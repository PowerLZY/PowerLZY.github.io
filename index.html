<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本博客主要用于记录个人学习笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="本博客主要用于记录个人学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta property="article:tag" content="Python、Machine Learing、CyberSecurity">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">相比到达的地方，同行的人更重要！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">本博客主要用于记录个人学习笔记</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2023/04/16/Untitled/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/16/Untitled/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-04-16 20:44:02" itemprop="dateCreated datePublished" datetime="2023-04-16T20:44:02+08:00">2023-04-16</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2023/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%880%EF%BC%89%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%880%EF%BC%89%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">机器学习理论</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-04-16 15:56:31 / 修改时间：20:38:02" itemprop="dateCreated datePublished" datetime="2023-04-16T15:56:31+08:00">2023-04-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="机器学习理论"><a href="#机器学习理论" class="headerlink" title="机器学习理论"></a>机器学习理论</h2><h3 id="一、-机器学习中参数模型和非参数模型理解"><a href="#一、-机器学习中参数模型和非参数模型理解" class="headerlink" title="一、 机器学习中参数模型和非参数模型理解"></a><strong>一、 机器学习中参数模型和非参数模型理解</strong></h3><blockquote>
<p>  参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/FrankieHello/article/details/94022594">https://blog.csdn.net/FrankieHello/article/details/94022594</a></p>
</blockquote>
<p><strong>参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型</strong>；非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p>
<p><strong>参数模型</strong>：线性回归、逻辑回归、感知机、基本型的SVM</p>
<p><strong>非参数模型</strong>：决策树、对偶型的SVM、朴素贝叶斯、神经网络</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%880%EF%BC%89%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2023/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8819%EF%BC%89%E3%80%90TODO%E3%80%91FP-growth/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8819%EF%BC%89%E3%80%90TODO%E3%80%91FP-growth/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-04-01 15:37:15 / 修改时间：21:58:55" itemprop="dateCreated datePublished" datetime="2023-04-01T15:37:15+08:00">2023-04-01</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>179</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="FP-growth"><a href="#FP-growth" class="headerlink" title="FP-growth"></a>FP-growth</h2><blockquote>
<ul>
<li>数据挖掘随笔（一）频繁模式挖掘与关联规则挖掘以及Apriori算法（python实现）：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410019734">https://zhuanlan.zhihu.com/p/410019734</a></li>
<li>数据挖掘随笔（二）FP-growth算法——一种用于频繁模式挖掘的模式增长方式(Python实现)：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/411594391">https://zhuanlan.zhihu.com/p/411594391</a></li>
</ul>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%888%EF%BC%89%E3%80%90Nan%E3%80%91CRF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%888%EF%BC%89%E3%80%90Nan%E3%80%91CRF/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-18 21:00:40 / 修改时间：22:17:32" itemprop="dateCreated datePublished" datetime="2022-07-18T21:00:40+08:00">2022-07-18</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/07/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8811%EF%BC%89CatBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8811%EF%BC%89CatBoost/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-02 15:21:08" itemprop="dateCreated datePublished" datetime="2022-07-02T15:21:08+08:00">2022-07-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-19 19:59:19" itemprop="dateModified" datetime="2022-07-19T19:59:19+08:00">2022-07-19</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="深入理解CatBoost"><a href="#深入理解CatBoost" class="headerlink" title="深入理解CatBoost"></a>深入理解CatBoost</h2><blockquote>
<p>  深入理解CatBoost - Microstrong的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102540344">https://zhuanlan.zhihu.com/p/102540344</a></p>
</blockquote>
<p><strong>本文主要内容概览：</strong></p>
<p><img src="https://pic1.zhimg.com/v2-f6a9520c6db0ba77ad620800cb36c054_b.jpg" alt="img"></p>
<h3 id="一、CatBoost简介"><a href="#一、CatBoost简介" class="headerlink" title="一、CatBoost简介"></a><strong>一、CatBoost简介</strong></h3><p>CatBoost是俄罗斯的搜索巨头Yandex在2017年开源的机器学习库，是Boosting族算法的一种。CatBoost和XGBoost、LightGBM并称为GBDT的三大主流神器，都是在GBDT算法框架下的一种改进实现。XGBoost被广泛的应用于工业界，LightGBM有效的提升了GBDT的计算效率，而Yandex的CatBoost号称是比XGBoost和LightGBM在算法准确率等方面表现更为优秀的算法。</p>
<p>CatBoost是一种基于对称决策树（oblivious trees）为基学习器实现的参数较少、支持类别型变量和高准确性的GBDT框架，主要解决的痛点是高效合理地处理类别型特征，这一点从它的名字中可以看出来，<strong>CatBoost是由Categorical和Boosting组成。此外，CatBoost还解决了梯度偏差（Gradient Bias）以及预测偏移（Prediction shift）的问题，从而减少过拟合的发生，进而提高算法的准确性和泛化能力。</strong></p>
<p><strong>与XGBoost、LightGBM相比，CatBoost的创新点有：</strong></p>
<ul>
<li><strong>嵌入了自动将类别型特征处理为数值型特征的创新算法。首先对categorical features做一些统计，计算某个类别特征（category）出现的频率，之后加上超参数，生成新的数值型特征（numerical features）。</strong></li>
<li><strong>Catboost还使用了组合类别特征，可以利用到特征之间的联系，这极大的丰富了特征维度</strong>。</li>
<li>采用排序提升的方法对抗训练集中的噪声点，从而避免梯度估计的偏差，进而解决预测偏移的问题。</li>
<li>采用了<strong>完全对称树作为基模型</strong>。</li>
</ul>
<h3 id="二、类别型特征"><a href="#二、类别型特征" class="headerlink" title="二、类别型特征"></a>二、<strong>类别型特征</strong></h3><p><strong>所谓类别型特征，即这类特征不是数值型特征，而是离散的集合</strong>，比如省份名（山东、山西、河北等），城市名（北京、上海、深圳等），学历（本科、硕士、博士等）。在梯度提升算法中，最常用的是将这些类别型特征转为数值型来处理，一般类别型特征会转化为一个或多个数值型特征。</p>
<p><strong>类别型特征基数比较低（low-cardinality features）</strong>，即该特征的所有值去重后构成的集合元素个数比较少，一般利用One-hot编码方法将特征转为数值型。One-hot编码可以在数据预处理时完成，也可以在模型训练的时候完成，从训练时间的角度，后一种方法的实现更为高效，CatBoost对于基数较低的类别型特征也是采用后一种实现。</p>
<p><strong>高基数类别型特征（high cardinality features）</strong>当中，比如 <code>user ID</code>，这种编码方式会产生大量新的特征，造成维度灾难。一种折中的办法是可以将类别分组成有限个的群体再进行One-hot编码。<strong>一种常被使用的方法是根据目标变量统计（Target Statistics，以下简称TS）进行分组</strong>，目标变量统计用于估算每个类别的目标变量期望值。甚至有人直接用TS作为一个新的数值型变量来代替原来的类别型变量。<strong><font color="red"> 重要的是，可以通过对TS数值型特征的阈值设置，基于对数损失、基尼系数或者均方差，得到一个对于训练集而言将类别一分为二的所有可能划分当中最优的那个。</font></strong></p>
<p>在LightGBM当中，类别型特征用每一步梯度提升时的梯度统计（Gradient Statistics，以下简称GS）来表示。虽然为建树提供了重要的信息，但是这种方法有以下两个缺点：</p>
<ul>
<li>增加计算时间，因为需要对每一个类别型特征，在迭代的每一步，都需要对GS进行计算；</li>
<li>增加存储需求，对于一个类别型变量，需要存储每一次分离每个节点的类别；</li>
</ul>
<p><strong>为了克服这些缺点，LightGBM以损失部分信息为代价将所有的长尾类别归为一类</strong>，作者声称这样处理高基数类别型特征时比One-hot编码还是好不少。不过如果采用TS特征，那么对于每个类别只需要计算和存储一个数字。</p>
<p>因此，采用TS作为一个新的数值型特征是最有效、信息损失最小的处理类别型特征的方法。TS也被广泛应用在点击预测任务当中，这个场景当中的类别型特征有用户、地区、广告、广告发布者等。接下来我们着重讨论TS，暂时将One-hot编码和GS放一边。</p>
<h4 id="2-1-目标变量统计（Target-Statistics）"><a href="#2-1-目标变量统计（Target-Statistics）" class="headerlink" title="2.1 目标变量统计（Target Statistics）"></a>2.1 <strong>目标变量统计（Target Statistics）</strong></h4><p><strong><font color="red"> CatBoost算法的设计初衷是为了更好的处理GBDT特征中的categorical features</font></strong>。在处理 GBDT特征中的categorical features的时候，最简单的方法是用 categorical feature 对应的标签的平均值来替换。在决策树中，标签平均值将作为节点分裂的标准。<strong>这种方法被称为 Greedy Target-based Statistics , 简称 Greedy TS</strong>，用公式来表达就是：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx%7D_k%5Ei%3D%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Bx_%7Bj%2Ck%7D%3Dx_%7Bi%2Ck%7D%5D%5Ccdot+Y_i%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Bx_%7Bj%2Ck%7D%3Dx_%7Bi%2Ck%7D%5D%7D+%5C%5C" alt="[公式]"></p>
<p>这种方法有一个显而易见的缺陷，就是通常特征比标签包含更多的信息，<strong><font color="red"> 如果强行用标签的平均值来表示特征的话，当训练数据集和测试数据集数据结构和分布不一样的时候会出条件偏移问题。</font></strong></p>
<p>一个标准的改进 Greedy TS的方式是添加先验分布项，这样可以减少噪声和低频率类别型数据对于数据分布的影响：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx%7D_k%5Ei%3D%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E%7Bp-1%7D%7B%5Bx_%7B%5Csigma_%7Bj%2Ck%7D%7D%3Dx_%7B%5Csigma_%7Bp%2Ck%7D%7D%5D%7DY_%7B%5Csigma_%7Bj%7D%7D%2Ba%5Ccdot+p%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bp-1%7D%7B%5Bx_%7B%5Csigma_%7Bj%2Ck%7D%7D%3Dx_%7B%5Csigma_%7Bp%2Ck%7D%7D%5D%7D%2Ba%7D+%5C%5C" alt="[公式]"></p>
<p> 其中 <img src="https://www.zhihu.com/equation?tex=p" alt="[公式]"> 是添加的先验项， <img src="https://www.zhihu.com/equation?tex=a+" alt="[公式]"> 通常是大于 <img src="https://www.zhihu.com/equation?tex=+0+" alt="[公式]"> 的权重系数。添加先验项是一个普遍做法，针对类别数较少的特征，它可以减少噪声数据。对于回归问题，一般情况下，先验项可取数据集label的均值。对于二分类，先验项是正例的先验概率。利用多个数据集排列也是有效的，但是，如果直接计算可能导致过拟合。CatBoost利用了一个比较新颖的计算叶子节点值的方法，这种方式(oblivious trees，对称树)可以避免多个数据集排列中直接计算会出现过拟合的问题。</p>
<p>当然，在论文《CatBoost: unbiased boosting with categorical features》中，还提到了其它几种改进Greedy TS的方法，分别有：Holdout TS、Leave-one-out TS、Ordered TS。我这里就不再翻译论文中的这些方法了，感兴趣的同学可以自己翻看一下原论文。</p>
<h4 id="2-2-特征组合"><a href="#2-2-特征组合" class="headerlink" title="2.2 特征组合"></a>2.2 <strong>特征组合</strong></h4><p>值得注意的是几个类别型特征的任意组合都可视为新的特征。例如，在音乐推荐应用中，我们有两个类别型特征：用户ID和音乐流派。如果有些用户更喜欢摇滚乐，将用户ID和音乐流派转换为数字特征时，根据上述这些信息就会丢失。结合这两个特征就可以解决这个问题，并且可以得到一个新的强大的特征。然而，组合的数量会随着数据集中类别型特征的数量成指数增长，因此不可能在算法中考虑所有组合。为当前树构造新的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=分割点&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">分割点</a>时，CatBoost会采用贪婪的策略考虑组合。对于树的第一次分割，不考虑任何组合。对于下一个分割，CatBoost将当前树的所有组合、类别型特征与数据集中的所有类别型特征相结合，并将新的组合类别型特征动态地转换为数值型特征。CatBoost还通过以下方式生成数值型特征和类别型特征的组合：树中选定的所有分割点都被视为具有两个值的类别型特征，并像类别型特征一样被进行组合考虑。</p>
<h4 id="2-3-CatBoost处理Categorical-features总结"><a href="#2-3-CatBoost处理Categorical-features总结" class="headerlink" title="2.3  CatBoost处理Categorical features总结"></a>2.3  <strong>CatBoost处理Categorical features总结</strong></h4><ul>
<li><strong>首先计算一些数据的statistics。计算某个category出现的频率，加上超参数，生成新的numerical features</strong>。这一策略要求同一标签数据不能排列在一起（即先全是0之后全是1这种方式），训练之前需要打乱数据集。</li>
<li>使用数据的不同排列（实际上是4个）。在每一轮建立树之前，先扔一轮骰子，决定使用哪个排列来生成树。</li>
<li>考虑使用categorical features的不同组合。例如颜色和种类组合起来，可以构成类似于blue dog这样的特征。当需要组合的categorical features变多时，CatBoost只考虑一部分<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=combinations&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">combinations</a>。在选择第一个节点时，只考虑选择一个特征，例如A。在生成第二个节点时，考虑A和任意一个categorical feature的组合，选择其中最好的。就这样使用贪心算法生成combinations。</li>
<li><strong>除非向gender这种维数很小的情况，不建议自己生成One-hot编码向量，最好交给算法来处理。</strong></li>
</ul>
<h2 id="三、CatboostQ-amp-A"><a href="#三、CatboostQ-amp-A" class="headerlink" title="三、CatboostQ&amp;A"></a>三、CatboostQ&amp;A</h2><h4 id="3-1-CatBoost与XGBoost、LightGBM的联系与区别？"><a href="#3-1-CatBoost与XGBoost、LightGBM的联系与区别？" class="headerlink" title="3.1 CatBoost与XGBoost、LightGBM的联系与区别？"></a>3.1 <strong>CatBoost与XGBoost、LightGBM的联系与区别？</strong></h4><p>（1）2014年3月XGBoost算法首次被<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=陈天奇&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">陈天奇</a>提出，但是直到2016年才逐渐著名。2017年1月微软发布LightGBM第一个稳定版本。2017年4月Yandex开源CatBoost。自从XGBoost被提出之后，很多文章都在对其进行各种改进，CatBoost和LightGBM就是其中的两种。</p>
<p>（2）<strong>CatBoost处理类别型特征十分灵活，可直接传入类别型特征的列标识，模型会自动将其使用One-hot编码，还可通过设置 one_hot_max_size参数来限制One-hot特征向量的长度</strong>。如果不传入类别型特征的列标识，那么CatBoost会把所有列视为数值特征。对于One-hot编码超过设定的one_hot_max_size值的特征来说，CatBoost将会使用一种高效的encoding方法，与mean encoding类似，但是会降低过拟合。处理过程如下：</p>
<ul>
<li>将输入样本集随机排序，并生成多组随机排列的情况；</li>
<li>将浮点型或属性值标记转化为整数；</li>
<li>将所有的类别型特征值结果都根据以下公式，转化为数值结果；</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=avg%5C_target+%3D+%5Cfrac%7BcountInClass+%2B+prior%7D%7BtotalCount+%2B+1%7D+%5C%5C" alt="[公式]"> </p>
<p>其中 countInClass 表示在当前类别型特征值中有多少样本的标记值是1；prior 是分子的初始值，根据初始参数确定。totalCount 是在所有样本中（包含当前样本）和当前样本具有相同的类别型特征值的样本数量。</p>
<p>LighGBM 和 CatBoost 类似，也可以通过使用特征名称的输入来处理类别型特征数据，它没有对数据进行独热编码，因此速度比独热编码快得多。LighGBM 使用了一个特殊的算法来确定属性特征的分割值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = lgb.Dataset(data, label=label, feature_name=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>], categorical_feature=[<span class="string">&#x27;c3&#x27;</span>])</span><br><span class="line"><span class="comment"># 注意，在建立适用于 LighGBM 的数据集之前，需要将类别型特征变量转化为整型变量，此算法不允许将字符串数据传给类别型变量参数。</span></span><br></pre></td></tr></table></figure>
<p>（3）XGBoost 和 CatBoost、 LighGBM 算法不同，XGBoost 本身无法处理类别型特征，而是像随机森林一样，只接受数值数据。因此在将类别型特征数据传入 XGBoost 之前，必须通过各种编码方式：例如序号编码、独热编码和二进制编码等对数据进行处理。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/06/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%888%EF%BC%89%E3%80%90Nan%E3%80%91%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%888%EF%BC%89%E3%80%90Nan%E3%80%91%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-22 12:54:43" itemprop="dateCreated datePublished" datetime="2022-06-22T12:54:43+08:00">2022-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-18 22:17:36" itemprop="dateModified" datetime="2022-07-18T22:17:36+08:00">2022-07-18</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h2><blockquote>
<p>  如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？ - Scofield的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/35866596/answer/236886066">https://www.zhihu.com/question/35866596/answer/236886066</a></p>
</blockquote>
<h3 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h3><h4 id="1-1-随机过程"><a href="#1-1-随机过程" class="headerlink" title="1.1 随机过程"></a>1.1 随机过程</h4><p><strong>随机过程就是一些统计模型，利用这些统计模型可以对自然界的一些事物进行预测和处理</strong>，比如天气预报，比如股票，比如市场分析，比如人工智能。它的应用还真是多了去了。</p>
<h4 id="1-2-马尔可夫链-（Markov-Chain）"><a href="#1-2-马尔可夫链-（Markov-Chain）" class="headerlink" title="1.2 马尔可夫链 （Markov Chain）"></a>1.2 马尔可夫链 （Markov Chain）</h4><blockquote>
<p>  马尔可夫链 （Markov Chain）是什么鬼 - 车卡门的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26453269">https://zhuanlan.zhihu.com/p/26453269</a></p>
</blockquote>
<p>马尔可夫链就是这样一个任性的过程，它将来的状态分布只取决于现在，跟过去无关！实际上就是一个随机变量随时间按照Markov性进行变化的过程。</p>
<h4 id="1-3-马尔可夫模型（Hidden-Markov-Models）"><a href="#1-3-马尔可夫模型（Hidden-Markov-Models）" class="headerlink" title="1.3 马尔可夫模型（Hidden Markov Models）"></a>1.3 马尔可夫模型（Hidden Markov Models）</h4><p>既是马尔可夫模型，就一定存在<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=马尔可夫链&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;64187492&quot;}">马尔可夫链</a>，该马尔可夫链服从马尔可夫性质：即无记忆性。也就是说，这一时刻的状态，受且只受前一时刻的影响，而不受更往前时刻的状态的影响。在这里我们仍然使用非常简单的天气模型来做说明。</p>
<p><img src="https://picx.zhimg.com/80/648a55725e67d718d97d6a475891d70b_1440w.png" alt="img" style="zoom:50%;"></p>
<p>在这个马尔可夫模型中，存在三个状态，Sunny， Rainy， Cloudy，同时图片上标的是各个状态间的转移概率（如果不明白什么是转移概率，那建议先去学习什么是马尔可夫再来看HMM）。</p>
<p><strong><font color="red"> 现在我们要说明什么是 HMM。既是隐形，说明这些状态是观测不到的，相应的，我们可以通过其他方式来『猜测』或是『推断』这些状态，这也是 HMM 需要解决的问题之一。</font></strong></p>
<blockquote>
<p>  举个例子，我女朋友现在在北京工作，而我还在法国读书。每天下班之后，她会根据天气情况有相应的活动：或是去商场购物，或是去公园散步，或是回家收拾房间。我们有时候会通电话，她会告诉我她这几天做了什么，而闲着没事的我呢，则要通过她的行为猜测这几天对应的天气最有可能是什么样子的。</p>
<p>  以上就是一个简单的 HMM，天气状况属于状态序列，而她的行为则属于观测序列。天气状况的转换是一个马尔可夫序列。而根据天气的不同，有相对应的概率产生不同的行为。在这里，为了简化，把天气情况简单归结为晴天和雨天两种情况。雨天，她选择去散步，购物，收拾的概率分别是0.1，0.4，0.5， 而如果是晴天，她选择去散步，购物，收拾的概率分别是0.6，0.3，0.1。而天气的转换情况如下：这一天下雨，则下一天依然下雨的概率是0.7，而转换成晴天的概率是0.3；这一天是晴天，则下一天依然是晴天的概率是0.6，而转换成雨天的概率是0.4. 同时还存在一个初始概率，也就是第一天下雨的概率是0.6， 晴天的概率是0.4.</p>
<p>  <img src="https://pic4.zhimg.com/80/792e033ff9b0418b3b6c9bbaef30fd83_1440w.png" alt="img" style="zoom:50%;"></p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/05/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%883%EF%BC%89%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%883%EF%BC%89%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-07 13:51:03" itemprop="dateCreated datePublished" datetime="2022-05-07T13:51:03+08:00">2022-05-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-07 19:16:00" itemprop="dateModified" datetime="2022-07-07T19:16:00+08:00">2022-07-07</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="「融合」机器学习模型：一种提升预测能力的方法"><a href="#「融合」机器学习模型：一种提升预测能力的方法" class="headerlink" title="「融合」机器学习模型：一种提升预测能力的方法"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33589222">「融合」机器学习模型：一种提升预测能力的方法</a></h2><p>没有哪个机器学习模型可以常胜，如何找到当前问题的最优解是一个永恒的问题。</p>
<p>幸运的是，<strong>结合/融合/整合 (integration/ combination/ fusion)多个机器学习模型往往可以提高整体的预测能力。</strong>这是一种非常有效的提升手段，在多分类器系统(multi-classifier system)和集成学习(ensemble learning)中，融合都是最重要的一个步骤。</p>
<p>一般来说，<strong>模型融合或多或少都能提高的最终的预测能力，且一般不会比最优子模型差</strong>。举个实用的例子，Kaggle比赛中常用的stacking方法就是模型融合，通过结合多个各有所长的子学习器，我们实现了更好的预测结果。基本的理论假设是：<strong>不同的子模型在不同的数据上有不同的表达能力，我们可以结合他们擅长的部分，得到一个在各个方面都很“准确”的模型</strong>。当然，最基本的假设是子模型的误差是互相独立的，这个一般是不现实的。但即使子模型间的误差有相关性，适当的结合方法依然可以各取其长，从而达到提升效果。</p>
<p>我们今天介绍几种简单、有效的模型结合方法。</p>
<h3 id="1-案例分析"><a href="#1-案例分析" class="headerlink" title="1. 案例分析"></a><strong>1. 案例分析</strong></h3><p>让我们给出一个简单的分析。假设我们有天气数据X和对应的标签y，现在希望实现一个可以预测明天天气的模型 <img src="https://www.zhihu.com/equation?tex=%5Cpsi" alt="[公式]"> 。但我们并不知道用什么算法效果最好，于是尝试了十种算法，包括</p>
<ul>
<li>算法1: 逻辑回归 - <img src="https://www.zhihu.com/equation?tex=C_%7B1%7D" alt="[公式]"> </li>
<li>算法2：支持向量机（SVM）-  <img src="https://www.zhihu.com/equation?tex=C_%7B2%7D" alt="[公式]"> </li>
<li>…</li>
<li>算法10：随机森林 -  <img src="https://www.zhihu.com/equation?tex=C_%7B10%7D" alt="[公式]"> </li>
</ul>
<p>结果发现他们表现都一般，在验证集上的误分率比较高。我们现在期待找到一种方法，可以有效提高最终预测结果。</p>
<h3 id="2-平均法-投票法"><a href="#2-平均法-投票法" class="headerlink" title="2. 平均法/投票法"></a><strong>2. 平均法/投票法</strong></h3><p>一种比较直白的方法就是对让10个算法模型同时对需要预测的数据进行预测，并对结果取平均数/众数。假设10个分类器对于测试数据 <img src="https://www.zhihu.com/equation?tex=X_%7Bt%7D" alt="[公式]"> 的预测结果是<img src="https://www.zhihu.com/equation?tex=%5BC_%7B1%7D%28X_t%29%2CC_%7B2%7D%28X_t%29%2C...%2CC_%7B10%7D%28X_t%29%5D%3D%5B0%2C1%2C1%2C1%2C1%2C1%2C0%2C1%2C1%2C0%5D" alt="[公式]"> ，那很显然少数服从多数，我们应该选择1作为 <img src="https://www.zhihu.com/equation?tex=X_%7Bt%7D" alt="[公式]"> 的预测结果。如果取平均值的话也可以那么会得到0.7，高于阈值0.5，因此是等价的。</p>
<p>但这个时候需要有几个注意的地方：</p>
<p><strong>首先，不同分类器的输出结果取值范围不同</strong>，不一定是[0,1]，而可以是无限定范围的值。举例，逻辑回归的输出范围是0-1（概率），而k-近邻的输出结果可以是大于0的任意实数，其他算法的输出范围可能是负数。<strong>因此整合多个分类器时，需要注意不同分类器的输出范围，并统一这个取值范围</strong>。</p>
<ul>
<li>比如可以先转化为如<strong>二分类结果</strong>，把输出的范围统一后再进行整合。但这种方法的问题在于我们丢失了很多信息，0.5和0.99都会被转化为1，但明显其可靠程度差别很大。</li>
<li>也可以转化为排序（ranking），再对不同的ranking进行求平均。</li>
<li>更加稳妥的方法是对每个分类器的输出结果做标准化，也就是调整到正态分布上去。之后就可以对多个调整后的结果进行整合。同理，用归一化也可以有类似的效果。</li>
</ul>
<p><strong>其次，就是整合稳定性的问题</strong>。采用平均法的另一个风险在于可能被极值所影响。正态分布的取值是 <img src="https://www.zhihu.com/equation?tex=%5B-%5Cinfty%2C%2B%5Cinfty%5D" alt="[公式]"> ，在少数情况下平均值会受到少数极值的影响。一个常见的解决方法是，用中位数（median)来代替平均数进行整合。</p>
<p><strong>同时，模型整合面临的另一个问题是子模型良莠不齐</strong>。如果10个模型中有1个表现非常差，那么会拖累最终的效果，适得其反。==因此，简单、粗暴的把所有子模型通过平均法整合起来效果往往一般。==</p>
<h3 id="3-寻找优秀的子模型准而不同"><a href="#3-寻找优秀的子模型准而不同" class="headerlink" title="3. 寻找优秀的子模型准而不同"></a>3. 寻找优秀的子模型准而不同</h3><p>不难看出，一个较差的子模型会拖累整体的集成表现，那么这就涉及到另一个问题？什么样的子模型是优秀的。</p>
<p>一般来说，我们希望子模型：<strong>准而不同 -&gt; accurate but diversified</strong>。好的子模型应该首先是准确的，这样才会有所帮助。其次不同子模型间应该有差别，比如独立的误差，这样作为一个整体才能起到<strong>互补作用</strong>。</p>
<p>因此，如果想实现良好的结合效果，就必须对子模型进行筛选，去粗取精。在这里我们需要做出一点解释，我们今天说的融合方法和bagging还有boosting中的思路不大相同。==bagging和boosting中的子模型都是<strong>很简单的且基数庞大</strong>，而我们今天的模型融合是<strong>结合少量但较为复杂的模型</strong>。==</p>
<h3 id="4-筛选方法：赋予不同子模型不同的权重"><a href="#4-筛选方法：赋予不同子模型不同的权重" class="headerlink" title="4. 筛选方法：赋予不同子模型不同的权重"></a><strong>4. 筛选方法：赋予不同子模型不同的权重</strong></h3><p>因此我们不能再简单的取平均了，而应该给优秀的子模型更大的权重。在这种前提下，一个比较直白的方法就是根据子<strong>模型的准确率给出一个参考权重</strong> <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> ，子模型越准确那么它的权重就更大，对于最终预测的影响就更强： <img src="https://www.zhihu.com/equation?tex=w_%7Bi%7D%3D%5Cfrac%7BAcc%28C_%7Bi%7D%29%7D%7B%5Csum_%7B1%7D%5E%7B10%7D%7BAcc%28C_%7Bj%7D%29%7D%7D" alt="[公式]"> 。简单取平均是这个方法的一个特例，即假设子模型准确率一致。</p>
<h3 id="5-更进一步：学习分类器权重"><a href="#5-更进一步：学习分类器权重" class="headerlink" title="5. 更进一步：学习分类器权重"></a><strong>5. 更进一步：学习分类器权重</strong></h3><p>在4中提到的方法在一定程度上可以缓解问题，但效果有限。那么另一个思路是，我们是否可以学习每个分类器的权重呢？</p>
<p>答案是肯定，这也就是Stacking的核心思路。通过增加一层来学习子模型的权重。</p>
<p><img src="https://pic3.zhimg.com/v2-13396e65c2bcc1c270ca536310686d07_720w.jpg?source=d16d100b" alt="img"></p>
<p><strong>图片来源</strong>：<a target="_blank" rel="noopener" href="https://www.quora.com/What-is-stacking-in-machine-learning">https://www.quora.com/What-is-stacking-in-machine-learning</a></p>
<p>更多有关于stacking的讨论可以参考我最近的文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32896968">「Stacking」与「神经网络」</a>。简单来说，就是加一层逻辑回归或者SVM，把子模型的输出结果当做训练数据，来自动赋予不同子模型不同的权重。</p>
<p>==<strong>一般来看，这种方法只要使用得当，效果应该比简单取平均值、或者根据准确度计算权重的效果会更好。</strong>==</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/04/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8814%EF%BC%89%E3%80%90Nan%E3%80%91LDA(%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8814%EF%BC%89%E3%80%90Nan%E3%80%91LDA(%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B)/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-04-20 12:38:36" itemprop="dateCreated datePublished" datetime="2022-04-20T12:38:36+08:00">2022-04-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-05 16:55:36" itemprop="dateModified" datetime="2022-07-05T16:55:36+08:00">2022-07-05</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>421</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="LDA-Latent-Dirichlet-Allocation-主题模型"><a href="#LDA-Latent-Dirichlet-Allocation-主题模型" class="headerlink" title="LDA(Latent Dirichlet Allocation) 主题模型"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36394491"><em>LDA</em>(Latent Dirichlet Allocation) 主题模型</a></h2><p><strong>同一个主题，在不同的文章中，他出现的比例(概率)是不同的</strong>，看到这里，读者可能已经发现，文档和主题之间的关系和主题和词汇的关系是多么惊人的类似！</p>
<blockquote>
<p>  LDA于2003年由 David Blei, Andrew Ng和 Michael I. Jordan提出，因为模型的简单和有效，掀起了主题模型研究的波浪。虽然说LDA模型简单，但是它的数学推导却不是那么平易近人，一般初学者会深陷数学细节推导中不能自拔。于是牛人们看不下去了，纷纷站出来发表了各种教程。国内方面rickjin有著名的《<a href="https://link.zhihu.com/?target=http%3A//www.52nlp.cn/author/rickjin">LDA数学八卦</a>》，国外的Gregor Heinrich有著名的《<a href="https://link.zhihu.com/?target=https%3A//users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf">Parameter estimation for text analysis</a>》。其实有了这两篇互补的通俗教程，大家沉住心看个4、5遍，基本就可以明白LDA为什么是简单的了。那么其实也没我什么事了，然而心中总有一种被大牛点播的豁然开朗的快感，实在是不吐不快啊。</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:53" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:53+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-08-21 23:00:53" itemprop="dateModified" datetime="2022-08-21T23:00:53+08:00">2022-08-21</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>机器学习中的监督学习本质上是给定一系列训练样本 <img src="https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29" alt="[公式]"> ，尝试学习 <img src="https://www.zhihu.com/equation?tex=x%5Crightarrow+y" alt="[公式]"> 的映射关系，使得给定一个 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，即便这个 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 不在训练样本中，也能够得到尽量接近真实 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 的输出 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="[公式]"> 。而损失函数（Loss Function）则是这个过程中关键的一个组成部分，用来<strong>衡量模型的输出</strong> <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="[公式]"> <strong>与真实的</strong> <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> <strong>之间的差距</strong>，给模型的优化指明方向。</p>
<p>本文将介绍机器学习、深度学习中分类与回归常用的几种损失函数，包括<strong>均方差损失 Mean Squared Loss、平均绝对误差损失 Mean Absolute Error Loss、Huber Loss、分位数损失 Quantile Loss、交叉熵损失函数 Cross Entropy Loss、Hinge 损失 Hinge Loss</strong>。主要介绍各种损失函数的基本形式、原理、特点等方面。</p>
<p><img src="https://pic3.zhimg.com/80/v2-d42728029b6f2f9db86fc34c63dd5ff8_1440w.jpg?source=1940ef5c" alt="img" style="zoom: 67%;"></p>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在正文开始之前，先说下关于 Loss Function、Cost Function 和 Objective Function 的区别和联系。在机器学习的语境下这三个术语经常被交叉使用。</p>
<ul>
<li><strong>损失函数</strong> Loss Function 通常是<strong>针对单个训练样本而言</strong>，给定一个模型输出 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="[公式]"> 和一个真实 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> ，损失函数输出一个实值损失 <img src="https://www.zhihu.com/equation?tex=L%3Df%28y_i%2C+%5Chat%7By_i%7D%29" alt="[公式]"></li>
<li><strong>代价函数</strong> Cost Function 通常是<strong>针对整个训练集</strong>（或者在使用 mini-batch gradient descent 时一个 mini-batch）的总损失 <img src="https://www.zhihu.com/equation?tex=J%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D+f%28y_i%2C%5Chat%7By_i%7D%29" alt="[公式]"></li>
<li><strong>目标函数</strong> Objective Function 是一个更通用的术语，表示任意希望被优化的函数，用于机器学习领域和非机器学习领域（比如运筹优化）</li>
</ul>
<p>一句话总结三者的关系就是：<font color="red"> <strong>A loss function is a part of a cost function which is a type of an objective function.</strong></font></p>
<p>由于损失函数和代价函数只是在针对样本集上有区别，因此在本文中统一使用了损失函数这个术语，但下文的相关公式实际上采用的是代价函数 Cost Function 的形式，请读者自行留意。</p>
<h4 id="结构风险函数"><a href="#结构风险函数" class="headerlink" title="结构风险函数"></a>结构风险函数</h4><p>损失函数（loss function）是用来估量模型的预测值f(x)与真实值$Y$不一致的程度，它是一个非负实数值函数，通常使用$L(Y,f(x))$来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下的式子：</p>
<p><img src="image-20220821223950768.png" alt="image-20220821223950768" style="zoom:50%;"></p>
<p>前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty term）,它可以是L1，也可以是L2等其他的正则函数。整个式子表示的意思是找到使目标函数最小时的θ值。下面列出集中常见的损失函数。</p>
<h3 id="一、-对数损失函数（逻辑回归）MLE-【交叉熵损失函数】"><a href="#一、-对数损失函数（逻辑回归）MLE-【交叉熵损失函数】" class="headerlink" title="一、 对数损失函数（逻辑回归）MLE  【交叉熵损失函数】"></a>一、 对数损失函数（逻辑回归）MLE  【交叉熵损失函数】</h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52100927">https://zhuanlan.zhihu.com/p/52100927</a></p>
<p>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/tsyccnh/article/details/79163834">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉</a></p>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35709485">损失函数｜交叉熵损失函数</a></p>
</blockquote>
<p>有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。<strong>平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到</strong>，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，<strong>它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数</strong>，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：</p>
<p><strong>最小化负的似然函数</strong>（即$maxF(y,f(x))—&gt;min−F(y,f(x))$)。从损失函数的视角来看，它就成了<strong>log损失函数了。</strong></p>
<h4 id="原理解释1：-条件概率下方便计算极大似然估计"><a href="#原理解释1：-条件概率下方便计算极大似然估计" class="headerlink" title="原理解释1：==条件概率下方便计算极大似然估计=="></a>原理解释1：<strong>==条件概率下方便计算极大似然估计==</strong></h4><p>Log损失函数的标准形式：</p>
<p>$L(Y,P(Y|X))=−logP(Y|X)$</p>
<p>刚刚说到，<strong>取对数是为了方便计算极大似然估计</strong>，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数$L(Y.P(Y|X))$表达的是样本在分类$Y$的情况下，使概率$P(Y|X)$达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以$logP(Y|X)$也会达到最大值，因此在前面加上负号之后，最大化$P(Y|X)$就等价于最小化$L$了。</p>
<p><strong>logistic回归</strong>的$P(y|x)$表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：</p>
<p><img src="image-20220322202521355.png" alt="image-20220322202521355" style="zoom:50%;"></p>
<p>将上面的公式合并在一起，可得到第i个样本正确预测的概率：</p>
<p><img src="image-20220322202548296.png" alt="image-20220322202548296" style="zoom:50%;"></p>
<p>上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布为：</p>
<p><img src="image-20220322202618734.png" alt="image-20220322202618734" style="zoom:50%;"></p>
<p>将上式代入到对数损失函数中，得到最终的损失函数为：</p>
<p><img src="image-20220322202653661.png" alt="image-20220322202653661" style="zoom:50%;"></p>
<h4 id="原理解释2：相对熵（KL散度）推理"><a href="#原理解释2：相对熵（KL散度）推理" class="headerlink" title="原理解释2：相对熵（KL散度）推理"></a>原理解释2：相对熵（KL散度）推理</h4><blockquote>
<p>  相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异.$DKL$的值越小，表示q分布和p分布越接近.</p>
</blockquote>
<h5 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵:"></a>相对熵:</h5><p><img src="image-20220330133351613.png" alt="image-20220330133351613" style="zoom:50%;"></p>
<h5 id="相对熵-信息熵-交叉熵-："><a href="#相对熵-信息熵-交叉熵-：" class="headerlink" title="相对熵 = 信息熵 + 交叉熵 ："></a>相对熵 = 信息熵 + 交叉熵 ：</h5><p><img src="image-20220330134202064.png" alt="image-20220330134202064" style="zoom:50%;"></p>
<p><strong>【对数损失函数（Log loss function）】和【交叉熵损失函数（Cross-entroy loss funtion）】在很多文献内是一致的，因为他们的表示式的本质是一样的。</strong></p>
<h3 id="二、-平方损失函数（线性回归，GBDT，最小二乘法，Ordinary-Least-Squares）MSE"><a href="#二、-平方损失函数（线性回归，GBDT，最小二乘法，Ordinary-Least-Squares）MSE" class="headerlink" title="二、 平方损失函数（线性回归，GBDT，最小二乘法，Ordinary Least Squares）MSE"></a>二、 平方损失函数（线性回归，GBDT，最小二乘法，Ordinary Least Squares）MSE</h3><p>最小二乘法是线性回归的一种，OLS 将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central limit theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。换言之，OLS是基于<strong>距离</strong>的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：</p>
<ul>
<li>简单，计算方便；</li>
<li>欧氏距离是一种很好的相似性度量标准；</li>
<li>在不同的表示域变换后特征性质不变。</li>
</ul>
<p>平方损失（Square loss）的标准形式如下：$L(Y,f(X))=(Y−f(x))^2$当样本个数为n时，此时的损失函数变为：</p>
<p><img src="image-20220322202912962.png" alt="image-20220322202912962" style="zoom:50%;"></p>
<p>$Y−f(X)$ 表示的是<strong>残差</strong>，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。</p>
<p>而在实际应用中，通常会使用<strong>均方差</strong>（MSE）作为一项衡量指标，公式如下：</p>
<p><img src="image-20220322202957484.png" alt="image-20220322202957484" style="zoom:50%;"></p>
<h3 id="三、-指数损失函数（Adaboost）"><a href="#三、-指数损失函数（Adaboost）" class="headerlink" title="三、 指数损失函数（Adaboost）"></a>三、 指数损失函数（Adaboost）</h3><blockquote>
<p>  Adaboost训练误差以指数下降。所以说，指数损失本身并没有带来优化上的特殊，优点在于计算和表达简单。</p>
</blockquote>
<p>学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到$fm(x)$:</p>
<p><img src="image-20220322203050695.png" alt="image-20220322203050695" style="zoom:50%;"></p>
<p><strong>Adaboost</strong>每次迭代时的目的是为了找到最小化下列式子时的参数 $a$ 和 $G$：</p>
<p><img src="image-20220322203141435.png" alt="image-20220322203141435" style="zoom:50%;"></p>
<p>而指数损失函数(exp-loss）的标准形式如下:</p>
<p><img src="image-20220322203221432.png" alt="image-20220322203221432" style="zoom:50%;"></p>
<p>可以看出，Adaboost的目标式子就是指数损失，在给定N个样本的情况下，Adaboost的损失函数为：</p>
<p><img src="image-20220322203238853.png" alt="image-20220322203238853" style="zoom:50%;"></p>
<h3 id="四、-Hinge-合页损失函数-（SVM，advGAN）"><a href="#四、-Hinge-合页损失函数-（SVM，advGAN）" class="headerlink" title="四、 ==Hinge 合页损失函数==（SVM，advGAN）"></a>四、 ==Hinge 合页损失函数==（SVM，advGAN）</h3><p><img src="image-20220401165315551.png" alt="image-20220401165315551" style="zoom:50%;"></p>
<p>线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数：</p>
<p><img src="image-20220322205741804.png" alt="image-20220322205741804" style="zoom:50%;"></p>
<p>目标函数的第一项是经验损失或经验风险函数：</p>
<p><img src="image-20220322205801232.png" alt="image-20220322205801232" style="zoom:50%;"></p>
<p>称为<strong>合页损失函数</strong>（hinge loss function）。下标”+”表示以下取正值的函数：</p>
<p><img src="image-20220322205844003.png" alt="image-20220322205844003" style="zoom:50%;"></p>
<p>这就是说，当样本点$(xi,yi)$被正确分类且函数间隔（确信度）$yi(w·xi+b)$大于1时，损失是0，否则损失是$1−yi(w·xi+b)$。目标函数的第二项是系数为 $λ$ 的 $w$ 的 $L2$ 范数，是正则化项。</p>
<p>接下来证明线性支持向量机原始最优化问题：</p>
<p><img src="image-20220322210000477.png" alt="image-20220322210000477" style="zoom:50%;"></p>
<p><img src="image-20220322210121285.png" alt="image-20220322210121285" style="zoom:50%;"></p>
<p>先令$[1−yi(w·xi+b)]+=ξi$，则$ξi≥0$，第二个约束条件成立；由$[1−yi(w·xi+b)]+=ξi$，当$1−yi(w·xi+b)&gt;0$时，有$yi(w·xi+b)=1−ξi$;当$1−yi(w·xi+b)≤0$时，$ξi=0$，有$yi(w·xi+b)≥1−ξi$，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作</p>
<p><img src="image-20220322210943775.png" alt="image-20220322210943775" style="zoom:50%;"></p>
<p>若取 $λ=1/2C$ 则:</p>
<p><img src="image-20220322211012150.png" alt="image-20220322211012150" style="zoom:50%;"></p>
<h3 id="五、Softmax函数和Sigmoid函数的区别与联系"><a href="#五、Softmax函数和Sigmoid函数的区别与联系" class="headerlink" title="五、Softmax函数和Sigmoid函数的区别与联系"></a>五、Softmax函数和Sigmoid函数的区别与联系</h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356976844">https://zhuanlan.zhihu.com/p/356976844</a></p>
</blockquote>
<h4 id="5-1-分类任务"><a href="#5-1-分类任务" class="headerlink" title="5.1 分类任务"></a>5.1 分类任务</h4><h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><blockquote>
<p>  Sigmoid =<strong>==多标签分类问题==</strong>=多个正确答案=非独占输出（例如胸部X光检查、住院）。构建分类器，解决有多个正确答案的问题时，用Sigmoid函数分别处理各个原始输出值。</p>
<p>  Softmax =<strong>多类别分类问题</strong>=只有一个正确答案=互斥输出（例如手写数字，鸢尾花）。构建分类器，解决只有唯一正确答案的问题时，用Softmax函数处理各个原始输出值。Softmax函数的分母综合了原始输出值的所有因素，这意味着，Softmax函数得到的不同概率之间相互关联。</p>
</blockquote>
<p><strong>Sigmoid函数</strong>是一种logistic函数，它将任意的值转换到 <img src="https://www.zhihu.com/equation?tex=%5B0%2C+1%5D" alt="[公式]"> 之间，如图1所示，函数表达式为： <img src="https://www.zhihu.com/equation?tex=Sigmoid%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D" alt="[公式]"> 。</p>
<p>它的导函数为： <img src="https://www.zhihu.com/equation?tex=Sigmoid%5E%7B%27%7D%28x%29%3DSigmoid%28x%29%5Ccdot+%281-Sigmoid%28x%29%29" alt="[公式]"> 。</p>
<p><img src="https://pic1.zhimg.com/80/v2-17889947ad7bc75ba672e41363b54788_1440w.jpg" alt="img">图1：Sigmoid函数</p>
<p><strong>优点</strong>：</p>
<ol>
<li>Sigmoid函数的输出在(0,1)之间，输出范围有限，优化稳定，可以用作<strong>输出层</strong>。</li>
<li>连续函数，便于<strong>求导</strong>。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><p>最明显的就是<strong>饱和性</strong>，从上图也不难看出其两侧导数逐渐趋近于0，容易造成<strong>梯度消失</strong>。</p>
<p>2.激活函数的偏移现象。Sigmoid函数的输出值均大于0，使得输出不是0的均值，这会导致后一层的神经元将得到上一层非0均值的信号作为输入，这会对梯度产生影响。</p>
</li>
<li><p>计算复杂度高，因为Sigmoid函数是指数形式。</p>
</li>
</ol>
<h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4><p><strong>Softmax函数</strong>，又称<strong>归一化指数函数</strong>，函数表达式为： <img src="https://www.zhihu.com/equation?tex=Softmax%28x%29%3D%5Cfrac%7Be%5E%7Bx_%7Bi%7D%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%7Be%5E%7Bx_%7Bj%7D%7D%7D%7D" alt="[公式]"> 。</p>
<p><img src="https://pic2.zhimg.com/80/v2-3c2edc906b299c33ae2e08376bd76689_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>Softmax函数是二分类函数Sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。</strong>如图2所示，Softmax直白来说就是将原来输出是3,1,-3通过Softmax函数一作用，就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标。</p>
<p>由于Softmax函数先拉大了输入向量元素之间的差异（通过指数函数），然后才归一化为一个概率分布，在应用到分类问题时，它使得各个类别的概率差异比较显著，最大值产生的概率更接近1，这样输出分布的形式更接近真实分布。</p>
<p><strong>Softmax可以由三个不同的角度来解释。从不同角度来看softmax函数，可以对其应用场景有更深刻的理解：</strong></p>
<ol>
<li><strong>softmax可以当作argmax的一种平滑近似</strong>，与arg max操作中暴力地选出一个最大值（产生一个one-hot向量）不同，softmax将这种输出作了一定的平滑，即将one-hot输出中最大值对应的1按输入元素值的大小分配给其他位置。</li>
<li><strong>softmax将输入向量归一化映射到一个类别概率分布</strong>，即 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 个类别上的概率分布（前文也有提到）。这也是为什么在深度学习中常常将softmax作为MLP的最后一层，并配合以交叉熵损失函数（对分布间差异的一种度量)。</li>
<li>从<strong>概率图模型</strong>的角度来看，softmax的这种形式可以理解为一个概率无向图上的联合概率。因此你会发现，条件最大熵模型与softmax回归模型实际上是一致的，诸如这样的例子还有很多。由于概率图模型很大程度上借用了一些热力学系统的理论，因此也可以从物理系统的角度赋予softmax一定的内涵。</li>
</ol>
<h4 id="5-2-总结"><a href="#5-2-总结" class="headerlink" title="5.2 总结"></a>5.2 总结</h4><ol>
<li>如果模型输出为非互斥类别，且可以同时选择多个类别，则采用Sigmoid函数计算该网络的原始输出值。</li>
<li>如果模型输出为<strong>互斥类别</strong>，且只能选择一个类别，则采用Softmax函数计算该网络的原始输出值。</li>
<li><strong>Sigmoid函数</strong>可以用来解决<strong>多标签问题</strong>，<strong>Softmax</strong>函数用来解决<strong>单标签问题</strong>。</li>
<li>对于某个分类场景，当Softmax函数能用时，Sigmoid函数一定可以用。</li>
</ol>
<h3 id="6-损失函数Q-amp-A"><a href="#6-损失函数Q-amp-A" class="headerlink" title="6 损失函数Q&amp;A"></a>6 损失函数Q&amp;A</h3><h4 id="平方误差损失函数和交叉熵损失函数分别适合什么场景？"><a href="#平方误差损失函数和交叉熵损失函数分别适合什么场景？" class="headerlink" title="==平方误差损失函数和交叉熵损失函数分别适合什么场景？=="></a>==平方误差损失函数和交叉熵损失函数分别适合什么场景？==</h4><p>一般还说，平方损失函数更适合输出为连续，并且最后一层不含sigmod或softmax激活函数的神经网络；交叉熵损失函数更适合二分类或多分类的场景。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-16 20:44:24" itemprop="dateModified" datetime="2023-04-16T20:44:24+08:00">2023-04-16</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="六、A-B-测试"><a href="#六、A-B-测试" class="headerlink" title="六、A/B 测试"></a>六、A/B 测试</h2><blockquote>
<p>  【AB测试最全干货】史上最全知识点及常见面试题（上篇） - 数据分析狗一枚的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/375902281">https://zhuanlan.zhihu.com/p/375902281</a></p>
</blockquote>
<h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>科学家门捷列夫说「没有测量，就没有科学」，在AI场景下我们同样需要定量的数值化指标来指导我们更好地应用模型对数据进行学习和建模。</p>
<p>事实上，在机器学习领域，对模型的测量和评估至关重要。选择与问题相匹配的评估方法，能帮助我们快速准确地发现在模型选择和训练过程中出现的问题，进而对模型进行优化和迭代。本文我们系统地讲解一下机器学习模型评估相关知识。</p>
<h4 id="6-1-模型评估的目标"><a href="#6-1-模型评估的目标" class="headerlink" title="6.1 模型评估的目标"></a>6.1 模型评估的目标</h4><p><strong>模型评估的目标是选出泛化能力强的模型完成机器学习任务</strong>。实际的机器学习任务往往需要进行大量的实验，经过反复调参、使用多种模型算法（甚至多模型融合策略）来完成自己的机器学习问题，并观察哪种模型算法在什么样的参数下能够最好地完成任务。</p>
<p>但是我们无法提前获取「未知的样本」，因此我们会基于已有的数据进行切分来完成模型训练和评估，借助于切分出的数据进行评估，可以很好地判定模型状态（过拟合 or 欠拟合），进而迭代优化。</p>
<p>在建模过程中，为了获得泛化能力强的模型，我们需要一整套方法及评价指标。</p>
<ul>
<li><strong>评估方法</strong>：为保证客观地评估模型，对数据集进行的有效划分实验方法。</li>
<li><strong>性能指标</strong>：量化地度量模型效果的指标。</li>
</ul>
<h4 id="6-2-离线与在线实验方法"><a href="#6-2-离线与在线实验方法" class="headerlink" title="6.2 离线与在线实验方法"></a>6.2 离线与在线实验方法</h4><p>进行评估的实验方法可以分为「离线」和「在线」两种。</p>
<h4 id="离线实验方法："><a href="#离线实验方法：" class="headerlink" title="离线实验方法："></a>离线实验方法：</h4><blockquote>
<p>  在<strong>离线评估</strong>中，经常使用<strong>准确率（Accuracy）、查准率（Precision）、召回率（Recall）、ROC、AUC、PRC</strong>等指标来评估模型。</p>
</blockquote>
<p><strong>模型评估通常指离线试验</strong>。原型设计（Prototyping）阶段及离线试验方法，包含以下几个过程：</p>
<ul>
<li>使用历史数据训练一个适合解决目标任务的一个或多个机器学习模型。</li>
<li>对模型进行验证（Validation）与离线评估（Offline Evaluation）。</li>
<li>通过评估指标选择一个较好的模型。</li>
</ul>
<h4 id="在线实验方法："><a href="#在线实验方法：" class="headerlink" title="在线实验方法："></a>在线实验方法：</h4><blockquote>
<p>  <strong>在线评估</strong>与离线评估所用的评价指标不同，一般使用一些商业评价指标，如<strong>用户生命周期值（Customer Lifetime value）、广告点击率（Click Through Rate）、用户流失率</strong>（Customer Churn Rate）等标。</p>
</blockquote>
<p>除了离线评估之外，其实还有一种在线评估的实验方法。由于模型是在老的模型产生的数据上学习和验证的，而线上的数据与之前是不同的，因此离线评估并不完全代表线上的模型结果。因此我们需要在线评估，来验证模型的有效性。</p>
<p><img src="https://www.zhihu.com/equation?tex=A%2FB%20%5Cquad%20Test" alt="公式"> <strong>是目前在线测试中最主要的方法</strong>。<img src="https://www.zhihu.com/equation?tex=A%2FB%20%5Cquad%20Test" alt="公式"> 是为同一个目标制定两个方案让一部分用户使用 <img src="https://www.zhihu.com/equation?tex=A" alt="公式"> 方案，另一部分用户使用 <img src="https://www.zhihu.com/equation?tex=B" alt="公式"> 方案，记录下用户的使用情况，看哪个方案更符合设计目标。如果不做AB实验直接上线新方案，新方案甚至可能会毁掉你的产品。</p>
<h4 id="6-3-模型离线评估后，为什么要进行ab测试？"><a href="#6-3-模型离线评估后，为什么要进行ab测试？" class="headerlink" title=" 6.3 模型离线评估后，为什么要进行ab测试？"></a><strong><font color="red"> 6.3 模型离线评估后，为什么要进行ab测试？</font></strong></h4><ul>
<li><strong>离线评估无法消除过拟合的影响</strong>，因此离线评估结果无法代替线上的评估效果</li>
<li><strong>离线评估过程中无法模拟线上的真实环境，例如数据丢失、样本反馈延迟</strong></li>
<li>线上的<strong>某些商业指标例如收益、留存等无法通过离线计算</strong></li>
</ul>
<h4 id="6-4-如何进行线上ab测试？"><a href="#6-4-如何进行线上ab测试？" class="headerlink" title="6.4 如何进行线上ab测试？"></a>6.4 <strong>如何进行线上ab测试？</strong></h4><p>进行ab测试的主要手段时对用户进行分桶，即将<strong>用户分成实验组和对照组</strong>。实验组使用新模型，对照组使用base模型。<strong>分桶过程中需要保证样本的独立性和采样的无偏性</strong>，确保每个用户只划分到一个桶中，分桶过程中需要保证user id是一个<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=随机数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;440144351&quot;}">随机数</a>，才能保证数据无偏的。</p>
<h2 id="七、模型评估"><a href="#七、模型评估" class="headerlink" title="七、模型评估"></a>七、模型评估</h2><h4 id="7-1-holdout"><a href="#7-1-holdout" class="headerlink" title="7.1 holdout"></a>7.1 holdout</h4><p><strong>留出法是机器学习中最常见的评估方法之一，它会从训练数据中保留出验证样本集，这部分数据不用于训练，而用于模型评估</strong>。</p>
<h4 id="7-2-交叉验证"><a href="#7-2-交叉验证" class="headerlink" title="7.2 交叉验证"></a>7.2 交叉验证</h4><p><strong>留出法的数据划分，可能会带来偏差</strong>。在机器学习中，另外一种比较常见的评估方法是交叉验证法—— <img src="https://www.zhihu.com/equation?tex=K" alt="公式"> <strong>折交叉验证对 <img src="https://www.zhihu.com/equation?tex=K" alt="公式"> 个不同分组训练的结果进行平均来减少方差</strong>。</p>
<h4 id="7-3-自助法"><a href="#7-3-自助法" class="headerlink" title="7.3 自助法"></a>7.3 自助法</h4><p>Bootstrap 是一种用小样本估计总体值的一种非参数方法，在进化和生态学研究中应用十分广泛。<strong>Bootstrap通过有放回抽样生成大量的伪样本，通过对伪样本进行计算，获得统计量的分布，从而估计数据的整体分布</strong>。</p>
<h2 id="八、超参数调优"><a href="#八、超参数调优" class="headerlink" title="八、超参数调优"></a>八、超参数调优</h2><p>神经网咯是有许多超参数决定的，例如网络深度，学习率，正则等等。如何寻找最好的超参数组合，是一个老人靠经验，新人靠运气的任务。</p>
<h4 id="8-1-网格搜索"><a href="#8-1-网格搜索" class="headerlink" title="8.1 网格搜索"></a>8.1 网格搜索</h4><h4 id="8-2-随机搜索"><a href="#8-2-随机搜索" class="headerlink" title="8.2 随机搜索"></a>8.2 随机搜索</h4><h4 id="8-3-贝叶斯优化"><a href="#8-3-贝叶斯优化" class="headerlink" title="==8.3 贝叶斯优化=="></a>==8.3 贝叶斯优化==</h4><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/390373572"><em>贝叶斯优化</em>(原理+代码解读)</a></p>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27916208">LightGBM调参指南(带贝叶斯优化代码)</a></p>
<ul>
<li>贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息</li>
<li>贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸</li>
<li><p>贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部最优</p>
<h4 id="可用的贝叶斯优化框架"><a href="#可用的贝叶斯优化框架" class="headerlink" title="可用的贝叶斯优化框架"></a>可用的贝叶斯优化框架</h4></li>
</ul>
<ol>
<li>BayesianOptimization：<a href="https://link.zhihu.com/?target=https%3A//github.com/fmfn/BayesianOptimization">https://github.com/fmfn/BayesianOptimization</a></li>
<li>清华开源的openbox：<a href="https://link.zhihu.com/?target=https%3A//open-box.readthedocs.io/zh_CN/latest/index.html">https://open-box.readthedocs.io/zh_CN/latest/index.html</a></li>
<li>华为开源的HEBO：<a href="https://link.zhihu.com/?target=https%3A//github.com/huawei-noah/HEBO">https://github.com/huawei-noah/HEBO</a></li>
<li><strong>Hyperopt</strong>：<a href="https://link.zhihu.com/?target=http%3A//hyperopt.github.io/hyperopt/">http://hyperopt.github.io/hype</a></li>
</ol>
</blockquote>
<h4 id="贝叶斯优化什么-【黑盒优化】"><a href="#贝叶斯优化什么-【黑盒优化】" class="headerlink" title="贝叶斯优化什么?【黑盒优化】"></a>贝叶斯优化什么?【黑盒优化】</h4><p>求助 gradient-free 的优化算法了，这类算法也很多了，<strong>贝叶斯优化就属于无梯度优化算法</strong>中的一种，它希望在尽可能少的试验情况下去尽可能获得优化命题的全局最优解。</p>
<p><img src="https://pic1.zhimg.com/v2-75933a225ab1875a27188013ce0d8740_b.jpg" alt="img" style="zoom: 33%;"></p>
<ul>
<li>目标函数 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> 及其导数未知，否则就可以用梯度下降等方法求解。</li>
<li>计算目标函数时间成本大，意味着像蚁群算法、遗传算法这种方法也失效了，因为计算一次要花费很多时间。</li>
</ul>
<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>贝叶斯优化，是一种使用<strong>贝叶斯定理来指导搜索以找到目标函数的最小值或最大值的方法</strong>，就是在每次迭代的时候，利用之前观测到的历史信息（先验知识）来进行下一次优化，通俗点讲，<strong>就是在进行一次迭代的时候，先回顾下之前的迭代结果，结果太差的 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 附近就不去找了，尽量往结果好一点的 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 附近去找最优解，</strong>这样一来搜索的效率就大大提高了，这其实和人的思维方式也有点像，每次在学习中试错，并且在下次的时候根据这些经验来找到最优的策略。</p>
<h4 id="贝叶斯优化过程"><a href="#贝叶斯优化过程" class="headerlink" title="贝叶斯优化过程"></a>贝叶斯优化过程</h4><p>首先，假设有一个这样的函数 <img src="https://www.zhihu.com/equation?tex=c%28x%29" alt="[公式]"> ，我们需要找到他的最小值，如下图所示，这也是我们所需要优化的目标函数，但是我们并不能够知道他的具体形状以及表达形式是怎么样的。</p>
<p><img src="https://pic3.zhimg.com/v2-ddb9527a36ddcdb539d573fa5124d576_b.jpg" alt="img"></p>
<p>贝叶斯优化是通过一种叫做代理优化的方式来进行的，就是不知道真实的目标函数长什么样，我们就用一个<strong>代理函数（surrogate function）来代替目标函数</strong>，<strong>而这个代理函数就可以通过先采样几个点，再通过这几个点来给他拟合出来</strong>，如下图虚线所示：</p>
<p><img src="https://pic2.zhimg.com/v2-53769125b87835a74445a472415c22a1_b.jpg" alt="img"></p>
<p>基于构造的代理函数，<strong>我们就可以在可能是最小值的点附近采集更多的点</strong>，或者在还没有采样过的区域来采集更多的点，有了更多点，就可以<strong>更新代理函数</strong>，使之更逼近真实的目标函数的形状，这样的话也更容易找到目标函数的最小值，这个采样的过程同样可以通过构建一个采集函数来表示，也就是知道了当前代理函数的形状，如何选择下一个 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 使得收益最大。</p>
<p><strong>然后重复以上过程，最终就可以找到函数的最小值点了，这大致就是贝叶斯优化的一个过程：</strong></p>
<ol>
<li><strong>初始化一个代理函数的先验分布</strong></li>
<li><strong>选择数据点 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，使得采集函数 <img src="https://www.zhihu.com/equation?tex=a%28x%29" alt="[公式]"> 取最大值</strong></li>
<li><strong>在目标函数 <img src="https://www.zhihu.com/equation?tex=+c%28x%29" alt="[公式]"> 中评估数据点 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 并获取其结果 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"></strong> </li>
<li><strong>使用新数据 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="[公式]"> 更新代理函数，得到一个后验分布（作为下一步的先验分布)</strong></li>
<li>重复2-4步，直到达到最大迭代次数</li>
</ol>
<p>举个例子，如图所示，一开始只有两个点（t=2），代理函数的分布是紫色的区域那块，然后根据代理函数算出一个采集函数（绿色线），取采集函数的最大值所在的 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> （红色三角处），算出 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> ，然后根据新的点 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="[公式]"> 更新代理函数和采集函数（t=3），继续重复上面步骤，选择新的采集函数最大值所在的 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，算出 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> ，再更新代理函数和采集函数，然后继续迭代。</p>
<p><img src="https://pic1.zhimg.com/v2-82ed7dc24fca93f3c2c5820ab519a5f8_b.jpg" alt="img" style="zoom: 67%;"></p>
<p>问题的核心就在于代理函数和采集函数如何构建，常用的代理函数有：</p>
<ol>
<li><strong>高斯过程（Gaussian processes）</strong></li>
<li><strong>Tree Parzer Estimator</strong></li>
<li><strong>概率随机森林：针对类别型变量</strong></li>
</ol>
<p>采集函数则需要兼顾两方面的性质：</p>
<ol>
<li>利用当前已开发的区域（Exploitation）：即在当前最小值附近继续搜索</li>
<li>探索尚未开发的区域（Exploration）：即在还没有搜索过的区域里面搜索，可能那里才是全局最优解</li>
</ol>
<p><strong>常用的采集函数有：</strong></p>
<ol>
<li>Probability of improvement（PI）</li>
<li>Expected improvement（EI）</li>
<li>Confidence bound criteria，包括LCB和UCB</li>
</ol>
<h4 id="8-4-Hyperopt"><a href="#8-4-Hyperopt" class="headerlink" title="8.4 Hyperopt"></a>8.4 Hyperopt</h4><p>Hyperopt 是一个强大的 Python 库，用于超参数优化，由 jamesbergstra 开发。Hyperopt 使用贝叶斯优化的形式进行参数调整，允许你为给定模型获得最佳参数。它可以在大范围内优化具有数百个参数的模型。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
