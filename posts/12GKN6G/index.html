<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="【机器学习】决策树（中）——Random Forest、Adaboost、GBDT   https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;classes.html#module-sklearn.ensemble   机器学习算法中GBDT与Adaboost的区别与联系是什么？ - Frankenstein的回答 - 知乎 https:&#x2F;&#x2F;www.zhihu.com&#x2F;q">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习（10）集成学习*">
<meta property="og:url" content="https://powerlzy.github.io/posts/12GKN6G/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="【机器学习】决策树（中）——Random Forest、Adaboost、GBDT   https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;classes.html#module-sklearn.ensemble   机器学习算法中GBDT与Adaboost的区别与联系是什么？ - Frankenstein的回答 - 知乎 https:&#x2F;&#x2F;www.zhihu.com&#x2F;q">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/v2-1ac553d300784e8d158bcc686e7cf66d_1440w.jpg?source=172ae18b">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-a0a3cb02f629f3db360fc68b4c2153c0_1440w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-3aab53d50ab65e11ad3c9e3decf895c2_1440w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-f6787a16c23950d129a7927269d5352a_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-92ff83c7c6acc0dea6bc53ffe815e8bc_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-60c942f91d33d9dedf9dd2c7d482af5d_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=M">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=N">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=K">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k%28k%3C%3DK%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=K">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csqrt%7BK%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=m">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%28y%2CF%29+%3D+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bn%7Dexp%28-y_iF_%7Bk%7D%28x_i%29%29++%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++L%28y%2C+F%29+%26%3D+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B%28-y_i%29+%28F_%7Bk-1%7D%28x_i%29+%2B+%5Calpha_k+f_k%28x_i%29%29%5D++%5C%5C+%26%3D++%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B-y_i+F_%7Bk-1%7D%28x_i%29+-y_i++%5Calpha_k+f_k%28x_i%29%5D+%5C%5C+%26%3D++%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B-y_i+F_%7Bk-1%7D%28x_i%29+%5D+exp%5B-y_i++%5Calpha_k+f_k%28x_i%29%5D+++%5Cend%7Balign%7D++%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F_%7Bk-1%7D%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w_%7Bk%2Ci%7D+%3D+exp%28-y_iF_%7Bk-1%7D%28x_i%29%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%28y%2C+F%28x%29%29+%3D%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bk%2Ci%7Dexp%5B-y_i%5Calpha_k+f_k%28x_i%29%5D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_k%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_k%28x%29+%3Dargmin%5C%3B+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bk%2Ci%7DI%28y_i+%5Cneq+f_k%28x_i%29%29+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_k%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+%5Calpha_k+%3D+%5Cfrac%7B1%7D%7B2%7Dlog%5Cfrac%7B1-e_k%7D%7Be_k%7D++%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=e_k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+e_k+%3D+%5Cfrac%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7D%5E%7B%E2%80%99%7DI%28y_i+%5Cneq+f_k%28x_i%29%29%7D%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7D%5E%7B%E2%80%99%7D%7D+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7DI%28y_i+%5Cneq+f_k%28x_i%29%29+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F_%7Bk%7D%28x%29+%3D+F_%7Bk-1%7D%28x%29+%2B+%5Calpha_kf_k%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w_%7Bk%2B1%2Ci%7D%3Dw_%7Bk%2Ci%7Dexp%5B-y_i%5Calpha_kf_k%28x%2Ci%29%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w_%7Bk%2B1%2Ci%7D+%3D+w_%7Bki%7Dexp%5B-y_i%5Calpha_kf_k%28x_i%29%5D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmu+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmu">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=0%3C%5Cmu%5Cleq1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmu">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_%7Bi%7D%28x%29+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F_k%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cln+%5Cfrac%7Bp%7D%7B1-p%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-+%5Csum_%7Bm%3D0%7D%5EM+h_m%28x%29%7D%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_m%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=loss%28x_i%2C+y_i%29+%3D+-y_i+%5Clog+%5Chat%7By_i%7D+-+%281-y_i%29+%5Clog%281-%5Chat%7By_i%7D%29%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F%28x%29+%3D+%5Csum_%7Bm%3D0%7D%5Ek+h_m%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=loss%28x_i%2C+y_i%7CF%28x%29%29+%3D+y_i+%5Clog+%5Cleft%28+1%2Be%5E%7B-F%28x_i%29%7D+%5Cright%29+%2B+%281-y_i%29+%5Cleft%5B+F%28x_i%29+%2B+%5Clog+%5Cleft%28+1%2Be%5E%7B-F%28x_i%29%7D+%5Cright%29+%5Cright%5D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+F%28x%29%7D%7C_%7Bx_i%2Cy_i%7D+%3D+y_i+-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-F%28x_i%29%7D%7D%3D+y_i+-+%5Chat%7By_i%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5C%7B+x_i%2C+y_i+-+%5Chat%7By_i%7D+%5C%7D_%7Bi%3D1%7D%5En">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F_0%28x%29+%3D+h_0%28x%29+%3D+%5Clog+%5Cfrac%7Bp_1%7D%7B1-p_1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=m+%3D+1%2C2%2C...%2CM">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=g_i+%3D+%5Chat%7By_i%7D+-+y_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5C%7B%28x_i%2C+-g_i%29%5C%7D_%7Bi%3D1%7D%5En">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=t_m%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-F_%7Bm-1%7D%28x%29%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Crho_m+%3D+%5Cmathop%7B%5Cmathrm%7Bargmin%7D%7D%5Climits_%7B%5Crho%7D+%5Csum_i+loss%28x_i%2C+y_i%7CF_%7Bm-1%7D%28x%29%2B%5Crho+t_m%28x%29%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F_m%28x%29+%3D+F_%7Bm-1%7D%28x%29+%2B+%5Calpha+%5Crho_m+t_m%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F_M%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_1%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=P%28y%3D2%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_2%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=...+...%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=P%28y%3Dk%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_k%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=F_k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=loss+%3D+-%5Csum_%7Bi%3D1%7D%5Ek+y_i+%5Clog+P%28y_i%7Cx%29+%3D+-%5Csum_%7Bi%3D1%7D%5Ek+y_i+%5Clog+%5Cfrac%7Be%5E%7BF_i%28x%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5Ek+e%5E%7BF_j%28x%29%7D%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i%5C+%28i%3D1...k%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+F_q%7D+%3D+y_q+-+%5Cfrac%7Be%5E%7BF_q%28x%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5Ek+e%5E%7BF_j%28x%29%7D%7D+%3D+y_q+-+%5Chat%7By_q%7D%5C%5C">
<meta property="article:published_time" content="2022-03-11T13:14:35.439Z">
<meta property="article:modified_time" content="2023-03-09T08:58:25.623Z">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-1ac553d300784e8d158bcc686e7cf66d_1440w.jpg?source=172ae18b">


<link rel="canonical" href="https://powerlzy.github.io/posts/12GKN6G/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://powerlzy.github.io/posts/12GKN6G/","path":"posts/12GKN6G/","title":"机器学习（10）集成学习*"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习（10）集成学习* | PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PowerLZY's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">【机器学习】决策树（中）——Random Forest、Adaboost、GBDT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">1. 集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.1 Bagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.2 Boosting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.3 Stacking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;1.4 Stacking vs 神经网络&#x3D;&#x3D;</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.4.1 Stacking是一种表示学习(representation learning)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.4.2  Stacking和神经网络从某种角度看有异曲同工之妙，神经网络也可以被看作是集成学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.4.3 Stacking 的输出层为什么用逻辑回归？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.4.4 Stacking是否需要多层？第一层的分类器是否越多越好？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">2. 偏差与方差</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.1 集成学习的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.2 Bagging 的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.3 Boosting 的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.4 小结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">3. Random Forest（Bagging）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.1 思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.2 处理缺失值的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.3 优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">优点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">缺点：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;3.4 基学习期的选择？&#x3D;&#x3D;</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">为什么集成学习的基分类器通常是决策树？还有什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">4 Adaboost (Boosting) 样本权重更新?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">4.1 思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">4.2 细节</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;4.2.1 损失函数 ???&#x3D;&#x3D;</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">4.2.2 正则化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">4.3 优缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">5. GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">5.1 思想</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">5.1.1 回归树（Regression Decision Tree）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">5.1.2 梯度迭代（Gradient Boosting）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">5.1.3 缩减（Shrinkage）添加权重、基数增大</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">5.2 优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">优点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">5.3 GBDT 与 Adaboost 的对比</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">相同：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">不同：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text"> 5.4 GBDT算法用于分类问题、二分类1维、多分类k维（one vs all）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">6 集成学习Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text"> 6.1 为什么gbdt和随机森林(稍好点)都不太适用直接用高维稀疏特征训练集？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">原因：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">方法：【LightGBM 互斥捆绑算法】</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">选择svm和lr这种能提供最佳分割平面的算法可能会更好；</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">6.1 为什么集成学习的基分类器通常是决策树？还有什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">6.2 可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text"> 6.3 为什么可以利用GBDT算法实现特征组合和筛选？【GBDT+LR】</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/12GKN6G/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习（10）集成学习* | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习（10）集成学习*
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:14:35" itemprop="dateCreated datePublished" datetime="2022-03-11T21:14:35+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-09 16:58:25" itemprop="dateModified" datetime="2023-03-09T16:58:25+08:00">2023-03-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>23 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2><span id="机器学习决策树中random-forest-adaboost-gbdt">【机器学习】决策树（中）——Random Forest、Adaboost、GBDT</span></h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble</a></p>
<p>  机器学习算法中GBDT与Adaboost的区别与联系是什么？ - Frankenstein的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/54626685/answer/140610056">https://www.zhihu.com/question/54626685/answer/140610056</a></p>
<p>  GBDT学习笔记 - 许辙的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/169382376">https://zhuanlan.zhihu.com/p/169382376</a></p>
<p>  GBDT - 王多鱼的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38057220">https://zhuanlan.zhihu.com/p/38057220</a></p>
</blockquote>
<p><img src="https://pic2.zhimg.com/v2-1ac553d300784e8d158bcc686e7cf66d_1440w.jpg?source=172ae18b" alt="【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细）"></p>
<p>本文主要介绍基于集成学习的决策树，其主要通过不同学习框架生产基学习器，并综合所有基学习器的预测结果来改善单个基学习器的识别率和泛化性。</p>
<p>==模型的准确度可由偏差和方差共同决定：==</p>
<p>$\text { Error }=\text { bias }^{2}+\operatorname{var}+\xi$</p>
<p><strong>模型总体期望：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
E(F) &=E\left(\sum_{i}^{m} r_{i} f_{i}\right) \\
&=\sum_{i}^{m} r_{i} E\left(f_{i}\right)
\end{aligned}</script><p><strong>模型总体方差</strong>:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{Var}(F) &=\operatorname{Var}\left(\sum_{i}^{m} r_{i} f_{i}\right) \\
&=\sum_{i}^{m} \operatorname{Var}\left(r_{i} f_{i}\right)+\sum_{i \neq j}^{m} \operatorname{Cov}\left(r_{i} f_{i}, r_{j} f_{j}\right) \\
&=\sum_{i}^{m} r_{i}{ }^{2} \operatorname{Var}\left(f_{i}\right)+\sum_{i \neq j}^{m} \rho r_{i} r_{j} \sqrt{\operatorname{Var}\left(f_{i}\right)} \sqrt{\operatorname{Var}\left(f_{j}\right)} \\
&=m r^{2} \sigma^{2}+m(m-1) \rho r^{2} \sigma^{2} \\
&=m r^{2} \sigma^{2}(1-\rho)+m^{2} r^{2} \sigma^{2} \rho
\end{aligned}</script><div class="table-container">
<table>
<thead>
<tr>
<th>集成学习</th>
<th>Bagging</th>
<th>Boosting</th>
<th>Stacking</th>
</tr>
</thead>
<tbody>
<tr>
<td>思想</td>
<td>对训练集进行<strong>有放回抽样</strong>得到子训练集</td>
<td>基模型的训练是有<strong>顺序</strong>的，每个基模型都会在前一个基模型学习的基础上进行学习；基于贪心策略的前向加法</td>
<td><strong>预测值</strong>将作为训练样本的特征值，进行训练得到最终预测结果。</td>
</tr>
<tr>
<td>样本抽样</td>
<td>有放回地抽取数据集</td>
<td>训练集不变</td>
<td></td>
</tr>
<tr>
<td>样本权重</td>
<td>样本权重相等</td>
<td>不断调整样本的权重</td>
<td></td>
</tr>
<tr>
<td>优化目标</td>
<td>减小的是方差</td>
<td>减小的是偏差</td>
<td></td>
</tr>
<tr>
<td>基模型</td>
<td><strong>强模型（偏差低，方差高）</strong></td>
<td><strong>弱模型（偏差高，方差低）</strong>而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</td>
<td><strong>强模型（偏差低，方差高）</strong></td>
</tr>
<tr>
<td>相关性</td>
<td></td>
<td>对于 Boosting 来说，由于基模型共用同一套训练集，所以基模型间具有强相关性，故模型间的相关系数近似等于 1</td>
<td></td>
</tr>
<tr>
<td>模型偏差</td>
<td><strong>整体模型的偏差与基模型近似</strong>。(<script type="math/tex">\mu</script>)</td>
<td>基于贪心策略的前向加法，随着基模型数的增多，偏差减少。</td>
<td></td>
</tr>
<tr>
<td>模型方差</td>
<td>随着<strong>模型的增加可以降低整体模型的方差</strong>，故其基模型需要为强模型；(<script type="math/tex">\frac{\sigma^{2}(1-\rho)}{m}+\sigma^{2} \rho</script>)</td>
<td><strong>整体模型的方差与基模型近似</strong>（<script type="math/tex">\sigma^{2}</script>）</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="1-集成学习">1. 集成学习</span></h2><p>常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。三种集成学习框架在基学习器的产生和综合结果的方式上会有些区别，我们先做些简单的介绍。</p>
<h3><span id="11-bagging">1.1 Bagging</span></h3><p>Bagging 全称叫 <strong>Bootstrap aggregating</strong>，，==每个基学习器都会对训练集进行<strong>有放回抽样</strong>得到子训练集==，比较著名的采样法为 0.632 自助法（<strong>Bootstrap</strong>）。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是<strong>投票法</strong>，票数最多的类别为预测类别。</p>
<p><img src="https://pic1.zhimg.com/80/v2-a0a3cb02f629f3db360fc68b4c2153c0_1440w.jpg" alt="img"></p>
<h3><span id="12-boosting">1.2 Boosting</span></h3><p><strong>Boosting 训练过程为阶梯状，基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果</strong>，用的比较多的综合方式为加权法。</p>
<p><img src="https://pic3.zhimg.com/80/v2-3aab53d50ab65e11ad3c9e3decf895c2_1440w.jpg" alt="img"></p>
<h3><span id="13-stacking">1.3 Stacking</span></h3><p><strong>Stacking 是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行的预测，==其预测值将作为训练样本的特征值==，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终预测结果。</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-f6787a16c23950d129a7927269d5352a_1440w.jpg" alt="img"></p>
<p>==那么，为什么集成学习会好于单个学习器呢？原因可能有三：==</p>
<ul>
<li><p>训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；</p>
</li>
<li><p>假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；</p>
</li>
<li><p>可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。</p>
</li>
</ul>
<h3><span id="14-stacking-vs-神经网络">==<strong>1.4 Stacking</strong> vs <strong>神经网络</strong>==</span></h3><blockquote>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32896968">https://zhuanlan.zhihu.com/p/32896968</a></p>
<p><strong>本文的核心观点是提供一种对于stacking的理解，即与神经网络对照来看。</strong>当然，在<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/59769987/answer/269367049">阿萨姆：为什么做stacking之后，准确率反而降低了？</a>中我已经说过stacking不是万能药，但往往很有效。通过与神经网络的对比，读者可以从另一个角度加深对stacking的理解。</p>
</li>
</ul>
</blockquote>
<h4><span id="141-stacking是一种表示学习representation-learning">1.4.1 Stacking是一种表示学习(representation learning)</span></h4><p><strong>表示学习指的是模型从原始数据中自动抽取有效特征的过程</strong>，比如深度学习就是一种表示学习的方法。关于表示学习的理解可以参考：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/264417928/answer/283087276">阿萨姆：人工智能（AI）是如何处理数据的？</a></p>
<p>原始数据可能是杂乱无规律的。在stacking中，通过第一层的多个学习器后，有效的特征被学习出来了。从这个角度来看，stacking的第一层就是特征抽取的过程。在[1]的研究中，上排是未经stacking的数据，下排是经过stacking(多个无监督学习算法)处理后的数据，我们显著的发现红色和蓝色的数据在下排中分界更为明显。<strong>==数据经过了压缩处理。这个小例子说明了，有效的stacking可以对原始数据中的特征有效的抽取==</strong>。</p>
<p><img src="https://pic1.zhimg.com/80/v2-92ff83c7c6acc0dea6bc53ffe815e8bc_1440w.jpg" alt="img" style="zoom:80%;"></p>
<h4><span id="142-stacking和神经网络从某种角度看有异曲同工之妙神经网络也可以被看作是集成学习">1.4.2  <strong>Stacking和神经网络从某种角度看有异曲同工之妙，神经网络也可以被看作是集成学习</strong></span></h4><p>承接上一点，stacking的学习能力主要来自于对于特征的表示学习，这和神经网络的思路是一致的。这也是为什么我说“第一层”，“最后一层”。</p>
<p>而且神经网络也可以被看做是一种集成学习，主要取决于不同神经元、层对于不同特征的理解不同。从浅层到深层可以理解为一种从具体到抽象的过程。</p>
<p><strong>Stacking中的第一层可以等价于神经网络中的前 n-1层，而stacking中的最终分类层可以类比于神经网络中最后的输出层。</strong>不同点在于，<strong>stacking中不同的分类器通过异质来体现对于不同特征的表示</strong>，神经网络是从同质到异质的过程且有分布式表示的特点(distributed representation)。Stacking中应该也有分布式的特点，主要表现在多个分类器的结果并非完全不同，而有很大程度的相同之处。</p>
<p>但同时这也提出了一个挑战，多个分类器应该尽量在保证效果好的同时尽量不同，stacking集成学习框架的对于基分类器的两个要求：</p>
<ul>
<li>差异化(diversity)要大</li>
<li>准确性(accuracy)要高</li>
</ul>
<h4><span id="143-stacking-的输出层为什么用逻辑回归"><strong>1.4.3 Stacking 的输出层为什么用逻辑回归？</strong></span></h4><blockquote>
<p>  <strong>表示学习的过拟合问题</strong>：</p>
<ul>
<li>仅包含学习到的特征</li>
<li>交叉验证</li>
<li>简单模型：<strong>逻辑回归</strong></li>
</ul>
</blockquote>
<p>如果你看懂了上面的两点，你应该可以理解stacking的有效性主要来自于特征抽取。<strong>而表示学习中，如影随形的问题就是过拟合，试回想深度学习中的过拟合问题。</strong></p>
<p>在[3]中，周志华教授也重申了stacking在使用中的过拟合问题。因为第二层的特征来自于对于第一层数据的学习，那么第二层数据中的特征中不该包括原始特征，<strong>以降低过拟合的风险</strong>。举例：</p>
<ul>
<li>第二层数据特征：仅包含学习到的特征</li>
<li>第二层数据特征：包含学习到的特征 + 原始特征</li>
</ul>
<p>另一个例子是，stacking中一般都用交叉验证来避免过拟合，足可见这个问题的严重性。</p>
<p>为了降低过拟合的问题，第二层分类器应该是较为简单的分类器，广义线性如逻辑回归是一个不错的选择。<strong>在特征提取的过程中，我们已经使用了复杂的非线性变换，因此在输出层不需要复杂的分类器</strong>。这一点可以对比神经网络的激活函数或者输出层，都是很简单的函数，一点原因就是不需要复杂函数并能控制复杂度。</p>
<h4><span id="144-stacking是否需要多层第一层的分类器是否越多越好"><strong>1.4.4 Stacking是否需要多层？第一层的分类器是否越多越好？</strong></span></h4><p>通过以上分析，stacking的表示学习不是来自于多层堆叠的效果，而是<strong>来自于不同学习器对于不同特征的学习能力</strong>，并有效的结合起来。一般来看，2层对于stacking足够了。多层的stacking会面临更加复杂的过拟合问题，且收益有限。</p>
<p>第一层分类器的数量对于特征学习应该有所帮助，<strong>经验角度看越多的基分类器越好。即使有所重复和高依赖性，我们依然可以通过特征选择来处理</strong>，问题不大。</p>
<h2><span id="2-偏差与方差">2. 偏差与方差</span></h2><p>上节介绍了集成学习的基本概念，这节我们主要介绍下如何从偏差和方差的角度来理解集成学习。</p>
<h4><span id="21-集成学习的偏差与方差">2.1 集成学习的偏差与方差</span></h4><p><strong>==偏差（Bias）描述的是预测值和真实值之差==</strong>；<strong>==方差（Variance）描述的是预测值作为随机变量的离散程==度</strong>。放一场很经典的图：</p>
<p><img src="https://pic2.zhimg.com/80/v2-60c942f91d33d9dedf9dd2c7d482af5d_1440w.jpg" alt="img"></p>
<p><strong>模型</strong>的<strong>偏差</strong>与<strong>方差</strong></p>
<ul>
<li><strong>偏差：</strong>描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的 High Variance，点会很分散。低偏差对应的点都打在靶心附近，所以喵的很准，但不一定很稳；</li>
<li><strong>方差：</strong>描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图 High Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。</li>
</ul>
<p>我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型，<strong>但并不是所有集成学习框架中的基模型都是弱模型</strong>。<strong>Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting 中的基模型为弱模型（偏差高，方差低）</strong>。</p>
<h4><span id="22-bagging-的偏差与方差">2.2 Bagging 的偏差与方差</span></h4><ul>
<li><strong>整体模型的期望等于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。</strong></li>
<li><strong>整体模型的方差小于等于基模型的方差，当且仅当相关性为 1 时取等号，随着基模型数量增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。</strong>但是，模型的准确度一定会无限逼近于 1 吗？并不一定，当基模型数增加到一定程度时，方差公式第一项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。</li>
</ul>
<h4><span id="23-boosting-的偏差与方差">2.3 Boosting 的偏差与方差</span></h4><ul>
<li>整体模型的方差等于基模型的方差，如果基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting 框架中的基模型必须为弱模型。</li>
<li>此外 Boosting 框架中采用基于贪心策略的前向加法，整体模型的期望由基模型的期望累加而成，所以随着基模型数的增多，整体模型的期望值增加，整体模型的准确度提高。</li>
</ul>
<h4><span id="24-小结">2.4 小结</span></h4><ul>
<li>我们可以使用<strong>==模型的偏差和方差来近似描述模型的准确度==</strong>；</li>
<li>对于 Bagging 来说，整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型；</li>
<li>对于 Boosting 来说，整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</li>
</ul>
<h2><span id="3-random-forestbagging">3. Random Forest（Bagging）</span></h2><blockquote>
<ol>
<li>随机森林具有<strong>防止过拟合能力</strong>，精度比大多数单个算法要好；<ol>
<li>随机森林分类器可以<strong>处理缺失值</strong>；</li>
</ol>
</li>
<li><strong>==于有袋外数据(OOB)==，可以在模型生成过程中取得真实误差的无偏估计，且不损失训练数据量在训练过程中，能够检测到feature间的互相影响，且可以得出feature的重要性，具有一定参考意义；</strong><ol>
<li>每棵树可以独立、同时生成，容易做成<strong>并行化方法</strong>；</li>
</ol>
</li>
<li>具有一定的特征选择能力。</li>
</ol>
</blockquote>
<p><strong>Random Forest（随机森林），用随机的方式建立一个森林。RF 算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。</strong></p>
<p>对于分类问题，其输出的类别是由个别树输出的众数所决定的。在回归问题中，把每一棵决策树的输出进行平均得到最终的回归结果。</p>
<h3><span id="31-思想">3.1 思想</span></h3><p>Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：</p>
<ul>
<li><strong>样本随机：</strong>假设训练数据集共有 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 个对象的数据，从样本数据中采取有放回（<strong>Boostrap</strong>）随机抽取 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 个样本（因为是有放回抽取，有些数据可能被选中多次，有些数据可能不被选上)，每一次取出的样本不完全相同，这些样本组成了决策树的训练数据集；</li>
<li><strong>特征随机：</strong>假设每个样本数据都有 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 个特征，从所有特征中随机地选取 <img src="https://www.zhihu.com/equation?tex=k%28k%3C%3DK%29" alt="[公式]"> 个特征，选择最佳分割属性作为节点建立CART决策树，决策树成长期间 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 的大小始终不变（<strong>在Python中构造随机森林模型的时候，默认取特征的个数 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 的平方根，即 <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BK%7D" alt="[公式]"></strong> )；</li>
<li>重复前面的步骤，建立 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 棵CART树，这些树都要完全的成长且不被修剪，这些树形成了森林；</li>
<li>根据这些树的预测结果进行投票，决定样本的最后预测类别。（针对回归模型，是根据这些决策树模型的平均值来获取最终的结果）</li>
</ul>
<p>随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；<strong>==随机选择特征是指在每个节点在分裂过程中都是随机选择特征的==</strong>（区别与每棵树随机选择一批特征）。</p>
<blockquote>
<p>  这种随机性导致随机森林的==偏差会有稍微的增加==（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的==方差减小==，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。</p>
</blockquote>
<p>随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算==不剪枝也不会出现过拟合==。</p>
<h3><span id="32-处理缺失值的方法">3.2 处理缺失值的方法</span></h3><ul>
<li>方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是<strong>分类变量(categorical var)缺失，用众数补上</strong>，如果是<strong>连续型变量(numerical var)缺失，用中位数补</strong>。</li>
<li>方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用<strong>proximity矩阵进行加权平均的方法补缺失值</strong>。然后迭代4-6次，这个补缺失值的思想和KNN有些类似。</li>
</ul>
<h3><span id="33-优缺点">3.3 优缺点</span></h3><h4><span id="优点"><strong>优点</strong>：</span></h4><ol>
<li><strong>模型准确率高</strong>：随机森林既可以处理分类问题，也可以处理回归问题，即使存在部分数据缺失的情况，随机森林也能保持很高的分类精度。</li>
<li><strong>能够处理数量庞大的高维度的特征</strong>，且不需要进行降维（因为特征子集是随机选择的）；</li>
<li><strong>易于并行化</strong>，在大数据集上有很大的优势；</li>
<li><p><strong>可解释性</strong>：可以生成树状结构，判断各个特征的重要性；</p>
<blockquote>
<p>  在sklearn中，随机森林<strong>基于每棵树分裂时的GINI指数下降量</strong>来判断各个特征的重要性。但是这种方法存在一个问题：当特征是连续的时候或者是类别很多的离散特征时，该方法会将这些特征的重要性增加。</p>
<p>  解决方法：对特征编码，使得特征的取值数量相近。</p>
</blockquote>
</li>
<li><strong>对异常值、缺失值不敏感；</strong></li>
<li><strong>随机森林有袋外数据（OOB），因此不需要单独划分交叉验证集</strong>。</li>
</ol>
<h4><span id="缺点">缺点：</span></h4><ul>
<li>随机森林解决回归问题的效果不如分类问题；（因为它的预测不是天生连续的，在解决回归问题时，随机森林并不能为训练数据以外的对象给出答案）</li>
<li><strong>树之间的相关性越大，错误率就越大</strong>；</li>
<li><strong>当训练数据噪声较大时，容易产生过拟合现象。</strong></li>
</ul>
<h3><span id="34-基学习期的选择">==3.4 基学习期的选择？==</span></h3><h4><span id="为什么集成学习的基分类器通常是决策树还有什么">为什么集成学习的基分类器通常是决策树？还有什么？</span></h4><p>基分类器通常是决策树：样本权重、方便调节、随机性；</p>
<ul>
<li><strong>==决策树可以较方便地将样本权重整合到训练过程中，而不需要通过过采样来调整样本权重。==</strong></li>
<li>树的表达能力和泛化能力，<strong>方便调节</strong>（可以通过树的层数来调节）</li>
<li>样本的扰动对决策树的影响较大，<strong><font color="red"> 因此不同子样本集合生成的决策树基分类器随机性较大。这样的不稳定的分类器更适合作为基分类器。</font></strong>此外树节点分类时随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li>
</ul>
<h4><span id="可以将随机森林的基分类器由决策树替换成线性分类器或k-nn吗">可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></h4><p>Bagging主要好处是集成后的方差，比基分类器小。bagging采用的基分类，最好是本身对样本分布较为敏感。而线性分类器和K-NN都是较为稳定的分类器（参数模型？）甚至<strong>可能因为采样，而导致他们再训练中更难收敛，从而增大了集成分类器的偏差。</strong></p>
<h2><span id="4-adaboost-boosting-样本权重更新">4 Adaboost (Boosting) 样本权重更新?</span></h2><p>AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：<strong>前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的==足够小的错误率或达到预先指定的最大迭代次数==。</strong></p>
<h3><span id="41-思想">4.1 思想</span></h3><p><strong>==Adaboost 迭代算法有三步：==</strong></p>
<ul>
<li>初始化训练样本的权值分布，每个样本具有相同权重；</li>
<li>训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；</li>
<li>将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，<strong>加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重</strong>。</li>
</ul>
<h3><span id="42-细节">4.2 细节</span></h3><h5><span id="421-损失函数">==<strong>4.2.1 损失函数 ???</strong>==</span></h5><p>Adaboost 模型是<strong>加法模型</strong>，学习算法为<strong>前向分步学习算法</strong>，损失函数为<strong>指数函数的分类问题</strong>。</p>
<p><strong>加法模型</strong>：最终的强分类器是由若干个弱分类器<strong>加权平均</strong>得到的。</p>
<p><strong>前向分布学习算法</strong>：算法是通过一轮轮的弱学习器学习，<strong>利用前一个弱学习器的结果来更新后一个弱学习器的训练集权重</strong>。第 k 轮的强学习器为：</p>
<script type="math/tex; mode=display">
F_{k}(x)=\sum_{i=1}^{k} \alpha_{i} f_{i}(x)=F_{k-1}(x)+\alpha_{k} f_{k}(x)</script><p><strong>定义损失函数为 n 个样本的指数损失函数</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28y%2CF%29+%3D+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bn%7Dexp%28-y_iF_%7Bk%7D%28x_i%29%29++%5C%5C" alt="[公式]"></p>
<p>利用前向分布学习算法的关系可以得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++L%28y%2C+F%29+%26%3D+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B%28-y_i%29+%28F_%7Bk-1%7D%28x_i%29+%2B+%5Calpha_k+f_k%28x_i%29%29%5D++%5C%5C+%26%3D++%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B-y_i+F_%7Bk-1%7D%28x_i%29+-y_i++%5Calpha_k+f_k%28x_i%29%5D+%5C%5C+%26%3D++%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B-y_i+F_%7Bk-1%7D%28x_i%29+%5D+exp%5B-y_i++%5Calpha_k+f_k%28x_i%29%5D+++%5Cend%7Balign%7D++%5C%5C" alt="[公式]"></p>
<p><strong>因为 <img src="https://www.zhihu.com/equation?tex=F_%7Bk-1%7D%28x%29" alt="[公式]"> 已知==，所以令 <img src="https://www.zhihu.com/equation?tex=w_%7Bk%2Ci%7D+%3D+exp%28-y_iF_%7Bk-1%7D%28x_i%29%29" alt="[公式]"> ，==随着每一轮迭代而将这个式子带入损失函数，损失函数转化为：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=L%28y%2C+F%28x%29%29+%3D%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bk%2Ci%7Dexp%5B-y_i%5Calpha_k+f_k%28x_i%29%5D+%5C%5C" alt="[公式]"></p>
<p>我们求 <img src="https://www.zhihu.com/equation?tex=f_k%28x%29" alt="[公式]"> ，可以得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=f_k%28x%29+%3Dargmin%5C%3B+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bk%2Ci%7DI%28y_i+%5Cneq+f_k%28x_i%29%29+%5C%5C" alt="[公式]"></p>
<p>将 <img src="https://www.zhihu.com/equation?tex=f_k%28x%29" alt="[公式]"> 带入损失函数，并对 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 求导，使其等于 0，则就得到了：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Calpha_k+%3D+%5Cfrac%7B1%7D%7B2%7Dlog%5Cfrac%7B1-e_k%7D%7Be_k%7D++%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=e_k" alt="[公式]"> 即为我们前面的<strong>分类误差率</strong>。</p>
<p><img src="https://www.zhihu.com/equation?tex=+e_k+%3D+%5Cfrac%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7D%5E%7B%E2%80%99%7DI%28y_i+%5Cneq+f_k%28x_i%29%29%7D%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7D%5E%7B%E2%80%99%7D%7D+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7DI%28y_i+%5Cneq+f_k%28x_i%29%29+%5C%5C" alt="[公式]"></p>
<p><strong>最后看样本权重的更新</strong>。利用 <img src="https://www.zhihu.com/equation?tex=F_%7Bk%7D%28x%29+%3D+F_%7Bk-1%7D%28x%29+%2B+%5Calpha_kf_k%28x%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=w_%7Bk%2B1%2Ci%7D%3Dw_%7Bk%2Ci%7Dexp%5B-y_i%5Calpha_kf_k%28x%2Ci%29%5D" alt="[公式]"> ，即可得：</p>
<p><img src="https://www.zhihu.com/equation?tex=w_%7Bk%2B1%2Ci%7D+%3D+w_%7Bki%7Dexp%5B-y_i%5Calpha_kf_k%28x_i%29%5D+%5C%5C" alt="[公式]"></p>
<p>这样就得到了样本权重更新公式。</p>
<h4><span id="422-正则化"><strong>4.2.2 正则化</strong></span></h4><p><strong>为了防止 Adaboost 过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长</strong>（learning rate）。对于前面的弱学习器的迭代,加上正则化项 <img src="https://www.zhihu.com/equation?tex=%5Cmu+" alt="[公式]"> 我们有：</p>
<script type="math/tex; mode=display">
F_{k}(x)=F_{k-1}(x)+\mu \alpha_{k} f_{k}(x)</script><p><img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 的取值范围为 <img src="https://www.zhihu.com/equation?tex=0%3C%5Cmu%5Cleq1" alt="[公式]"> 。对于同样的训练集学习效果，较小的 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>
<h3><span id="43-优缺点">4.3 优缺点</span></h3><p><strong>4.3.1 优点</strong></p>
<ol>
<li>分类精度高；</li>
<li>可以<strong>用各种回归分类模型来构建弱学习器，非常灵活</strong>；</li>
<li>不容易发生过拟合。</li>
</ol>
<p><strong>4.3.2 缺点</strong></p>
<ol>
<li>对异常点敏感，异常点会获得较高权重。</li>
</ol>
<h2><span id="5-gbdt">5. GBDT</span></h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://www.cnblogs.com/modifyrong/p/7744987.html">https://www.cnblogs.com/modifyrong/p/7744987.html</a></p>
</blockquote>
<p><strong>GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，该算法由多棵决策树组成，从名字中我们可以看出来它是属于 Boosting 策略。GBDT 是被公认的泛化能力较强的算法。</strong></p>
<h3><span id="51-思想">5.1 思想</span></h3><p><strong>GBDT是boosting算法的一种，按照boosting的思想，在GBDT算法的每一步，用一棵决策树去拟合当前学习器的残差，获得一个新的弱学习器。将这每一步的决策树组合起来，就得到了一个强学习器</strong>。GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和 Shrinkage（一个重要演变）</p>
<h4><span id="511-回归树regression-decision-tree"><strong>5.1.1 回归树（Regression Decision Tree）</strong></span></h4><p>如果认为 GBDT 由很多分类树那就大错特错了（虽然调整后也可以分类）。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。GBDT 的核心在于累加所有树的结果作为最终结果，所以 GBDT 中的树都是<strong>回归树</strong>，不是分类树，这一点相当重要。</p>
<p><strong><font color="red"> 回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。</font></strong></p>
<h4><span id="512-梯度迭代gradient-boosting"><strong>5.1.2 梯度迭代（Gradient Boosting）</strong></span></h4><p>上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，GBDT 的每一棵树都是以之前树得到的<strong>残差【负梯度】</strong>来更新目标值，这样每一棵树的<strong>值加起来</strong>即为 GBDT 的预测值。</p>
<p>模型的预测值可以表示为：</p>
<script type="math/tex; mode=display">
F_{k}(x)=\sum_{i=1}^{k} f_{i}(x)</script><p><img src="https://www.zhihu.com/equation?tex=f_%7Bi%7D%28x%29+" alt="[公式]"> 为<strong>基模型与其权重的乘积</strong>，模型的训练目标是使预测值 <img src="https://www.zhihu.com/equation?tex=F_k%28x%29" alt="[公式]"> 逼近真实值 y，也就是说要让每个基模型的预测值逼近各自要预测的部分真实值。<strong>==贪心==</strong>的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：</p>
<script type="math/tex; mode=display">
F_{k}(x)=F_{k-1}(x)+f_{k}(x)</script><p>其实很简单，其<strong>==残差其实是最小均方损失函数关于预测值的反向梯度(划重点)==</strong>：<strong>用负梯度的解作为样本新的真实值</strong>。基于残差 GBDT 容易对异常值敏感。</p>
<script type="math/tex; mode=display">
-\frac{\partial\left(\frac{1}{2}\left(y-F_{k}(x)\right)^{2}\right)}{\partial F_{k}(x)}=y-F_{k}(x)</script><p>很明显后续的模型会对第 4 个值关注过多，这不是一种好的现象，所以一般回归类的损失函数会用<strong>绝对损失或者 Huber 损失函数</strong>来代替平方损失函数。</p>
<script type="math/tex; mode=display">
L(y, F)=|y-F|</script><script type="math/tex; mode=display">
L(y, F)= \begin{cases}\frac{1}{2}(y-F)^{2} & |y-F| \leq \delta \\ \delta(|y-F|-\delta / 2) & |y-F|>\delta\end{cases}</script><p>GBDT 的 Boosting 不同于 Adaboost 的 Boosting，<strong>==GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0==</strong>，这样后面的树就能专注于那些被分错的样本。</p>
<blockquote>
<p>  <strong><font color="red"> 最后补充一点拟合残差的问题，无论损失函数是什么形式，每个决策树拟合的都是负梯度。只有当损失函数是均方损失时，负梯度刚好是残差</font></strong>，也就是说<strong>拟合残差只是针对均方损失的特例</strong>，并不能说GBDT的迭代的过程是拟合残差。</p>
</blockquote>
<h4><span id="513-缩减shrinkage添加权重-基数增大"><strong>5.1.3 缩减（Shrinkage）</strong>添加权重、基数增大</span></h4><blockquote>
<p>  <strong><font color="red"> gbdt中的步长和参数中的学习率作用是什么？详细讲一讲？</font></strong></p>
<ul>
<li>参数中的学习率用于梯度下降</li>
</ul>
</blockquote>
<p>Shrinkage 的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。</p>
<p>Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。<strong>本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大</strong>。</p>
<h3><span id="52-优缺点">5.2 优缺点</span></h3><h4><span id="优点"><strong>优点</strong></span></h4><ol>
<li>可以自动进行特征组合，拟合非线性数据；在稠密数据集上泛化能力和表达能力很好。</li>
<li>可以灵活处理各种类型的数据，不需要对数据预处理和归一化。</li>
<li>预测可以并行，计算数据很快。</li>
</ol>
<h4><span id="缺点"><strong>缺点</strong></span></h4><ol>
<li>对异常点敏感。</li>
</ol>
<h3><span id="53-gbdt-与-adaboost-的对比">5.3 GBDT 与 Adaboost 的对比</span></h3><h4><span id="相同"><strong>相同：</strong></span></h4><ol>
<li>都是 Boosting 家族成员，使用弱分类器；</li>
<li>都使用前向分布算法；</li>
</ol>
<h4><span id="不同"><strong>不同：</strong></span></h4><ol>
<li><strong>迭代思路不同</strong>：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；</li>
<li><strong>损失函数不同</strong>：AdaBoost 采用的是<strong>指数损失</strong>，GBDT 使用的是<strong>绝对损失</strong>或者 <strong>Huber 损失函数</strong>；</li>
</ol>
<h3><span id="54-gbdt算法用于分类问题-二分类1维-多分类k维one-vs-all"><strong><font color="red"> 5.4 GBDT算法用于分类问题、二分类1维、多分类k维（one vs all）</font></strong></span></h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46445201">https://zhuanlan.zhihu.com/p/46445201</a></p>
</blockquote>
<p>将GBDT应用于回归问题，相对来说比较容易理解。因为<strong>回归问题的损失函数一般为平方差损失函数</strong>，这时的残差，恰好等于预测值与实际值之间的差值。每次拿一棵决策树去拟合这个差值，使得残差越来越小，这个过程还是比较intuitive的。</p>
<p><strong><font color="red"> 将GBDT用于分类问题，类似于逻辑回归、FM模型用于分类问题，其实是在用一个线性模型或者包含交叉项的非线性模型，去拟合所谓的对数几率 <img src="https://www.zhihu.com/equation?tex=%5Cln+%5Cfrac%7Bp%7D%7B1-p%7D" alt="[公式]"> 。</font></strong>而GBDT也是一样，只是用一系列的梯度提升树去拟合这个对数几率，实际上最终得到的是一系列CART回归树。其分类模型可以表达为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-+%5Csum_%7Bm%3D0%7D%5EM+h_m%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p>其中<img src="https://www.zhihu.com/equation?tex=h_m%28x%29" alt="[公式]"> 就是学习到的决策树。清楚了这一点之后，我们便可以参考逻辑回归，单样本 <img src="https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29" alt="[公式]"> 的损失函数可以表达为<strong>交叉熵</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss%28x_i%2C+y_i%29+%3D+-y_i+%5Clog+%5Chat%7By_i%7D+-+%281-y_i%29+%5Clog%281-%5Chat%7By_i%7D%29%5C%5C" alt="[公式]"></p>
<p>假设第k步迭代之后当前学习器为 <img src="https://www.zhihu.com/equation?tex=F%28x%29+%3D+%5Csum_%7Bm%3D0%7D%5Ek+h_m%28x%29" alt="[公式]"> ，将 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]"> 的表达式带入之后， 可将损失函数写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss%28x_i%2C+y_i%7CF%28x%29%29+%3D+y_i+%5Clog+%5Cleft%28+1%2Be%5E%7B-F%28x_i%29%7D+%5Cright%29+%2B+%281-y_i%29+%5Cleft%5B+F%28x_i%29+%2B+%5Clog+%5Cleft%28+1%2Be%5E%7B-F%28x_i%29%7D+%5Cright%29+%5Cright%5D%5C%5C" alt="[公式]"></p>
<p><strong>可以求得损失函数相对于当前学习器的负梯度为：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+F%28x%29%7D%7C_%7Bx_i%2Cy_i%7D+%3D+y_i+-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-F%28x_i%29%7D%7D%3D+y_i+-+%5Chat%7By_i%7D%5C%5C" alt="[公式]"></p>
<p>可以看到，同回归问题很类似，下一棵决策树的训练样本为： <img src="https://www.zhihu.com/equation?tex=%5C%7B+x_i%2C+y_i+-+%5Chat%7By_i%7D+%5C%7D_%7Bi%3D1%7D%5En" alt="[公式]"> ，其所需要拟合的残差为真实标签与预测概率之差。于是便有下面GBDT应用于二分类的算法：</p>
<ul>
<li><p><img src="https://www.zhihu.com/equation?tex=F_0%28x%29+%3D+h_0%28x%29+%3D+%5Clog+%5Cfrac%7Bp_1%7D%7B1-p_1%7D" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=p_1" alt="[公式]"> 是训练样本中y=1的比例，利用先验信息来初始化学习器</p>
</li>
<li><p>For <img src="https://www.zhihu.com/equation?tex=m+%3D+1%2C2%2C...%2CM" alt="[公式]"> ：</p>
<ul>
<li>计算 <img src="https://www.zhihu.com/equation?tex=g_i+%3D+%5Chat%7By_i%7D+-+y_i" alt="[公式]"> ，并使用训练集 <img src="https://www.zhihu.com/equation?tex=%5C%7B%28x_i%2C+-g_i%29%5C%7D_%7Bi%3D1%7D%5En" alt="[公式]"> <strong>训练一棵回归树</strong> <img src="https://www.zhihu.com/equation?tex=t_m%28x%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-F_%7Bm-1%7D%28x%29%7D%7D" alt="[公式]"></li>
<li>通过一维最小化损失函数找到树的最优权重： <img src="https://www.zhihu.com/equation?tex=%5Crho_m+%3D+%5Cmathop%7B%5Cmathrm%7Bargmin%7D%7D%5Climits_%7B%5Crho%7D+%5Csum_i+loss%28x_i%2C+y_i%7CF_%7Bm-1%7D%28x%29%2B%5Crho+t_m%28x%29%29" alt="[公式]"></li>
<li><strong>考虑shrinkage</strong>，可得这一轮迭代之后的学习器 <img src="https://www.zhihu.com/equation?tex=F_m%28x%29+%3D+F_%7Bm-1%7D%28x%29+%2B+%5Calpha+%5Crho_m+t_m%28x%29" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 为学习率</li>
</ul>
</li>
<li><p>得到最终学习器为： <img src="https://www.zhihu.com/equation?tex=F_M%28x%29" alt="[公式]"></p>
</li>
</ul>
<p>以上就是将GBDT应用于二分类问题的算法流程。类似地，对于多分类问题，则需要考虑以下softmax模型：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_1%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D2%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_2%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=...+...%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3Dk%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_k%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><strong>其中 <img src="https://www.zhihu.com/equation?tex=F_1" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=F_k" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 个不同的tree ensemble。每一轮的训练实际上是训练了 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 棵树去拟合softmax的每一个分支模型的负梯度</strong>【one-hot中的一维】。softmax模型的单样本损失函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss+%3D+-%5Csum_%7Bi%3D1%7D%5Ek+y_i+%5Clog+P%28y_i%7Cx%29+%3D+-%5Csum_%7Bi%3D1%7D%5Ek+y_i+%5Clog+%5Cfrac%7Be%5E%7BF_i%28x%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5Ek+e%5E%7BF_j%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><strong>这里的 <img src="https://www.zhihu.com/equation?tex=y_i%5C+%28i%3D1...k%29" alt="[公式]"> 是样本label在k个类别上作one-hot编码之后的取值，只有一维为1，其余都是0。由以上表达式不难推导：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+F_q%7D+%3D+y_q+-+%5Cfrac%7Be%5E%7BF_q%28x%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5Ek+e%5E%7BF_j%28x%29%7D%7D+%3D+y_q+-+%5Chat%7By_q%7D%5C%5C" alt="[公式]"></p>
<p>可见，这k棵树同样是拟合了样本的真实标签与预测概率之差，与二分类的过程非常类似。</p>
<h2><span id="6-集成学习qampa">6 集成学习Q&amp;A</span></h2><h4><span id="61-为什么gbdt和随机森林稍好点都不太适用直接用高维稀疏特征训练集"><strong><font color="red"> 6.1 为什么gbdt和随机森林(稍好点)都不太适用直接用高维稀疏特征训练集？</font></strong></span></h4><h5><span id="原因">原因：</span></h5><p>gbdt这类boosting或者rf这些bagging集成分类器模型的算法，是典型的贪心算法，在当前节点总是选择对当前数据集来说最好的选择</p>
<p>一个6层100树的模型，要迭代2^(5 4 3 2 1 0)<em>100次<em>*,每次都根据当前节点最大熵或者最小误差分割来选择变量</em></em></p>
<p><strong>那么，高维稀疏数据集里很多“小而美”的数据就被丢弃了</strong>，因为它对当前节点来说不是最佳分割方案(比如，关联分析里，支持度很低置信度很高的特征)</p>
<p>但是高维数据集里面，对特定的样本数据是有很强预测能力的，比如你买叶酸，买某些小的孕妇用品品类，对应这些人6个月后买奶粉概率高达40%，但叶酸和孕妇用品销量太小了，用户量全网万分之一都不到，这种特征肯定是被树算法舍弃的，哪怕这些特征很多很多。。它仍是被冷落的份。。。</p>
<h5><span id="方法lightgbm-互斥捆绑算法">方法：【LightGBM 互斥捆绑算法】</span></h5><h5><span id="选择svm和lr这种能提供最佳分割平面的算法可能会更好">选择svm和lr这种能提供最佳分割平面的算法可能会更好；</span></h5><p>但如果top.特征已经能够贡献很大的信息量了，比如刚才孕妇的案例，你用了一个孕妇用品一级类目的浏览次数购买金额购买次数这样的更大更强的特征包含了这些高维特征的信息量，那可能gbdt会更好</p>
<p>实际情况的数据集是，在数据仓库里的清洗阶段，你可以选择把它做成高维的特征，也可以选择用算法把它做成低维的特征，一般有</p>
<p>1-在数据清洗阶段，或用类目升级(三级类目升级到二三级类目)范围升级的方式来做特征，避免直接清洗出来高维特征</p>
<p>2-在特征生成后，<strong>利用数据分析结论简单直接的用多个高维特征合并</strong>(<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加减乘除&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A574865175}">加减乘除</a>逻辑判断都行，随你合并打分)的方式来做特征，前提你hold得住工作量判断量，但这个如果业务洞察力强效果有可能特别好</p>
<p>3-在特征工程的特征处理阶段，我们可以用<strong>PCA因子构建等降维算法做特征整合</strong>，对应训练集，也这么搞，到时候回归或预测的时候，就用这个因子或者主成分的值来做特征</p>
<h4><span id="61-为什么集成学习的基分类器通常是决策树还有什么">6.1 为什么集成学习的基分类器通常是决策树？还有什么？</span></h4><p>基分类器通常是决策树：样本权重、方便调节、随机性；</p>
<ul>
<li><strong>==决策树可以较方便地将样本权重整合到训练过程中，而不需要通过过采样来调整样本权重。==</strong></li>
<li>树的表达能力和泛化能力，<strong>方便调节</strong>（可以通过树的层数来调节）</li>
<li>样本的扰动对决策树的影响较大，<strong><font color="red"> 因此不同子样本集合生成的决策树基分类器随机性较大。这样的不稳定的分类器更适合作为基分类器。</font></strong>此外树节点分类时随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li>
</ul>
<h4><span id="62-可以将随机森林的基分类器由决策树替换成线性分类器或k-nn吗">6.2 可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></h4><p>Bagging主要好处是集成后的方差，比基分类器小。bagging采用的基分类，最好是本身对样本分布较为敏感。而线性分类器和K-NN都是较为稳定的分类器（参数模型？）甚至可能因为采样，而导致他们再训练中更难收敛，从而增大了集成分类器的偏差。</p>
<h4><span id="63-为什么可以利用gbdt算法实现特征组合和筛选gbdtlr"><strong><font color="red"> 6.3 为什么可以利用GBDT算法实现特征组合和筛选？【GBDT+LR】</font></strong></span></h4><p>GBDT模型是有一组有序的树模型组合起来的，前面的树是由对大多数样本有明显区分度的特征分裂构建而成，经过前面的树，仍然存在少数残差较大的样本，后面的树主要由能对这些少数样本有区分度的特征分裂构建。优先选择对整体有区分度的特征，然后再选择对少数样本有区分度的特征，这样才更加合理，所以<strong>GBDT子树节点分裂是一个特征选择的过程，而子树的多层结构则对特征组合的过程，最终实现特征的组合和筛选。</strong></p>
<p><strong>GBDT+LR融合方案：</strong></p>
<p>（1）利用GBDT模型训练数据，最终得到一系列弱分类器的cart树。</p>
<p>（2）<strong>生成新的训练数据。将原训练数据重新输入GBDT模型，对于每一个样本，都会经过模型的一系列树，对于每棵树，将样本落到的叶子节点置为1，其他叶子为0，然后将叶子节点的数字从左至右的拼接起来，形成该棵树的特征向量，最后将所有树的特征向量拼接起来，形成新的数据特征，之后保留原样本标签形成新的训练数据。</strong></p>
<p>（3）将上一步得到的训练数据作为输入数据输入到LR模型中进行训练</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>lzy
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://powerlzy.github.io/posts/12GKN6G/" title="机器学习（10）集成学习*">https://powerlzy.github.io/posts/12GKN6G/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/YFRZTY/" rel="prev" title="机器学习（11）LGB*">
                  <i class="fa fa-chevron-left"></i> 机器学习（11）LGB*
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/1N8XMT6/" rel="next" title="机器学习（16）聚类*-HDBSCAN">
                  机器学习（16）聚类*-HDBSCAN <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
