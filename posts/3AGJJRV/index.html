<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="【机器学习】决策树（上）——ID3、C4.5、CART决策树是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。本文将分三篇介绍决策树，第一篇介绍基本树（包括 ID3、C4.5、CART），第二篇介绍 Random Forest、Adaboost、GBDT，第三篇介绍 Xgboost 和 LightGBM。     算法 ID3（&#x3D;&#x3D;分类&#x3D;&#x3D;） C4.">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习（9）决策树">
<meta property="og:url" content="https://powerlzy.github.io/posts/3AGJJRV/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="【机器学习】决策树（上）——ID3、C4.5、CART决策树是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。本文将分三篇介绍决策树，第一篇介绍基本树（包括 ID3、C4.5、CART），第二篇介绍 Random Forest、Adaboost、GBDT，第三篇介绍 Xgboost 和 LightGBM。     算法 ID3（&#x3D;&#x3D;分类&#x3D;&#x3D;） C4.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Gain%28D%2CA%29%3DH%28D%29-H%28D%7CA%29++%5C%5C">
<meta property="og:image" content="https://powerlzy.github.io/posts/3AGJJRV/image-20220321203204744.png">
<meta property="og:image" content="https://powerlzy.github.io/posts/3AGJJRV/image-20220320215056933.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T_0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_%5Calpha%28T%29%3DC%28T%29%2B%5Calpha%7CT%7C++%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C%28T%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%7CT%7C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C%28T%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%7CT%7C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7BN_1%28node%29%7D%7BN_1%28root%29%7D+%3E+%5Cfrac%7BN_0%28node%29%7D%7BN_0%28root%29%7D++%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B0.5%2C+0.2%2C+0.8%2C+0.9%2C+1.2%2C+2.1%2C+3.2%2C+4.5%5D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B0.2%2C+0.5%2C+0.8%2C+0.9%2C+1.2%2C+2.1%2C+3.2%2C+4.5%5D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B0.35%2C+0.65%2C+0.85%2C+1.05%2C+1.65%2C+2.65%2C+3.85%5D+%5C%5C">
<meta property="og:image" content="https://picx.zhimg.com/v2-9081bc3cd5f2ec069212b79d5c5ff7d3_b.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C">
<meta property="og:image" content="https://powerlzy.github.io/posts/3AGJJRV/image-20220321203204744.png">
<meta property="og:image" content="https://powerlzy.github.io/posts/3AGJJRV/image-20220322131856478.png">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-0e87b3bf410cd798efd05a2837b83589_1440w.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-ee1ddd22da5171fa44e079582cefe20a_1440w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5EN%5B1-y_i%28w%C2%B7x_i+%2B+b%29%5D_%2B+%2B+%5Clambda%7C%7Cw%7C%7C%5E2+%5C%5C+%5Bz%5D_%2B+%3D+%5Cbegin%7Bequation%7D+%5Cleft%5C%7B++++++++++++++%5Cbegin%7Barray%7D%7Blr%7D+++++++++++z%2C+z%3E0+%26++%5C%5C++++++++++++++0.z%5Cleq0+%26+++++++++++++++%5Cend%7Barray%7D++%5Cright.+%5Cend%7Bequation%7D+%5C%5C+">
<meta property="og:image" content="https://powerlzy.github.io/posts/3AGJJRV/image-20220316144906846.png">
<meta property="og:image" content="https://powerlzy.github.io/posts/3AGJJRV/image-20220316145437180.png">
<meta property="article:published_time" content="2022-02-25T09:29:37.718Z">
<meta property="article:modified_time" content="2023-01-29T08:20:53.855Z">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=C_k">


<link rel="canonical" href="https://powerlzy.github.io/posts/3AGJJRV/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://powerlzy.github.io/posts/3AGJJRV/","path":"posts/3AGJJRV/","title":"机器学习（9）决策树"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习（9）决策树 | PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PowerLZY's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">【机器学习】决策树（上）——ID3、C4.5、CART</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">1. ID3【删特征】</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.1 思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.2 划分标准</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.3 缺点【没有剪枝、特征偏好、缺失值】</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">2. C4.5</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2.2 划分标准</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2.3 剪枝策略</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.3.1 预剪枝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.3.2 后剪枝【悲观剪枝方法】  </span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2.4 缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">3. CART</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.1 思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.2 划分标准</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.3 缺失值处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.4 剪枝策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.5 类别不平衡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.6 连续值处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.6.1 分类树</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.7 回归树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">连续值处理：&#x3D;&#x3D;RSS残差平方和&#x3D;&#x3D;</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">预测方式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3.7 CART分类树建模时，预测变量中存在连续和离散时，会自动分别进行处理吗？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">4. 总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">一、决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.1 介绍决策树ID2、C4.5、CART, 3种决策树及其区别和适应场景？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.2 决策树处理连续值的方法?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.3 决策树处理缺失值的方式？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.3.1 在特征值缺失的情况下进行划分特征的选择？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.3.2 选定该划分特征，对于缺失该特征值的样本如何处理？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.4 决策树如何剪枝？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.5 决策树特征选择？特征重要性判断？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.6 SVM、LR、决策树的对比？</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3AGJJRV/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习（9）决策树 | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习（9）决策树
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-25 17:29:37" itemprop="dateCreated datePublished" datetime="2022-02-25T17:29:37+08:00">2022-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-29 16:20:53" itemprop="dateModified" datetime="2023-01-29T16:20:53+08:00">2023-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/" itemprop="url" rel="index"><span itemprop="name">决策树</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>21 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2><span id="机器学习决策树上id3-c45-cart">【机器学习】决策树（上）——ID3、C4.5、CART</span></h2><p><strong>决策树</strong>是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。本文将分三篇介绍决策树，第一篇介绍基本树（包括 <strong>ID3、C4.5、CART</strong>），第二篇介绍 <strong>Random Forest、Adaboost、GBDT</strong>，第三篇介绍 <strong>Xgboost</strong> 和 <strong>LightGBM</strong>。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>ID3（==分类==）</th>
<th>C4.5（==分类==）</th>
<th>CART（==分类和回归==）</th>
</tr>
</thead>
<tbody>
<tr>
<td>思想</td>
<td>奥卡姆剃刀：越是小型的决策树越优于大的决策树;ID3 算法的核心思想就是以<strong>信息增益</strong>来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间。</td>
<td>C4.5 算法最大的特点是<strong>克服了 ID3 对特征数目的偏重</strong>这一缺点，引入<strong>信息增益率</strong>来作为分类标准。</td>
<td>CART 算法的二分法可以<strong>简化决策树的规模</strong>，提高生成决策树的效率。CART 包含的基本过程有<strong>分裂</strong>，<strong>剪枝</strong>和<strong>树选择</strong>。</td>
</tr>
<tr>
<td><strong>划分标准</strong></td>
<td><strong>信息增益</strong>  =  类别熵 - 特征类别熵                                <strong>类别熵</strong>：$H(D)=-\sum_{k=1}^{K} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>} \log _{2} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}$                 <strong>特征类别熵</strong>：$H(D \mid A)=\sum_{i=1}^{n} \frac{\left</td>
<td>D_{i}\right</td>
<td>}{</td>
<td>D</td>
<td>} H\left(D_{i}\right)$</td>
<td>先从候选划分特征中找到信息增益高于平均值的特征，再从中选择<strong>增益率</strong>最高的。</td>
<td><strong>Gini 系数</strong>作为变量的<strong>不纯度量</strong>，<strong>减少了大量的对数运算</strong>；$G i n i(D)=\sum_{k=1}^{K} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}\left(1-\frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}\right)$</td>
</tr>
<tr>
<td>剪枝策略</td>
<td><strong>无</strong></td>
<td><strong>悲观剪枝策略</strong></td>
<td>基于<strong>代价复杂度剪枝</strong></td>
</tr>
<tr>
<td>数据差异</td>
<td><strong>离散</strong>数据且<strong>缺失值</strong>敏感</td>
<td><strong>离散</strong>、<strong>连续特征离散化</strong>；【排序+离散化】</td>
<td><strong>连续型、离散型</strong></td>
</tr>
<tr>
<td><strong>连续值处理</strong></td>
<td>无</td>
<td><strong>排序</strong>并取相邻两样本值的<strong>平均数</strong>。</td>
<td><strong>排序</strong>并取相邻两样本值的<strong>平均数</strong>。<strong>CART 分类树</strong>【<strong>基尼系数</strong>】。<strong>回归树</strong>【<strong>和方差度量</strong>】。</td>
</tr>
<tr>
<td>缺失值处理</td>
<td><strong>无</strong></td>
<td>1、有缺失值特征，用没有缺失的样本子集所占比重来折算；2、将样本同时划分到所有子节点</td>
<td><strong>代理测试</strong>来估计缺失值</td>
</tr>
<tr>
<td>类别不平衡</td>
<td><strong>无</strong></td>
<td><strong>无</strong></td>
<td><strong>先验机制</strong>：其作用相当于对数据自动重加权，对类别进行均衡。</td>
</tr>
<tr>
<td><strong>==缺点==</strong></td>
<td>1、ID3 没有剪枝策略，容易过拟合；2、信息增益准则对可取值<strong>数目较多的特征有所偏好</strong>，类似“编号”的特征其信息增益接近于 1； 3、只能用于处理离散分布的特征； 没有考虑缺失值。</td>
<td>1、<strong>多叉树</strong>。2、<strong>只能用于分类</strong>。3、熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>。4、驻留于内存的数据集。</td>
<td>熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>。</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>划分标准的差异：</strong>ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li>
<li><strong>使用场景的差异：</strong>ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li>
<li><strong>样本数据的差异：</strong>ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li>
<li><strong>样本特征的差异：</strong>ID3 和 C4.5 层级之间只使用一次特征，==CART 可多次重复使用特征==；</li>
<li><strong>剪枝策略的差异：</strong>ID3 没有剪枝策略，C4.5 是通过<strong>悲观剪枝策略</strong>来修正树的准确性，而 CART 是通过<strong>代价复杂度</strong>剪枝。</li>
</ul>
<h2><span id="1-id3删特征">1. ID3【删特征】</span></h2><p>ID3 算法是建立在奥卡姆剃刀[<strong>“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情”</strong>]（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。</p>
<h3><span id="11-思想">1.1 思想</span></h3><p>从信息论的知识中我们知道：信息熵越大，从而样本纯度越低，。ID3 算法的核心思想就是以<strong>信息增益</strong>来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。 其大致步骤为：</p>
<ol>
<li>初始化特征集合和数据集合；</li>
<li>计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；</li>
<li>更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；</li>
<li>重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。</li>
</ol>
<h3><span id="12-划分标准">1.2 划分标准</span></h3><p>ID3 使用的分类标准是信息增益，它表示得知特征 A 的信息而使得样本集合不确定性减少的程度。</p>
<p>数据集的<strong>信息熵</strong>：</p>
<p>$H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}$</p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=C_k" alt="[公式]"> 表示集合 D 中属于第 k 类样本的样本子集。针对某个特征 A，对于数据集 D 的条件熵 $H(D \mid A)$为：</p>
<p>$\begin{aligned} H(D \mid A) &amp;=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right) \\ &amp;=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|}\left(\sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}\right) \end{aligned}$</p>
<p><strong>信息增益</strong> = 信息熵 - 条件熵。信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</p>
<p><img src="https://www.zhihu.com/equation?tex=Gain%28D%2CA%29%3DH%28D%29-H%28D%7CA%29++%5C%5C" alt="[公式]"></p>
<p>信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</p>
<h3><span id="13-缺点没有剪枝-特征偏好-缺失值">1.3 缺点【没有剪枝、特征偏好、缺失值】</span></h3><ul>
<li>ID3 没有剪枝策略，容易过拟合；</li>
<li>信息增益准则==对可取值数目较多的特征==有所偏好，类似“编号”的特征其信息增益接近于 1；</li>
<li>只能用于处理离散分布的特征；</li>
<li>没有考虑缺失值。</li>
</ul>
<h2><span id="2-c45">2. C4.5</span></h2><p>C4.5 算法最大的特点是克服了 ID3 对==特征数目的偏重==这一缺点，引入信息增益率来作为分类标准。</p>
<p>C4.5 相对于 ID3 的缺点对应有以下改进方式： </p>
<ul>
<li>引入<strong>悲观剪枝策略进行后剪枝</strong>； </li>
<li>引入<strong>信息增益率</strong>作为划分标准； </li>
<li><strong>将连续特征离散化</strong>，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； </li>
<li>对于<strong>缺失值的处理</strong>可以分为两个子问题：</li>
<li>问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）<ul>
<li>C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；</li>
</ul>
</li>
<li>问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里） <ul>
<li>C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</li>
</ul>
</li>
</ul>
<h3><span id="22-划分标准">2.2 划分标准</span></h3><p>利用信息增益率可以克服信息增益的缺点，其公式为</p>
<p>$\begin{aligned} \operatorname{Gain}_{\text {ratio }}(D, A) &amp;=\frac{\operatorname{Gain}(D, A)}{H_{A}(D)}     \\ H_{A}(D)=-\sum_{i=1}^{n}   \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|} \end{aligned}$</p>
<p>信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个<strong>启发式方法</strong>：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。</p>
<h3><span id="23-剪枝策略">2.3 剪枝策略</span></h3><p>为什么要剪枝：<strong>过拟合的树在泛化能力的表现非常差。</strong></p>
<p><strong>预剪枝和悲观剪枝</strong></p>
<h4><span id="231-预剪枝"><strong>2.3.1 预剪枝</strong></span></h4><p>在节点划分前来确定是否继续增长，及早停止增长的主要方法有：</p>
<ul>
<li>节点内数据样本低于<strong>某一阈值</strong>；</li>
<li>所有节点特征都已分裂；</li>
<li>节点划分前准确率比划分后准确率高。</li>
</ul>
<p>预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。</p>
<h4><span id="232-后剪枝悲观剪枝方法-httpgitlinuxnet2019-06-04-c45"><strong>2.3.2 后剪枝</strong>【悲观剪枝方法】  </span></h4><p>在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。</p>
<p>C4.5 采用的<strong>悲观剪枝方法</strong>，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。<strong>C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率</strong>。</p>
<p>后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。</p>
<h3><span id="24-缺点">2.4 缺点</span></h3><ul>
<li><strong>剪枝策略</strong>可以再优化；</li>
<li>C4.5 用的是<strong>多叉树</strong>，用二叉树效率更高；</li>
<li>C4.5 只能用于<strong>分类</strong>；</li>
<li>C4.5 使用的熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>；</li>
<li>C4.5 在构造树的过程中，<strong>对数值属性值需要按照其大小进行排序</strong>，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li>
</ul>
<h2><span id="3-cart">3. CART</span></h2><p>ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。</p>
<h3><span id="31-思想">3.1 思想</span></h3><p>CART 包含的基本过程有分裂，剪枝和树选择。 </p>
<ul>
<li><strong>分裂：</strong>分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去； </li>
<li><strong>剪枝：</strong>采用<strong>代价复杂度剪枝</strong>，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树； </li>
<li><strong>树选择：</strong>用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。</li>
</ul>
<p>CART 在 C4.5 的基础上进行了很多提升。 </p>
<ul>
<li>C4.5 为多叉树，运算速度慢，CART 为<strong>二叉树</strong>，运算速度快； </li>
<li>C4.5 只能分类，CART 既可以分类也可以<strong>回归</strong>； </li>
<li>CART 使用 ==<strong>Gini 系数作为变量的不纯度量</strong>，减少了<strong>大量的对数运算</strong>；== </li>
<li>CART 采用<strong>代理测试来估计缺失值</strong>，而 C4.5 以不同概率划分到不同节点中； </li>
<li>CART 采用<strong>“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法</strong>。</li>
</ul>
<h3><span id="32-划分标准">3.2 划分标准</span></h3><p><strong>熵模型拥有大量耗时的对数运算</strong>，基尼指数在简化模型的同时还保留了熵模型的优点。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。</p>
<p>$\begin{aligned} \operatorname{Gini}(D) &amp;=\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|}\left(1-\frac{\left|C_{k}\right|}{|D|}\right) =1- \sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}  &amp;\operatorname{Gini}(D \mid A) =\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \operatorname{Gini}\left(D_{i}\right) \end{aligned}$</p>
<p>==<strong>基尼指数</strong>反映了从<strong>数据集中随机抽取两个样本，其类别标记不一致的概率</strong>==。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等，<strong>基尼指数可以理解为熵模型的一阶泰勒展开。</strong></p>
<blockquote>
<p>  <strong><em>基尼指数是信息熵中﹣logP在P=1处一阶泰勒展开后的结果！所以两者都可以用来度量数据集的纯度</em></strong></p>
</blockquote>
<h3><span id="33-缺失值处理">3.3 缺失值处理</span></h3><p>上文说到，模型对于缺失值的处理会分为两个子问题：</p>
<ul>
<li><strong>如何在特征值缺失的情况下进行划分特征的选择？</strong></li>
</ul>
<p>对于问题 1，<strong>CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响</strong>（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。</p>
<ul>
<li><strong>选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？</strong></li>
</ul>
<p>对于问题 2，CART 算法的机制是为树的每个节点都找到<strong>代理分裂器</strong>，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是<strong>代替缺失值特征作为划分特征的特征</strong>），<strong>当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理</strong>，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。</p>
<h3><span id="34-剪枝策略">3.4 剪枝策略</span></h3><p><strong>基于代价复杂度的剪枝</strong>:<a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv11066239">https://www.bilibili.com/read/cv11066239</a></p>
<p>采用一种<strong>“基于代价复杂度的剪枝</strong>”方法进行<strong>后剪枝</strong>，这种方法会生成一系列树，每<strong>个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点</strong>。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。<strong>这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树</strong>。</p>
<blockquote>
<p>  从完整子树 $T0$ 开始， 通过在 $Ti$ 子树序列中裁剪真实误差最小【考虑叶子节点的个数】的子树，得到 $Ti+1$。 </p>
<p>  <img src="image-20220321203204744.png" alt="image-20220321203204744" style="zoom: 25%;">【剪枝之后的误差 - 剪枝前的误差 / 叶子节点数 - 1】</p>
<p>  每次误差增加率最小的节点，得到一系列的子树，从中选择效果最好的【独立剪枝数据集】和【K折交叉验证】</p>
</blockquote>
<p><img src="image-20220320215056933.png" alt="image-20220320215056933" style="zoom:50%;"></p>
<p>我们来看具体看一下代价复杂度剪枝算法：</p>
<p>首先我们将最大树称为 <img src="https://www.zhihu.com/equation?tex=T_0" alt="[公式]"> ，我们希望减少树的大小来防止过拟合，但又担心去掉节点后预测误差会增大，所以我们定义了一个损失函数来达到这两个变量之间的平衡。损失函数定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=C_%5Calpha%28T%29%3DC%28T%29%2B%5Calpha%7CT%7C++%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"> 为任意子树， <img src="https://www.zhihu.com/equation?tex=C%28T%29" alt="[公式]"> 为预测误差， <img src="https://www.zhihu.com/equation?tex=%7CT%7C" alt="[公式]"> 为子树 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"> 的叶子节点个数， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 是参数， <img src="https://www.zhihu.com/equation?tex=C%28T%29" alt="[公式]"> 衡量训练数据的拟合程度， <img src="https://www.zhihu.com/equation?tex=%7CT%7C" alt="[公式]"> 衡量树的复杂度， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> <strong>权衡拟合程度与树的复杂度</strong>。</p>
<h3><span id="35-类别不平衡">3.5 类别不平衡</span></h3><p>CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其子冻消除不需要建模人员采取其他操作。</p>
<p>CART 使用了一种先验机制，其作用相当于对类别进行加权。这种先验机制嵌入于 CART 算法判断分裂优劣的运算里，在 CART 默认的分类模式中，总是要计算每个节点关于根节点的类别频率的比值，这就相当于对数据自动重加权，对类别进行均衡。</p>
<p>对于一个二分类问题，节点 node 被分成类别 1 当且仅当：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BN_1%28node%29%7D%7BN_1%28root%29%7D+%3E+%5Cfrac%7BN_0%28node%29%7D%7BN_0%28root%29%7D++%5C%5C" alt="[公式]"></p>
<p>比如二分类，根节点属于 1 类和 0 类的分别有 20 和 80 个。在子节点上有 30 个样本，其中属于 1 类和 0 类的分别是 10 和 20 个。如果 10/20&gt;20/80，该节点就属于 1 类。</p>
<p>通过这种计算方式就无需管理数据真实的类别分布。假设有 K 个目标类别，就可以确保根节点中每个类别的概率都是 1/K。这种默认的模式被称为“先验相等”。</p>
<p>先验设置和加权不同之处在于先验不影响每个节点中的各类别样本的数量或者份额。先验影响的是每个节点的类别赋值和树生长过程中分裂的选择。</p>
<h3><span id="36-连续值处理">3.6 连续值处理</span></h3><h4><span id="361-分类树">3.6.1 分类树</span></h4><ul>
<li><p><strong><font color="red">如果特征值是连续值：CART的处理思想与C4.5是相同的，即将连续特征值离散化。唯一不同的地方是度量的标准不一样，</font></strong> <strong>CART采用基尼指数，而C4.5采用信息增益比</strong>。</p>
</li>
<li><p>如果当前节点为连续属性，<strong>CART树中该属性（剩余的属性值）后面还可以参与子节点的产生选择过程</strong>。</p>
</li>
</ul>
<h3><span id="37-回归树">3.7 回归树</span></h3><p><strong>CART（Classification and Regression Tree，分类回归树），从名字就可以看出其不仅可以用于分类，也可以应用于回归</strong>。其回归树的建立算法上与分类树部分相似，这里简单介绍下不同之处。</p>
<h5><span id="连续值处理rss残差平方和"><strong>连续值处理</strong>：==RSS<strong>残差平方和</strong>==</span></h5><p>对于连续值的处理，<strong>CART 分类树采用基尼系数的大小来度量特征的各个划分点</strong>。<strong>在回归模型中，我们使用常见的和方差度量方式</strong>，对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> ，求出使 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 各自<strong>集合的均方差最小</strong>，同时 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=c_1" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 数据集的样本输出均值， <img src="https://www.zhihu.com/equation?tex=c_2" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 数据集的样本输出均值。</p>
<h5><span id="预测方式"><strong>预测方式</strong></span></h5><p>对于决策树建立后做预测的方式，上面讲到了 CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。</p>
<h3><span id="37-cart分类树建模时预测变量中存在连续和离散时会自动分别进行处理吗">3.7 CART分类树建模时，预测变量中存在连续和离散时，会自动分别进行处理吗？</span></h3><blockquote>
<p>  在使用sklearn的决策树CART建模时，预测变量中存在连续和离散时，会自动分别进行处理吗？ - 月来客栈的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/472579561/answer/2002434993">https://www.zhihu.com/question/472579561/answer/2002434993</a></p>
</blockquote>
<p><strong>对于这种连续型的特征变量，Sklearn中的具体做法（包括ID3、CART、随机森林等）是先对连续型特征变量进行排序处理</strong>，<strong><font color="red"> 然后取所有连续两个值的均值来离散化整个连续型特征变量。</font></strong></p>
<p>假设现在某数据集其中一个特征维度为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.5%2C+0.2%2C+0.8%2C+0.9%2C+1.2%2C+2.1%2C+3.2%2C+4.5%5D+%5C%5C" alt="[公式]"></p>
<p>则首先需要对其进行排序处理，排序后的结果为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.2%2C+0.5%2C+0.8%2C+0.9%2C+1.2%2C+2.1%2C+3.2%2C+4.5%5D+%5C%5C" alt="[公式]"></p>
<p>接着再计算所有连续两个值之间的平均值：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.35%2C+0.65%2C+0.85%2C+1.05%2C+1.65%2C+2.65%2C+3.85%5D+%5C%5C" alt="[公式]"></p>
<p>这样，便得到了该特征离散化后的结果。最后在构造<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=决策树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;2002434993&quot;}">决策树</a>时，只需要使用式最后离散化后的特征进行划分指标的计算即可。同时，值得一说的地方是<strong>目前Sklearn在实际处理时，会把所有的特征均看作连续型变量进行处理</strong>。</p>
<p>下图所示为iris数据集根据sklearn中CART算法所建模的决策树的可视化结果：</p>
<p><img src="https://picx.zhimg.com/v2-9081bc3cd5f2ec069212b79d5c5ff7d3_b.jpg" alt="img" style="zoom:50%;"></p>
<p>从图中可以看到，<code>petal width</code>这个特征在前两次分类时的分割点分别为0.8和1.75。下面先来看看原始特征<code>petal width</code>的取值情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span>  <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">1.4</span> <span class="number">2.5</span> <span class="number">1.3</span> <span class="number">2.1</span> <span class="number">1.5</span> <span class="number">0.2</span> <span class="number">2.</span>  <span class="number">1.</span>  <span class="number">0.2</span> <span class="number">0.3</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">0.2</span> <span class="number">0.5</span> <span class="number">1.3</span> <span class="number">0.2</span> <span class="number">1.2</span> <span class="number">2.2</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">2.</span>  <span class="number">0.2</span> <span class="number">1.8</span> <span class="number">1.9</span> <span class="number">1.</span>  <span class="number">1.5</span> <span class="number">2.3</span> <span class="number">1.3</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">1.9</span> <span class="number">0.2</span> <span class="number">0.2</span> <span class="number">1.1</span> <span class="number">1.7</span> <span class="number">0.2</span> <span class="number">2.4</span> <span class="number">0.2</span> <span class="number">0.6</span> <span class="number">1.8</span> <span class="number">1.1</span> <span class="number">2.3</span> <span class="number">1.6</span> <span class="number">1.4</span> <span class="number">2.3</span> <span class="number">1.3</span> <span class="number">0.2</span> <span class="number">0.1</span> <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">0.3</span> <span class="number">0.2</span> <span class="number">1.5</span> <span class="number">2.4</span> <span class="number">0.3</span> <span class="number">2.1</span> <span class="number">2.5</span> <span class="number">0.2</span> <span class="number">1.4</span> <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">1.4</span> <span class="number">2.3</span> <span class="number">0.2</span> <span class="number">2.1</span> <span class="number">1.5</span> <span class="number">2.</span>  <span class="number">1.</span>  <span class="number">1.4</span> <span class="number">1.4</span> <span class="number">0.3</span> <span class="number">1.3</span> <span class="number">1.2</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">1.8</span> <span class="number">2.1</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">2.5</span> <span class="number">1.6</span> <span class="number">0.1</span> <span class="number">2.4</span> <span class="number">0.2</span> <span class="number">1.5</span> <span class="number">1.9</span> <span class="number">1.8</span> <span class="number">1.3</span> <span class="number">1.8</span> <span class="number">1.3</span> <span class="number">1.3</span> <span class="number">2.</span>  <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">1.7</span> <span class="number">0.2</span> <span class="number">1.2</span> <span class="number">2.1</span>]</span><br></pre></td></tr></table></figure>
<p>可以发现上面并没有0.8和1.75这两个取值。接着按上面的方法先排序，再取相邻两个值的平均作为离散化的特征，其结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0.1</span>, <span class="number">0.15000000000000002</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, </span><br><span class="line"><span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.25</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.35</span>, <span class="number">0.4</span>, <span class="number">0.4</span>,</span><br><span class="line"> <span class="number">0.45</span>, <span class="number">0.55</span>, <span class="number">0.8</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.05</span>, <span class="number">1.1</span>, <span class="number">1.15</span>, <span class="number">1.2</span>, <span class="number">1.2</span>, <span class="number">1.25</span>, <span class="number">1.3</span>,</span><br><span class="line"> <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.35</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.45</span>, <span class="number">1.5</span>, </span><br><span class="line"><span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.55</span>, <span class="number">1.6</span>, <span class="number">1.65</span>, <span class="number">1.7</span>, <span class="number">1.75</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, </span><br><span class="line"><span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.85</span>, <span class="number">1.9</span>, <span class="number">1.9</span>, <span class="number">1.95</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.05</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, </span><br><span class="line"><span class="number">2.1500000000000004</span>, <span class="number">2.25</span>, <span class="number">2.3</span>, <span class="number">2.3</span>, <span class="number">2.3</span>, <span class="number">2.3499999999999996</span>, <span class="number">2.4</span>, <span class="number">2.4</span>, <span class="number">2.45</span>, <span class="number">2.5</span>, <span class="number">2.5</span>]</span><br></pre></td></tr></table></figure>
<h2><span id="4-总结">4. 总结</span></h2><p>最后通过总结的方式对比下 ID3、C4.5 和 CART 三者之间的差异。</p>
<p>除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：</p>
<ul>
<li><strong>划分标准的差异：</strong>ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li>
<li><strong>使用场景的差异：</strong>ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li>
<li><strong>样本数据的差异：</strong>ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li>
<li><strong>样本特征的差异：</strong>ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征（连续型）；</li>
<li><strong>剪枝策略的差异：</strong>ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。</li>
</ul>
<h2><span id="一-决策树">一、决策树</span></h2><h3><span id="11-介绍决策树id2-c45-cart-3种决策树及其区别和适应场景">1.1 介绍决策树ID2、C4.5、CART, 3种决策树及其区别和适应场景？</span></h3><h3><span id="12-决策树处理连续值的方法">1.2 决策树处理连续值的方法?</span></h3><p><strong>ID3 只能离散型</strong>。<strong>C4.5 将连续特征离散化</strong>，假设 n 个样本的连续特征 A 有 m 个取值，<strong>C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点</strong>，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； </p>
<p><strong>CART分类树：离散化+基尼指数，</strong></p>
<p><strong>CART回归树：均方差之和度量方式</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C" alt="[公式]"></p>
<h3><span id="13-决策树处理缺失值的方式">1.3 决策树处理缺失值的方式？</span></h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/84519568">ID3、c4.5、cart、rf到底是如何处理缺失值的？</a></p>
</blockquote>
<h4><span id="131-在特征值缺失的情况下进行划分特征的选择">1.3.1 <strong>在特征值缺失的情况下进行划分特征的选择？</strong></span></h4><p><strong>ID3</strong> 没有缺失值处理；</p>
<p><strong>C4.5</strong>：对于具有缺失值特征，用没有缺失的<strong>样本子集所占比重来折算</strong>；</p>
<p><strong>CART</strong>：<strong>初期</strong>：分裂特征评估时只能使用在该特征上没有缺失值的那部分数据<strong>；后续</strong>：CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响。</p>
<h4><span id="132-选定该划分特征对于缺失该特征值的样本如何处理">1.3.2 <strong>选定该划分特征，对于缺失该特征值的样本如何处理？</strong></span></h4><p><strong>ID3</strong> 没有缺失值处理；</p>
<p><strong>C4.5</strong>：<strong>将样本同时划分到所有子节点</strong>，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</p>
<p>==<strong>CART</strong>：==sklearn中的cart的实现是没有对缺失值做任何处理的，也就是说sklearn的cart无法处理存在缺失值的特征。</p>
<h3><span id="14-决策树如何剪枝">1.4 决策树如何剪枝？</span></h3><ul>
<li><strong>预剪枝</strong>：在树的生成过程中，提前停止生长。简单，适合解决大规模问题。<ul>
<li>深度</li>
<li>节点样本数</li>
<li>对测试集准确率的提升过小</li>
</ul>
</li>
<li><strong>后剪枝</strong>：生成一颗完全生长的二叉树，从低向上剪枝，将子树删除用叶子节点代替。【类别：多数投票】常见的剪枝方法：错误率降低剪枝（REP）、<strong>悲观剪枝（PEP）、代价复杂度剪枝（CCP）</strong>、最小误差剪枝（MEP）等。</li>
</ul>
<p><strong>代价复杂度剪枝（CCP）【CART树】</strong></p>
<p>从完整子树 $T0$ 开始， 通过在 $Ti$ 子树序列中裁剪真实误差最小【考虑叶子节点的个数】的子树，得到 $Ti+1$。 </p>
<p><img src="image-20220321203204744.png" alt="image-20220321203204744" style="zoom: 25%;">【剪枝之后的误差 - 剪枝前的误差 / 叶子节点数 - 1】</p>
<p>每次误差增加率最小的节点，得到一系列的子树，从中选择效果最好的【独立剪枝数据集】和【K折交叉验证】</p>
<h3><span id="15-决策树特征选择特征重要性判断">1.5 决策树特征选择？特征重要性判断？</span></h3><p><strong>XGBoost</strong>：</p>
<ul>
<li><p>该特征在所有树中被用作分割样本的特征的总次数。</p>
</li>
<li><p>该特征在其出现过的所有树中产生的平均增益。</p>
</li>
<li><p>该特征在其出现过的所有树中的平均覆盖范围。</p>
<blockquote>
<p>  注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。</p>
</blockquote>
</li>
</ul>
<h3><span id="16-svm-lr-决策树的对比">1.6 SVM、LR、决策树的对比？</span></h3><blockquote>
<p>  逻辑回归，决策树，支持向量机 选择方案: <a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1435642">https://cloud.tencent.com/developer/article/1435642</a></p>
<p>  广义线性模型？</p>
<p>  sigmod、softmax</p>
<p>  为什么逻辑回归的连续值也需要离散化？</p>
<p>  为什么逻辑回归要用交叉熵？</p>
<p>  交叉熵和KL散度（相对熵）和GAN的损失函数的区别？</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>线性回归</th>
<th>LR 逻辑回归</th>
<th>SVM</th>
<th>朴素贝叶斯</th>
<th>决策树</th>
</tr>
</thead>
<tbody>
<tr>
<td>场景</td>
<td>【回归问题】</td>
<td>逻辑回归 = 线性回归 + Sigmoid 函数（非线形）【分类问题】==【参数模型】==【统计方法】</td>
<td>【分类问题】【几何方法】【非参数模型】</td>
<td>【生成式模型】</td>
<td>【分类问题】【回归问题】【非参数模型】</td>
</tr>
<tr>
<td><strong>思想</strong></td>
<td></td>
<td><strong>思路：</strong>先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。<strong>细节</strong>：通过<strong>非线性映射减小了离分类平面较远的点的权重</strong>，相对提升了与分类最相关的数据点的权重；</td>
<td><strong>思想</strong>：SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</td>
<td></td>
<td><strong>思想</strong>：用启发算法来度量特征选择，选择特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间。</td>
</tr>
<tr>
<td><strong>关键样本</strong></td>
<td></td>
<td><strong>所有样本</strong>（通过非线性映射，大大减小了离分类平面较远的点的权重）</td>
<td><strong>支持向量</strong>（超平面到距离最近的不同标记样本集合）</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>目标函数</strong></td>
<td></td>
<td>$y=\frac{1}{1+e^{-\left(w^{T} x+b\right)}}$ 【极大似然函数】</td>
<td><img src="image-20220322131856478.png" alt="image-20220322131856478" style="zoom:150%;"> <img src="https://pic2.zhimg.com/80/v2-0e87b3bf410cd798efd05a2837b83589_1440w.png" alt="img"></td>
<td></td>
<td>信息增益、信息增益率、Gini指数</td>
</tr>
<tr>
<td><strong>损失函数</strong></td>
<td></td>
<td><img src="https://pic3.zhimg.com/80/v2-ee1ddd22da5171fa44e079582cefe20a_1440w.png" alt="img" style="zoom:150%;"></td>
<td><strong><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/47746939">HingeLoss</a></strong>【合页损失函数】：<img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5EN%5B1-y_i%28w%C2%B7x_i+%2B+b%29%5D_%2B+%2B+%5Clambda%7C%7Cw%7C%7C%5E2+%5C%5C+%5Bz%5D_%2B+%3D+%5Cbegin%7Bequation%7D+%5Cleft%5C%7B++++++++++++++%5Cbegin%7Barray%7D%7Blr%7D+++++++++++z%2C+z%3E0+%26++%5C%5C++++++++++++++0.z%5Cleq0+%26+++++++++++++++%5Cend%7Barray%7D++%5Cright.+%5Cend%7Bequation%7D+%5C%5C+" alt="[公式]" style="zoom:150%;"></td>
<td></td>
<td>信息增益、信息增益率、Gini指数【方差和】</td>
</tr>
<tr>
<td>决策面</td>
<td></td>
<td>线性可分</td>
<td>【核函数映射】从而使得样本数据线性可分</td>
<td></td>
<td>矩形【非线性】</td>
</tr>
<tr>
<td>连续值处理</td>
<td></td>
<td>==离散化？==</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td></td>
<td>类别的概率</td>
<td>类别</td>
<td></td>
<td>类别、回归</td>
</tr>
<tr>
<td>过拟合</td>
<td></td>
<td><strong>正则化</strong> L1: <img src="image-20220316144906846.png" alt="image-20220316144906846" style="zoom:50%;"> L2: <img src="image-20220316145437180.png" alt="image-20220316145437180" style="zoom:50%;"></td>
<td>\</td>
<td></td>
<td>预剪枝和后剪枝</td>
</tr>
<tr>
<td>优势</td>
<td></td>
<td><strong>本质其实是为了模型参数服从某一分布</strong>；1、对观测样本的概率值输出 2、实现简单高效3、<strong>多重共线性的问题可以通过L2正则化来应对</strong>。 4、大量的工业界解决方案5、支持online learning</td>
<td>1、可以处理<strong>高维特征</strong> 2、使用<strong>核函数</strong>轻松应对非线的性特征空间 3、分类面不依赖于所有数据4、关重要的<strong>关键样本</strong></td>
<td></td>
<td>1、直观的决策过程 2、能够处理非线性特征 3、考虑了<strong>特征相关性</strong></td>
</tr>
<tr>
<td>劣势</td>
<td></td>
<td>1、特征空间太大时表现不太好 2、对于大量的分类变量无能为力 3、对于非线性特征需要做特征变换 4、依赖所有的样本数据</td>
<td>1、<strong>对于大量的观测样本，效率会很低</strong> 2、找到一个“合适”的核函数还是很tricky的</td>
<td></td>
<td>1、极易过拟合 2、无法输出score，只能给出直接的分类结果</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>  <strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/Alphonse_Huang/article/details/114278377">多重共线性问题</a></strong></p>
<p>  多重共线性问题就是指一个解释变量的变化引起另一个解释变量地变化。多重共<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=线性&amp;spm=1001.2101.3001.7020">线性</a>是使用线性回归算法时经常要面对的一个问题。在其他算法中，例如决策树或者朴素贝叶斯，前者的建模过程时逐渐递进，每次都只有一个变量参与，这种机制含有抗多重共线性干扰的功能；后者假设变量之间是相互独立的。但对于回归算法来说，都要同时考虑多个预测因子，因此多重共线性不可避免。</p>
<ul>
<li>PCA等降维方法。因为在原始特征空间中变量之间相关性大，很容易想到通过降低维度的形式来去除这种共线性。</li>
<li>正则化。使用<strong>岭回归（L2</strong>）或者lasso回归（L1）或者elasticnet回归（L1+L2）</li>
<li><p>逐步回归法</p>
<p><strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/FrankieHello/article/details/94022594">机器学习中参数模型和非参数模型理解</a></strong></p>
<p>参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p>
</li>
</ul>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>lzy
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://powerlzy.github.io/posts/3AGJJRV/" title="机器学习（9）决策树">https://powerlzy.github.io/posts/3AGJJRV/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
            </div>
            <div class="post-nav-item">
                <a href="/posts/3HRFFWP/" rel="next" title="机器学习（11）XGB*">
                  机器学习（11）XGB* <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
