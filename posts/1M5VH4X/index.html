<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="LSTM和GRU算法简单梳理🍭  前言 - 从反向传播推导到梯度消失and爆炸的原因及解决方案？   从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导） - 韦伟的文章 - 知乎 https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;76772734  本质上是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习（6）LSTM*">
<meta property="og:url" content="https://powerlzy.github.io/posts/1M5VH4X/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="LSTM和GRU算法简单梳理🍭  前言 - 从反向传播推导到梯度消失and爆炸的原因及解决方案？   从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导） - 韦伟的文章 - 知乎 https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;76772734  本质上是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=NET_%7Bo1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=OUT_%7Bo1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=NET_%7Bh1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=OUT_%7Bh1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma%28z%29+%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csigma%5E%7B%5Cprime%7D%28z%29+%26%3D%5Cleft%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D%5Cright%29%5E%7B%5Cprime%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7B-z%7D%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B1%2Be%5E%7B-z%7D-1%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B%5Csigma%28z%29%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Csigma%28z%29%281-%5Csigma%28z%29%29+%5Cend%7Baligned%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D%5Csum+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget+-+output%7D%29%5E%7B2%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget1+-+out_o1%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget2+-+out_o2%7D%29%5E%7B2%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_%7B5%7D%7D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B2%7D%28%7Btarget_1+-+out_%7Bo1%7D%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28+%7Btarget_2+-+out_%7Bo2%7D%7D%29%5E%7B2%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bo1%7D%7D%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+w_5%7D+%3Dout_%7Bh_1%7D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%28out_%7Bo1%7D+-+target_1%29+%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+out_%7Bh_1%7D++%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D%2B%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo2%7D%7D%7B%5Cpartial+net_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo2%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3Dw_5%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%3D+%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+i_1w_1%2Bi_2w_2%7D%7B%5Cpartial+w_1%7D+%3Di_1%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D++%26%3D%5Cleft%28%28out_%7Bo1%7D+-+target_1%29%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+w_5+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29+%5C%5C+%26%2B%7B%5Cleft%28%28out_%7Bo2%7D+-+target_2%29%5Ccdot+%28%5Csigma%28net_%7Bo2%7D%29%281-%5Csigma%28net_%7Bo2%7D%29%29%29%5Ccdot+w_7+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29%7D+%5C%5C+%5Cend%7Baligned%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=out_%7Bo1%7D+-+target_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_%7Bo1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+C_%7Bo1%7D+%5Ccdot+%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_5+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1+%2B+C_%7Bo2%7D+%5Ccdot+%5Csigma%28net_%7Bo2%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_7+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_%7Bo1%7D">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-0f2ded75fbecc449a25bfd58b8c58d35_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y_i%3D%5Csigma%5Cleft%28z_i%5Cright%29%3D%5Csigma%5Cleft%28w_ix_i%2Bb_i%5Cright%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-ea9beb6c28c7d4e89be89dc5f4cbae2e_1440w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+y_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+y_%7B4%7D%7D%7B%5Cpartial+z_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B4%7D%7D%7B%5Cpartial+x_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B4%7D%7D%7B%5Cpartial+z_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B3%7D%7D%7B%5Cpartial+x_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B3%7D%7D%7B%5Cpartial+z_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B2%7D%7D%7B%5Cpartial+x_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B2%7D%7D%7B%5Cpartial+z_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B1%7D%7D%7B%5Cpartial+b_%7B1%7D%7D%7D+%5C%5C+%7B%3DC_%7By4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B4%7D%5Cright%29+w_%7B4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B3%7D%5Cright%29+w_%7B3%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B2%7D%5Cright%29+w_%7B2%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B1%7D%5Cright%29%7D%5Cend%7Barray%7D+%5C%5C">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-208a4aa5dc657fe86919f3549d853793_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq0.25">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cgeq1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma%28z%29%5E%7B%5Cprime%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=z%3Dwx%2Bb">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Loss%3D%28y-W%5ETx%29%5E2%2B+%5Calpha+%7C%7CW%7C%7C%5E2%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctanh+%28x%29%3D%5Cfrac%7Be%5E%7Bx%7D-e%5E%7B-x%7D%7D%7Be%5E%7Bx%7D%2Be%5E%7B-x%7D%7D%5C%5C">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-66a7e4fcf11a2d85c15e7bf7b88b2d1b_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Coperatorname%7BRe%7D+%5Coperatorname%7Blu%7D%28%5Cmathrm%7Bx%7D%29%3D%5Cmax+%28%5Cmathrm%7Bx%7D%2C+0%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D%7B0%2C+x%3C0%7D+%5C%5C+%7Bx%2C+x%3E0%7D%5Cend%7Barray%7D%5Cright%5C%7D%5C%5C">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-55475ee2d90cd7257a39f62549a65769_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=leakrelu%3D%5Cbegin%7Bequation%7D+f%28x%29%3D+%5Cbegin%7Bcases%7D+x%2C+%26+%7Bx%5Cgt+0%7D+%5C%5C%5C%5C+x%2Ak%2C+%26+%7Bx%5Cleq+0%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-3ab1bd8fb85542a0c85eb907b73fa327_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7Bx%2C%7D+%26+%7B%5Ctext+%7B+if+%7D+x%3E0%7D+%5C%5C+%7B%5Calpha%5Cleft%28e%5E%7Bx%7D-1%5Cright%29%2C%7D+%26+%7B%5Ctext+%7B+otherwise+%7D%7D%5Cend%7Barray%7D%5Cright.%5C%5C">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-ec3c80e51129bd76d49cad6e52d449c2_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_3%3Df_2%28w%5ET%2Ax%2Bb%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cpartial+f_2%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+f_1%7Dw">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-3134d24348c47ca2001d37fef1c3f8bf_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot+%5Cfrac%7B%5Cpartial+x_%7BL%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot%5Cleft%281%2B%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D+F%5Cleft%28x_%7Bi%7D%2C+W_%7Bi%7D%5Cright%29%5Cright%29%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-ab844e07a86f910d2852198c3117ddb7_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_%7B0%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_%7B1%7D%3DW_%7Bx%7DX_%7B1%7D%2BW_%7Bs%7DS_%7B0%7D%2Bb_%7B1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=O_%7B1%7D%3DW_%7Bo%7DS_%7B1%7D%2Bb_%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_%7B2%7D%3DW_%7Bx%7DX_%7B2%7D%2BW_%7Bs%7DS_%7B1%7D%2Bb_%7B1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=O_%7B2%7D%3DW_%7Bo%7DS_%7B2%7D%2Bb_%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_%7B3%7D%3DW_%7Bx%7DX_%7B3%7D%2BW_%7Bs%7DS_%7B2%7D%2Bb_%7B1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=O_%7B3%7D%3DW_%7Bo%7DS_%7B3%7D%2Bb_%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7B3%7D%3D%5Cfrac%7B1%7D%7B2%7D%28Y_%7B3%7D-O_%7B3%7D%29%5E%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bt%3D0%7D%5E%7BT%7D%7BL_%7Bt%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7Bx%7D+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7Bs%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7Bo%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=b_%7B1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=b_%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D%E3%80%81W_%7B0%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7B0%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bo%7D%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7B0%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_%7Bt%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_%7Bt%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Csum_%7Bk%3D0%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%7B%5Cpartial%7BS_%7Bt%7D%7D%7D%7D%28%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D%29%5Cfrac%7B%5Cpartial%7BS_%7Bk%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_%7Bs%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_%7Bj%7D%3Dtanh%28W_%7Bx%7DX_%7Bj%7D%2BW_%7Bs%7DS_%7Bj-1%7D%2Bb_%7B1%7D%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7Btanh%5E%7B%27%7D%7DW_%7Bs%7D">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-a5590a65bf7a93bbaafc1a6b03cf3862_1440w.png">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-5602237fa98e90614cea748aa6a8b6d3_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-acee3e085ee62fe162bcac5cd135b54c_1440w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-ccf7b5baee04f3f24bd04637df9bcd3a_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-1428c54d3ae79cf12616e7051c07799d_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=a">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=O_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z%EF%BC%8CZ_i%EF%BC%8CZ_f%EF%BC%8CZ_o">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=X_t+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z%EF%BC%8CZ_i%EF%BC%8CZ_f%EF%BC%8CZ_o">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=X_t">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-b74b5413f6897f2890b3ede634f60efb_1440w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z%3Dtanh%28W%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_i%3D%5Csigma%28W_i%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_f%3D%5Csigma%28W_f%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_o%3D%5Csigma%28W_o%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=X_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z_i+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=X_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z_f%EF%BC%8CZ_o">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Z_i%EF%BC%8CZ_f%EF%BC%8CZ_o">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-10f9ec56794c9f89ca2b6ce86c7693ee_1440w.jpg?source=d16d100b">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-867b66c85751aa3bfeeb4956054e0eb8_1440w.jpg?source=d16d100b">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_%7Bt%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_%7Bt%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_%7Bt%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csigma">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=sigmoid">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctanh">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+%5Ctanh">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++f_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7Bf%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bf%7D%5Cright%29++%5Cend%7Bequation%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+i_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bi%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bi%7D%5Cright%29+%5C%5C+%5Ctilde%7BC%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W_%7BC%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7BC%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_%7Bt%7D%3DC_%7Bt-1%7D%5Codot+f_t%2Bi_t%5Codot+%5Ctilde%7BC_t%7D%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D++o_t%26%3D%5Csigma%7B%28W_o%5Ccdot%5Bh_%7Bt-1%7D%2Cx_t%5D%2Bb_o%29%7D%5C%5C+h_t%26%3Do_t%5Codot+%5Ctanh%7B%28C_t%29%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-e616cbe445d60aee532ef3d9db0fb8f6_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=LSTM">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=sigmoid">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+sigmoid">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5B0%2C+1%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++f_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7Bf%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bf%7D%5Cright%29++%5Cend%7Bequation%7D%5C%5C">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-1dbddd39ff6a039631b8ff1e4bc294a5_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=sigmoid">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctanh">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+%5Ctilde%7BC_t%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+i_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bi%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bi%7D%5Cright%29+%5C%5C+%5Ctilde%7BC%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W_%7BC%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7BC%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-82f48ff07dd75ee4346ab44c573fe0b7_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+C_%7Bt-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_%7Bt-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+i_t%5Codot+%5Ctilde%7BC_t%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_%7Bt%7D%3DC_%7Bt-1%7D%5Codot+f_t%2Bi_t%5Codot+%5Ctilde%7BC_t%7D%5C%5C">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-6335c8e72a056eeb80970c88988f6bd2_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=o_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=sigmoid">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctanh">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D++o_t%26%3D%5Csigma%7B%28W_o%5Ccdot%5Bh_%7Bt-1%7D%2Cx_t%5D%2Bb_o%29%7D%5C%5C+h_t%26%3Do_t%5Codot+%5Ctanh%7B%28C_t%29%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-7547e5c91e1590bf67e0641e9e29ec66_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Gated%5C+Recurrent%5C+Unit%2C%5C+GRU">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=LSTM">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+z_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bz%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+r_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Br%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+%5Ctilde%7Bh%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W+%5Ccdot%5Cleft%5Br_%7Bt%7D+%5Codot+h_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+h_%7Bt%7D+%26%3D%5Cleft%281-z_%7Bt%7D%5Cright%29+%5Codot+h_%7Bt-1%7D%2Bz_%7Bt%7D+%5Codot+%5Ctilde%7Bh%7D_%7Bt%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=GRU">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=r_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=z_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=LSTM">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=LSTM">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctilde%7BC%7D_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=r_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=r_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+z_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=GRU">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-10f9ec56794c9f89ca2b6ce86c7693ee_1440w.jpg?source=d16d100b">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+c_t+%3D+f_t%5Codot+c_%7Bt-1%7D+%2B+i_t+%5Codot+%5Chat%7Bc_t%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+h_%7Bt-1%7D+%5Crightarrow+i_t+%5Crightarrow+c_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_t%3D1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+c_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=dl%2Fdc_t">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=dl%2Fdc_%7Bt-1%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f%5Cleft%5BW+%5Cldots%5Cleft%5BW+f%5Cleft%5BW+f%5Cleft%5BW+f_%7Bi%7D%5Cright%5D%5Cright%5D%5Cright%5D%5Cright%5D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+W_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+f_%7B2%7D%2B%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D+f_%7B1%7D%2B%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B1%7D%7D%7B%5Cpartial+a_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+a_%7B1%7D%7D%7B%5Cpartial+W_%7B1%7D%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B1%7D%7D%3D1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W">
<meta property="article:published_time" content="2022-03-16T13:45:12.682Z">
<meta property="article:modified_time" content="2022-07-13T14:55:53.403Z">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg">


<link rel="canonical" href="https://powerlzy.github.io/posts/1M5VH4X/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://powerlzy.github.io/posts/1M5VH4X/","path":"posts/1M5VH4X/","title":"深度学习（6）LSTM*"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习（6）LSTM* | PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PowerLZY's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">LSTM和GRU算法简单梳理🍭</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">
前言 - 从反向传播推导到梯度消失and爆炸的原因及解决方案？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">一、反向传播推导到梯度消失and爆炸的原因及解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.1 &#x3D;&#x3D;反向传播推导：&#x3D;&#x3D;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.2
&#x3D;&#x3D;梯度消失，爆炸产生原因：&#x3D;&#x3D;</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.3 梯度消失、爆炸解决方案？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">解决方案一（预训练加微调）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">解决方案二（梯度剪切、正则）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">
解决方案三（改变激活函数）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">解决方案四（batchnorm）：【梯度消失】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">
解决方案五（残差结构）：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">二、LSTM 框架结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">前言：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;LSTM迭代过程&#x3D;&#x3D;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.1 LSTM之遗忘门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.2 LSTM之输入门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.3 LSTM之细胞状态更新</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.4 LSTM之输出门</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">三、GRU 框架结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">参考资料⬇️</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">LSTM Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1、为什么LSTM可以解决梯度消失和梯度爆炸？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2、为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">为什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">能更换吗？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3、能不能把tanh换成relu？</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">264</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1M5VH4X/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习（6）LSTM* | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习（6）LSTM*
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:45:12" itemprop="dateCreated datePublished" datetime="2022-03-16T21:45:12+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 22:55:53" itemprop="dateModified" datetime="2022-07-13T22:55:53+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN/" itemprop="url" rel="index"><span itemprop="name">RNN</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2><span id="lstm和gru算法简单梳理">LSTM和GRU算法简单梳理🍭</span></h2>
<h3><span id="前言-从反向传播推导到梯度消失and爆炸的原因及解决方案"><font color="red">
前言 - 从反向传播推导到梯度消失and爆炸的原因及解决方案？</font></span></h3>
<blockquote>
<ul>
<li>从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导）
- 韦伟的文章 - 知乎 https://zhuanlan.zhihu.com/p/76772734</li>
</ul>
<p><strong>本质上</strong>是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。其实<strong>梯度爆炸和梯度消失问题都是因为网络太深</strong>，网络权值更新不稳定造成的，本质上是因为<strong>梯度反向传播中的连乘效应</strong>。</p>
</blockquote>
<h3><span id="一-反向传播推导到梯度消失and爆炸的原因及解决方案">一、反向传播推导到梯度消失and爆炸的原因及解决方案</span></h3>
<h4><span id="11-反向传播推导">1.1 ==反向传播推导：==</span></h4>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以上图为例开始推起来，先说明几点，i1，i2是输入节点，h1，h2为隐藏层节点，o1，o2为输出层节点，除了输入层，其他两层的节点结构为下图所示：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>举例说明，<img src="https://www.zhihu.com/equation?tex=NET_%7Bo1%7D" alt="[公式]"> 为输出层的输入，也就是隐藏层的输出经过线性变换后的值，
<img src="https://www.zhihu.com/equation?tex=OUT_%7Bo1%7D" alt="[公式]"> 为经过激活函数sigmoid后的值；同理 <img src="https://www.zhihu.com/equation?tex=NET_%7Bh1%7D" alt="[公式]">
为隐藏层的输入，也就是输入层经过线性变换后的值， <img src="https://www.zhihu.com/equation?tex=OUT_%7Bh1%7D" alt="[公式]">
为经过激活函数sigmoid 的值。只有这两层有激活函数，输入层没有。</p>
<blockquote>
<p><strong>定义一下sigmoid的函数：</strong> <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29+%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D" alt="[公式]"> <strong>说一下sigmoid的求导：</strong></p>
</blockquote>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csigma%5E%7B%5Cprime%7D%28z%29+%26%3D%5Cleft%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D%5Cright%29%5E%7B%5Cprime%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7B-z%7D%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B1%2Be%5E%7B-z%7D-1%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B%5Csigma%28z%29%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Csigma%28z%29%281-%5Csigma%28z%29%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>定义一下损失函数，这里的损失函数是均方误差函数，即：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D%5Csum+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget+-+output%7D%29%5E%7B2%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>具体到上图，就是：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget1+-+out_o1%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget2+-+out_o2%7D%29%5E%7B2%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>到这里，所有前提就交代清楚了，前向传播就不推了，默认大家都会，下面推反向传播。</p>
<ul>
<li><strong>第一个反向传播（热身）</strong></li>
</ul>
<p>先来一个简单的热热身，求一下损失函数对W5的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D" alt="[公式]"></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>首先根据链式求导法则写出对W5求偏导的总公式，再把图拿下来对照（如上），可以看出，需要计算三部分的求导【损失函数、激活函数、线性函数】，下面就一步一步来：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_%7B5%7D%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B2%7D%28%7Btarget_1+-+out_%7Bo1%7D%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28+%7Btarget_2+-+out_%7Bo2%7D%7D%29%5E%7B2%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bo1%7D%7D%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+w_5%7D+%3Dout_%7Bh_1%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>综上三个步骤，得到总公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%28out_%7Bo1%7D+-+target_1%29+%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+out_%7Bh_1%7D++%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<ul>
<li><strong>第二个反向传播：</strong></li>
</ul>
<p>接下来，要求损失函数对w1的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D" alt="[公式]"></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>还是把图摆在这，方便看，先写出总公式，对w1求导有个地方要注意，w1的影响不仅来自o1还来自o2，从图上可以一目了然，所以总公式为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D%2B%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo2%7D%7D%7B%5Cpartial+net_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo2%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>所以总共分为左右两个式子，分别又对应5个步骤，详细写一下左边，右边同理：</p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]"></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3Dw_5%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%3D+%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+i_1w_1%2Bi_2w_2%7D%7B%5Cpartial+w_1%7D+%3Di_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>右边也是同理，就不详细写了，写一下总的公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D++%26%3D%5Cleft%28%28out_%7Bo1%7D+-+target_1%29%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+w_5+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29+%5C%5C+%26%2B%7B%5Cleft%28%28out_%7Bo2%7D+-+target_2%29%5Ccdot+%28%5Csigma%28net_%7Bo2%7D%29%281-%5Csigma%28net_%7Bo2%7D%29%29%29%5Ccdot+w_7+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29%7D+%5C%5C+%5Cend%7Baligned%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>这个公式只是对如此简单的一个网络结构的一个节点的偏导，就这么复杂。。亲自推完才深深的意识到。。。</p>
<p>为了后面描述方便，把上面的公式化简一下， <img src="https://www.zhihu.com/equation?tex=out_%7Bo1%7D+-+target_1" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]"> ，
<img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D" alt="[公式]"> ，则：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+C_%7Bo1%7D+%5Ccdot+%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_5+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1+%2B+C_%7Bo2%7D+%5Ccdot+%5Csigma%28net_%7Bo2%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_7+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="12梯度消失爆炸产生原因">1.2
<strong>==梯度消失，爆炸产生原因：==</strong></span></h4>
<p>从上式其实已经能看出来，求和操作其实不影响，主要是是看乘法操作就可以说明问题，可以看出，损失函数对w1的偏导，与
<img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]">
，权重w，sigmoid的导数有关，明明还有输入i为什么不提？因为如果是多层神经网络的中间某层的某个节点，那么就没有输入什么事了。所以产生影响的就是刚刚提的三个因素。</p>
<p>再详细点描述，如图，多层神经网络：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-0f2ded75fbecc449a25bfd58b8c58d35_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25631496">PENG：神经网络训练中的梯度消失与梯度爆炸282
赞同 · 26 评论文章</a></p>
<p>假设（假设每一层只有一个神经元且对于每一层 <img src="https://www.zhihu.com/equation?tex=y_i%3D%5Csigma%5Cleft%28z_i%5Cright%29%3D%5Csigma%5Cleft%28w_ix_i%2Bb_i%5Cright%29" alt="[公式]">，其中<img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]">为sigmoid函数），如图：</p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-ea9beb6c28c7d4e89be89dc5f4cbae2e_1440w.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>则：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+y_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+y_%7B4%7D%7D%7B%5Cpartial+z_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B4%7D%7D%7B%5Cpartial+x_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B4%7D%7D%7B%5Cpartial+z_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B3%7D%7D%7B%5Cpartial+x_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B3%7D%7D%7B%5Cpartial+z_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B2%7D%7D%7B%5Cpartial+x_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B2%7D%7D%7B%5Cpartial+z_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B1%7D%7D%7B%5Cpartial+b_%7B1%7D%7D%7D+%5C%5C+%7B%3DC_%7By4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B4%7D%5Cright%29+w_%7B4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B3%7D%5Cright%29+w_%7B3%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B2%7D%5Cright%29+w_%7B2%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B1%7D%5Cright%29%7D%5Cend%7Barray%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>看一下sigmoid函数的求导之后的样子：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-208a4aa5dc657fe86919f3549d853793_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>发现sigmoid函数求导后最大最大也只能是0.25。</strong></p>
<p>再来看W，一般我们初始化权重参数W时，通常都小于1，用的最多的应该是0，1正态分布吧。</p>
<p><font color="red"><strong>所以 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq0.25" alt="[公式]">
，多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说几乎不更新，这就是梯度消失的根本原因。</strong></font></p>
<p>再来看看<strong>梯度爆炸</strong>的原因，也就是说如果 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cgeq1" alt="[公式]">
时，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。sigmoid的函数是不可能大于1了，上图看的很清楚，那只能是w了，这也就是经常看到别人博客里的一句话，初始权重过大，一直不理解为啥。。现在明白了。</p>
<p>但梯度爆炸的情况一般不会发生，对于sigmoid函数来说， <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29%5E%7B%5Cprime%7D" alt="[公式]"> 的大小也与w有关，因为 <img src="https://www.zhihu.com/equation?tex=z%3Dwx%2Bb" alt="[公式]">
，除非该层的输入值<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">在一直一个比较小的范围内。</p>
<p>其实<strong>梯度爆炸和梯度消失问题都是因为网络太深</strong>，网络权值更新不稳定造成的，本质上是因为<strong>梯度反向传播中的连乘效应</strong>。</p>
<p>==<strong>所以，总结一下，为什么会发生梯度爆炸和消失：</strong>==</p>
<blockquote>
<p>本质上是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。</p>
</blockquote>
<h3><span id="13-梯度消失-爆炸解决方案">1.3 梯度消失、爆炸解决方案？</span></h3>
<blockquote>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33006526">DoubleV：详解深度学习中的梯度消失、爆炸原因及其解决方法</a></p>
<ul>
<li>预训练加微调</li>
<li>梯度剪切、正则</li>
</ul>
</blockquote>
<h4><span id="解决方案一预训练加微调"><strong>解决方案一（预训练加微调）：</strong></span></h4>
<p>提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（<strong>fine-tunning</strong>）。</p>
<p>Hinton在训练深度信念网络（Deep Belief
Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p>
<h4><span id="解决方案二梯度剪切-正则"><strong>解决方案二（梯度剪切、正则）：</strong></span></h4>
<p><strong>梯度剪切</strong>这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p>
<p><strong>正则化</strong>是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss%3D%28y-W%5ETx%29%5E2%2B+%5Calpha+%7C%7CW%7C%7C%5E2%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]">
是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</p>
<p>注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些</p>
<h4><span id="解决方案三改变激活函数"><strong><font color="red">
解决方案三（改变激活函数）：</font></strong></span></h4>
<p>首先说明一点，<strong>tanh激活函数不能有效的改善这个问题</strong>，先来看tanh的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Ctanh+%28x%29%3D%5Cfrac%7Be%5E%7Bx%7D-e%5E%7B-x%7D%7D%7Be%5E%7Bx%7D%2Be%5E%7B-x%7D%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>再来看tanh的导数图像：</p>
<p><img src="https://pic4.zhimg.com/80/v2-66a7e4fcf11a2d85c15e7bf7b88b2d1b_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>发现虽然比sigmoid的好一点，sigmoid的最大值小于0.25，tanh的最大值小于1，但仍是小于1的，所以并不能解决这个问题。</strong></p>
<p><strong>Relu</strong>:思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Coperatorname%7BRe%7D+%5Coperatorname%7Blu%7D%28%5Cmathrm%7Bx%7D%29%3D%5Cmax+%28%5Cmathrm%7Bx%7D%2C+0%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D%7B0%2C+x%3C0%7D+%5C%5C+%7Bx%2C+x%3E0%7D%5Cend%7Barray%7D%5Cright%5C%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-55475ee2d90cd7257a39f62549a65769_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>从上图中，我们可以很容易看出，<strong>relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</strong></p>
<p><strong>relu</strong>的主要贡献在于：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练</li>
</ul>
<p>同时也存在一些<strong>缺点</strong>：</p>
<ul>
<li><strong>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</strong></li>
<li>输出不是以0为中心的</li>
</ul>
<p><strong>leakrelu</strong></p>
<p>leakrelu就是为了解决relu的0区间带来的影响，其数学表达为： <img src="https://www.zhihu.com/equation?tex=leakrelu%3D%5Cbegin%7Bequation%7D+f%28x%29%3D+%5Cbegin%7Bcases%7D+x%2C+%26+%7Bx%5Cgt+0%7D+%5C%5C%5C%5C+x%2Ak%2C+%26+%7Bx%5Cleq+0%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]">
其中k是leak系数，一般选择0.1或者0.2，或者通过学习而来解决死神经元的问题。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3ab1bd8fb85542a0c85eb907b73fa327_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>leakrelu解决了0区间带来的影响，而且包含了relu的所有优点</p>
<p><strong>elu</strong></p>
<p>elu激活函数也是为了解决relu的0区间带来的影响，其数学表达为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7Bx%2C%7D+%26+%7B%5Ctext+%7B+if+%7D+x%3E0%7D+%5C%5C+%7B%5Calpha%5Cleft%28e%5E%7Bx%7D-1%5Cright%29%2C%7D+%26+%7B%5Ctext+%7B+otherwise+%7D%7D%5Cend%7Barray%7D%5Cright.%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其函数及其导数数学形式为：</p>
<p><img src="https://pic3.zhimg.com/80/v2-ec3c80e51129bd76d49cad6e52d449c2_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>但是elu相对于leakrelu来说，计算要更耗时间一些，因为有e。</p>
<h4><span id="解决方案四batchnorm梯度消失"><strong>解决方案四（batchnorm）：</strong>【梯度消失】</span></h4>
<p><strong>Batchnorm</strong>是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch
normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。</p>
<p>具体的batchnorm原理非常复杂，在这里不做详细展开，此部分大概讲一下batchnorm解决梯度的问题上。具体来说就是反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：
正向传播中<img src="https://www.zhihu.com/equation?tex=f_3%3Df_2%28w%5ET%2Ax%2Bb%29" alt="[公式]">，那么反向传播中，<img src="https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cpartial+f_2%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+f_1%7Dw" alt="[公式]">，反向传播式子中有w的存在，所以<img src="https://www.zhihu.com/equation?tex=w" alt="[公式]">的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出做scale和shift的方法，通过一定的规范化手段，<strong>把每层神经网络任意神经元这个输入值的分布【假设原始是正态分布】强行拉回到接近均值为0方差为1的标准正太分布，即严重偏离的分布强制拉回比较标准的分布，<font color="red">
这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生</font>，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<h4><span id="解决方案五残差结构"><strong><font color="red">
解决方案五（残差结构）：</font></strong></span></h4>
<p><img src="https://pic4.zhimg.com/80/v2-3134d24348c47ca2001d37fef1c3f8bf_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>如图，把输入加入到某层中，这样求导时，总会有个1在，这样就不会梯度消失了。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot+%5Cfrac%7B%5Cpartial+x_%7BL%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot%5Cleft%281%2B%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D+F%5Cleft%28x_%7Bi%7D%2C+W_%7Bi%7D%5Cright%29%5Cright%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>式子的第一个因子 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D" alt="[公式]"> 表示的损失函数到达 L
的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p>
<p><code>注：上面的推导并不是严格的证</code>，只为帮助理解</p>
<p>==<strong>解决方案六（LSTM）：</strong>==</p>
<p>在介绍这个方案之前，有必要来推导一下RNN的反向传播，<strong>因为关于梯度消失的含义它跟DNN不一样！不一样！不一样！</strong></p>
<p>先推导再来说，从这copy的：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28687529">沉默中的思索：RNN梯度消失和爆炸的原因565
赞同</a></p>
<p>RNN结构如图：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-ab844e07a86f910d2852198c3117ddb7_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>假设我们的时间序列只有三段， <img src="https://www.zhihu.com/equation?tex=S_%7B0%7D" alt="[公式]">
为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下： <img src="https://www.zhihu.com/equation?tex=S_%7B1%7D%3DW_%7Bx%7DX_%7B1%7D%2BW_%7Bs%7DS_%7B0%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B1%7D%3DW_%7Bo%7DS_%7B1%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B2%7D%3DW_%7Bx%7DX_%7B2%7D%2BW_%7Bs%7DS_%7B1%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B2%7D%3DW_%7Bo%7DS_%7B2%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B3%7D%3DW_%7Bx%7DX_%7B3%7D%2BW_%7Bs%7DS_%7B2%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B3%7D%3DW_%7Bo%7DS_%7B3%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p>假设在t=3时刻，损失函数为 <img src="https://www.zhihu.com/equation?tex=L_%7B3%7D%3D%5Cfrac%7B1%7D%7B2%7D%28Y_%7B3%7D-O_%7B3%7D%29%5E%7B2%7D" alt="[公式]"> 。</p>
<p>则对于一次训练任务的损失函数为 <img src="https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bt%3D0%7D%5E%7BT%7D%7BL_%7Bt%7D%7D" alt="[公式]"> ，即每一时刻损失值的累加。</p>
<p>使用随机梯度下降法训练RNN其实就是对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D+" alt="[公式]"> 、
<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]">
、 <img src="https://www.zhihu.com/equation?tex=W_%7Bo%7D" alt="[公式]"> 以及 <img src="https://www.zhihu.com/equation?tex=b_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=b_%7B2%7D" alt="[公式]">
求偏导，并不断调整它们以使L尽可能达到最小的过程。</p>
<p>现在假设我们我们的时间序列只有三段，t1，t2，t3。</p>
<p><strong>我们只对t3时刻的 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D%E3%80%81W_%7B0%7D" alt="[公式]"> 求偏导（其他时刻类似）：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7B0%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bo%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>可以看出对于 <img src="https://www.zhihu.com/equation?tex=W_%7B0%7D" alt="[公式]">
求偏导并没有长期依赖，但是对于 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导，会随着时间序列产生长期依赖</strong>。因为 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]">
随着时间序列向前传播，而 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]"> 又是
<img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]">的函数。</p>
<p>根据上述求偏导的过程，我们可以得出任意时刻对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导的公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Csum_%7Bk%3D0%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%7B%5Cpartial%7BS_%7Bt%7D%7D%7D%7D%28%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D%29%5Cfrac%7B%5Cpartial%7BS_%7Bk%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>任意时刻对<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]"> 求偏导的公式同上。</p>
<p><font color="red"> 如果加上激活函数， <img src="https://www.zhihu.com/equation?tex=S_%7Bj%7D%3Dtanh%28W_%7Bx%7DX_%7Bj%7D%2BW_%7Bs%7DS_%7Bj-1%7D%2Bb_%7B1%7D%29" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D" alt="[公式]"> = <img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7Btanh%5E%7B%27%7D%7DW_%7Bs%7D" alt="[公式]">激活函数tanh和它的导数图像在上面已经说过了，所以原因在这就不赘述了，还是一样的，激活函数导数小于1。</font></p>
<blockquote>
<p>==<strong>现在来解释一下，为什么说RNN和DNN的梯度消失问题含义不一样？</strong>==</p>
<ol type="1">
<li><strong>先来说DNN中的反向传播：</strong>在上文的DNN反向传播中，我推导了两个权重的梯度，第一个梯度是直接连接着输出层的梯度，求解起来并没有梯度消失或爆炸的问题，因为它没有连乘，只需要计算一步。第二个梯度出现了连乘，也就是说越靠近输入层的权重，梯度消失或爆炸的问题越严重，可能就会消失会爆炸。<strong>一句话总结一下，DNN中各个权重的梯度是独立的，该消失的就会消失，不会消失的就不会消失。</strong></li>
<li><strong>再来说RNN：</strong>RNN的特殊性在于，它的权重是共享的。抛开W_o不谈，因为它在某时刻的梯度不会出现问题（某时刻并不依赖于前面的时刻），但是W_s和W_x就不一样了，每一时刻都由前面所有时刻共同决定，是一个相加的过程，这样的话就有个问题，当距离长了，计算最前面的导数时，最前面的导数就会消失或爆炸，但当前时刻整体的梯度并不会消失，因为它是求和的过程，当下的梯度总会在，只是前面的梯度没了，但是更新时，由于权值共享，所以整体的梯度还是会更新，<strong>通常人们所说的梯度消失就是指的这个，指的是当下梯度更新时，用不到前面的信息了，因为距离长了，前面的梯度就会消失，也就是没有前面的信息了，但要知道，整体的梯度并不会消失，因为当下的梯度还在，并没有消失。</strong></li>
<li><strong>一句话概括：</strong>RNN的梯度不会消失，RNN的梯度消失指的是当下梯度用不到前面的梯度了，但DNN靠近输入的权重的梯度是真的会消失。</li>
</ol>
</blockquote>
<p>说完了RNN的反向传播及梯度消失的含义，终于该说<strong>为什么LSTM可以解决这个问题了</strong>，这里默认大家都懂LSTM的结构，对结构不做过多的描述。<strong>见第三节</strong>。【LSTM通过它的“门控装置”有效的缓解了这个问题，这也就是为什么我们现在都在使用LSTM而非普通RNN。】</p>
<h3><span id="二-lstm-框架结构">二、LSTM 框架结构</span></h3>
<h4><span id="前言">前言：</span></h4>
<blockquote>
<p>LSTM是RNN的一种变体，更高级的RNN，那么它的本质还是一样的，还记得RNN的特点吗，<strong>可以有效的处理序列数据，</strong>当然LSTM也可以，还记得RNN是如何处理有效数据的吗，是不是<strong>每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息</strong>，如图，我们把存每一时刻信息的地方叫做Memory
Cell，中文就是记忆细胞，可以这么理解。</p>
<p><img src="https://pic3.zhimg.com/80/v2-a5590a65bf7a93bbaafc1a6b03cf3862_1440w.png" alt="img" style="zoom:50%;"></p>
<p><strong>RNN什么信息它都存下来，因为它没有挑选的能力，而LSTM不一样，它会选择性的存储信息，因为它能力强，它有门控装置，它可以尽情的选择。</strong>如下图，普通RNN只有中间的Memory
Cell用来存所有的信息，而从下图我们可以看到，<strong>LSTM多了三个Gate</strong>。</p>
<p><img src="https://pic4.zhimg.com/80/v2-5602237fa98e90614cea748aa6a8b6d3_1440w.jpg" alt="img" style="zoom:50%;"></p>
<ul>
<li><strong>Input
Gate</strong>：输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory
Cell。</li>
<li><strong>Output Gate</strong>：输出门，每一时刻是否有信息从Memory
Cell输出取决于这一道门。</li>
<li><strong>Forget Gate</strong>：遗忘门，每一时刻Memory
Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory
Cell里的值清除，也就是遗忘掉。</li>
</ul>
<p>在了解LSTM的内部结构之前，我们需要先回顾一下普通RNN的结构，以免在这里很多读者被搞懵，如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-acee3e085ee62fe162bcac5cd135b54c_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>我们可以看到，左边是为了简便描述RNN的工作原理而画的缩略图，右边是展开之后，每个时间点之间的流程图，<strong>注意，我们接下来看到的LSTM的结构图，是一个时间点上的内部结构，就是整个工作流程中的其中一个时间点，也就是如下图：</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-ccf7b5baee04f3f24bd04637df9bcd3a_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>注意，<strong>上图是普通RNN的一个时间点的内部结构</strong>，上面已经讲过了公式和原理，<strong>LSTM的内部结构更为复杂，不过如果这么类比来学习，我认为也没有那么难</strong>。</p>
<p><img src="https://pic2.zhimg.com/80/v2-1428c54d3ae79cf12616e7051c07799d_1440w.jpg" alt="img" style="zoom:50%;"></p>
<ul>
<li>Cell：memory cell，也就是一个记忆存储的地方，这里就类似于普通RNN的
<img src="https://www.zhihu.com/equation?tex=S_t" alt="[公式]">
，都是用来存储信息的，这里面的信息都会保存到下一时刻，其实标准的叫法应该是
<img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]">
，因为这里对应神经网络里的隐藏层，所以是hidden的缩写，无论普通RNN还是LSTM其实t时刻的记忆细胞里存的信息，都应该被称为
<img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"></li>
<li><img src="https://www.zhihu.com/equation?tex=a" alt="[公式]">
是这一时刻的输出，也就是类似于普通RNN里的 <img src="https://www.zhihu.com/equation?tex=O_t" alt="[公式]"></li>
<li>四个 <img src="https://www.zhihu.com/equation?tex=Z%EF%BC%8CZ_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]"> ，这四个相辅相成，才造就了中间的Memory
Cell里的值，你肯恩要问普通RNN里有个 <img src="https://www.zhihu.com/equation?tex=X_t+" alt="[公式]">
作为输入，那LSTM的输入在哪？别着急，其实这四个 <img src="https://www.zhihu.com/equation?tex=Z%EF%BC%8CZ_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]"> 都有输入向量 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]">
的参与。对了，在解释这四个分别是什么之前，我要先解释一下上图的所有这个符号：<img src="https://pic4.zhimg.com/80/v2-b74b5413f6897f2890b3ede634f60efb_1440w.png" alt="img" style="zoom:50%;">都代表一个激活函数，<strong>LSTM里常用的激活函数有两个，一个是tanh，一个是sigmoid</strong>。</li>
<li><figure>
<img src="https://www.zhihu.com/equation?tex=Z%3Dtanh%28W%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_i%3D%5Csigma%28W_i%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_f%3D%5Csigma%28W_f%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_o%3D%5Csigma%28W_o%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure></li>
<li><img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
<strong>是最为普通的输入</strong>，可以从上图中看到， <img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
是通过该时刻的输入 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]"> 和上一时刻存在memory cell里的隐藏层信息 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
向量拼接，再与权重参数向量 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
点积，得到的值经过激活函数tanh最终会得到一个数值。</li>
<li><img src="https://www.zhihu.com/equation?tex=Z_i" alt="[公式]">
<strong>input gate的缩写i，所以也就是输入门的门控装置</strong>， <img src="https://www.zhihu.com/equation?tex=Z_i+" alt="[公式]">
同样也是通过该时刻的输入 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]">
和上一时刻隐藏状态，也就是上一时刻存下来的信息 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
向量拼接，在与权重参数向量 <img src="https://www.zhihu.com/equation?tex=W_i" alt="[公式]">
点积（注意每个门的权重向量都不一样，这里的下标i代表input的意思，也就是输入门)。得到的值经过激活函数sigmoid的最终会得到一个0-1之间的一个数值，用来作为<strong>输入门的控制信号</strong>。</li>
<li>以此类推，就不详细讲解 <img src="https://www.zhihu.com/equation?tex=Z_f%EF%BC%8CZ_o" alt="[公式]">
了，分别是缩写forget和output的门控装置，原理与上述输入门的门控装置类似。上面说了，只有
<img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
是输入，其他的三个都是门控装置，负责把控每一阶段的信息记录与遗忘，具体是怎样的呢？我们先来看公式：<strong>首先解释一下，经过这个sigmod激活函数后，得到的
<img src="https://www.zhihu.com/equation?tex=Z_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]">
都是在0到1之间的数值，1表示该门完全打开，0表示该门完全关闭</strong>，</li>
</ul>
</blockquote>
<h4><span id="lstm迭代过程">==LSTM迭代过程==</span></h4>
<blockquote>
<p>LSTM和GRU算法简单梳理🍭: https://zhuanlan.zhihu.com/p/72500407</p>
</blockquote>
<p><img src="https://pic3.zhimg.com/80/v2-10f9ec56794c9f89ca2b6ce86c7693ee_1440w.jpg?source=d16d100b" alt="img" style="zoom: 33%;"></p>
<p><img src="https://pic2.zhimg.com/80/v2-867b66c85751aa3bfeeb4956054e0eb8_1440w.jpg?source=d16d100b" alt="img" style="zoom:67%;"></p>
<p><img src="https://www.zhihu.com/equation?tex=h_%7Bt%7D" alt="[公式]"> ：当前序列的隐藏状态、 <img src="https://www.zhihu.com/equation?tex=x_%7Bt%7D" alt="[公式]">
：当前序列的输入数据、 <img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D" alt="[公式]">
：当前序列的细胞状态、 <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]"> ：
<img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
激活函数、 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]"> ： <img src="https://www.zhihu.com/equation?tex=+%5Ctanh" alt="[公式]">
激活函数。</p>
<p><strong>遗忘门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++f_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7Bf%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bf%7D%5Cright%29++%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>输入门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+i_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bi%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bi%7D%5Cright%29+%5C%5C+%5Ctilde%7BC%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W_%7BC%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7BC%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>细胞状态更新：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D%3DC_%7Bt-1%7D%5Codot+f_t%2Bi_t%5Codot+%5Ctilde%7BC_t%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>输出门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D++o_t%26%3D%5Csigma%7B%28W_o%5Ccdot%5Bh_%7Bt-1%7D%2Cx_t%5D%2Bb_o%29%7D%5C%5C+h_t%26%3Do_t%5Codot+%5Ctanh%7B%28C_t%29%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="21-lstm之遗忘门">2.1 LSTM之遗忘门</span></h4>
<p><img src="https://pic3.zhimg.com/80/v2-e616cbe445d60aee532ef3d9db0fb8f6_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>遗忘门是控制是否遗忘的</strong>，在 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]">
中即以一定的概率控制是否遗忘上一层的细胞状态。图中输入的有前一序列的隐藏状态
<img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]"> 和当前序列的输入数据 <img src="https://www.zhihu.com/equation?tex=x_t" alt="[公式]"> ，通过一个
<img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
激活函数得到遗忘门的输出 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]"> 。因为 <img src="https://www.zhihu.com/equation?tex=+sigmoid" alt="[公式]">
函数的取值在 <img src="https://www.zhihu.com/equation?tex=%5B0%2C+1%5D" alt="[公式]"> 之间，所以 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]">
表示的是遗忘前一序列细胞状态的概率，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++f_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7Bf%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bf%7D%5Cright%29++%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="22-lstm之输入门">2.2 LSTM之输入门</span></h4>
<p><img src="https://pic2.zhimg.com/80/v2-1dbddd39ff6a039631b8ff1e4bc294a5_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>输入门是用来决定哪些数据是需要更新的</strong>，由 <img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
层决定；然后，一个 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]"> 层为新的候选值创建一个向量 <img src="https://www.zhihu.com/equation?tex=+%5Ctilde%7BC_t%7D" alt="[公式]"> ，这些值能够加入到当前细胞状态中，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+i_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bi%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bi%7D%5Cright%29+%5C%5C+%5Ctilde%7BC%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W_%7BC%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7BC%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="23-lstm之细胞状态更新">2.3 LSTM之细胞状态更新</span></h4>
<p><img src="https://pic4.zhimg.com/80/v2-82f48ff07dd75ee4346ab44c573fe0b7_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>前面的遗忘门和输入门的结果都会作用于细胞状态</strong> <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
，<strong>在决定需要遗忘和需要加入的记忆之后，就可以更新前一序列的细胞状态
<img src="https://www.zhihu.com/equation?tex=+C_%7Bt-1%7D" alt="[公式]"> 到当前细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
了</strong>，前一序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_%7Bt-1%7D" alt="[公式]">
乘以遗忘门的输出 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]"> 表示决定遗忘的信息， <img src="https://www.zhihu.com/equation?tex=+i_t%5Codot+%5Ctilde%7BC_t%7D" alt="[公式]"> 表示新的记忆信息，数学表达式为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D%3DC_%7Bt-1%7D%5Codot+f_t%2Bi_t%5Codot+%5Ctilde%7BC_t%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="24-lstm之输出门">2.4 LSTM之输出门</span></h4>
<p><img src="https://pic3.zhimg.com/80/v2-6335c8e72a056eeb80970c88988f6bd2_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>在得到当前序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
后，就可以计算当前序列的输出隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"></strong>
了，隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"> 的更新由两部分组成，第一部分是 <img src="https://www.zhihu.com/equation?tex=o_t" alt="[公式]">
，它由前一序列的隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
和当前序列的输入数据 <img src="https://www.zhihu.com/equation?tex=x_t" alt="[公式]"> 通过激活函数 <img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
得到，第二部分由当前序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]"> 经过 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]">
激活函数后的结果组成，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D++o_t%26%3D%5Csigma%7B%28W_o%5Ccdot%5Bh_%7Bt-1%7D%2Cx_t%5D%2Bb_o%29%7D%5C%5C+h_t%26%3Do_t%5Codot+%5Ctanh%7B%28C_t%29%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h3><span id="三-gru-框架结构">三、GRU 框架结构</span></h3>
<p><img src="https://pic3.zhimg.com/80/v2-7547e5c91e1590bf67e0641e9e29ec66_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>循环门单元( <img src="https://www.zhihu.com/equation?tex=Gated%5C+Recurrent%5C+Unit%2C%5C+GRU" alt="[公式]">
)，它组合了遗忘门和输入门到一个单独的更新门当中，也合并了细胞状态 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">
和隐藏状态</strong> <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]">，并且还做了一些其他的改变使得其模型比标准 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]">
模型更简单，其数学表达式式为： <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+z_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bz%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+r_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Br%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+%5Ctilde%7Bh%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W+%5Ccdot%5Cleft%5Br_%7Bt%7D+%5Codot+h_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+h_%7Bt%7D+%26%3D%5Cleft%281-z_%7Bt%7D%5Cright%29+%5Codot+h_%7Bt-1%7D%2Bz_%7Bt%7D+%5Codot+%5Ctilde%7Bh%7D_%7Bt%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C+" alt="[公式]"></p>
<p>首先介绍 <img src="https://www.zhihu.com/equation?tex=GRU" alt="[公式]"> 的两个门，它们分别是重置门 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]"> 和更新门
<img src="https://www.zhihu.com/equation?tex=z_t" alt="[公式]">
，计算方法与 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]"> 中门的计算方法是一致的；然后是计算候选隐藏层 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> ，该候选隐藏层和 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]"> 中的 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BC%7D_t" alt="[公式]"> 类似，都可以看成是当前时刻的新信息，<strong>其中 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]">
用来控制需要保留多少之前的记忆，如果 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=0" alt="[公式]"> 则表示 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> 只保留当前序列的输入信息；最后 <img src="https://www.zhihu.com/equation?tex=+z_t" alt="[公式]">
控制需要从前一序列的隐藏层 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
中遗忘多少信息和需要加入多少当前序列的隐藏层信息</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> ，从而得到当前序列的输出隐藏层信息 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"> ，而 <img src="https://www.zhihu.com/equation?tex=GRU" alt="[公式]">
是没有输出门的。</p>
<p>GRU和LSTM的性能差不多，但GRU参数更少，更简单，所以训练效率更高。但是，如果数据的依赖特别长且数据量很大的话，LSTM的效果可能会稍微好一点，毕竟参数量更多。所以默认推荐使用LSTM。</p>
<h4><span id="参考资料️">参考资料⬇️</span></h4>
<p><a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding
LSTM Networks</a></p>
<h2><span id="lstm-qampa">LSTM Q&amp;A</span></h2>
<h3><span id="1-为什么lstm可以解决梯度消失和梯度爆炸">1、为什么LSTM可以解决梯度消失和梯度爆炸？</span></h3>
<p><img src="https://pic3.zhimg.com/80/v2-10f9ec56794c9f89ca2b6ce86c7693ee_1440w.jpg?source=d16d100b" alt="img" style="zoom: 33%;"></p>
<p><strong>参考（这个老哥说的是最好的）：</strong><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/34878706">LSTM如何来避免梯度弥散和梯度爆炸？</a></p>
<ul>
<li>==<strong>LSTM 中梯度的传播有很多条路径</strong>，<img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+c_t+%3D+f_t%5Codot+c_%7Bt-1%7D+%2B+i_t+%5Codot+%5Chat%7Bc_t%7D" alt="[公式]">
这条路径上只有逐元素相乘和相加的操作，梯度流最稳定==；但是其他路径（例如
<img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+h_%7Bt-1%7D+%5Crightarrow+i_t+%5Crightarrow+c_t" alt="[公式]"> )上梯度流与普通 RNN
类似，照样会发生相同的权重矩阵反复连乘。</li>
<li><strong>LSTM 刚提出时没有遗忘门</strong>，或者说相当于 <img src="https://www.zhihu.com/equation?tex=f_t%3D1" alt="[公式]">
，这时候在 <img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+c_t" alt="[公式]"> 直接相连的短路路径上，<img src="https://www.zhihu.com/equation?tex=dl%2Fdc_t" alt="[公式]">
可以无损地传递给 <img src="https://www.zhihu.com/equation?tex=dl%2Fdc_%7Bt-1%7D" alt="[公式]">
，从而<strong>这条路径</strong>上的梯度畅通无阻，不会消失。类似于 ResNet
中的残差连接。</li>
<li>但是在<strong>其他路径</strong>上，LSTM 的梯度流和普通 RNN
没有太大区别，依然会爆炸或者消失。由于总的远距离梯度 =
各条路径的远距离梯度之和，即便其他远距离路径梯度消失了，只要保证有一条远距离路径（就是上面说的那条高速公路）梯度不消失，总的远距离梯度就不会消失（正常梯度
+ 消失梯度 = 正常梯度）。因此 LSTM
通过改善<strong>一条路径</strong>上的梯度问题拯救了<strong>总体的远距离梯度</strong>。</li>
<li>同样，因为总的远距离梯度 =
各条路径的远距离梯度之和，高速公路上梯度流比较稳定，但其他路径上梯度有可能爆炸，此时总的远距离梯度
= 正常梯度 + 爆炸梯度 = 爆炸梯度，因此 <strong>LSTM
仍然有可能发生梯度爆炸</strong>。不过，<strong>==由于 LSTM
的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于
1），因此 LSTM
发生梯度爆炸的频率要低得多==</strong>。实践中梯度爆炸一般通过梯度裁剪来解决。</li>
<li>对于现在常用的带遗忘门的 LSTM 来说，4 中的分析依然成立，而 3
分为两种情况：其一是遗忘门接近 1（例如模型初始化时会把 forget bias
设置成较大的正数，让遗忘门饱和），这时候远距离梯度不消失；其二是<strong>遗忘门接近
0，但这时模型是故意阻断梯度流的，这不是 bug 而是
feature</strong>（例如情感分析任务中有一条样本 “A，但是
B”，模型读到“但是”后选择把遗忘门设置成 0，遗忘掉内容
A，这是合理的）。当然，常常也存在 f 介于 [0, 1]
之间的情况，在这种情况下只能说 LSTM
改善（而非解决）了梯度消失的状况。</li>
</ul>
<h3><span id="2-为什么lstm模型中既存在sigmoid又存在tanh两种激活函数">2、为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数？</span></h3>
<p>关于激活函数的选取，在LSTM中，遗忘门、输入门和输出门使用
Sigmoid函数作为激活函数;在<strong>生成候选记忆</strong>时，使用双曲正切函数<strong>tanh</strong>作为激活函数。值得注意的是，这两个激活函数都是<strong>饱和</strong>的也就是说在<strong>输入达到一定值的情况下，输出就不会发生明显变化</strong>了。如果是用非饱和的激活图数，例如ReLU，那么将<strong>难以实现门控的效果。</strong></p>
<ul>
<li><p>Sigmoid的输出在0-1之同，符合门控的物理定义，且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关，在生成候选记亿时，</p></li>
<li><p><strong>tanh函数，是因为其输出在-1-1之间，这与大多数场景下特征分布是0中心的吻合</strong>。此外，<strong>tanh函数在输入为0近相比
Sigmoid函数有更大的梯度，通常使模型收敛更快。</strong></p></li>
</ul>
<p>激活函数的选择也不是一成不变的。<strong>例如在原始的LSTM中，使用的激活函数是
Sigmoid函数的变种，h(x)=2sigmoid(x)-1,g(x)＝4
sigmoid(x)-2，这两个函数的范国分别是[-1，1]和[-2，2]</strong>。并且在原始的LSTM中，只有输入门和输出门，没有遗忘门，其中输入经过输入门后是直接与记忆相加的，所以输入门控g(x)的值是0中心的。</p>
<p><strong>后来经过大量的研究和实验，人们发现增加遗忘门对LSTM的性能有很大的提升</strong>且<strong>h(x)使用tanh比2
sigmoid(x)-1要好</strong>，所以现代的LSTM采用
Sigmoid和tanh作为激活函数。<strong>事实上在门控中，使用
Sigmoid函数是几乎所有现代神经网络模块的共同选择</strong>。例如在<strong>门控循环单元和注意力机制</strong>中，也广泛使用
Sigmoid i函数作为门控的激活函数。</p>
<h4><span id="为什么">为什么？</span></h4>
<ol type="1">
<li>门是控制开闭的，全开时值为1，全闭值为０。用于遗忘和保留信息。</li>
<li>对于求值的激活函数无特殊要求。</li>
</ol>
<h4><span id="能更换吗">能更换吗？</span></h4>
<ol type="1">
<li>门是控制开闭的，全开时值为1，全闭值为０。用于遗忘和保留信息。门的激活函数只能是值域为０到１的，最常见的就是sigmoid。</li>
<li>对于求值的激活函数无特殊要求。</li>
</ol>
<h3><span id="3-能不能把tanh换成relu">3、能不能把tanh换成relu？</span></h3>
<p><strong>不行</strong>？对于梯度爆炸的问题用梯度裁剪解决就行了。</p>
<ol type="1">
<li><strong>会造成输出值爆炸</strong>。RNN共享参数矩阵，长程的话相当于多个相乘，最后输出类似于
<img src="https://www.zhihu.com/equation?tex=f%5Cleft%5BW+%5Cldots%5Cleft%5BW+f%5Cleft%5BW+f%5Cleft%5BW+f_%7Bi%7D%5Cright%5D%5Cright%5D%5Cright%5D%5Cright%5D" alt="[公式]"> ，其中是 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 激活函数，如果 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
有一个大于1的特征值，且使用relu激活函数，那最后的输出值会爆炸。但是使用tanh激活函数，能够把输出值限制在-1和1之间。</li>
<li><strong>这里relu并不能解决梯度消失或梯度爆炸的问题</strong>。假设有t=3，最后一项输出反向传播对W求导，
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+W_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+f_%7B2%7D%2B%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D+f_%7B1%7D%2B%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B1%7D%7D%7B%5Cpartial+a_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+a_%7B1%7D%7D%7B%5Cpartial+W_%7B1%7D%7D" alt="[公式]"> 。我们用最后一项做分析，即使使用了<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=relu&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22350613342%22%7D">relu</a>，
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B1%7D%7D%3D1" alt="[公式]"> ，还是会有两个 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
相乘，并不能解决梯度消失或梯度爆炸的问题。</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>lzy
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://powerlzy.github.io/posts/1M5VH4X/" title="深度学习（6）LSTM*">https://powerlzy.github.io/posts/1M5VH4X/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/3MJS4P5/" rel="prev" title="深度学习（4）CNN">
                  <i class="fa fa-chevron-left"></i> 深度学习（4）CNN
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/31DSKYD/" rel="next" title="Python常见问题">
                  Python常见问题 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
