<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Transformer  Transformer：（Self-attention）自注意力机制的序列到序列的模型  Transformer 代码完全解读! 如何从浅入深理解transformer？ Transformer和GNN有什么联系吗？ 详解Transformer （Attention Is All You Need） Transformer代码+面试细节   一、模型结构概述 如下是Tra">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习（9）Transformer-code">
<meta property="og:url" content="https://powerlzy.github.io/posts/PSEWDM/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="Transformer  Transformer：（Self-attention）自注意力机制的序列到序列的模型  Transformer 代码完全解读! 如何从浅入深理解transformer？ Transformer和GNN有什么联系吗？ 详解Transformer （Attention Is All You Need） Transformer代码+面试细节   一、模型结构概述 如下是Tra">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-7d8daf8e5dbba5ed3f26f3e03f61d395_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-f70598121a15f713eeafa4bc696d528c_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-ab0188042d72481d479f8951dc0d702c_1440w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-4a57b7e6f8a4a7260c4e841f393f873a_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-ee127bacaf444e5c3612ca819b53bb8c_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-4f3a1f34553d5d568c99e8d2ace9e6c0_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=A%28Q%2CK%2CV%29%3D%5Cmathrm%7BSoftmax%7D%28QK%5ET%29V+%5C%5C+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Q%2CK%2CV%3An%5Ctimes+d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=QK%5ET">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5Ctimes+d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d%5Ctimes+n">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5Ctimes+n">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5Ctimes+n">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5Ctimes+d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5Ctimes+d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=r">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=r">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28rd%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%28rnd%29%7D">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-be94b689af1b76a1f64a2581709d67cd_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-e0f18101e6c6c621c87bcb880eb3c795_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathrm%7BMultiHead%7D%28Q%2CK%2CV%29%3D%5Cmathrm%7BConcat%28head_1%2C...%2Chead_h%29%7DW%5EO+%5C%5C+%5Cmathrm%7Bwhere%5Cquad+head_i%7D%3DA%28QW_i%5EQ%2CKW_i%5EK%2CVW_i%5EV%29%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d_q%2Cd_k%2Cd_v">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d_q%3Dd_k%3Dd_v%3D%5Cfrac%7Bd%7D%7Bh%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d_k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d_v">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n+%5Ctimes+d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d%5Ctimes+%5Cfrac%7Bd%7D%7Bh%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28nd%5E2%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5Ctimes+%5Cfrac%7Bd%7D%7Bh%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%7D%7Bh%7D%5Ctimes+%7Bn%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7B%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5Ctimes+d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5Ctimes+d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d%5Ctimes+d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28nd%5E2%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%2Bnd%5E2%29">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-776124756aeaa1aab51f630819d372b7_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-fda2b501fe89662dd5f76326b102c650_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-27f90c6393de75bc8d237fca3e4758b8_1440w.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-f70598121a15f713eeafa4bc696d528c_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=m">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=m+%5Csim+Bernoulli%28%5CPhi%28x%29%29+%2C+~where+~%5CPhi%28x%29+%3D+P%28X+%3C%3D+x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=X+%5Csim+N%280%2C+1%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5CPhi%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5CPhi%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=x">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=GELU%28x%29+%3D+%5CPhi%28x%29+%2A+I%28x%29+%2B+%281+-+%5CPhi%28x%29%29+%2A+0x+%3D+x%5CPhi%28x%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5CPhi%28x%29">
<meta property="og:image" content="https://pic3.zhimg.com/v2-0fde0599700a8045a7c1b7d006de33fa_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-c55ded292a81733eb0944abdc4332d43_b.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Q%2CK%2CV">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_Q%2CW_K%2CW_V">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29+%3D+softmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d_k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Q%2CK">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=d_k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n%5E2d">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=O%28n%5E2d%29">
<meta property="article:published_time" content="2022-06-10T13:00:41.874Z">
<meta property="article:modified_time" content="2022-07-13T13:12:11.597Z">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/80/v2-7d8daf8e5dbba5ed3f26f3e03f61d395_1440w.jpg">


<link rel="canonical" href="https://powerlzy.github.io/posts/PSEWDM/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://powerlzy.github.io/posts/PSEWDM/","path":"posts/PSEWDM/","title":"深度学习（9）Transformer-code"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习（9）Transformer-code | PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PowerLZY's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">一、模型结构概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">二、模型输入</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.1 Embedding层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.2 位置编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.3
Encoder和Decoder都包含输入模块</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">三、Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.1 编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.2 编码器层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.3 注意力机制 Self-Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.4 多头注意力机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.5 前馈全连接层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.6. 规范化层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.7 掩码及其作用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">四、 Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">4.1 解码器整体结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">4.2 解码器层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">五、模型输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">六、模型构建</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">Transformer Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">Feed forward network
(FFN)的作用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">GELU原理？相比RELU的优点？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">图像：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">导数形式：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;为什么用layernorm不用batchnorm？&#x3D;&#x3D;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;Multi-head Self-Attention&#x3D;&#x3D;</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">多头注意力是怎样的呢？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">多头机制为什么有效？</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;为什么要做scaled dot
product?&#x3D;&#x3D;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;为什么用双线性点积模型（即Q，K两个向量）？&#x3D;&#x3D;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;Transformer的非线性来自于哪里？&#x3D;&#x3D;</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">254</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/PSEWDM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="深度学习（9）Transformer-code | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习（9）Transformer-code
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-10 21:00:41" itemprop="dateCreated datePublished" datetime="2022-06-10T21:00:41+08:00">2022-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 21:12:11" itemprop="dateModified" datetime="2022-07-13T21:12:11+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Seq2Seq/" itemprop="url" rel="index"><span itemprop="name">Seq2Seq</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>19k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>35 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1><span id="transformer">Transformer</span></h1>
<blockquote>
<p>Transformer：（Self-attention）自注意力机制的序列到序列的模型</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/415318478">Transformer
代码完全解读!</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/471328838/answer/1996725528">如何从浅入深理解transformer？</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/428626879/answer/1556915218">Transformer和GNN有什么联系吗？</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221"><strong>详解<em>Transformer</em>
（Attention Is All You Need）</strong></a></li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/438634058"><em>Transformer</em>代码+面试细节</a></strong></li>
</ul>
</blockquote>
<h3><span id="一-模型结构概述">一、模型结构概述</span></h3>
<p>如下是Transformer的两个结构示意图：</p>
<p><img src="https://pic2.zhimg.com/80/v2-7d8daf8e5dbba5ed3f26f3e03f61d395_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>上图是从一篇英文博客中截取的Transformer的结构简图，下图是原论文中给出的结构简图，更细粒度一些，可以结合着来看。</p>
<p><img src="https://pic1.zhimg.com/80/v2-f70598121a15f713eeafa4bc696d528c_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>模型大致分为<code>Encoder</code>(编码器)和<code>Decoder</code>(解码器)两个部分，分别对应上图中的左右两部分。</p>
<p><strong>编码器</strong>由N个相同的层堆叠在一起(我们后面的实验取N=6)，每一层又有两个子层：</p>
<ul>
<li>第一个子层是一个<code>Multi-Head Attention</code>(<strong>==多头的自注意机制==</strong>)
<ul>
<li>Self-attention多个头类似于cnn中多个卷积核的作用，使用多头注意力，能够从不同角度提取信息，提高信息提取的全面性。</li>
</ul></li>
<li>第二个子层是一个简单的<code>Feed Forward</code>(全连接前馈网络)</li>
<li>两个子层都添加了一个<strong>残差连接</strong>+==layer
normalization==的操作。</li>
</ul>
<p><strong>解码器</strong>同样是堆叠了N个相同的层，不过和编码器中每层的结构稍有不同。</p>
<ul>
<li>第一个子层是一个<code>Multi-Head Attention</code>(<strong>==多头的自注意机制==</strong>)</li>
<li>第二个子层是一个简单的<code>Feed Forward</code>(全连接前馈网络)</li>
<li><strong>==Masked Multi-Head Attention==</strong></li>
<li>每个子层同样也用了<strong>==residual==</strong>以及layer
normalization。</li>
</ul>
<p>模型的输入由<code>Input Embedding</code>和<code>Positional Encoding</code>(位置编码)两部分组合而成。</p>
<p>模型的输出由Decoder的输出简单的经过softmax得到。</p>
<h3><span id="二-模型输入">二、<strong>模型输入</strong></span></h3>
<p>首先我们来看模型的输入是什么样的，先明确模型输入，后面的模块理解才会更直观。输入部分包含两个模块，<code>Embedding</code>和<code>Positional Encoding</code>。</p>
<h4><span id="21-embedding层"><strong>2.1 Embedding层</strong></span></h4>
<p><strong>Embedding层的作用是将某种格式的输入数据，例如文本，转变为模型可以处理的向量表示，来描述原始数据所包含的信息</strong>。<code>Embedding</code>层输出的可以理解为当前时间步的特征，如果是文本任务，这里就可以是<code>Word Embedding</code>，如果是其他任务，就可以是任何合理方法所提取的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        类的初始化函数</span></span><br><span class="line"><span class="string">        d_model：指词嵌入的维度</span></span><br><span class="line"><span class="string">        vocab:指词表的大小</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment">#之后就是调用nn中的预定义层Embedding，获得一个词嵌入对象self.lut</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="comment">#最后就是将d_model传入类中</span></span><br><span class="line">        self.d_model =d_model</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Embedding层的前向传播逻辑</span></span><br><span class="line"><span class="string">        参数x：这里代表输入给模型的单词文本通过词表映射后的one-hot向量</span></span><br><span class="line"><span class="string">        将x传给self.lut并与根号下self.d_model相乘作为结果返回</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        embedds = self.lut(x)</span><br><span class="line">        <span class="keyword">return</span> embedds * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<h4><span id="22-位置编码">2.2 <strong>位置编码</strong></span></h4>
<p><strong><code>Positional Encodding</code>位置编码的作用是为模型提供当前时间步的前后出现顺序的信息</strong>。因为Transformer不像RNN那样的循环结构有前后不同时间步输入间天然的先后顺序，所有的时间步是同时输入，并行推理的，因此在时间步的特征中融合进位置编码的信息是合理的。位置编码可以有很多选择，可以是固定的，也可以设置成可学习的参数。这里，我们使用固定的位置编码。<strong>具体地，使用不同频率的sin和cos函数来进行位置编码</strong>，如下所示：
<span class="math display">\[
\begin{gathered}
P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text {model
}}}\right) \\
P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\text {model
}}}\right)
\end{gathered}
\]</span>
其中pos代表时间步的下标索引，向量也就是第pos个时间步的位置编码，编码长度同<code>Embedding</code>层，这里我们设置的是512。上面有两个公式，代表着位置编码向量中的元素，奇数位置和偶数位置使用两个不同的公式。思考：<strong>为什么上面的公式可以作为位置编码？</strong>我的理解：在上面公式的定义下，<strong><font color="red">
时间步p和时间步p+k的位置编码的内积，即是与p无关，只与k有关的定值（不妨自行证明下试试）。也就是说，任意两个相距k个时间步的位置编码向量的内积都是相同的，这就相当于蕴含了两个时间步之间相对位置关系的信息。</font></strong>此外，每个时间步的位置编码又是唯一的，这两个很好的性质使得上面的公式作为位置编码是有理论保障的。下面是位置编码模块的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        位置编码器类的初始化函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        共有三个参数，分别是</span></span><br><span class="line"><span class="string">        d_model：词嵌入维度</span></span><br><span class="line"><span class="string">        dropout: dropout触发比率</span></span><br><span class="line"><span class="string">        max_len：每个句子的最大长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings</span></span><br><span class="line">        <span class="comment"># 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。</span></span><br><span class="line">        <span class="comment"># 这样计算是为了避免中间的数值计算结果超出float的范围，</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>因此，可以认为，最终模型的输入是若干个时间步对应的embedding，每一个时间步对应一个embedding，可以理解为是当前时间步的一个综合的特征信息，即包含了本身的语义信息，又包含了当前时间步在整个句子中的位置信息。</p>
<h4><span id="23encoder和decoder都包含输入模块"><strong>2.3
Encoder和Decoder都包含输入模块</strong></span></h4>
<p>此外有一个点刚刚接触Transformer的同学可能不太理解，<strong>编码器和解码器两个部分都包含输入，且两部分的输入的结构是相同的，只是推理时的用法不同，编码器只推理一次，而解码器是类似RNN那样循环推理，不断生成预测结果的。</strong></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-ab0188042d72481d479f8951dc0d702c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>怎么理解？假设我们现在做的是一个法语-英语的机器翻译任务，想把<code>Je suis étudiant</code>翻译为<code>I am a student</code>。那么我们输入给编码器的就是时间步数为3的embedding数组，编码器只进行一次并行推理，即获得了对于输入的法语句子所提取的若干特征信息。而对于解码器，是循环推理，逐个单词生成结果的。最开始，由于什么都还没预测，我们会将编码器提取的特征，以及一个句子起始符传给解码器，解码器预期会输出一个单词<code>I</code>。然后有了预测的第一个单词，我们就将<code>I</code>输入给解码器，会再预测出下一个单词<code>am</code>，再然后我们将<code>I am</code>作为输入喂给解码器，以此类推直到预测出句子终止符完成预测。</p>
<h3><span id="三-encoder">三、<strong>Encoder</strong></span></h3>
<h4><span id="31-编码器"><strong>3.1 编码器</strong></span></h4>
<p><strong><font color="red">
编码器作用是用于对输入进行特征提取，为解码环节提供有效的语义信息整体来看编码器由N个编码器层简单堆叠而成</font></strong>，因此实现非常简单，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个clones函数，来更方便的将某个结构复制若干份</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Encoder</span></span><br><span class="line"><span class="string">    The encoder is composed of a stack of N=6 identical layers.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># 调用时会将编码器层传进来，我们简单克隆N分，叠加在一起，组成完整的Encoder</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p><strong>上面的代码中有一个小细节，就是编码器的输入除了x，也就是embedding以外，还有一个<code>mask</code>，为了介绍连续性</strong>，这里先忽略，后面会讲解。下面我们来看看单个的编码器层都包含什么，如何实现。</p>
<h4><span id="32-编码器层"><strong>3.2 编码器层</strong></span></h4>
<p>每个编码器层由两个子层连接结构组成：<strong>第一个子层包括一个多头自注意力层和规范化层以及一个残差连接</strong>；<strong>第二个子层包括一个前馈全连接层和规范化层以及一个残差连接</strong>；如下图所示：</p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-4a57b7e6f8a4a7260c4e841f393f873a_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>可以看到，两个子层的结构其实是一致的，只是中间核心层的实现不同</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-ee127bacaf444e5c3612ca819b53bb8c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://pic1.zhimg.com/80/v2-4f3a1f34553d5d568c99e8d2ace9e6c0_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们先定义一个SubLayerConnection类来描述这种结构关系:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    实现子层连接结构的类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 原paper的方案</span></span><br><span class="line">        <span class="comment">#sublayer_out = sublayer(x)</span></span><br><span class="line">        <span class="comment">#x_norm = self.norm(x + self.dropout(sublayer_out))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 稍加调整的版本</span></span><br><span class="line">        sublayer_out = sublayer(x)</span><br><span class="line">        sublayer_out = self.dropout(sublayer_out)</span><br><span class="line">        x_norm = x + self.norm(sublayer_out)</span><br><span class="line">        <span class="keyword">return</span> x_norm</span><br></pre></td></tr></table></figure>
<p>注：上面的实现中，我对残差的链接方案进行了小小的调整，和原论文有所不同。<strong>把x从norm中拿出来，保证永远有一条“高速公路”，这样理论上会收敛的快一些，但我无法确保这样做一定是对的，请一定注意</strong>。定义好了SubLayerConnection，我们就可以实现EncoderLayer的结构了.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;EncoderLayer is made up of two sublayer: self-attn and feed forward&quot;</span>                                                                                                         </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size   <span class="comment"># embedding&#x27;s dimention of model, 默认512</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># attention sub layer</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="comment"># feed forward sub layer</span></span><br><span class="line">        z = self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<p>继续往下拆解，我们需要了解 attention层 和
feed_forward层的结构以及如何实现。</p>
<h4><span id="33-注意力机制-self-attention">3.3 注意力机制 Self-Attention</span></h4>
<p>人类在观察事物时，无法同时仔细观察眼前的一切，只能聚焦到某一个局部。通常我们大脑在简单了解眼前的场景后，能够很快把注意力聚焦到最有价值的局部来仔细观察，从而作出有效判断。或许是基于这样的启发，大家想到了在算法中利用注意力机制。注意力计算：它需要三个指定的输入Q（query），K（key），V（value），然后通过下面公式得到注意力的计算结果。</p>
<blockquote>
<figure>
<img src="https://www.zhihu.com/equation?tex=A%28Q%2CK%2CV%29%3D%5Cmathrm%7BSoftmax%7D%28QK%5ET%29V+%5C%5C+" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<ul>
<li><figure>
<img src="https://www.zhihu.com/equation?tex=Q%2CK%2CV%3An%5Ctimes+d" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure></li>
<li>相似度计算 <img src="https://www.zhihu.com/equation?tex=QK%5ET" alt="[公式]"> ： <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]"> 与
<img src="https://www.zhihu.com/equation?tex=d%5Ctimes+n" alt="[公式]"> 运算，得到 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="[公式]">
矩阵，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29" alt="[公式]"></li>
<li>softmax计算：对每行做softmax，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%29" alt="[公式]"> ，则n行的复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2%29" alt="[公式]"></li>
<li>加权和： <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]">
运算，得到 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]"> 矩阵，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29" alt="[公式]"></li>
</ul>
<p>故最后self-attention的时间复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29" alt="[公式]">; 对于受限的self-attention，每个元素仅能和周围 <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]">
个元素进行交互，即和 <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]"> 个 <img src="https://www.zhihu.com/equation?tex=d" alt="[公式]"> 维向量做内积运算，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28rd%29" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 个元素的总时间复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%28rnd%29%7D" alt="[公式]"></p>
</blockquote>
<p><strong>计算流程图如下：</strong></p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-be94b689af1b76a1f64a2581709d67cd_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>可以这么简单的理解，<strong>当前时间步的注意力计算结果，是一个组系数
*
每个时间步的特征向量value的累加，而这个系数，通过当前时间步的query和其他时间步对应的key做内积得到，这个过程相当于用自己的query对别的时间步的key做查询，判断相似度，决定以多大的比例将对应时间步的信息继承过来</strong>。下面是注意力模块的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    <span class="comment">#首先取query的最后一维的大小，对应词嵌入维度</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#按照注意力公式，将query与key的转置相乘，这里面key是将最后两个维度进行转置，再除以缩放系数得到注意力得分张量scores</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="comment">#接着判断是否使用掩码张量</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment">#使用tensor的masked_fill方法，将掩码张量和scores张量每个位置一一比较，如果掩码张量则对应的scores张量用-1e9这个置来替换</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    <span class="comment">#对scores的最后一维进行softmax操作，使用F.softmax方法，这样获得最终的注意力张量</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#之后判断是否使用dropout进行随机置0</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="comment">#最后，根据公式将p_attn与value张量相乘获得最终的query注意力表示，同时返回注意力张量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<h4><span id="34-多头注意力机制"><strong>3.4 多头注意力机制</strong></span></h4>
<p><strong>刚刚介绍了attention机制，在搭建EncoderLayer时候所使用的Attention模块，实际使用的是多头注意力，可以简单理解为多个注意力模块组合在一起。</strong></p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-e0f18101e6c6c621c87bcb880eb3c795_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><font color="red">
多头注意力机制的作用：这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元表达，实验表明可以从而提升模型效果。</font></strong></p>
<blockquote>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BMultiHead%7D%28Q%2CK%2CV%29%3D%5Cmathrm%7BConcat%28head_1%2C...%2Chead_h%29%7DW%5EO+%5C%5C+%5Cmathrm%7Bwhere%5Cquad+head_i%7D%3DA%28QW_i%5EQ%2CKW_i%5EK%2CVW_i%5EV%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>对于multi-head attention，假设有 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 个head，这里
<img src="https://www.zhihu.com/equation?tex=h" alt="[公式]">
是一个常数，对于每个head，首先需要把三个矩阵分别映射到 <img src="https://www.zhihu.com/equation?tex=d_q%2Cd_k%2Cd_v" alt="[公式]">
维度。这里考虑一种简化情况： <img src="https://www.zhihu.com/equation?tex=d_q%3Dd_k%3Dd_v%3D%5Cfrac%7Bd%7D%7Bh%7D" alt="[公式]"> 。(对于dot-attention计算方式， <img src="https://www.zhihu.com/equation?tex=d_k" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=d_v" alt="[公式]">
可以不同)。</p>
<ul>
<li>输入线性映射的复杂度： <img src="https://www.zhihu.com/equation?tex=n+%5Ctimes+d" alt="[公式]"> 与
<img src="https://www.zhihu.com/equation?tex=d%5Ctimes+%5Cfrac%7Bd%7D%7Bh%7D" alt="[公式]"> 运算，忽略常系数，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28nd%5E2%29" alt="[公式]"> 。</li>
<li>Attention操作复杂度：主要在相似度计算及加权和的开销上， <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+%5Cfrac%7Bd%7D%7Bh%7D" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%7D%7Bh%7D%5Ctimes+%7Bn%7D" alt="[公式]"> 运算，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7B%7D" alt="[公式]"></li>
<li>输出线性映射的复杂度：concat操作拼起来形成 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]">
的矩阵，然后经过输出线性映射，保证输入输出相同，所以是 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]"> 与
<img src="https://www.zhihu.com/equation?tex=d%5Ctimes+d" alt="[公式]"> 计算，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28nd%5E2%29" alt="[公式]"></li>
</ul>
<p>故最后的复杂度为： <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%2Bnd%5E2%29" alt="[公式]"></p>
</blockquote>
<p>举个更形象的例子，<strong>bank是银行的意思，如果只有一个注意力模块，那么它大概率会学习去关注类似money、loan贷款这样的词。如果我们使用多个多头机制，那么不同的头就会去关注不同的语义，比如bank还有一种含义是河岸，那么可能有一个头就会去关注类似river这样的词汇，这时多头注意力的价值就体现出来了</strong>。下面是多头注意力机制的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment">#在类的初始化时，会传入三个参数，h代表头数，d_model代表词嵌入的维度，dropout代表进行dropout操作时置0比率，默认是0.1</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="comment">#在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，这是因为我们之后要给每个头分配等量的词特征，也就是embedding_dim/head个</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment">#得到每个头获得的分割词向量维度d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        <span class="comment">#传入头数h</span></span><br><span class="line">        self.h = h</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#创建linear层，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用，为什么是四个呢，这是因为在多头注意力中，Q,K,V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        <span class="comment">#self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None</span></span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment">#前向逻辑函数，它输入参数有四个，前三个就是注意力机制需要的Q,K,V，最后一个是注意力机制中可能需要的mask掩码张量，默认是None</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            <span class="comment">#使用unsqueeze扩展维度，代表多头中的第n头</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        <span class="comment"># 首先利用zip将输入QKV与三个线性层组到一起，然后利用for循环，将输入QKV分别传到线性层中，做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结构进行维度重塑，多加了一个维度h代表头，这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，计算机会根据这种变换自动计算这里的值，然后对第二维和第三维进行转置操作，为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，从attention函数中可以看到，利用的是原始输入的倒数第一和第二维，这样我们就得到了每个头的输入</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        <span class="comment"># 得到每个头的输入后，接下来就是将他们传入到attention中，这里直接调用我们之前实现的attention函数，同时也将mask和dropout传入其中</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        <span class="comment"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法。这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，所以，下一步就是使用view重塑形状，变成和输入形状相同。  </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment">#最后使用线性层列表中的最后一个线性变换得到最终的多头注意力结构的输出</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<h4><span id="35-前馈全连接层"><strong>3.5 前馈全连接层</strong></span></h4>
<p><strong>EncoderLayer中另一个核心的子层是 Feed Forward
Layer</strong>，我们这就介绍一下。在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-776124756aeaa1aab51f630819d372b7_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><font color="red"> Feed Forward Layer
其实就是简单的由两个前向全连接层组成，核心在于，Attention模块每个时间步的输出都整合了所有时间步的信息，==而Feed
Forward
Layer每个时间步只是对自己的特征的一个进一步整合，与其他时间步无关。==</font></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment">#初始化函数有三个输入参数分别是d_model，d_ff，和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，因为我们希望输入通过前馈全连接层后输入和输出的维度不变，第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出，最后一个是dropout置0比率。</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#输入参数为x，代表来自上一层的输出，首先经过第一个线性层，然后使用F中的relu函数进行激活，之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<p>到这里Encoder中包含的主要结构就都介绍了，上面的代码中涉及了两个小细节还没有介绍，<strong>layer
normalization 和 mask</strong>，下面来简单讲解一下。</p>
<h4><span id="36-规范化层"><strong>3.6. 规范化层</strong></span></h4>
<p><strong>规范化层的作用：它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后输出可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常慢</strong>。因此都会在一定层后接规范化层进行数值的规范化，使其特征数值在合理范围内。Transformer中使用的normalization手段是layer
norm，实现代码很简单，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, feature_size, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="comment">#初始化函数有两个参数，一个是features,表示词嵌入的维度,另一个是eps它是一个足够小的数，在规范化公式的分母中出现,防止分母为0，默认是1e-6。</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        <span class="comment">#根据features的形状初始化两个参数张量a2，和b2，第一初始化为1张量，也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，这两个张量就是规范化层的参数。因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，使其即能满足规范化要求，又能不改变针对目标的表征，最后使用nn.parameter封装，代表他们是模型的参数</span></span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(feature_size))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(feature_size))</span><br><span class="line">        <span class="comment">#把eps传到类中</span></span><br><span class="line">        self.eps = eps</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment">#输入参数x代表来自上一层的输出，在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致，接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果。</span></span><br><span class="line">    <span class="comment">#最后对结果乘以我们的缩放参数，即a2,*号代表同型点乘，即对应位置进行乘法操作，加上位移参b2，返回即可</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<h4><span id="37-掩码及其作用"><strong>3.7 掩码及其作用</strong></span></h4>
<p><strong>掩码：掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有0和1；代表位置被遮掩或者不被遮掩。</strong>掩码的作用：<strong><font color="red">
在transformer中，掩码主要的作用有两个，一个是屏蔽掉无效的padding区域，一个是屏蔽掉来自“未来”的信息。</font></strong></p>
<p><strong>Encoder中的掩码主要是起到第一个作用，Decoder中的掩码则同时发挥着两种作用</strong>。屏蔽掉无效的padding区域：我们训练需要组batch进行，就以机器翻译任务为例，一个batch中不同样本的输入长度很可能是不一样的，此时我们要设置一个最大句子长度，然后对空白区域进行padding填充，而填充的区域无论在Encoder还是Decoder的计算中都是没有意义的，因此需要用mask进行标识，屏蔽掉对应区域的响应。屏蔽掉来自未来的信息：我们已经学习了attention的计算流程，它是会综合所有时间步的计算的，那么在解码的时候，就有可能获取到未来的信息，这是不行的。因此，这种情况也需要我们使用mask进行屏蔽。现在还没介绍到Decoder，如果没完全理解，可以之后再回过头来思考下。mask的构造代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="comment">#生成向后遮掩的掩码张量，参数size是掩码张量最后两个维度的大小，它最后两维形成一个方阵</span></span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    <span class="comment">#然后使用np.ones方法向这个形状中添加1元素，形成上三角阵</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="comment">#最后将numpy类型转化为torch中的tensor，内部做一个1- 的操作。这个其实是做了一个三角阵的反转，subsequent_mask中的每个元素都会被1减。</span></span><br><span class="line">    <span class="comment">#如果是0，subsequent_mask中的该位置由0变成1</span></span><br><span class="line">    <span class="comment">#如果是1，subsequect_mask中的该位置由1变成0</span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>以上便是编码器部分的全部内容，有了这部分内容的铺垫，解码器的介绍就会轻松一些。</p>
<h3><span id="四-decoder"><strong>四、 Decoder</strong></span></h3>
<h4><span id="41-解码器整体结构"><strong>4.1 解码器整体结构</strong></span></h4>
<p>解码器的作用：根据编码器的结果以及上一次预测的结果，输出序列的下一个结果。整体结构上，解码器也是由N个相同层堆叠而成。构造代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用类Decoder来实现解码器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="comment">#初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        <span class="comment">#首先使用clones方法克隆了N个layer，然后实例化一个规范化层，因为数据走过了所有的解码器层后最后要做规范化处理。</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="comment">#forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，source_mask，target_mask代表源数据和目标数据的掩码张量，然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，得出最后的结果，再进行一次规范化返回即可。</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h4><span id="42-解码器层"><strong>4.2 解码器层</strong></span></h4>
<p><strong>每个解码器层由三个子层连接结构组成，第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接，第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接，第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接。</strong></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-fda2b501fe89662dd5f76326b102c650_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>解码器层中的各个子模块，如，多头注意力机制，规范化层，前馈全连接都与编码器中的实现相同。</p>
<p>有一个细节需要注意，第一个子层的多头注意力和编码器中完全一致，<strong><font color="red">
第二个子层，它的多头注意力模块中，query来自上一个子层，key 和 value
来自编码器的输出。</font></strong>可以这样理解，就是第二层负责，利用解码器已经预测出的信息作为query，去编码器提取的各种特征中，查找相关信息并融合到当前特征中，来完成预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用DecoderLayer的类实现解码器层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="comment">#初始化函数的参数有5个，分别是size，代表词嵌入的维度大小，同时也代表解码器的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，第三个是src_attn,多头注意力对象，这里Q!=K=V，第四个是前馈全连接层对象，最后就是dropout置0比率</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        <span class="comment">#按照结构图使用clones函数克隆三个子层连接对象</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="comment">#forward函数中的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量memory，以及源数据掩码张量和目标数据掩码张量，将memory表示成m之后方便使用。</span></span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        <span class="comment">#将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，最后一个参数时目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据。</span></span><br><span class="line">        <span class="comment">#比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用。</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        <span class="comment">#接着进入第二个子层，这个子层中常规的注意力机制，q是输入x;k,v是编码层输出memory，同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄露，而是遮蔽掉对结果没有意义的padding。</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="comment">#最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果，这就是我们的解码器结构</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3><span id="五-模型输出"><strong>五、模型输出</strong></span></h3>
<p>输出部分就很简单了，每个时间步都过一个 线性层 + softmax层</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-27f90c6393de75bc8d237fca3e4758b8_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>线性层的作用：通过对上一步的线性变化得到指定维度的输出，也就是转换维度的作用。转换后的维度对应着输出类别的个数，如果是翻译任务，那就对应的是文字字典的大小。</strong></p>
<h3><span id="六-模型构建">六、<strong>模型构建</strong></span></h3>
<p>下面是Transformer总体架构图，回顾一下，再看这张图，是不是每个模块的作用都有了基本的认知。</p>
<p><img src="https://pic1.zhimg.com/80/v2-f70598121a15f713eeafa4bc696d528c_1440w.jpg" alt="img" style="zoom: 50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model Architecture</span></span><br><span class="line"><span class="comment">#使用EncoderDecoder类来实现编码器-解码器结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. </span></span><br><span class="line"><span class="string">    Base for this and many other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span><br><span class="line">        <span class="comment">#初始化函数中有5个参数，分别是编码器对象，解码器对象,源数据嵌入函数，目标数据嵌入函数，以及输出部分的类别生成器对象.</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed    <span class="comment"># input embedding module(input embedding + positional encode)</span></span><br><span class="line">        self.tgt_embed = tgt_embed    <span class="comment"># ouput embedding module</span></span><br><span class="line">        self.generator = generator    <span class="comment"># output generation module</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="comment">#在forward函数中，有四个参数，source代表源数据，target代表目标数据,source_mask和target_mask代表对应的掩码张量,在函数中，将source source_mask传入编码函数，得到结果后与source_mask target 和target_mask一同传给解码函数</span></span><br><span class="line">        memory = self.encode(src, src_mask)</span><br><span class="line">        res = self.decode(memory, src_mask, tgt, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="comment">#编码函数，以source和source_mask为参数,使用src_embed对source做处理，然后和source_mask一起传给self.encoder</span></span><br><span class="line">        src_embedds = self.src_embed(src)</span><br><span class="line">        <span class="keyword">return</span> self.encoder(src_embedds, src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="comment">#解码函数，以memory即编码器的输出，source_mask target target_mask为参数,使用tgt_embed对target做处理，然后和source_mask,target_mask,memory一起传给self.decoder</span></span><br><span class="line">        target_embedds = self.tgt_embed(tgt)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(target_embedds, memory, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Full Model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构建模型</span></span><br><span class="line"><span class="string">    params:</span></span><br><span class="line"><span class="string">        src_vocab:</span></span><br><span class="line"><span class="string">        tgt_vocab:</span></span><br><span class="line"><span class="string">        N: 编码器和解码器堆叠基础模块的个数</span></span><br><span class="line"><span class="string">        d_model: 模型中embedding的size，默认512</span></span><br><span class="line"><span class="string">        d_ff: FeedForward Layer层中embedding的size，默认2048</span></span><br><span class="line"><span class="string">        h: MultiHeadAttention中多头的个数，必须被d_model整除</span></span><br><span class="line"><span class="string">        dropout:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1><span id="transformer-qampa">Transformer Q&amp;A</span></h1>
<blockquote>
<p>3，Transformer的Feed Forward层在训练的时候到底在训练什么？ -
zzzzzzz的回答 - 知乎
https://www.zhihu.com/question/499274875/answer/2250085650</p>
</blockquote>
<h4><span id="feed-forward-networkffn的作用"><strong>Feed forward network
(FFN)的作用？</strong></span></h4>
<p>Transformer在抛弃了 LSTM 结构后，FFN
中的激活函数成为了一个主要的提供<strong>非线性变换</strong>的单元。</p>
<h4><span id="gelu原理相比relu的优点"><strong>GELU原理？相比RELU的优点？</strong></span></h4>
<p>ReLU会<strong>确定性</strong>的将输入乘上一个0或者1(当x&lt;0时乘上0，否则乘上1)，Dropout则是随机乘上0。而GELU虽然也是将输入乘上0或1，但是输入到底是乘以0还是1，是在<strong>取决于输入自身</strong>的情况下<strong>随机</strong>选择的。</p>
<p>什么意思呢？具体来说：</p>
<p>我们将神经元的输入 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 乘上一个服从伯努利分布的 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]">
。而该伯努利分布又是依赖于 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=m+%5Csim+Bernoulli%28%5CPhi%28x%29%29+%2C+~where+~%5CPhi%28x%29+%3D+P%28X+%3C%3D+x%29" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中， <img src="https://www.zhihu.com/equation?tex=X+%5Csim+N%280%2C+1%29" alt="[公式]">，那么 <img src="https://www.zhihu.com/equation?tex=%5CPhi%28x%29" alt="[公式]">
就是标准正态分布的累积分布函数。这么做的原因是因为神经元的输入 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">
往往遵循正态分布，尤其是深度网络中普遍存在Batch
Normalization的情况下。当<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">减小时，<img src="https://www.zhihu.com/equation?tex=%5CPhi%28x%29" alt="[公式]">的值也会减小，此时<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">被“丢弃”的可能性更高。所以说这是<strong>随机依赖于输入</strong>的方式。</p>
<p>现在，给出GELU函数的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=GELU%28x%29+%3D+%5CPhi%28x%29+%2A+I%28x%29+%2B+%281+-+%5CPhi%28x%29%29+%2A+0x+%3D+x%5CPhi%28x%29" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5CPhi%28x%29" alt="[公式]">
是上文提到的标准正态分布的累积分布函数。因为这个函数没有解析解，所以要用近似函数来表示。</p>
<h5><span id="图像">图像：</span></h5>
<p><img src="https://pic3.zhimg.com/v2-0fde0599700a8045a7c1b7d006de33fa_b.jpg" alt="img" style="zoom:50%;"></p>
<h5><span id="导数形式">导数形式：</span></h5>
<p><img src="https://pic4.zhimg.com/v2-c55ded292a81733eb0944abdc4332d43_b.jpg" alt="img" style="zoom:50%;"></p>
<p>GELU和RELU一样，可以解决梯度消失，所以，GELU的优点就是在ReLU上增加随机因素，x越小越容易被mask掉。</p>
<h4><span id="为什么用layernorm不用batchnorm">==<strong>为什么用layernorm不用batchnorm？</strong>==</span></h4>
<p>对于RNN来说，sequence的长度是不一致的，所以用很多padding来表示无意义的信息。如果BN会导致有意义的embedding损失信息。所以，BN一般用于CNN，而LN用于RNN。</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=layernorm&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22438634058%22%7D">layernorm</a>是在hidden
size的维度进行的，跟batch和seq_len无关。每个hidden
state都计算自己的均值和方差，这是因为不同hidden
state的量纲不一样。beta和gamma的维度都是(hidden_size,)，经过白化的hidden
state * beta + gamma得到最后的结果。</p>
<p>LN在BERT中主要起到白化的作用，增强模型稳定性（如果删除则无法收敛）</p>
<h4><span id="multi-head-self-attention">==Multi-head Self-Attention==</span></h4>
<p>如果是<strong>单头</strong>注意力，就是每个位置的embedding对应 <img src="https://www.zhihu.com/equation?tex=Q%2CK%2CV" alt="[公式]">
三个向量，这三个向量分别是embedding点乘 <img src="https://www.zhihu.com/equation?tex=W_Q%2CW_K%2CW_V" alt="[公式]">
矩阵得来的。每个位置的Q向量去乘上所有位置的K向量，其结果经过softmax变成attention
score，以此作为权重对所有V向量做<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加权求和&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22438634058%22%7D">加权求和</a>即可。</p>
<p>用公式表示为：<img src="https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29+%3D+softmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=d_k" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=Q%2CK" alt="[公式]"> 向量的hidden size。除以 <img src="https://www.zhihu.com/equation?tex=d_k" alt="[公式]"> 叫做scaled
dot product.</p>
<ul>
<li><h5><span id="多头注意力是怎样的呢"><strong>多头</strong>注意力是怎样的呢？</span></h5></li>
</ul>
<p>Transformer中先通过切头（<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=spilt&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22438634058%22%7D">spilt</a>）再分别进行Scaled
Dot-Product Attention。</p>
<p><strong>step1</strong>：一个768维的hidden向量，被映射成Q，K，V。
然后三个向量分别切分成12(head_num)个小的64维的向量，每一组小向量之间做attention。不妨假设batch_size为32，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=seqlen&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22438634058%22%7D">seqlen</a>为512，隐层维度为768，12个head。</p>
<blockquote>
<p>hidden(32 x 512 x 768) -&gt; Q(32 x 512 x 768) -&gt; 32 x 12 x 512 x
64 hidden(32 x 512 x 768) -&gt; K(32 x 512 x 768) -&gt; 32 x 12 x 512 x
64 hidden(32 x 512 x 768) -&gt; V(32 x 512 x 768) -&gt; 32 x 12 x 512 x
64</p>
</blockquote>
<p><strong>step2</strong>：然后Q和K之间做attention，得到一个32 x 12 x
512 x 512的权重矩阵（时间复杂度O( <img src="https://www.zhihu.com/equation?tex=n%5E2d" alt="[公式]">
))，然后根据这个权重矩阵加权V中切分好的向量，得到一个32 x 12 x 512 x 64
的向量，拉平输出为768向量。</p>
<blockquote>
<p>32 x 12 x 512 x 64(query_hidden) * 32 x 12 x 64 x 512(key_hidden)
-&gt; 32 x 12 x 512 x 512 32 x 12 x 64 x 512(value_hidden) * 32 x 12 x
512 x 512 (权重矩阵) -&gt; 32 x 12 x 512 x 64</p>
</blockquote>
<p>然后再还原成 -&gt; 32 x 512 x 768
。简言之是12个头，每个头都是一个64维度，分别去与其他的所有位置的hidden
embedding做attention然后再合并还原。</p>
<ul>
<li><h5><span id="多头机制为什么有效">多头机制为什么有效？</span></h5></li>
</ul>
<p>类似于CNN中通过多通道机制进行特征选择。Transformer中使用切头(split)的方法，是为了在不增加复杂度（
<img src="https://www.zhihu.com/equation?tex=O%28n%5E2d%29" alt="[公式]"> )的前提下享受类似CNN中“不同卷积核”的优势。</p>
<h4><span id="为什么要做scaled-dotproduct">==为什么要做scaled dot
product?==</span></h4>
<p>当输入信息的维度 d 比较高，会导致 softmax
函数接近饱和区，梯度会比较小。因此，缩放点积模型可以较好地解决这一问题。</p>
<h4><span id="为什么用双线性点积模型即qk两个向量">==为什么用双线性点积模型（即Q，K两个向量）？==</span></h4>
<p>双线性点积模型使用Q，K两个向量，而不是只用一个Q向量，这样引入非对称性，更具健壮性（Attention对角元素值不一定是最大的，也就是说当前位置对自身的注意力得分不一定最高）。</p>
<h4><span id="transformer的非线性来自于哪里">==Transformer的非线性来自于哪里？==</span></h4>
<ul>
<li>FFN的gelu激活函数</li>
<li>self-attention：注意self-attention是非线性的（因为有相乘和softmax）</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>lzy
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://powerlzy.github.io/posts/PSEWDM/" title="深度学习（9）Transformer-code">https://powerlzy.github.io/posts/PSEWDM/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/2D5Z22P/" rel="prev" title="模型训练（3）Adaptive Learning Rate">
                  <i class="fa fa-chevron-left"></i> 模型训练（3）Adaptive Learning Rate
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/S7AAWG/" rel="next" title="深度学习（9）Transformer*-p2">
                  深度学习（9）Transformer*-p2 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
