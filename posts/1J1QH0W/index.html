<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic Regression 因其简单、可并行化、可解释强深受工业界喜爱。Logistic 回归的本质是：假设数据服从这个Logistic分布，然后使用极大似然估计做参数的估计。 逻辑回归的思路是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情">
<meta property="og:type" content="article">
<meta property="og:title" content="线性模型（2）逻辑回归">
<meta property="og:url" content="https://powerlzy.github.io/posts/1J1QH0W/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic Regression 因其简单、可并行化、可解释强深受工业界喜爱。Logistic 回归的本质是：假设数据服从这个Logistic分布，然后使用极大似然估计做参数的估计。 逻辑回归的思路是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pica.zhimg.com/v2-35393b75f51c81bb3c09774e76a7d91c_1440w.jpg?source=172ae18b">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-b15289fd1162a807e11949e5396c7989_1440w.jpg">
<meta property="og:image" content="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200201527.png">
<meta property="og:image" content="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200201006.png">
<meta property="og:image" content="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200201154.png">
<meta property="og:image" content="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200244353.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-a352bc374e80df1299a4d63d39ce4606_1440w.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-efc752bd6d1ce09dbf2e18b9766570eb_1440w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-a352bc374e80df1299a4d63d39ce4606_1440w.jpg">
<meta property="og:image" content="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200216158.png">
<meta property="og:image" content="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200217870.png">
<meta property="og:image" content="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200223146.png">
<meta property="og:image" content="https://pic1.zhimg.com/v2-1d4dd37a5e63e40fb4c5a25a6067e7dc_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-7b24efc290bf6503e5ae8f2255c36121_b.png">
<meta property="og:image" content="https://pic1.zhimg.com/v2-e321aeb2cacf2215b1a9e75f5154dcf8_b.png">
<meta property="og:image" content="https://pic3.zhimg.com/v2-02c43a6ba09360753ee62c4c0871e2aa_b.png">
<meta property="article:published_time" content="2022-03-08T10:56:01.328Z">
<meta property="article:modified_time" content="2023-04-20T11:08:22.567Z">
<meta property="article:author" content="lzy">
<meta property="article:tag" content="线性回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pica.zhimg.com/v2-35393b75f51c81bb3c09774e76a7d91c_1440w.jpg?source=172ae18b">


<link rel="canonical" href="https://powerlzy.github.io/posts/1J1QH0W/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://powerlzy.github.io/posts/1J1QH0W/","path":"posts/1J1QH0W/","title":"线性模型（2）逻辑回归"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>线性模型（2）逻辑回归 | PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PowerLZY's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">一、逻辑回归
【深度学习的组成单元】</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.1 Logistic 分布
[位置参数、形状参数]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.2 Logistic 回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.3 代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.4 求解【 梯度下降 和 牛顿法 】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">LR-L1 的梯度下降方式？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text"> 1.5 正则化
L1和L2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">L1 正则化
【零均值拉普拉斯分布】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">L2
正则化【零均值正态分布】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">L1 和 L2 的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">
L1正则化使得模型参数具有稀疏性的原理？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">(1)
解空间约束条件 ： KKT条件【互斥松弛条件 + 约束条件大于0】</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">(2)
函数叠加：（0点成为最值的可能)导数为0的可能性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">(3) 贝叶斯先验：分布图像</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">简单总结下：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">1.6 并行化
[对目标函数梯度计算的并行化 ？？？]</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">二、模型的对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2.1 线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2.2 最大熵模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">最大熵模型的学习(推导过程)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2.3 SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2.4 GBDT模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">三、应用场景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">四、逻辑回归 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1、线性回归和逻辑的区别与联系？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2、线性回归为什么用平方损失函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3、
为什么不用平方误差，用交叉熵损失【w
的初始化，导数值可能很小：梯度消失】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">4、
LR为什么适合离散特征【易于迭代、加快计算，简化模型和增加泛化能力】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">5、LR推导（伯努利过程，极大似然，损失函数，梯度下降）有没有最优解？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">6、LR可以用来处理非线性问题么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">
7、LR为什么用sigmoid函数，这个函数有什么优点和缺点？为什么不用其他函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">8、逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？（值越大可能性越高，但不能说是概率）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">9、手推逻辑回归目标函数，正类是1，反类是-1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">10、scikit-learn源码LR的实现？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">11、为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好，为什么把特征组合之后还能提升？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">12、Naive
bayes，SVM和logistic regression的区别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">13、为什么LR可以用来做CTR预估？【场景】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">14、了解其他的分类模型吗，问LR缺点，LR怎么推导（当时我真没准备好，写不出来）写LR目标函数，目标函数怎么求最优解（也不会）讲讲LR的梯度下降，梯度下降有哪几种，逻辑函数是啥？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">学习率？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">15、LR
如何做回归预测？【回归任务】?????</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">
16、当在高维稀疏特征的场景下，LR的效果一般会比GBDT好？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">高维稀疏特征对模型训练带来哪些影响呢？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">高维稀疏特征解决方案？</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">17、SVM和logistic回归分别在什么情况下使用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text"> 18、
什么是广义线性模型？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link"><span class="nav-text">
Bernoulli分布的指数分布族形式:</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">19、LR的训练方</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">参考文献</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">121</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1J1QH0W/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="线性模型（2）逻辑回归 | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          线性模型（2）逻辑回归
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-08 18:56:01" itemprop="dateCreated datePublished" datetime="2022-03-08T18:56:01+08:00">2022-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-20 19:08:22" itemprop="dateModified" datetime="2023-04-20T19:08:22+08:00">2023-04-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">线性模型</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>20k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>36 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><strong>Logistic Regression</strong>
虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic
Regression 因其简单、可并行化、可解释强深受工业界喜爱。<strong>Logistic
回归的本质是：假设数据服从这个Logistic分布，然后使用极大似然估计做参数的估计。</strong></p>
<p><strong>逻辑回归的思路</strong>是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</p>
<span id="more"></span>
<p><img src="https://pica.zhimg.com/v2-35393b75f51c81bb3c09774e76a7d91c_1440w.jpg?source=172ae18b" alt="【机器学习】逻辑回归（非常详细）" style="zoom:50%;"></p>
<h2><span id="一-逻辑回归深度学习的组成单元">一、逻辑回归
【深度学习的组成单元】</span></h2>
<h3><span id="11-logistic-分布位置参数-形状参数">1.1 Logistic 分布
[位置参数、形状参数]</span></h3>
<p>分布函数：<span class="math inline">\(F(x)=P(X \leq
x)=\frac{1}{1+e^{-(x-\mu) / \gamma}}\)</span>
【<strong>Softmax</strong>函数、对数几率函数】</p>
<p>密度函数：<span class="math inline">\(f(x)=F^{\prime}(X \leq
x)=\frac{e^{-(x-\mu) / \gamma}}{\gamma\left(1+e^{-(x-\mu) /
\gamma}\right)^{2}}\)</span></p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-b15289fd1162a807e11949e5396c7989_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>其中, <span class="math inline">\(\mu\)</span> 表示位置参数, <span class="math inline">\(\gamma&gt;0\)</span> 为形状参数。</p>
<p><font color="red"> Logistic
分布是由其位置和尺度参数定义的连续分布。Logistic
分布的形状与正态分布的形状相似, 但是 Logistic 分布的尾部更长,
所以我们可以使用 Logistic
分布来建模比正态分布具有更长尾部和更高波峰的数据分布。</font>在深度学习中常用到的
Sigmoid 函数就是 Logistic 的分布函数在 <span class="math inline">\(\mu=0, \gamma=1\)</span> 的特殊形式。</p>
<p>决策边界可以表示为 <span class="math inline">\(w_1 x_1+w_2
x_2+b=0\)</span> ，假设某个样本点 <span class="math inline">\(h_w(x)=w_1
x_1+w_2 x_2+b&gt;0\)</span> 那么可以判断它的 类别为 1 ,
这个过程其实是感知机。</p>
<h3><span id="12-logistic-回归">1.2 Logistic 回归</span></h3>
<p>Logistic 回归还需要加一层, 它要找到分类概率 <span class="math inline">\(P(Y=1)\)</span> 与输入向量 <span class="math inline">\(x\)</span> 的直接关系, 然后通过比较概率值来判断类
别。考虑到 <span class="math inline">\(w^T x+b\)</span> 取值是连续的,
因此它不能拟合离散变量。可以考虑用它来拟合条件概率 <span class="math inline">\(p(Y=1 \mid x)\)</span>, 因
为概率的取值也是连续的。</p>
<p>最理想的是单位阶跃函数：</p>
<p><span class="math display">\[
p(y=1 \mid x)=\left\{\begin{array}{ll}0, &amp; z&lt;0 \\ 0.5, &amp; z=0
\\ 1, &amp; z&gt;0\end{array}, \quad z=w^{T} x+b\right.
\]</span></p>
<p>但是这个阶跃函数不可微, 对数几率函数是一个常用的替代函数： <span class="math display">\[
y=\frac{1}{1+e^{-\left(w^T x+b\right)}}
\]</span></p>
<p><span class="math display">\[
\ln \frac{y}{1-y}=w^{T} x+b
\]</span></p>
<p>我们将 <span class="math inline">\(\mathrm{y}\)</span> 视为 <span class="math inline">\(\mathrm{x}\)</span> 为正例的概率, 则 1- <span class="math inline">\(\mathrm{y}\)</span> 为 <span class="math inline">\(\mathrm{x}\)</span>
为其反例的概率。两者的比值称为几率 (odds), 指该事件发生与不
发生的概率比值, 若事件发生的概率为 <span class="math inline">\(\mathrm{p}\)</span> 。则对数几率: <span class="math display">\[
\ln (o d d s)=\ln \frac{y}{1-y}
\]</span> 将 <span class="math inline">\(\mathrm{y}\)</span>
视为类后验概率估计, 重写公式有: <span class="math display">\[
\begin{aligned}
&amp; w^T x+b=\ln \frac{P(Y=1 \mid x)}{1-P(Y=1 \mid x)} \\
&amp; P(Y=1 \mid x)=\frac{1}{1+e^{-\left(w^T x+b\right)}}
\end{aligned}
\]</span></p>
<p>也就是说，<strong>输出 Y=1 的对数几率是由输入 x
的线性函数表示的模型</strong>，这就是<strong>逻辑回归模型</strong>。当
<span class="math inline">\(w^T x+b\)</span>的值越接近正无穷，<span class="math inline">\(P(Y=1 \mid x)\)</span> 概率值也就越接近
1。因此<font color="red"><strong>逻辑回归的思路是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</strong> </font></p>
<p>在这我们思考个问题，我们使用<strong>对数几率（sigmod）的意义在哪？</strong>通过上述推导我们可以看到
<strong>Logistic
回归实际上是使用线性回归模型的预测值逼近分类任务真实标记的对数几率，其优点有：</strong>【无假设分布？得到概率、易求解】</p>
<p><font color="red"> Sigmoid 函数到底起了什么作？</font></p>
<ul>
<li><p><strong>逻辑回归是在线性回归的基础上加了一个 Sigmoid
函数（非线性）映射，使得逻辑回归称为了一个优秀的分类算法。</strong>本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。</p></li>
<li><p>线性回归是在实数域范围内进行预测，而<strong>分类范围则需要在
[0,1]</strong>，逻辑回归减少了预测范围；<strong>线性回归在实数域上敏感度一致</strong>，而<strong>逻辑回归在
0 附近敏感</strong>，在远离 0
点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。</p></li>
<li><p><font color="red"><strong>无需实现假设数据分布</strong></font>：直接对<strong>分类的概率</strong>建模，从而避免了假设分布不准确带来的问题（区别于生成式模型）；</p></li>
<li><p>不仅可预测出类别，还能得到该<font color="red">
<strong>预测的概率</strong></font>，这对一些利用概率辅助决策的任务很有用；</p></li>
<li><p><font color="red"><strong>对数几率函数是任意阶可导的凸函数</strong></font>，有许多数值优化算法都可以求出最优解。</p></li>
</ul>
<h3><span id="13-代价函数">1.3 代价函数</span></h3>
<p><strong>逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。</strong>设：
<span class="math display">\[
\begin{aligned}
&amp; P(Y=1 \mid x)=p(x) \\
&amp; P(Y=0 \mid x)=1-p(x)
\end{aligned}
\]</span> 似然函数： <span class="math display">\[
L(w)=\prod\left[p\left(x_i\right)\right]^{y_i}\left[1-p\left(x_i\right)\right]^{1-y_i}
\]</span> 为了更方便求解，我们对等式两边同取对数，写成对数似然函数：
<span class="math display">\[
\begin{aligned}
L(w) &amp; =\sum\left[y_i \ln p\left(x_i\right)+\left(1-y_i\right) \ln
\left(1-p\left(x_i\right)\right)\right] \\
&amp; =\sum\left[y_i \ln
\frac{p\left(x_i\right)}{1-p\left(x_i\right)}+\ln
\left(1-p\left(x_i\right)\right)\right] \\
&amp; =\sum\left[y_i\left(w \cdot x_i\right)-\ln \left(1+e^{w \cdot
x_i}\right)\right]
\end{aligned}
\]</span> <strong><font color="red">
逻辑回归模型中，我们最大化似然函数和最小化损失函数实际上是等价的。</font></strong>我们对预测结果的概率表示取似然函数，取似然函数就是将模型对样本的概率预测值累乘起来。得到如下的似然函数由于该式比较麻烦涉及连乘法，所以我们对其去对数操作得到<strong>对数似然函数</strong>。</p>
<p><span class="math display">\[
l(\theta)=\log L(\theta)=\sum_{i=1}^m\left(y^{(i)} \log
h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log
\left(1-h_\theta\left(x^{(i)}\right)\right)\right)
\]</span></p>
<p>上述利用的是最大似然估计原理：<strong>极大似然估计就是就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大</strong>。</p>
<p>当似然函数求得最大值时，模型能够最大可能的满足当前的样本，求最大值使用梯度向上法，我们可以对似然函数加个负号，通过求等价问题的最小值来求原问题的最大值，这样我们就可以使用极大似然估计法。</p>
<p>令: <span class="math display">\[
J(\theta)=-\frac{1}{m} l(\theta)
\]</span> 这样我们就能得到损失函数的最终形式 <span class="math display">\[
J(\theta)=-\frac{1}{m} \sum_{i=1}^n\left(y^{(i)} \log
h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log
\left(1-h_\theta\left(x^{(i)}\right)\right)\right)
\]</span> 化简得： <span class="math display">\[
=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\log
\left(1+e^{\theta^T x^{(i)}}\right)\right]
\]</span></p>
<h3><span id="14-求解-梯度下降-和-牛顿法">1.4 求解【 梯度下降 和 牛顿法 】</span></h3>
<p>求解逻辑回归的方法有非常多，我们这里主要聊下<strong>梯度下降</strong>和<strong>牛顿法</strong>。优化的主要目标是找到一个方向，参数朝这个方向移动之后使得损失函数的值能够减小，这个方向往往由一阶偏导或者二阶偏导各种组合求得。<strong>逻辑回归的损失函数</strong>是：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200201527.png" alt="image-20220320172656203" style="zoom:50%;"></p>
<ul>
<li><p><strong>随机梯度下降</strong></p>
<p>梯度下降是通过 <strong>J(w) 对 w
的一阶导数来找下降方向</strong>，并且以迭代的方式来更新参数，更新方式为:</p></li>
</ul>
<p><span class="math display">\[
\begin{gathered}g_i=\frac{\partial J(w)}{\partial
w_i}=\left(p\left(x_i\right)-y_i\right) x_i \\ w_i^{k+1}=w_i^k-\alpha
g_i\end{gathered}
\]</span></p>
<p>​ 其中 k
为迭代次数。每次更新参数后，可以通过比较梯度下降小于阈值或者到达最大迭代次数来停止迭代。</p>
<blockquote>
<p>求导：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200201006.png" alt="image-20220330213027011" style="zoom:50%;"></p>
<p>这就是交叉熵对参数的导数：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200201154.png" alt="image-20220330213041006" style="zoom:50%;"></p>
</blockquote>
<ul>
<li><p><strong>牛顿法</strong></p>
<p><strong>牛顿法的基本思路是, 在现有极小点估计值的附近对 <span class="math inline">\(f(x)\)</span> 做二阶泰勒展开,
进而找到极小点的下一个估计值</strong>。假设 <span class="math inline">\(w^k\)</span> 为当前的极小值估计值，那么有: <span class="math display">\[
\varphi(w)=J\left(w^k\right)+J^{\prime}\left(w^k\right)\left(w-w^k\right)+\frac{1}{2}
J^{\prime \prime}\left(w^k\right)\left(w-w^k\right)^2
\]</span> 然后令 <span class="math inline">\(\varphi^{\prime}(w)=0\)</span>, 得到了 <span class="math inline">\(w^{k+1}=w^k-\frac{J^{\prime}\left(w^k\right)}{J^{\prime
\prime}\left(w^k\right)}\)</span> 。因此有迭代更新式: <span class="math display">\[
w^{k+1}=w^k-\frac{J^{\prime}\left(w^k\right)}{J^{\prime
\prime}\left(w^k\right)}=w^k-H_k^{-1} \cdot g_k
\]</span> 其中 <span class="math inline">\(H_k^{-1}\)</span> 为海森矩阵:
<span class="math display">\[
H_{m n}=\frac{\partial^2 J(w)}{\partial w_m \partial
w_n}=h_w\left(x^{(i)}\right)\left(1-p_w\left(x^{(i)}\right)\right)
x_m^{(i)} x_n^{(i)}
\]</span> 此外, 这个方法需要目标函数是二阶连续可微的, 本文中的 <span class="math inline">\(J(w)\)</span> 是符合要求的。</p></li>
<li><p><strong>拟牛顿法</strong>：为了避免存储海塞矩阵的逆矩阵</p>
<p>通过拟合的方式找到一个近似的海塞矩阵，拟牛顿法的可行性在于严格的近似方法，只要不是差的特别远就能起到相应下调效果。</p>
<blockquote>
<p>DFP、BFGS、L-BFGS:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200244353.png" alt="image-20220509171547743" style="zoom:50%;"></p>
</blockquote>
<h4><span id="lr-l1-的梯度下降方式">LR-L1 的梯度下降方式？</span></h4>
<p>OWL-QN：用L-BFCS求解L1正则化的LR</p></li>
</ul>
<h3><span id="15-正则化l1和l2"><strong><font color="red"> 1.5 正则化
L1和L2</font></strong></span></h3>
<blockquote>
<ul>
<li><strong>本质其实是为了模型参数服从某一分布</strong>；</li>
<li><strong>正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现；</strong></li>
</ul>
<p><strong>为什么希望参数具有稀疏性？</strong></p>
<p>相当于对模型进行了一次特征选择，只留下比较重要的特征，提高模型的泛化能力；</p>
</blockquote>
<p><strong><font color="red">
正则化是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。</font></strong>如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。</p>
<p>正则化一般会采用 L1 范式或者 L2 范式, 其形式分别为 <span class="math inline">\(\Phi(w)=\|x\|_1\)</span> 和 <span class="math inline">\(\Phi(w)=\|x\|_2 。\)</span></p>
<p><img src="https://pic3.zhimg.com/80/v2-a352bc374e80df1299a4d63d39ce4606_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h4><span id="l1-正则化零均值拉普拉斯分布"><strong>L1 正则化</strong>
【零均值拉普拉斯分布】</span></h4>
<p><strong>LASSO 回归, 相当于为模型添加了这样一个先验知识</strong>：
<span class="math inline">\(\mathbf{w}\)</span>
服从<strong>零均值拉普拉斯分布</strong>。首先看看拉普拉斯分布长什
么样子: <span class="math display">\[
f(w \mid \mu, b)=\frac{1}{2 b} \exp \left(-\frac{|w-\mu|}{b}\right)
\]</span> 由于引入了先验知识, 所以似然函数这样写: <span class="math display">\[
\begin{aligned}
L(w) &amp; =P(y \mid w, x) P(w) \\
&amp; =\prod_{i=1}^N
p\left(x_i\right)^{y_i}\left(1-p\left(x_i\right)\right)^{1-y_i}
\prod_{j=1}^d \frac{1}{2 b} \exp
\left(-\frac{\left|w_j\right|}{b}\right)
\end{aligned}
\]</span> 取 <span class="math inline">\(\log\)</span>
再取负，得到目标函数: <span class="math display">\[
-\ln L(w)=-\sum_i\left[y_i \ln p\left(x_i\right)+\left(1-y_i\right) \ln
\left(1-p\left(x_i\right)\right)\right]+\frac{1}{2 b^2}
\sum_j\left|w_j\right|
\]</span> 等价于原始损失函数的后面加上了 L1 正则, 因此 L1
正则的本质其实是为模型增加了“模型参数服从零均值拉普拉
斯分布"这一先验知识。</p>
<h4><span id="l2正则化零均值正态分布"><strong>L2
正则化</strong>【零均值正态分布】</span></h4>
<p>Ridge 回归, 相当于为模型添加了这样一个先验知识： <span class="math inline">\(w\)</span> 服从<strong>零均值正态分布</strong>。
首先看看正态分布长什么样子: <span class="math display">\[
f(w \mid \mu, \sigma)=\frac{1}{\sqrt{2 \pi} \sigma} \exp
\left(-\frac{(w-\mu)^2}{2 \sigma^2}\right)
\]</span> 由于引入了先验知识, 所以似然函数这样写: <span class="math display">\[
\begin{aligned}
L(w) &amp; =P(y \mid w, x) P(w) \\
&amp; =\prod_{i=1}^N
p\left(x_i\right)^{y_i}\left(1-p\left(x_i\right)\right)^{1-y_i}
\prod_{j=1}^d \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{w_j^2}{2
\sigma^2}\right) \\
&amp; =\prod_{i=1}^N
p\left(x_i\right)^{y_i}\left(1-p\left(x_i\right)\right)^{1-y_i}
\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{w^T w}{2
\sigma^2}\right)
\end{aligned}
\]</span> 取 In 再取负，得到目标函数: <span class="math display">\[
-\ln L(w)=-\sum_i\left[y_i \ln p\left(x_i\right)+\left(1-y_i\right) \ln
\left(1-p\left(x_i\right)\right)\right]+\frac{1}{2 \sigma^2} w^T w
\]</span> 等价于原始的损失函数后面加上了 <span class="math inline">\(L
2\)</span> 正则, 因此 <span class="math inline">\(L 2\)</span>
正则的本质其实是为模型增加了““<strong>模型参数服从零均值正态分
布</strong>"这一先验知识。</p>
<h4><span id="l1-和-l2-的区别">L1 和 L2 的区别</span></h4>
<blockquote>
<ul>
<li><strong><font color="red"> 解空间约束条件 ： KKT条件【互斥松弛条件 +
约束条件大于0】</font></strong></li>
<li><strong><font color="red">
函数叠加：（0点成为最值的可能）导数为0的可能性</font></strong></li>
<li><strong><font color="red"> 贝叶斯先验：分布图像</font></strong></li>
</ul>
</blockquote>
<p><strong>L1 正则化</strong>增加了所有权重 w
参数的绝对值之和<strong>逼迫更多 w 为零</strong>，也就是变稀疏（ L2
因为其导数也趋 0, 奔向零的速度不如 L1
给力了）。对<strong>稀疏规则趋之若鹜</strong>的一个关键原因在于它能<strong>实现特征的自动选择</strong>。L1
正则化的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为
0。</p>
<p><strong>L2 正则化</strong>中增加所有权重 w 参数的平方之和，逼迫所有
<strong>w 尽可能趋向零但不为零</strong>（L2 的导数趋于零）。因为在未加入
L2
正则化发生过拟合时，拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的变化很剧烈，也就是某些
w 值非常大。为此，L2 正则化的加入就惩罚了权重变大的趋势。</p>
<h4><span id="l1正则化使得模型参数具有稀疏性的原理"><strong><font color="red">
L1正则化使得模型参数具有稀疏性的原理？</font></strong></span></h4>
<h5><span id="1解空间约束条件-kkt条件互斥松弛条件-约束条件大于0">(1)
解空间约束条件 ： KKT条件【互斥松弛条件 + 约束条件大于0】</span></h5>
<p><strong>KKT
条件是指优化问题在最优处（包括基本型的最优值，对偶问题的最优值）必须满足的条件</strong>。</p>
<blockquote>
<p><strong>参考线性支持向量机的 KKT 条件:</strong></p>
<ul>
<li><strong>主问题可行</strong>: <span class="math inline">\(g_{i}\left(u^{\star}\right)=1-y_{i}\left(w^{\star
\top} x_{i}+b^{\star}\right) \leq 0\)</span> ；</li>
<li><strong><font color="red"> 对偶问题可行: <span class="math inline">\(\alpha_{i}^{\star} \geq
0\)</span></font></strong>;</li>
<li><strong>主变量最优</strong>: <span class="math inline">\(w^{\star}=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i},
\sum_{i=1}^{m} \alpha_{i} y_{i}=0\)</span>;</li>
<li><strong><font color="red"> 互补松弛: <span class="math inline">\(\alpha_{i}^{\star}
g_{i}\left(u^{\star}\right)=\alpha_{i}^{\star}\left(1-y_{i}\left(w^{\star
\top} x_{i}+b^{\star}\right)\right)=0\)</span> ；</font></strong></li>
</ul>
</blockquote>
<p><strong>原函数曲线等高线（同颜色曲线上，每一组<span class="math inline">\(w_1,w_2\)</span>带入后值都相同)</strong>：</p>
<p><img src="https://pic4.zhimg.com/80/v2-efc752bd6d1ce09dbf2e18b9766570eb_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>当加入 L1 正则化的时候, 我们先画出 <span class="math inline">\(\left|w_1\right|+\left|w_2\right|=F\)</span>
的图像, 也就是一个菱形, 代表这些曲线上的点算出来的
要使得这个菱形越小越好 ( <span class="math inline">\(F\)</span>
越小越好)。那么还和原来一样的话, 过中心紫色圈圈的那个菱形明显很大,
因此我 们要取到一个恰好的值。那么如何求值呢?</p>
<ol type="1">
<li>以同一条原曲线目标等高线来说, 现在以最外圈的红色等高线为例,
我们看到, 对于红色曲线上的每个点都可 做一个菱形, 根据上图可知,
当这个菱形与某条等高线相切（仅有一个交点）的时候, 这个菱形最小, 上图相
割对比较大的两个菱形对应的 L1 范数更大。用公式说这个时候能使得在相同的
<span class="math inline">\(\frac{1}{N} \sum_{i=1}^N\left(y_i-w^T
x_i\right)^2\)</span>, 由于 相切的时候的 <span class="math inline">\(C||
w \|_1\)</span> 小, 即 <span class="math inline">\(\left|w_1\right|+\left|w_2\right|\)</span>
所以能够使得 <span class="math inline">\(\frac{1}{N} \sum
i=1^N\left(y_i-w^T x_i\right)^2+C|| w \|_1\)</span> 更小;</li>
<li>有了第一条的说明我们可以看出, 最终加入 <span class="math inline">\(L
1\)</span> 范数得到的解一定是某个菱形和某条原函数等高线的切点。现
在有个比较重要的结论来了, <font color="red"> 我们经过观察可以看到,
几乎对于很多原函数等高曲线, 和某个菱形相交的时 候及其容易相交在坐标轴
(比如上图) , 也就是说最终的结果, 解的某些维度及其容易是 0,
比如上图最终解 是 <span class="math inline">\(w=(0, x)\)</span>
，这也就是我们所说的 L1 更容易得到稀疏解（解向量中 0
比较多)的原因;</font></li>
<li>当加入 <span class="math inline">\(L 2\)</span> 正则化的时候, 分析和
<span class="math inline">\(L 1\)</span> 正则化是类似的,
也就是说我们仅仅是从菱形变成了圆形而已, 同样还
是求原曲线和圆形的切点作为最终解。<strong>当然与 <span class="math inline">\(L 1\)</span> 范数比, 我们这样求的 <span class="math inline">\(L 2\)</span> 范数的从图上来看, 不容易交在
坐标轴上, 但是仍然比较靠近坐标轴。因此这也就是我们老说的, L2
范数能让解比较小 (靠近 0), 但是比 较平滑（不等于 0)。</strong></li>
</ol>
<h5><span id="2函数叠加0点成为最值的可能导数为0的可能性">(2)
函数叠加：（0点成为最值的可能)导数为0的可能性</span></h5>
<p>我们接下来从更严谨的方式来证明,
简而言之就是假设现在我们是一维的情况下 <span class="math inline">\(h(w)=f(w)+C|w|\)</span>, 其中 <span class="math inline">\(h(w)\)</span> 是目标函数, <span class="math inline">\(f(w)\)</span> 是没加 <span class="math inline">\(\mathrm{L} 1\)</span> 正则化项前的目标函数, <span class="math inline">\(C|w|\)</span> 是 <span class="math inline">\(\mathrm{L}\)</span> 正则项, 要使得 0
点成为最值可能的点, <strong>虽然在 0 点不可导, 但是我们只需要让 0
点左右的导数异号, 即 <span class="math inline">\(h_l^{\prime}(0)
h_r^{\prime}(0)=\left(f^{\prime}(0)+C\right)\left(f^{\prime}(0)-C\right)&lt;0\)</span>
即可也就 是 <span class="math inline">\(C&gt;\left|f^{\prime}(0)\right|\)</span> 的情况下,
0 点都是可能的最值点</strong>。相反, L2正则项在原点处的导数是0,
只要原目标函数在原点 处导数不为 0 , 那么最小值点就不会在原点, 所以 <span class="math inline">\(L 2\)</span> 只有减小w最对值的作用,
对解空间的稀疏性没有贡献。</p>
<h5><span id="3-贝叶斯先验分布图像">(3) 贝叶斯先验：分布图像</span></h5>
<p><img src="https://pic3.zhimg.com/80/v2-a352bc374e80df1299a4d63d39ce4606_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>从贝叶斯加入先验分布的角度解释，L1正则化相当于对模型参数引入拉普拉斯先验，L2正则化相当于与引入了高斯先验。高斯分布在0点是平滑的，拉普拉斯在0点处是一个尖峰。</strong></p>
<h4><span id="简单总结下"><strong>简单总结下：</strong></span></h4>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 45%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>正则化</th>
<th>L1 正则化</th>
<th>L2 正则化</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>服从分布</td>
<td>零均值拉普拉斯分布</td>
<td>零均值正态分布</td>
</tr>
<tr class="even">
<td>损失函数变化</td>
<td><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200216158.png" alt="image-20220316144906846" style="zoom:50%;"></td>
<td><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200217870.png" alt="image-20220316145437180" style="zoom:50%;"></td>
</tr>
<tr class="odd">
<td>模型参数w效果</td>
<td>逼迫更多 w 为零，【稀疏解】<strong><font color="red">
（解空间约束条件、函数叠加、贝叶斯先验）</font></strong></td>
<td>趋向零但不为零【平滑】</td>
</tr>
<tr class="even">
<td>作用</td>
<td>【特征选择】【降低模型复杂度】</td>
<td>【降低模型复杂度】</td>
</tr>
</tbody>
</table>
<h3><span id="16-并行化对目标函数梯度计算的并行化">1.6 并行化
[对目标函数梯度计算的并行化 ？？？]</span></h3>
<blockquote>
<p><strong>面试题解答8：逻辑斯蒂回归的并行化计算方法 - 舟晓南的文章 -
知乎</strong> https://zhuanlan.zhihu.com/p/447482293</p>
</blockquote>
<p>从逻辑回归的求解方法中我们可以看到，无论是随机梯度下降还是牛顿法，或者是没有提到的拟牛顿法，都是需要计算梯度的，因此<strong>逻辑回归的并行化最主要的就是对目标函数梯度计算的并行化</strong>。</p>
<p>我们看到目标函数的梯度向量计算中只需要进行向量间的点乘和相加，可以很容易将每个迭代过程拆分成相互独立的计算步骤，由不同的节点进行独立计算，然后归并计算结果。</p>
<ul>
<li>梯度下降是通过 <strong>J(w) 对 w
的一阶导数来找下降方向</strong>，并且以迭代的方式来更新参数，更新方式为:</li>
</ul>
<p><span class="math display">\[
\begin{gathered}
g_i=\frac{\partial J(w)}{\partial
w_i}=\left(p\left(x_i\right)-y_i\right) x_i \\
w_i^{k+1}=w_i^k-\alpha g_i
\end{gathered}
\]</span> 其中 <span class="math inline">\(\mathrm{k}\)</span>
为迭代次数。每次更新参数后, 可以通过比较 <span class="math inline">\(\left\|J\left(w^{k+1}\right)-J\left(w^k\right)\right\|\)</span>
小于阈值或者到达最大迭代次数来 停止迭代。</p>
<h2><span id="二-模型的对比">二、模型的对比</span></h2>
<h3><span id="21-线性回归">2.1 线性回归</span></h3>
<p><strong>逻辑回归是在线性回归的基础上加了一个 Sigmoid
函数（非线形）映射</strong>，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是<strong>分类问题</strong>，输出的是<strong>离散值</strong>，线性回归解决的是<strong>回归问题</strong>，输出的<strong>连续值</strong>。</p>
<p><strong>我们需要明确 Sigmoid 函数到底起了什么作用？</strong></p>
<ul>
<li>线性回归是在实数域范围内进行预测，而分类范围则需要在
[0,1]，<strong>逻辑回归减少了预测范围</strong>；</li>
<li>线性回归在实数域上敏感度一致，而<strong>逻辑回归在 0
附近敏感</strong>，在远离 0
点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。</li>
</ul>
<h3><span id="22-最大熵模型">2.2 <strong>最大熵模型</strong></span></h3>
<p>逻辑回归和最大熵模型本质上没有区别，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。<strong>最大熵原理是概率模型学习的一个准则。其认为在学习概率模型时，熵最大的模型是最好的模型。也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</strong></p>
<h4><span id="最大熵模型的学习推导过程">最大熵模型的学习(推导过程)</span></h4>
<p>最大熵模型推导 - Eccc的文章 - 知乎
https://zhuanlan.zhihu.com/p/47988480</p>
<h3><span id="23-svm">2.3 SVM</span></h3>
<p><strong>相同点：</strong></p>
<ul>
<li><strong>都是分类算法，本质上都是在找最佳分类超平面</strong>；</li>
<li>都是<strong>监督学习算法</strong>；</li>
<li>都是<strong>判别式模型</strong>，判别模型不关心数据是怎么生成的，它只关心数据之间的差别，然后用差别来简单对给定的一个数据进行分类；</li>
<li>都可以增加不同的正则项。</li>
</ul>
<p><strong>不同点：</strong></p>
<ul>
<li>LR 是一个<strong>统计</strong>的方法，SVM
是一个<strong>几何</strong>的方法；</li>
<li>SVM 的处理方法是只考虑 Support
Vectors，也就是<strong>和分类最相关的少数点去学习分类器</strong>。而逻辑回归通过非线性映射减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重；</li>
<li><strong>损失函数</strong>不同：<strong>LR
的损失函数是交叉熵</strong>，<strong>SVM 的损失函数是
HingeLoss</strong>，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。对
HingeLoss
来说，其零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这是支持向量机最大的优势所在，对训练样本数目的依赖大减少，而且提高了训练效率；</li>
<li><strong>LR 是参数模型，SVM
是非参数模型</strong>，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以
LR 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM
不直接依赖于分布；</li>
<li>LR 可以产生概率，SVM 不能；</li>
<li>LR 不依赖样本之间的距离，SVM 是基于距离的；</li>
<li>LR
相对来说模型更简单好理解，特别是大规模线性分类时并行计算比较方便。而 SVM
的理解和优化相对来说复杂一些，<strong>SVM
转化为对偶问题后，分类只需要计算与少数几个支持向量的距离</strong>，这个<strong>在进行复杂核函数计算时优势很明显</strong>，能够大大简化模型和计算。</li>
</ul>
<h3><span id="24-gbdt模型">2.4 GBDT模型</span></h3>
<p>先说说LR和GBDT的区别：</p>
<ul>
<li>LR是线性模型，<strong>可解释性强</strong>，<strong>很容易并行化</strong>，但学习能力有限，<strong>需要大量的人工特征工程；</strong></li>
<li>GBDT是非线性模型，具有天然的<strong>特征组合优势</strong>，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；</li>
</ul>
<p>当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。</p>
<h2><span id="三-应用场景">三、应用场景</span></h2>
<ul>
<li>CTR预估/推荐系统的learning to rank/各种分类场景。</li>
<li>某搜索引擎厂的广告CTR预估基线版是LR。</li>
<li>某电商搜索排序/广告CTR预估基线版是LR。</li>
<li>某电商的购物搭配推荐用了大量LR。</li>
<li>某现在一天广告赚1000w+的新闻app排序基线是LR。</li>
</ul>
<h2><span id="四-逻辑回归-qampa">四、逻辑回归 Q&amp;A</span></h2>
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/100763009">【机器学习面试总结】——
LR（逻辑回归）</a></li>
</ul>
</blockquote>
<h4><span id="1-线性回归和逻辑的区别与联系">1、线性回归和逻辑的区别与联系？</span></h4>
<p><strong>逻辑回归是在线性回归的基础上加了一个 Sigmoid
函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。</strong>本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。</p>
<table>
<colgroup>
<col style="width: 11%">
<col style="width: 44%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>算法</th>
<th>线性回归</th>
<th>逻辑回归</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>思想</td>
<td><strong>线性回归假设特征和结果满足线性关系</strong>；每个特征的强弱可以由参数体现。高斯分布</td>
<td><strong>逻辑回归</strong>是一个假设样本服从<strong>伯努利分布</strong>，利用极大似然估计和梯度下降求解的二分类模型，在分类、CTR预估领域有着广泛的应用。</td>
</tr>
<tr class="even">
<td>模型假设</td>
<td><strong><font color="red"> 线性回归假设 <span class="math inline">\(y\)</span> 的残差 <span class="math inline">\(\varepsilon\)</span> 服从正态分布 <span class="math inline">\(N\left(\mu,
\sigma^{2}\right)\)</span></font></strong></td>
<td><strong><font color="red"> 逻辑回归假设 <span class="math inline">\(y\)</span> 服从伯努利分布
(Bernoulli)</font></strong></td>
</tr>
<tr class="odd">
<td>应用场景</td>
<td>回归问题</td>
<td>分类问题，【非线性问题】</td>
</tr>
<tr class="even">
<td>目标函数【原因】</td>
<td><span class="math inline">\(J(\theta)=\frac{1}{2}
\sum_{i=1}^n\left(\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)\right)^2\)</span>
【<strong>平方损失函数</strong>】</td>
<td><span class="math inline">\(J(\theta)=-\frac{1}{m}
\sum_{i=1}^n\left(y^{(i)} \log
h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log
\left(1-h_\theta\left(x^{(i)}\right)\right)\right)\)</span>【<strong>交叉熵损失函数</strong>】</td>
</tr>
<tr class="odd">
<td>参数估计</td>
<td><strong>最小二乘法</strong>、梯度下降</td>
<td><strong>梯度下降</strong>、牛顿法</td>
</tr>
<tr class="even">
<td>并行化</td>
<td>无</td>
<td>对把目标函数<strong>梯度计算</strong>的并行</td>
</tr>
<tr class="odd">
<td>样本分布</td>
<td><strong>高斯分布</strong></td>
<td><strong>伯努利分布</strong></td>
</tr>
<tr class="even">
<td>优势</td>
<td></td>
<td><strong>对数几率</strong>：无假设分布？得到概率、<strong>易求解</strong></td>
</tr>
</tbody>
</table>
<h4><span id="2-线性回归为什么用平方损失函数">2、线性回归为什么用平方损失函数？</span></h4>
<p><strong>中心极限定理 + 高斯分布 + 最大似然估计</strong>
推导出来的待优化的目标函数与<strong>平方损失函数</strong>是等价的。因此可以得出结论：<strong>线性回归误差平方损失极小化与极大似然估计等价</strong>。其实在概率模型中，目标函数的原函数（或对偶函数）极小化（或极大化）与极大似然估计等价，这是一个带有普遍性的结论。比如在最大熵模型中，有对偶函数极大化与极大似然估计等价的结论。</p>
<h4><span id="3-为什么不用平方误差用交叉熵损失w的初始化导数值可能很小梯度消失">3、<strong><font color="red">
为什么不用平方误差，用交叉熵损失【w
的初始化，导数值可能很小：梯度消失】</font></strong></span></h4>
<p>假设目标函数是 <strong>MSE</strong>，即： <span class="math display">\[
\begin{gathered}
L=\frac{(y-\hat{y})^2}{2} \\
\frac{\partial L}{\partial w}=(\hat{y}-y) \sigma^{\prime}(w \cdot x) x
\end{gathered}
\]</span> 这里 Sigmoid 的导数项为: <span class="math display">\[
\sigma^{\prime}(w \cdot x)=w \cdot x(1-w \cdot x)
\]</span> 根据 <strong>w 的初始化，导数值可能很小</strong>（想象一下
Sigmoid
函数在输入较大时的梯度）而导致收敛变慢，而训练途中也可能因为该值过小而提早终止训练（<strong>梯度消失</strong>）。</p>
<p><strong>交叉熵</strong>的梯度如下，当模型输出概率偏离于真实概率时，梯度较大，加快训练速度，当拟合值接近于真实概率时训练速度变缓慢，<strong>没有
MSE 的问题</strong>。 <span class="math display">\[
g^{\prime}=\sum_{i=1}^N x_i\left(y_i-p\left(x_i\right)\right)
\]</span></p>
<ul>
<li>为什么不用平方损失函数？</li>
<li>交叉熵损失函数原理？</li>
</ul>
<h4><span id="4-lr为什么适合离散特征易于迭代-加快计算简化模型和增加泛化能力">4、<strong><font color="red">
LR为什么适合离散特征【易于迭代、加快计算，简化模型和增加泛化能力】</font></strong></span></h4>
<p>我们在使用逻辑回归的时候很少会把数据直接丢给 LR
来训练，我们一般会对特征进行离散化处理，这样做的优势大致有以下几点：</p>
<ol type="1">
<li><strong>离散特征的增加和减少都很容易，易于模型的快速迭代</strong>；</li>
<li><strong>离散后稀疏向量内积乘法运算速度更快</strong>，计算结果也方便存储，容易扩展；</li>
<li>离散后的特征<strong>对异常值更具鲁棒性</strong>，如 age&gt;30 为 1
否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；</li>
<li>LR
属于广义线性模型，表达能力有限，经过离散化后，<strong>每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；</strong></li>
<li>离散后特征可以<strong>进行特征交叉</strong>，提升表达能力，由 M+N
个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；</li>
<li>特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化；</li>
<li>特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险；</li>
</ol>
<p><strong><font color="red">
总的来说，特征离散化以后起到了易于迭代、加快计算，简化模型和增加泛化能力的作用。</font></strong></p>
<h4><span id="5-lr推导伯努利过程极大似然损失函数梯度下降有没有最优解">5、LR推导（伯努利过程，极大似然，损失函数，梯度下降）有没有最优解？</span></h4>
<h4><span id="6-lr可以用来处理非线性问题么">6、LR可以用来处理非线性问题么？</span></h4>
<p>引入kernel：</p>
<p>对特征先进行多项式转换：</p>
<h4><span id="7-lr为什么用sigmoid函数这个函数有什么优点和缺点为什么不用其他函数"><strong><font color="red">
7、LR为什么用sigmoid函数，这个函数有什么优点和缺点？为什么不用其他函数？</font></strong></span></h4>
<p><strong>Sigmoid函数由那个指数族分布，加上二项分布导出来的。损失函数是由最大似然估计求出的。</strong></p>
<blockquote>
<p>逻辑回归认为函数其概率服从伯努利分布,
将其写成指数族分布的形式。因为指数族分布是给定某些统计量
下熵最大的分布，例如伯努利分布就是只有两个取值且给定期望值为 <span class="math inline">\(\mu\)</span> 下的樀最大的分布。 也就是： <span class="math display">\[
  \begin{aligned}
  p(y ; \phi) &amp; =\phi^y(1-\phi)^{1-y} \\
  &amp; =\exp (y \log \phi+(1-y) \log (1-\phi)) \\
  &amp; =\exp \left(\left(\log \left(\frac{\phi}{1-\phi}\right)\right)
y+\log (1-\phi)\right)
  \end{aligned}
  \]</span> 符合: <span class="math inline">\(\quad p(y ; \eta)=b(y)
\exp \left(\eta^T T(y)-\alpha(\eta)\right)\)</span> 其中: <span class="math inline">\(\quad \alpha(\eta)=-\log (1-\phi)\)</span>
能够推导出sigmoid函数的形式。 <span class="math display">\[
  \eta=\log \left(\frac{\phi}{1-\phi}\right)
  \]</span> <span class="math display">\[
  \phi=\frac{e^\eta}{1+e^\eta}
  \]</span></p>
</blockquote>
<p><strong>Sigmoid 函数到底起了什么作用？</strong></p>
<ul>
<li><p><strong>逻辑回归是在线性回归的基础上加了一个 Sigmoid
函数（非线性）映射，使得逻辑回归称为了一个优秀的分类算法。</strong>本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。</p></li>
<li><p>线性回归是在实数域范围内进行预测，而<strong>分类范围则需要在
[0,1]</strong>，逻辑回归减少了预测范围；<strong>线性回归在实数域上敏感度一致</strong>，而<strong>逻辑回归在
0 附近敏感</strong>，在远离 0
点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。</p></li>
<li><p><font color="red"><strong>无需实现假设数据分布</strong></font>：直接对<strong>分类的概率</strong>建模，从而避免了假设分布不准确带来的问题（区别于生成式模型）；</p></li>
<li><p>不仅可预测出类别，还能得到该<font color="red">
<strong>预测的概率</strong></font>，这对一些利用概率辅助决策的任务很有用；</p></li>
<li><p><font color="red"><strong>对数几率函数是任意阶可导的凸函数</strong></font>，有许多数值优化算法都可以求出最优解。</p></li>
</ul>
<h4><span id="8-逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗值越大可能性越高但不能说是概率">8、逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？（值越大可能性越高，但不能说是概率）</span></h4>
<p>https://blog.csdn.net/zhaojc1995/article/details/114462504</p>
<ul>
<li><p><strong><font color="red">
不是概率，概率是可以做加减乘除计算的，但softmax只能比较大小，不能用于计算。</font></strong></p></li>
<li><p>并不完全等同。在多分类中，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=softmax&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%221771152146%22%7D">softmax</a>最终输出的数值，会和<strong>某个类别的样本数有关</strong>。因此它不能直接用作概率，但可以粗略的认为是与概率类似的东西。</p></li>
</ul>
<h4><span id="9-手推逻辑回归目标函数正类是1反类是-1">9、手推逻辑回归目标函数，正类是1，反类是-1</span></h4>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304200223146.png" alt="image-20220401164913393" style="zoom:50%;"></p>
<h4><span id="10-scikit-learn源码lr的实现">10、scikit-learn源码LR的实现？</span></h4>
<h4><span id="11-为什么lr需要归一化或者取对数为什么lr把特征离散化后效果更好为什么把特征组合之后还能提升">11、为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好，为什么把特征组合之后还能提升？</span></h4>
<ul>
<li><p><strong>为什么LR需要归一化或者取对数?</strong>
符合假设、利于分析、归一化也有利于梯度下降。</p></li>
<li><p>LR 特征离散化</p></li>
</ul>
<h4><span id="12-naivebayessvm和logistic-regression的区别">12、Naive
bayes，SVM和logistic regression的区别？</span></h4>
<h4><span id="13-为什么lr可以用来做ctr预估场景">13、为什么LR可以用来做CTR预估？【场景】</span></h4>
<p><strong>简单应用</strong>
预测用户在未来某个时间段是否会购买某个品类，如果把会购买标记为1，不会购买标记为0，就转换为一个二分类问题。我们用到的特征包括用户在美团的浏览，购买等历史信息，见下表</p>
<figure>
<img src="https://pic1.zhimg.com/v2-1d4dd37a5e63e40fb4c5a25a6067e7dc_b.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>其中提取的特征的时间跨度为30天，标签为2天。生成的训练数据大约在7000万量级（美团一个月有过行为的用户），我们人工把相似的小品类聚合起来，最后有18个较为典型的品类集合。如果用户在给定的时间内购买某一品类集合，就作为正例。哟了训练数据后，使用Spark版的LR算法对每个品类训练一个二分类模型，迭代次数设为100次的话模型训练需要40分钟左右，平均每个模型2分钟，测试集上的AUC也大多在0.8以上。训练好的模型会保存下来，用于预测在各个品类上的购买概率。预测的结果则会用于推荐等场景。</p>
<p>由于不同品类之间正负例分布不同，有些品类正负例分布很不均衡，我们还尝试了不同的采样方法，最终目标是提高下单率等线上指标。经过一些参数调优，品类偏好特征为推荐和排序带来了超过1%的下单率提升。</p>
<p>此外，由于LR模型的简单高效，易于实现，可以为后续模型优化提供一个不错的baseline，我们在排序等服务中也使用了LR模型。</p>
<h4><span id="14-了解其他的分类模型吗问lr缺点lr怎么推导当时我真没准备好写不出来写lr目标函数目标函数怎么求最优解也不会讲讲lr的梯度下降梯度下降有哪几种逻辑函数是啥">14、了解其他的分类模型吗，问LR缺点，LR怎么推导（当时我真没准备好，写不出来）写LR目标函数，目标函数怎么求最优解（也不会）讲讲LR的梯度下降，梯度下降有哪几种，逻辑函数是啥？</span></h4>
<p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行<strong>梯度下降</strong>来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有<strong>随机梯度下降</strong>，<strong>批梯度下降</strong>，<strong>small
batch
梯度下降</strong>三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。</p>
<p>（1）<strong>批梯度下降</strong>会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
（2）<strong>随机梯度下降</strong>是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
（3）<strong><font color="red"> 小批量梯度下降结合了sgd和batch
gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</font></strong></p>
<table style="width:100%;">
<colgroup>
<col style="width: 11%">
<col style="width: 20%">
<col style="width: 51%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>梯度下降</th>
<th>批梯度下降</th>
<th>随机梯度下降</th>
<th>小批量梯度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>样本</td>
<td>遍历所有的数据</td>
<td>1个样本</td>
<td>使用n个样本</td>
</tr>
<tr class="even">
<td>解</td>
<td>全局最优解</td>
<td>sgd会跳到新的和潜在更好的局部最优解</td>
<td>局部最优解</td>
</tr>
</tbody>
</table>
<p><strong>其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。</strong></p>
<h4><span id="学习率">学习率？</span></h4>
<p>（1）<strong>第一个是如何对模型选择合适的学习率</strong>。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。
（2）<strong>第二个是如何对参数选择合适的学习率</strong>。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。</p>
<h4><span id="15-lr如何做回归预测回归任务"><strong>15、LR
如何做回归预测？【回归任务】</strong>?????</span></h4>
<p>logistic regression是这么假设的：数据服从概率为p的二项分布，并且
logit(p)是特征的线性组合。</p>
<p>二项分布的取值就是两个，0,1，所以如果不修改假设，<strong>直接就把logistic
regression用于连续值的预测肯定是不合理的</strong>，因为没有哪个正常的连续取值的东西是服从二项分布的……</p>
<h4><span id="16-当在高维稀疏特征的场景下lr的效果一般会比gbdt好"><strong><font color="red">
16、当在高维稀疏特征的场景下，LR的效果一般会比GBDT好？</font></strong></span></h4>
<blockquote>
<p>高维稀疏特征 - 黄志超的文章 - 知乎
https://zhuanlan.zhihu.com/p/271055971</p>
<p>LR,gbdt,libfm这三种模型分别适合处理什么类型的特征,为了取得较好效果他们对特征有何要求？
- 凯菜的回答 - 知乎
https://www.zhihu.com/question/35821566/answer/225291663</p>
</blockquote>
<ul>
<li><p><strong>高维稀疏特征</strong>：向量表示了一个样本（或一个数据点）的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=特征向量&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22271055971%22%7D">特征向量</a>
，我们可以看到向量中存在着许多0，或者说0的数量&gt;&gt;其他值的数量。</p>
<ul>
<li><strong>产生的原因</strong>：数据特征的缺失；<strong>数据不恰当的处理（过多类别特征的one-hot）</strong></li>
</ul></li>
<li><h5><span id="高维稀疏特征对模型训练带来哪些影响呢">高维稀疏特征对模型训练带来哪些影响呢？</span></h5>
<ul>
<li><strong>LR</strong>：低维的稠密特征映射到了高维空间，低维线性不可分就转到高维空间；LR的目标就是找到一个<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=超平面&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22271055971%22%7D">超平面</a>对样本是的正负样本位于两侧，<strong>由于这个模型够简单，不会出现gbdt上过拟合的问题。</strong></li>
<li><strong>GBDT</strong>：树模型是以列为单位切分样本集的，当稀疏时，每一列的特征就会存在大量的0，切分的意义不大，且特征过于细时，容易过拟合<strong>（如年龄每一岁占一列特征）</strong></li>
</ul></li>
<li><h5><span id="高维稀疏特征解决方案">高维稀疏特征解决方案？</span></h5>
<ul>
<li><strong>embedding</strong></li>
<li>FM</li>
</ul></li>
</ul>
<h4><span id="17-svm和logistic回归分别在什么情况下使用">17、SVM和logistic回归分别在什么情况下使用？</span></h4>
<p><strong>n是feature的数量 m是样本数；</strong></p>
<p>1、如果n相对于m来说很大（n远大于m，n=10000，m=10-1000），则使用LR算法或者不带核函数的SVM（线性分类）</p>
<p>2、如果n很小，m的数量适中（n=1-1000，m=10-10000）：使用带有核函数的SVM算法</p>
<p>3、如果n很小，m很大（n=1-1000，m=50000+）：增加更多的feature然后使用LR算法或者不带核函数的SVM</p>
<h4><span id="18-什么是广义线性模型"><strong><font color="red"> 18、
什么是广义线性模型？</font></strong></span></h4>
<p><strong>线性回归和逻辑回归都是广义线性模型的一种特殊形式</strong>，介绍广义线性模型的一般求解步骤。
利用广义线性模型推导 出 <strong>多分类的Softmax
Regression</strong>。</p>
<p><strong>线性回归中我们假设：</strong></p>
<p><img src="https://pic2.zhimg.com/v2-7b24efc290bf6503e5ae8f2255c36121_b.png" alt="img" style="zoom: 67%;"></p>
<p><strong>逻辑回归中我们假设：</strong></p>
<p><img src="https://pic1.zhimg.com/v2-e321aeb2cacf2215b1a9e75f5154dcf8_b.png" alt="img" style="zoom:67%;"></p>
<p>其实它们都只是 广义线性模型 (GLMs)
的特例。提前透露：<strong>有了广义线性模型下 我们只需要把
符合指数分布的一般模型 的参数
转换成它对应的广义线性模型参数</strong>，然后按照广义线性模型的求解步骤即可轻松求解问题。</p>
<figure>
<img src="https://pic3.zhimg.com/v2-02c43a6ba09360753ee62c4c0871e2aa_b.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>指数分布族（ The exponential family）</strong></p>
<blockquote>
<p><strong>将拉格朗日乘数法应用于求解最大熵问题等同于求解对应的指数族分布问题。</strong></p>
<p>结合定理1和定理2，<strong>可以看到指数族分布的联合分布的统计量充分完备统计量，再构造其无偏估计则为UMVUE。这也是指数族分布应用广泛的原因。</strong></p>
</blockquote>
<p>首先我们定义一下什么是<strong>指数分布族</strong>, 它有如下形式:
<span class="math display">\[
p(y ; \eta)=b(y) \exp \left(\eta^T T(y)-a(\eta)\right)
\]</span> 简单介绍下其中的参数 (看不懂没关系）</p>
<ul>
<li><span class="math inline">\(\eta\)</span> 是 自然参数 ( natural
parameter)</li>
<li><span class="math inline">\(T(y)\)</span> 是充分统计量 (sufficient
statistic ) (一般情况下 <span class="math inline">\(T(y)=y\)</span>
)</li>
<li><span class="math inline">\(a(\eta)\)</span> 是 log partition
function <span class="math inline">\(\left(e^{-a(\eta)}\right.\)</span>
充当正规化常量的角色, 保证 <span class="math inline">\(\sum p(y ;
\eta)=1\)</span> )</li>
</ul>
<p>也就是所 <span class="math inline">\(\mathrm{T}, \mathrm{a},
\mathrm{b}\)</span> 确定了一种分布, <span class="math inline">\(\eta\)</span> 是 该分布的参数。 <strong>选择合适的
<span class="math inline">\(T, a, b\)</span> 我们可以得到 高斯分布 和
Bernoulli 分布。</strong></p>
<h5><span id="bernoulli分布的指数分布族形式"><strong><font color="red">
Bernoulli分布的指数分布族形式:</font></strong></span></h5>
<p><span class="math display">\[
p(y=1 ; \phi)=\phi ; p(y=0 ; \phi)=1-\phi
\]</span> <span class="math inline">\(=&gt;\)</span> <span class="math display">\[
\begin{aligned}
p(y ; \phi) &amp;=\phi^{y}(1-\phi)^{1-y} \\
&amp;=\exp (y \log \phi+(1-y) \log (1-\phi)) \\
&amp;=\exp \left(\left(\log \left(\frac{\phi}{1-\phi}\right)\right)
y+\log (1-\phi)\right)
\end{aligned}
\]</span> 即: 在如下参数下广义线性模型是 Bernoulli 分布 <span class="math display">\[
\begin{aligned}
\eta=\log (\phi /(1-\phi)) \Rightarrow \phi=1 /\left(1+e^{-\eta}\right)
\\
\\
T(y) &amp;=y \\
a(\eta) &amp;=-\log (1-\phi) \\
&amp;=\log \left(1+e^{\eta}\right) \\
b(y) &amp;=1
\end{aligned}
\]</span> ##### <strong><font color="red"> Gaussian
分布的指数分布族形式:</font></strong></p>
<p>在线性回归中, <span class="math inline">\(\sigma\)</span>
对于模型参数 <span class="math inline">\(\theta\)</span> 的选择没有影响,
为了推导方便我们将其设为 1 : <span class="math display">\[
\begin{aligned}
p(y ; \mu) &amp;=\frac{1}{\sqrt{2 \pi}} \exp
\left(-\frac{1}{2}(y-\mu)^{2}\right) \\
&amp;=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2} y^{2}\right) \cdot
\exp \left(\mu y-\frac{1}{2} \mu^{2}\right)
\end{aligned}
\]</span> 得到 对应的参数: <span class="math display">\[
\begin{aligned}
\eta &amp;=\mu \\
T(y) &amp;=y \\
a(\eta) &amp;=\mu^{2} / 2 \\
&amp;=\eta^{2} / 2 \\
b(y) &amp;=(1 / \sqrt{2 \pi}) \exp \left(-y^{2} / 2\right)
\end{aligned}
\]</span></p>
<h4><span id="19-lr的训练方">19、LR的训练方</span></h4>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li><p><strong>逻辑回归：入门到精通</strong>：http://www.tianyancha.com/research/LR_intro.pdf</p></li>
<li><p>美团技术团队《Logistic Regression
模型简介》https://tech.meituan.com/intro_to_logistic_regression.html</p></li>
<li><p>SVM和logistic回归分别在什么情况下使用？https://www.zhihu.com/question/21704547</p></li>
<li><p>逻辑斯蒂回归能否解决非线性分类问题？https://www.zhihu.com/question/29385169</p></li>
<li><p>为什么LR可以用来做CTR预估？https://www.zhihu.com/question/23652394</p></li>
<li><p><strong><font color="red">
评分卡基础—逻辑回归算法理解</font></strong> - 求是汪在路上的文章 - 知乎
https://zhuanlan.zhihu.com/p/111260930</p></li>
<li><p><strong>多分类</strong>：https://www.heywhale.com/mw/project/5ed755db946a0e002cb803f7</p></li>
</ul>
<blockquote>
<ul>
<li>LR推导（伯努利过程，极大似然，损失函数，梯度下降）有没有最优解？</li>
<li>LR可以用来处理非线性问题么？（还是lr啊 只不过是加了核的lr
这里加核是显式地把特征映射到高维
然后再做lr）怎么做？可以像SVM那样么？为什么？</li>
<li>为什么LR需要归一化或者取对数，<strong>为什么LR把特征离散化后效果更好</strong>，<strong>为什么把特征组合之后还能提升</strong>，反正这些基本都是增强了特征的表达能力，或者说更容易线性可分吧</li>
<li>美团技术团队《Logistic Regression
模型简介》https://tech.meituan.com/intro_to_logistic_regression.html</li>
<li><strong>SVM和logistic回归分别在什么情况下使用？</strong>https://www.zhihu.com/question/21704547</li>
<li><strong>逻辑斯蒂回归能否解决非线性分类问题？</strong>https://www.zhihu.com/question/29385169</li>
<li>为什么LR可以用来做CTR预估？https://www.zhihu.com/question/23652394</li>
<li>逻辑回归估计参数时的目标函数
（就是极大似然估计那部分），逻辑回归估计参数时的目标函数
（呵呵，第二次） 逻辑回归估计参数时的目标函数
如果加上一个先验的服从高斯分布的假设，会是什么样（天啦。我不知道，其实就是在后面乘一个东西，取log后就变成加一个东西，实际就变成一个正则项）</li>
<li>逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？（值越大可能性越高，但不能说是概率）</li>
<li>手推逻辑回归目标函数，正类是1，反类是-1，这里挖了个小坑，一般都是正例是1，反例是0的，他写的时候我就注意到这个坑了，然而写的太快又给忘了，衰，后来他提醒了一下，改了过来，就是极大似然函数的指数不一样，然后说我这里的面试就到这了。</li>
<li>看没看过scikit-learn源码LR的实现？（回头看了一下是调用的liblinear，囧）</li>
<li>为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好，为什么把特征组合之后还能提升，反正这些基本都是增强了特征的表达能力，或者说更容易线性可分吧</li>
<li>naive bayes和logistic
regression的区别http://m.blog.csdn.net/blog/muye5/19409615</li>
<li>LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？sigmoid函数由那个指数族分布，加上二项分布导出来的。损失函数是由最大似然估计求出的。</li>
<li>了解其他的分类模型吗，问LR缺点，LR怎么推导（当时我真没准备好，写不出来）写LR目标函数，目标函数怎么求最优解（也不会）讲讲LR的梯度下降，梯度下降有哪几种，逻辑函数是啥</li>
</ul>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>lzy
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://powerlzy.github.io/posts/1J1QH0W/" title="线性模型（2）逻辑回归">https://powerlzy.github.io/posts/1J1QH0W/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag"># 线性回归</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/3AGJJRV/" rel="prev" title="机器学习（9）决策树">
                  <i class="fa fa-chevron-left"></i> 机器学习（9）决策树
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/3TFM6N7/" rel="next" title="线性模型（1）线性回归">
                  线性模型（1）线性回归 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
