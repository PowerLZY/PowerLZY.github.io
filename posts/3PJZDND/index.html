<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="特征工程-特征处理   机器学习中的特征工程（四）—— &#x3D;&#x3D;特征离散化处理方法&#x3D;&#x3D;：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;918649ce379a   机器学习中的特征工程（三）—— &#x3D;&#x3D;序数和类别特征处理方法&#x3D;&#x3D;：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;3d828de72cd4   机器学习中的特征工程（二）—— &#x3D;&#x3D;数值类型数据处理&#x3D;&#x3D;：https:&#x2F;&#x2F;www.jia">
<meta property="og:type" content="article">
<meta property="og:title" content="特征工程（1）特征预处理*">
<meta property="og:url" content="https://powerlzy.github.io/posts/3PJZDND/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="特征工程-特征处理   机器学习中的特征工程（四）—— &#x3D;&#x3D;特征离散化处理方法&#x3D;&#x3D;：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;918649ce379a   机器学习中的特征工程（三）—— &#x3D;&#x3D;序数和类别特征处理方法&#x3D;&#x3D;：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;3d828de72cd4   机器学习中的特征工程（二）—— &#x3D;&#x3D;数值类型数据处理&#x3D;&#x3D;：https:&#x2F;&#x2F;www.jia">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://math.jianshu.com/math?formula=log_a(%5Calpha%5Ex">
<meta property="og:image" content="https://math.jianshu.com/math?formula=%5Calpha">
<meta property="og:image" content="https://math.jianshu.com/math?formula=x">
<meta property="og:image" content="https://math.jianshu.com/math?formula=%5Calpha%5E0%20%3D%201">
<meta property="og:image" content="https://math.jianshu.com/math?formula=log_a(1">
<meta property="og:image" content="https://math.jianshu.com/math?formula=(-%5Cinfty%2C%200">
<meta property="og:image" content="https://math.jianshu.com/math?formula=log_%7B10%7D(x">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220426154119370.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cphi_j%3D%5Csum_%7Bh%28i%29%3Dj%7D%7Bx_i%7D">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220426130800412.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220426130824066.png">
<meta property="og:image" content="https://math.jianshu.com/math?formula=w%20%3D%20%5Cfrac%20%7Bmax%20-%20min%7D%20%7Bn%7D">
<meta property="og:image" content="https://math.jianshu.com/math?formula=Entropy(y">
<meta property="og:image" content="https://math.jianshu.com/math?formula=m%3D2">
<meta property="og:image" content="https://math.jianshu.com/math?formula=P_1">
<meta property="og:image" content="https://math.jianshu.com/math?formula=P_2">
<meta property="og:image" content="https://math.jianshu.com/math?formula=Info_%7Bsplit%7D(x">
<meta property="og:image" content="https://math.jianshu.com/math?formula=Z_1%2C%20Z_2%2C%20...%2CZ_k">
<meta property="og:image" content="https://math.jianshu.com/math?formula=N(0%2C1">
<meta property="og:image" content="https://math.jianshu.com/math?formula=X%3D%5Csum_%7Bi%3D0%7D%5Ek%20Z_i%5E2">
<meta property="og:image" content="https://math.jianshu.com/math?formula=X%20%5Csim%20%5Cchi%5E2">
<meta property="og:image" content="https://math.jianshu.com/math?formula=%5Cchi%5E2%20%3D%20%5Csum%20%5Cfrac%20%7B(O-E">
<meta property="og:image" content="https://powerlzy.github.io/Users/apple/Library/Application%20Support/typora-user-images/image-20220708173638301.png">
<meta property="og:image" content="https://math.jianshu.com/math?formula=WOE_i%20%3D%20ln%5Clbrace%20%5Cfrac%20%7BP_%7By1%7D%7D%7BP_%7By0%7D%7D%20%5Crbrace%20%3D%20ln%5Clbrace%20%5Cfrac%20%7BB_i%20%2F%20B%7D%7BG_i%2FG%7D%20%5Crbrace">
<meta property="og:image" content="https://math.jianshu.com/math?formula=WOE_i">
<meta property="og:image" content="https://math.jianshu.com/math?formula=i">
<meta property="og:image" content="https://math.jianshu.com/math?formula=P_%7By1%7D">
<meta property="og:image" content="https://math.jianshu.com/math?formula=P_%7By0%7D">
<meta property="og:image" content="https://math.jianshu.com/math?formula=B_i%2FB">
<meta property="og:image" content="https://math.jianshu.com/math?formula=G_i%2FG">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_2">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=D_2">
<meta property="og:image" content="https://powerlzy.github.io/Library/Containers/com.tencent.xinWeChat/Data/Library/Application%20Support/com.tencent.xinWeChat/2.0b4.0.9/076b7987d1501ed1ebeee6aecab0dccc/Message/MessageTemp/f381afde205729b015fb0a76a283f729/Image/891647863651_.pic.jpg">
<meta property="article:published_time" content="2022-03-28T02:55:00.036Z">
<meta property="article:modified_time" content="2023-04-09T06:36:44.353Z">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://math.jianshu.com/math?formula=log_a(%5Calpha%5Ex">


<link rel="canonical" href="https://powerlzy.github.io/posts/3PJZDND/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://powerlzy.github.io/posts/3PJZDND/","path":"posts/3PJZDND/","title":"特征工程（1）特征预处理*"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>特征工程（1）特征预处理* | PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PowerLZY's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">特征工程-特征处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">一、 数值类型处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.1 数据归一化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">1.2 数据标准化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">&#x3D;&#x3D;1.3 对数转换&#x3D;&#x3D;</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">二、序数和类别特征处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.1 序数特征处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">Label Encoding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">2.2 类别特征处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">One-Hot编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">LabelBinarizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">虚拟编码Dummy Coding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text"> 特征哈希（feature hashing）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">多类别值处理方式 -&#x3D;&#x3D;- 基于统计的编码方法&#x3D;&#x3D;</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2.3 二阶</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">三、 特征离散化处理方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.1 等宽分箱（Equal-Width Binning)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.2 等频分箱（Equal-Frequency Binning）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.3 信息熵分箱【有监督】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.4 决策树分箱【有监督】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">3.5 卡方分箱 【有监督】</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">步骤：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text"> 3.6 WOE编码 【有监督】</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">四、缺失值处理解析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">4.1 缺失值的替换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">4.2 缺失值的删除</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">五、异常值处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">数据预处理Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text"> 1、LR为什么要离散化？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text"></span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">答案一（严林）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">答案二（周开拓）：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">2、树模型为什么离散化？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">Cart树的离散化：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">LGB直方图算法优点：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">3、归一化？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">4、赵兄-特征处理</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">239</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3PJZDND/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="特征工程（1）特征预处理* | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          特征工程（1）特征预处理*
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-09 14:36:44" itemprop="dateModified" datetime="2023-04-09T14:36:44+08:00">2023-04-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>26 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2><span id="特征工程-特征处理">特征工程-特征处理</span></h2><blockquote>
<p>  <strong>机器学习中的特征工程（四）—— ==特征离散化处理方法==：</strong><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/918649ce379a">https://www.jianshu.com/p/918649ce379a</a></p>
<p>  <strong>机器学习中的特征工程（三）—— ==序数和类别特征处理方法==</strong>：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/3d828de72cd4">https://www.jianshu.com/p/3d828de72cd4</a></p>
<p>  <strong>机器学习中的特征工程（二）—— ==数值类型数据处理==</strong>：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/b0cc0710ef55">https://www.jianshu.com/p/b0cc0710ef55</a></p>
<p>  机器学习中的特征工程（一）—— 概览：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/172677f4ea4c">https://www.jianshu.com/p/172677f4ea4c</a></p>
<p>  特征工程完全手册 - 从预处理、构造、选择、降维、不平衡处理，到放弃：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94994902">https://zhuanlan.zhihu.com/p/94994902</a></p>
<p>  <strong>这9个特征工程使用技巧，解决90%机器学习问题！</strong> - Python与数据挖掘的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/462744763">https://zhuanlan.zhihu.com/p/462744763</a></p>
</blockquote>
<h3><span id="一-数值类型处理">一、 数值类型处理</span></h3><blockquote>
<p>  <strong>pandas 显示所有列：</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示所有列</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#显示所有行</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_rows&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#设置value的显示长度为100，默认为50</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;max_colwidth&#x27;</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>  <strong>pandas 查看缺失特征:</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.isnull().<span class="built_in">sum</span>().sort_values(ascending = <span class="literal">False</span>) / train.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>  <strong>pandas 查看某一列的分布:</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[:,col_name].value_counts()</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>特征提取方式是可以深挖隐藏在数据背后更深层次的信息的</strong>。其次，数值类型数据也并不是直观看上去那么简单易用，因为不同的数值类型的计量单位不一样，比如个数、公里、千克、DB、百分比之类，同样数值的大小也可能横跨好几个量级，比如小到头发丝直径约为0.00004米， 大到热门视频播放次数成千上万次。</p>
<h4><span id="11-数据归一化">1.1 数据归一化</span></h4><blockquote>
<p>  <strong>为什么要数据归一化？</strong></p>
<p>   <a href="../../AI深度学习/深度学习（3）Normalization*.md">深度学习（3）Normalization*.md</a> </p>
<ul>
<li><strong>可解释性</strong>：<strong>回归模型【无正则化】</strong>中自变量X的量纲不一致导致了<strong>==回归系数无法直接解读==</strong>或者错误解读；需要将X都处理到统一量纲下，这样才可比【可解释性】；<strong>取决于我们的逻辑回归是不是用了正则化</strong>。如果你不用正则，标准化并不是必须的，如果用正则，那么标准化是必须的。</li>
<li><strong>距离计算</strong>：机器学习任务和统计学任务中有很多地方要用到<strong>==“距离”的计算==</strong>，比如<strong>PCA、KNN，kmeans和SVM</strong>等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；</li>
<li><p><strong>加速收敛</strong>：参数估计时使用<strong>==梯度下降==</strong>，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即<strong>==提升模型的收敛速度==</strong>。</p>
<p><strong>需要归一化的模型：</strong>利用梯度下降法求解的模型一般需要归一化，<strong>线性回归、LR、SVM、KNN、神经网络</strong></p>
</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\tilde{x}=\frac{x-\min (x)}{\max (x)-\min (x)}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># define data </span></span><br><span class="line">data = np.asarray([[<span class="number">100</span>, <span class="number">0.001</span>], </span><br><span class="line">                   [<span class="number">8</span>, <span class="number">0.05</span>], </span><br><span class="line">                   [<span class="number">50</span>, <span class="number">0.005</span>], </span><br><span class="line">                   [<span class="number">88</span>, <span class="number">0.07</span>], </span><br><span class="line">                   [<span class="number">4</span>, <span class="number">0.1</span>]])</span><br><span class="line"><span class="comment"># define min max scaler</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line"><span class="comment"># transform data</span></span><br><span class="line">scaled = scaler.fit_transform(data) </span><br></pre></td></tr></table></figure>
<h4><span id="12-数据标准化">1.2 数据标准化</span></h4><p>数据标准化是指通过改变数据的分布得到均值为0，标准差为1的服从标准正态分布的数据。主要目的是为了让不同特征之间具有相同的尺度（Scale），这样更有理化模型训练收敛。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment"># define standard scaler</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"><span class="comment"># transform data</span></span><br><span class="line">scaled = scaler.fit_transform(data) </span><br><span class="line"><span class="built_in">print</span>(scaled)</span><br></pre></td></tr></table></figure>
<h4><span id="13-对数转换">==1.3 对数转换==</span></h4><p>log函数的定义为<img src="https://math.jianshu.com/math?formula=log_a(%5Calpha%5Ex" alt="log_a(\alpha^x) = x">%20%3D%20x)，其中a是log函数的底数，<img src="https://math.jianshu.com/math?formula=%5Calpha" alt="\alpha">是一个正常数，<img src="https://math.jianshu.com/math?formula=x" alt="x">可以是任何正数。由于<img src="https://math.jianshu.com/math?formula=%5Calpha%5E0%20%3D%201" alt="\alpha^0 = 1">，我们有<img src="https://math.jianshu.com/math?formula=log_a(1" alt="log_a(1) = 0">%20%3D%200)。这意味着log函数可以将一些介于0~1之间小范围的数字映射到<img src="https://math.jianshu.com/math?formula=(-%5Cinfty%2C%200" alt="(-\infty, 0)">)范围内。比如当a=10时，函数<img src="https://math.jianshu.com/math?formula=log_%7B10%7D(x" alt="log_{10}(x)">)可以将[1,10]映射到[0,1]，将[1,100]映射到[1,2]。<strong>换句话说，log函数压缩了大数的范围，扩大了小数的范围</strong>。x越大，log(x)增量越慢。log(x)函数的图像如下：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426154119370.png" alt="image-20220426154119370" style="zoom: 25%;"></p>
<p><strong>Log函数可以极大压缩数值的范围，相对而言就扩展了小数字的范围。该转换方法适用于长尾分布且值域范围很大的特征，变换后的特征趋向于正态分布。</strong>对数值类型使用对数转换一般有以下几种好处：</p>
<ul>
<li>缩小数据的绝对数值</li>
<li>取对数后，可以将乘法计算转换成加法计算</li>
<li>在数据的整个值域中不同区间的差异带来的影响不同</li>
<li>取对数后不会改变数据的性质和相关关系，但压缩了变量的尺度。</li>
<li>得到的数据易消除异方差问题</li>
</ul>
<h3><span id="二-序数和类别特征处理">二、序数和类别特征处理</span></h3><p>本文主要说明特征工程中关于<strong>序数特征</strong>和<strong>类别特征</strong>的常用处理方法。主要包含<strong>LabelEncoder</strong>、<strong>One-Hot编码</strong>、<strong>DummyCoding</strong>、<strong>FeatureHasher</strong>以及要重点介绍的<strong>WOE编码</strong>。</p>
<h4><span id="21-序数特征处理">2.1 序数特征处理</span></h4><p><strong>序数特征指的是有序但无尺度的特征</strong>。比如表示‘学历’的特征，’高中’、’本科’、’硕士’，这些特征彼此之间是有顺序关系的，但是特征本身无尺度，并且也可能不是数值类型。在实际应用中，一般是字符类型居多，为了将其转换成模型能处理的形式，通常需要先进行编码，比如LabelEncoding。如果序数特征本身就是数值类型变量，则可不进行该步骤。下面依次介绍序数特征相关的处理方式。</p>
<ul>
<li><h4><span id="label-encoding">Label Encoding</span></h4></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">x = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">encoder = LabelEncoder()</span><br><span class="line">x1 = encoder.fit_transform(x)</span><br><span class="line"></span><br><span class="line">x2 = pd.Series(x).astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">x2.cat.codes.values</span><br><span class="line"><span class="comment"># pandas 因子化</span></span><br><span class="line">x2, uniques = pd.factorize(x)</span><br><span class="line"><span class="comment"># pandas 二值化</span></span><br><span class="line">x2 = pd.Series(x)</span><br><span class="line">x2 = (x2 &gt;= <span class="string">&#x27;b&#x27;</span>).astype(<span class="built_in">int</span>) <span class="comment">#令大于等于&#x27;b&#x27;的都为1</span></span><br></pre></td></tr></table></figure>
<h4><span id="22-类别特征处理">2.2 类别特征处理</span></h4><p><strong>类别特征由于没有顺序也没有尺度</strong>，因此处理较为麻烦，但是在CTR等领域却是非常常见的特征。比如<strong>商品的类型，颜色，用户的职业，兴趣</strong>等等。类别变量编码方法中最常使用的就是<strong>One-Hot编码</strong>，接下来结合具体实例来介绍。</p>
<ul>
<li><h4><span id="one-hot编码">One-Hot编码</span></h4></li>
</ul>
<p>One-Hot编码，又称为’独热编码’，其变换后的单列特征值只有一位是1。如下例所示，一个特征中包含3个不同的特征值(a,b,c)，编码转换后变成3个子特征，其中每个特征值中只有一位是有效位1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line"></span><br><span class="line">one_feature = [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">feature = label_encoder.fit_transform(one_feature)</span><br><span class="line">onehot_encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line">onehot_encoder.fit_transform(feature.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="labelbinarizer">LabelBinarizer</span></h4></li>
</ul>
<p>sklearn中的LabelBinarizer也具有同样的作用，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line">feature = np.array([<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">LabelBinarizer().fit_transform(feature)</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="虚拟编码dummy-coding">虚拟编码Dummy Coding</span></h4></li>
</ul>
<p>同样，<strong>pandas中也内置了对应的处理方式,使用起来比Sklearn更加方便</strong>，产生n-1个特征。实例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one_feature = [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">pd.get_dummies(one_feature, prefix=<span class="string">&#x27;test&#x27;</span>) <span class="comment"># 设置前缀test</span></span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="特征哈希feature-hashing"><strong><font color="red"> 特征哈希（feature hashing）</font></strong></span></h4></li>
</ul>
<p>按照上述编码方式，如果某个特征具有100个类别值，那么经过编码后将产生100个或99个新特征，这极大地增加了特征维度和特征的稀疏度，同时还可能会出现内存不足的情况。<strong>sklearn中的FeatureHasher接口采用了hash的方法，将不同的值映射到用户指定长度的数组中，使得输出特征的维度是固定的，该方法占用内存少，效率高，可以在多类别变量值中使用，但是由于采用了Hash函数的方式，所以具有冲突的可能，即不同的类别值可能映射到同一个特征变量值中。</strong></p>
<blockquote>
<p>  Feature hashing(特征哈希): <a target="_blank" rel="noopener" href="https://blog.csdn.net/laolu1573/article/details/79410187">https://blog.csdn.net/laolu1573/article/details/79410187</a></p>
<p>  <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing">https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing</a></p>
<p>  <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/264165760/answer/277634591">如何用通俗的语言解释CTR和推荐系统中常用的<em>Feature</em> <em>Hashing</em>技术以及其对应的优缺点？</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">h = FeatureHasher(n_features=<span class="number">5</span>, input_type=<span class="string">&#x27;string&#x27;</span>)</span><br><span class="line">test_cat = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;f&#x27;</span>,<span class="string">&#x27;g&#x27;</span>,<span class="string">&#x27;h&#x27;</span>,<span class="string">&#x27;i&#x27;</span>,<span class="string">&#x27;j&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">f = h.transform(test_cat)</span><br><span class="line">f.toarray()</span><br></pre></td></tr></table></figure>
<p><strong>如果hash的目标空间足够大，并且hash函数本身足够散列，不会损失什么特征信息。</strong></p>
<p>feature hashing简单来说和<strong>kernal的思想</strong>是类似的，就是把输入的特征映射到一个具有一些我们期望的较好性质的空间上去。在feature hasing这个情况下我们希望目标的空间具有如下的性质：</p>
<ol>
<li><strong>样本无关的维度大小，因为当在线学习，或者数据量非常大，提前对数据观察开销非常大的时候，这可以使得我们能够提前给算法分配存储和切分pattern。大大提高算法的工程友好性</strong>。</li>
<li>这个空间一般来说比输入的特征空间维度小很多。</li>
<li>另外我们假设在原始的特征空间里，样本的分布是非常稀疏的，只有很少一部分子空间是被取值的。</li>
<li><strong>保持内积的无偏</strong>（不变肯定是不可能的，因为空间变小了），否则很多机器学习方法就没法用了。</li>
</ol>
<blockquote>
<p>  <strong>原理</strong>：假设输入特征是一个N维的0/1取值的向量x。一个N-&gt;M的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=哈希函数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">哈希函数</a>h。那么 <img src="https://www.zhihu.com/equation?tex=%5Cphi_j%3D%5Csum_%7Bh%28i%29%3Dj%7D%7Bx_i%7D" alt="[公式]"></p>
<p>  好处：</p>
<ul>
<li>从某种程度上来讲，使得训练样本的特征在对应空间里的<strong>分布更均匀</strong>了。这个好处对于实际训练过程是非常大的，某种程度上起到了<strong>shuffle的作用</strong>。</li>
<li>特征的空间变小了，而且是一个可以预测的大小。比如说加入输入特征里有个东西叫做user_id，那么显然你也不知道到底有多少<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=userid&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">userid</a>的话，你需要先扫描一遍并且分配足够的空间给到它不然学着学着oom了。你也不能很好地提前优化<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=分片&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">分片</a>。</li>
<li><p>对在线学习非常友好。</p>
<p>坏处：</p>
</li>
<li><p>会给debug增加困难，为了debug你要保存记录h计算的过程数据，否则如果某个特征有毛病，你怎么知道到底是哪个原始特征呢？</p>
</li>
<li>没选好哈希函数的话，<strong>可能会造成碰撞</strong>，如果原始特征很稠密并且碰撞很严重，那可能会带来坏的训练效果。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hashing_vectorizer</span>(<span class="params">features, N</span>):</span><br><span class="line">  	x = [<span class="number">0</span>] * N</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">      	h = <span class="built_in">hash</span>(f)</span><br><span class="line">        idx = h % N</span><br><span class="line">        <span class="keyword">if</span> xt(f) == <span class="number">1</span>: <span class="comment"># xt 2值hash函数减少hash冲突</span></span><br><span class="line">          	x[idx] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">         		x[idx] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="多类别值处理方式-基于统计的编码方法">多类别值处理方式 -==- 基于统计的编码方法==</span></h4></li>
</ul>
<p>当类别值过多时，<strong>One-Hot 编码或者Dummy Coding都可能导致编码出来的特征过于稀疏</strong>，其次也会占用过多内存。<strong>如果使用FeatureHasher，n_features的设置不好把握，可能会造成过多冲突，造成信息损失</strong>。这里提供一种基于统计的编码方法，包括<strong>基于特征值的统计</strong>或者<strong>基于标签值的统计</strong>——基于标签的编码。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line"><span class="built_in">test</span> = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="built_in">df</span> = pd.DataFrame(<span class="built_in">test</span>, columns=[<span class="string">&#x27;alpha&#x27;</span>])</span><br><span class="line">sns.countplot(<span class="built_in">df</span>[<span class="string">&#x27;alpha&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426130800412.png" alt="image-20220426130800412" style="zoom:50%;"></p>
<p>首先我们将每个类别值出现的频数计算出来，比如我们设置阈值为1，那么所有小于阈值1的类别值都会被编码为同一类，大于1的类别值会分别编码，如果出现频数一样的类别值，既可以都统一分为一个类，也可以按照某种顺序进行编码，这个可以根据业务需要自行决定。那么根据上图，可以得到其编码值为：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426130824066.png" alt="image-20220426130824066" style="zoom:50%;"></p>
<p> <strong>即（a,c）分别编码为一个不同的类别，（e,b,d）编码为同一个类别。</strong></p>
<h3><span id="23-二阶">2.3 二阶</span></h3><blockquote>
<p>  w * h = s</p>
</blockquote>
<h3><span id="三-特征离散化处理方法">三、 特征离散化处理方法</span></h3><p><strong>特征离散化指的是将连续特征划分离散的过程</strong>：将原始定量特征的一个区间一一映射到单一的值。离散化过程也被表述成分箱（Binning）的过程。特征离散化常应用于<strong>逻辑回归</strong>和金融领域的评分卡中，同时在规则提取，特征分类中也有对应的应用价值。本文主要介绍几种常见的分箱方法，包括<strong>等宽分箱、等频分箱、信息熵分箱</strong>、<strong>基于决策树分箱、卡方分箱</strong>等。</p>
<p><strong>可以看到在分箱之后，数据被规约和简化，有利于理解和解</strong>释。总来说特征离散化，即 分箱之后会带来如下优势：</p>
<ul>
<li>有助于模型部署和应用，加快模型迭代</li>
<li>增强模型鲁棒性</li>
<li>增加非线性表达能力：连续特征不同区间对模型贡献或者重要程度不一样时，分箱后不同的权重能直接体现这种差异，离散化后的特征再进行特征 交叉衍生能力会进一步加强。</li>
<li>提升模型的泛化能力</li>
<li><strong>扩展数据在不同各类型算法中的应用范围</strong></li>
</ul>
<p>当然特征离散化也有其缺点，总结如下：</p>
<ul>
<li>分箱操作必定会导致一定程度的信息损失</li>
<li>增加流程：建模过程中加入了额外的的离散化步骤</li>
<li>影响模型稳定性： 当一个特征值处于分箱点的边缘时，此时微小的偏差会造成该特征值的归属从一箱跃迁到另外一箱，影响模型的稳定性。</li>
</ul>
<h4><span id="31-等宽分箱equal-width-binning">3.1 等宽分箱（Equal-Width Binning)</span></h4><p><strong>等宽分箱指的是每个分隔点或者划分点的距离一样，即等宽</strong>。实践中一般指定分隔的箱数，等分计算后得到每个分隔点。例如将数据序列分为n份，则 分隔点的宽度计算公式为：</p>
<p><img src="https://math.jianshu.com/math?formula=w%20%3D%20%5Cfrac%20%7Bmax%20-%20min%7D%20%7Bn%7D" alt="w = \frac {max - min} {n}"></p>
<p>这样就将原始数据划分成了n个等宽的子区间，一般情况下，分箱后每个箱内的样本数量是不一致的。使用pandas中的cut函数来实现等宽分箱，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value, cutoff = pd.cut(df[<span class="string">&#x27;mean radius&#x27;</span>], bins=<span class="number">4</span>, retbins=<span class="literal">True</span>, precision=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>等宽分箱计算简单，但是当数值方差较大时，即数据离散程度很大，那么很可能出现没有任何数据的分箱</strong>，这个问题可以通过自适应数据分布的分箱方法—等频分箱来避免</p>
<h4><span id="32-等频分箱equal-frequency-binning">3.2 等频分箱（Equal-Frequency Binning）</span></h4><p><strong>等频分箱理论上分隔后的每个箱内得到数据量大小一致</strong>，但是当某个值出现次数较多时，会出现等<strong>分边界是同一个值</strong>，导致同一数值分到不同的箱内，这是不正确的。具体的实现可以<strong>去除分界处的重复值</strong>，但这也导致每箱的数量不一致。如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1 = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">value, cutoff = pd.qcut(s1, <span class="number">3</span>, retbins=<span class="literal">True</span>)</span><br><span class="line">sns.countplot(value)</span><br></pre></td></tr></table></figure>
<p><strong>上述的等宽和等频分箱容易出现的问题是每箱中信息量变化不大</strong>。例如，等宽分箱不太适合分布不均匀的数据集、离群值；等频方法不太适合特定的值占比过多的数据集，如<strong>长尾分布</strong>。</p>
<h4><span id="33-信息熵分箱有监督">3.3 信息熵分箱【有监督】</span></h4><p><strong>如果分箱后箱内样本对y的区分度好，那么这是一个好的分箱</strong>。通过信息论理论，我们可知信息熵衡量了这种区分能力。当特征按照某个分隔点划分为上下两部分后能达到最大的信息增益，那么这就是一个好的分隔点。由上可知，信息熵分箱是有监督的分箱方法。<strong><font color="red"> 其实决策树的节点分裂原理也是基于信息熵。</font></strong></p>
<p>首先我们需要明确信息熵和信息增益的计算方式，分别如下：<br><img src="https://math.jianshu.com/math?formula=Entropy(y" alt="Entropy(y) = - \sum_{i=1}^m p_i log_2{p_i} \\ Gain(x) = Entropy(y) - Info_{split}(x)">%20%3D%20-%20%5Csum<em>%7Bi%3D1%7D%5Em%20p_i%20log_2%7Bp_i%7D%20%5C%5C%20Gain(x)%20%3D%20Entropy(y)%20-%20Info</em>%7Bsplit%7D(x))</p>
<p>在二分类问题中，<img src="https://math.jianshu.com/math?formula=m%3D2" alt="m=2">。 信息增益的物理含义表达为：x的分隔带来的信息对y的不确定性带来的增益。<br>对于二值化的单点分隔，如果我们找到一个分隔点将数据一分为二，分成<img src="https://math.jianshu.com/math?formula=P_1" alt="P_1">和<img src="https://math.jianshu.com/math?formula=P_2" alt="P_2">两部分，那么划分后的信息熵的计算方式为：</p>
<p><img src="https://math.jianshu.com/math?formula=Info_%7Bsplit%7D(x" alt="Info_{split}(x) = P1_{ratio}Entropy(x_{p1}) + P2_{ratio}Entropy(x_{p2})">%20%3D%20P1<em>%7Bratio%7DEntropy(x</em>%7Bp1%7D)%20%2B%20P2<em>%7Bratio%7DEntropy(x</em>%7Bp2%7D))</p>
<p>同时也可以看出，当分箱后，某个箱中的标签y的类别（0或者1）的比例相等时，其熵值最大，表明此特征划分几乎没有区分度。而当某个箱中的数据的标签y为单个类别时，那么该箱的熵值达到最小的0，即纯度最纯，最具区分度。从结果上来看，最大信息增益对应分箱后的总熵值最小。</p>
<h4><span id="34-决策树分箱有监督">3.4 决策树分箱【有监督】</span></h4><p><strong>由于决策树的结点选择和划分也是根据信息熵来计算的，因此我们其实可以利用决策树算法来进行特征分箱</strong>，具体做法如下：</p>
<p>还是以乳腺癌数据为例，首先取其中‘mean radius’字段，和标签字段‘target’来拟合一棵决策树，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>, max_depth=<span class="number">3</span>) <span class="comment"># 树最大深度为3</span></span><br><span class="line">dt.fit(df[<span class="string">&#x27;mean radius&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>), df[<span class="string">&#x27;target&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>接着我们取出这课决策树的所有叶节点的分割点的阈值，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qts = dt.tree_.threshold[np.where(dt.tree_.children_left &gt; -<span class="number">1</span>)]</span><br><span class="line">qts = np.sort(qts)</span><br><span class="line">res = [np.<span class="built_in">round</span>(x, <span class="number">3</span>) <span class="keyword">for</span> x <span class="keyword">in</span> qts.tolist()]</span><br></pre></td></tr></table></figure>
<h4><span id="35-卡方分箱-有监督">3.5 卡方分箱 【有监督】</span></h4><blockquote>
<p>  <strong>特征选择之卡方分箱、WOE/IV</strong> - 云水僧的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/101771771">https://zhuanlan.zhihu.com/p/101771771</a></p>
</blockquote>
<p><strong><font color="red"> 卡方检验可以用来评估两个分布的相似性，因此可以将这个特性用到数据分箱的过程中。卡方分箱认为：理想的分箱是在同一个区间内标签的分布是相同的</font>;</strong> <strong>卡方分布是概率统计常见的一种概率分布，是卡方检验的基础。卡方分布定义为</strong>：若n个独立的随机变量<img src="https://math.jianshu.com/math?formula=Z_1%2C%20Z_2%2C%20...%2CZ_k" alt="Z_1, Z_2, ...,Z_k">满足标准正态分布<img src="https://math.jianshu.com/math?formula=N(0%2C1" alt="N(0,1)">)，则n个随机变量的平方和<img src="https://math.jianshu.com/math?formula=X%3D%5Csum_%7Bi%3D0%7D%5Ek%20Z_i%5E2" alt="X=\sum_{i=0}^k Z_i^2">为服从自由度为k的卡方分布，记为<img src="https://math.jianshu.com/math?formula=X%20%5Csim%20%5Cchi%5E2" alt="X \sim \chi^2">。参数n称为自由度（样本中独立或能自由变化的自变量的个数)，不同的自由度是不同的分布。</p>
<p><strong>卡方检验</strong> ：卡方检验属于非参数假设检验的一种，其本质都是度量频数之间的差异。其假设为：<strong>观察频数与期望频数无差异或者两组变量相互独立不相关。</strong></p>
<p><img src="https://math.jianshu.com/math?formula=%5Cchi%5E2%20%3D%20%5Csum%20%5Cfrac%20%7B(O-E" alt="\chi^2 = \sum \frac {(O-E)^2}{E}">%5E2%7D%7BE%7D)</p>
<ul>
<li>卡方拟合优度检验：用于检验样本是否来自于某一个分布，比如检验某样本是否为正态分布</li>
<li>独立性卡方检验，查看两组类别变量分布是否有差异或者相关，以列联表的形式比较。以列联表形式的卡方检验中，卡方统计量由上式给出。</li>
</ul>
<h4><span id="步骤">步骤：</span></h4><p>卡方分箱是自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。基本思想: 对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p>
<p><strong>理想的分箱是在同一个区间内标签的分布是相同的</strong>。卡方分箱就是不断的计算相邻区间的卡方值（卡方值越小表示分布越相似），将分布相似的区间（卡方值最小的）进行合并，直到相邻区间的分布不同，达到一个理想的分箱结果。下面用一个例子来解释：</p>
<p><img src="/Users/apple/Library/Application Support/typora-user-images/image-20220708173638301.png" alt="image-20220708173638301" style="zoom: 33%;"></p>
<p> 由上图，第一轮中初始化是5个区间，分别计算相邻区间的卡方值。找到1.2是最小的，合并2、3区间，为了方便，将合并后的记为第2区间，因此得到4个区间。第二轮中，由于合并了区间，影响该区间与前面的和后面的区间的卡方值，因此重新计算1和2,2和4的卡方值，由于4和5区间没有影响，因此不需要重新计算，这样就得到了新的卡方值列表，找到最小的取值2.5，因此该轮会合并2、4区间，并重复这样的步骤，一直到满足终止条件。</p>
<h4><span id="36-woe编码-有监督"><strong><font color="red"> 3.6 WOE编码 【有监督】</font></strong></span></h4><blockquote>
<p>  <strong><font color="red"> 风控模型—WOE与IV指标的深入理解应用</font></strong>: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80134853">https://zhuanlan.zhihu.com/p/80134853</a></p>
</blockquote>
<p><strong>WOE（Weight of Evidence，证据权重）编码利用了标签信息，属于有监督的编码方式。该方式广泛用于金融领域信用风险模型中，是该领域的经验做法。</strong>下面先给出WOE的计算公式：<br> <img src="https://math.jianshu.com/math?formula=WOE_i%20%3D%20ln%5Clbrace%20%5Cfrac%20%7BP_%7By1%7D%7D%7BP_%7By0%7D%7D%20%5Crbrace%20%3D%20ln%5Clbrace%20%5Cfrac%20%7BB_i%20%2F%20B%7D%7BG_i%2FG%7D%20%5Crbrace" alt="WOE_i = ln\lbrace \frac {P_{y1}}{P_{y0}} \rbrace = ln\lbrace \frac {B_i / B}{G_i/G} \rbrace"><br> ==<strong><img src="https://math.jianshu.com/math?formula=WOE_i" alt="WOE_i">值可解释为第<img src="https://math.jianshu.com/math?formula=i" alt="i">类别中好坏样本分布比值的对数</strong>==。其中各个分量的解释如下：</p>
<ul>
<li><img src="https://math.jianshu.com/math?formula=P_%7By1%7D" alt="P_{y1}">表示该类别中坏样本的分布</li>
<li><img src="https://math.jianshu.com/math?formula=P_%7By0%7D" alt="P_{y0}">表示该类别中好样本的分布</li>
<li><img src="https://math.jianshu.com/math?formula=B_i%2FB" alt="B_i/B">表示该类别中坏样本的数量在总体坏样本中的占比</li>
<li><img src="https://math.jianshu.com/math?formula=G_i%2FG" alt="G_i/G">表示该类别中好样本的数量在总体好样本中的占比</li>
</ul>
<p>很明显，如果整个分数的值大于1，那么WOE值为正，否则为负，所以WOE值的取值范围为正负无穷。<br> <strong>WOE值直观上表示的实际上是“当前分组中坏客户占所有坏客户的比例”和“当前分组中好客户占所有坏客户的比例”的差异。</strong>转化公式以后，也可以理解为：当前这个组中坏客户和好客户的比值，和所有样本中这个比值的差异。这个差异为这两个比值的比值，再取对数来表示的。 WOE越大，这种差异越大，这个分组里的样本坏样本可能性就越大，WOE越小，差异越小，这个分组里的坏样本可能性就越小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 随机生成1000行数据</span></span><br><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;x&#x27;</span>: np.random.choice([<span class="string">&#x27;R&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="string">&#x27;B&#x27;</span>], <span class="number">1000</span>),</span><br><span class="line">    <span class="string">&#x27;y&#x27;</span>: np.random.randint(<span class="number">2</span>, size=<span class="number">1000</span>)</span><br><span class="line">&#125;)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<h3><span id="四-缺失值处理解析">四、缺失值处理解析</span></h3><blockquote>
<p>  看不懂你打我，史上最全的缺失值解析: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/379707046">https://zhuanlan.zhihu.com/p/379707046</a></p>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/137175585">https://zhuanlan.zhihu.com/p/137175585</a></p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>机器学习模型</th>
<th>是否支持缺失值</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>XGBoost</strong></td>
<td>是</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>是</td>
</tr>
<tr>
<td>线性回归</td>
<td>否</td>
</tr>
<tr>
<td>逻辑回归（LR）</td>
<td>否</td>
</tr>
<tr>
<td>随机森林（RF）</td>
<td>否</td>
</tr>
<tr>
<td>SVM</td>
<td>否</td>
</tr>
<tr>
<td>因子分解机(FM)</td>
<td>否</td>
</tr>
<tr>
<td>朴实贝叶斯（NB）</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="41-缺失值的替换">4.1 <strong>缺失值的替换</strong></span></h4><p><strong>scikit-learn中填充缺失值的API是Imputer类，使用方法如下：</strong></p>
<p>参数strategy有三个值可选：mean(平均值)，median(中位数)，most_frequent(众数)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rom sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 缺失值填补的时候必须得是float类型</span></span><br><span class="line"><span class="comment"># 缺失值要填充为np.nan，它是浮点型，strategy是填充的缺失值类型，这里填充平均数，axis代表轴，这里第0轴是列</span></span><br><span class="line">im = Imputer(missing_values=<span class="string">&#x27;NaN&#x27;</span>,strategy=<span class="string">&#x27;mean&#x27;</span>,axis=<span class="number">0</span>)</span><br><span class="line">data = im.fit_transform([[<span class="number">1</span>, <span class="number">2</span>], </span><br><span class="line">                         [np.nan, <span class="number">3</span>], </span><br><span class="line">                         [<span class="number">7</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<h4><span id="42-缺失值的删除">4.2 缺失值的删除</span></h4><h3><span id="五-异常值处理">五、异常值处理</span></h3><h2><span id="数据预处理qampa">数据预处理Q&amp;A</span></h2><h3><span id="1-lr为什么要离散化"><strong><font color="red"> 1、LR为什么要离散化？</font></strong></span></h3><h4><span id="学习-连续特征的离散化在什么情况下将连续的特征离散化之后可以获得更好的效果"></span></h4><p><strong>问题描述：</strong>发现CTR预估一般都是用LR，而且特征都是离散的，为什么一定要用离散特征呢？这样做的好处在哪里？求大拿们解答。</p>
<h4><span id="答案一严林"><strong>答案一（<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=严林&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">严林</a>）：</strong></span></h4><p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ol>
<li>离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>【鲁棒性】离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则为0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>【模型假设】<strong>逻辑回归属于广义线性模型，表达能力受限</strong>；单变量离散化为N个后，每个变量有独立的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li>
<li>【特征交叉】离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险；</li>
</ol>
<p><strong><font color="red"> <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=李沐&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">李沐</a>曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型”同“少量连续特征+复杂模型”的权衡。</font></strong></p>
<blockquote>
<p>  这里我写下我关于上面某些点的理解，有问题的欢迎指出：</p>
<ol>
<li><p>假设目前有两个连续的特征：『年龄』和『收入』，预测用户的『魅力指数』；</p>
<p>第三点: <strong>LR是广义线性模型</strong>，因此如果特征『年龄』不做离散化直接输入，那么只能得到『年龄』和魅力指数的一个线性关系。但是这种线性关系是不准确的，并非年龄越大魅力指一定越大；如果将年龄划分为M段，则可以针对每段有一个对应的权重；这种分段的能力为模型带来类似『折线』的能力，也就是所谓的非线性<br><strong>连续变量的划分，naive的可以通过人为先验知识划分，也可以通过训练单特征的决策树桩，根据Information Gain/Gini系数等来有监督的划分。</strong><br>假如『年龄』离散化后，共有N段，『收入』离散化后有M段；此时这两个离散化后的特征类似于<strong>CategoryFeature</strong>，对他们进行<strong>OneHotEncode</strong>，即可以得到 M + N的 01向量；例如： 0 1 0 0， 1 0 0 0 0；<br>第四点: <strong>特征交叉</strong>，可以理解为上述两个向量的互相作用，作用的方式可以例如是 &amp;和|操作（这种交叉方式可以产生一个 M * N的01向量；）<br>上面特征交叉，可以类比于决策树的决策过程。例如进行&amp;操作后，得到一个1，则可以认为产生一个特征 （a &lt; age &lt; b &amp;&amp; c &lt; income &lt; d）;将特征空间进行的非线性划分，也就是所谓的引入非线性；</p>
</li>
</ol>
</blockquote>
<h4><span id="答案二周开拓"><strong>答案二（周开拓）：</strong></span></h4><p><strong><font color="red"> 机器学习里当然并没有free lunch，一个方法能work，必定是有假设的。如果这个假设和真实的问题及数据比较吻合，就能work。</font></strong></p>
<p>对于LR这类的模型来说，假设基本如下：</p>
<ul>
<li><strong>局部<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=平坦性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">平坦性</a>，或者说连续性</strong>。对于连续特征x来说，在任何一个取值x0的邻域附近，这个特征对预估目标y的影响也在一个足够小的邻域内变化。比如，人年龄对点击率的影响，x0=30岁假设会产生一定的影响，那么x=31或者29岁，这个影响和x0=30岁的影响差距不会太大；</li>
<li><strong>x对y的影响，这个函数虽然局部比较平坦，但是不太规律，如果你知道这个影响是个严格的直线</strong>（或者你有先验知识知道这个影响一定可以近似于一个参数不太多的函数），<strong>显然也没必要去做离散化</strong>。当然这条基本对于绝大多数问题都是成立的，因为基本没有这种好事情。</li>
</ul>
<p>假设一个最简单的问题，binary classification，y=0/1，x是个连续值。你希望学到一个logloss足够小的y=f(x)。</p>
<p>那么有一种做法就是，在数据轴上切若干段，每一段观察训练样本里y为1的比例，以这个比例作为该段上y=f(x)的值。这个当然不是LR训练的过程，但是就是离散化的思想。你可以发现：</p>
<ul>
<li><strong>如果每一段里面都有足够多的样本，那么在这一段里的y=f(x)值的点估计就比较可信</strong>；</li>
<li><font color="red">如果x在数轴上分布不太均匀，比如是<strong>指数分布或者周期分布</strong>的，这么做可能会有问题，因而你要先<strong>对x取个log，或者去掉周期性</strong></font>；</li>
</ul>
<p>这就告诉了你应该怎么做离散化：<strong><font color="red"> 尽可能保证每个分段里面有足够多的样本，尽量让样本的分布在数轴上均匀一些。</font></strong></p>
<p>结语：<strong>本质上连续特征离散化，可以理解为连续信号怎么转化为数字信号，好比我们计算机画一条曲线，也是变成了画一系列线段的问题。</strong>用分段函数来表达一个连续的函数在大多数情况下，都是work的。想取得好的效果需要：</p>
<ul>
<li>你的分段足够小，以使得在每个分段内x对y的影响基本在一个不大的邻域内，或者你可以忍受这个变化的幅度；</li>
<li>你的分段足够大，以使得在每个分段内有足够的样本，以获得可信的f(x)也就是权重；</li>
<li>你的分段策略使得在每个x的分段中，样本的分布尽量均匀（当然这很难），一般会根据先验知识先对x做一些变化以使得变得均匀一些；</li>
<li>如果你有非常强的x对y的先验知识，比如严格线性之类的，也未必做离散化，但是这种先验在计算广告或者推荐系统里一般是不存在的，也许其他领域比如CV之类的里面是可能存在的；</li>
</ul>
<p>最后还有个特别大的<strong>LR用离散特征的好处就是LR的特征是并行的，每个特征是并行同权的</strong>，如果有异常值的情况下，如果这个异常值没见过，那么LR里因为没有这个值的权重，最后对<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=score&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">score</a>的贡献为0，最多效果不够好，但是不会错的太离谱。另外，如果你debug，很容易查出来是哪个段上的权重有问题，比较好定位和解决。</p>
<h3><span id="2-树模型为什么离散化"><strong><font color="red">2、树模型为什么离散化？</font></strong></span></h3><h4><span id="cart树的离散化">Cart树的离散化：</span></h4><p><strong>分类：</strong></p>
<ul>
<li><p><strong><font color="red">如果特征值是连续值：CART的处理思想与C4.5是相同的，即将连续特征值离散化。唯一不同的地方是度量的标准不一样，</font></strong> <strong>CART采用基尼指数，而C4.5采用信息增益比</strong>。</p>
</li>
<li><p>如果当前节点为连续属性，<strong>CART树中该属性（剩余的属性值）后面还可以参与子节点的产生选择过程</strong>。</p>
</li>
</ul>
<p><strong>回归：</strong></p>
<p>对于连续值的处理，<strong>CART 分类树采用基尼系数的大小来度量特征的各个划分点</strong>。<strong>在回归模型中，我们使用常见的和方差度量方式</strong>，对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> ，求出使 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 各自<strong>集合的均方差最小</strong>，同时 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=c_1" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 数据集的样本输出均值， <img src="https://www.zhihu.com/equation?tex=c_2" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 数据集的样本输出均值。</p>
<h4><span id="lgb直方图算法优点">LGB直方图算法优点：</span></h4><p><strong>内存小、复杂度降低、直方图加速【分裂、并行通信、缓存优化】</strong></p>
<ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于 直方图算法，则只需要(1x样本数x维 度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的 bin 值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p>
</li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为$k$的树的时间复杂度：对特征所有取值的排序为$O(NlogN)$，$N$为样本点数目，若有$D$维特征，则$O(kDNlogN)$，而直方图算法需要$O(kD \times bin)$ (bin是histogram 的横轴的数量，一般远小于样本数量$N$)。</p>
</li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>==两个维度==</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的$k$个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
</li>
<li><p><strong>数据并行优化</strong>，用 histgoram 可以大幅降低通信代价。用 pre-sorted 算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst 在并行的时候也使用 histogram 进行通信。</p>
</li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM 所使用直方图算法对 Cache 天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</p>
</li>
</ul>
<h3><span id="3-归一化">3、归一化？</span></h3><h3><span id="4-赵兄-特征处理">4、赵兄-特征处理</span></h3><p><img src="../../../../../Library/Containers/com.tencent.xinWeChat/Data/Library/Application Support/com.tencent.xinWeChat/2.0b4.0.9/076b7987d1501ed1ebeee6aecab0dccc/Message/MessageTemp/f381afde205729b015fb0a76a283f729/Image/891647863651_.pic.jpg" alt="891647863651_.pic"></p>
<ul>
<li>滑动窗口、随机采样、加权差分特征</li>
<li>均值插补插补，稀疏矩阵技术问题</li>
<li>分层统计、差分特征</li>
<li><strong>二阶交叉统计</strong></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>lzy
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://powerlzy.github.io/posts/3PJZDND/" title="特征工程（1）特征预处理*">https://powerlzy.github.io/posts/3PJZDND/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/T8EY5H/" rel="prev" title="特征工程（2）特征选择">
                  <i class="fa fa-chevron-left"></i> 特征工程（2）特征选择
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/1D4WS02/" rel="next" title="深度学习（2）【draft】神经网络">
                  深度学习（2）【draft】神经网络 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
