<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Quick Introduction of Batch Normalization 本篇是一个很快地介绍,Batch Normalization 这个技术 一、Changing Landscape（不变化的景观） 之前才讲过说,我们能不能够直接改error surface 的 landscape,我们觉得说 error surface 如果很崎嶇的时候,它比较难 train,那我们能不能够直接把山">
<meta property="og:type" content="article">
<meta property="og:title" content="模型训练（5）Batch Normalization">
<meta property="og:url" content="https://powerlzy.github.io/posts/1HFEDWZ/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="Quick Introduction of Batch Normalization 本篇是一个很快地介绍,Batch Normalization 这个技术 一、Changing Landscape（不变化的景观） 之前才讲过说,我们能不能够直接改error surface 的 landscape,我们觉得说 error surface 如果很崎嶇的时候,它比较难 train,那我们能不能够直接把山">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616212321383.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616212341359.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616212421658.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616212503953.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616212559236.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221138101.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221152221.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221202386.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221211793.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221239128.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221403703.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221515889.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221555203.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616221637917.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616222053826.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616222338443.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616222438574.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616222534083.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616225043073.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616225149673.png">
<meta property="og:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616225308853.png">
<meta property="article:published_time" content="2022-06-09T03:34:59.000Z">
<meta property="article:modified_time" content="2022-07-13T15:19:15.172Z">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://powerlzy.github.io/Library/Application%20Support/typora-user-images/image-20220616212321383.png">


<link rel="canonical" href="https://powerlzy.github.io/posts/1HFEDWZ/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://powerlzy.github.io/posts/1HFEDWZ/","path":"posts/1HFEDWZ/","title":"模型训练（5）Batch Normalization"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>模型训练（5）Batch Normalization | PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PowerLZY's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">Quick Introduction of
Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">一、Changing
Landscape（不变化的景观）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">Feature Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">Considering Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">那為什麼要加上 \(β\) 跟 \(γ\) 呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">这样不会不同
dimension 的分布,它的 range 又都不一样了吗？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">Testing</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">这个
Batch Normalization 在 inference,或是 testing
的时候,会有什麼样的问题呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">但如果今天在
testing 的时候,根本就没有 batch,那我们要怎麼算这个 \(μ\),跟怎麼算这个 \(\sigma\) 呢？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">Comparison</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-text">Internal Covariate Shift?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-text">為什麼 Batch
Normalization 会比较好呢？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-text">To learn more ……</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">255</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">52</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1HFEDWZ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="模型训练（5）Batch Normalization | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          模型训练（5）Batch Normalization
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-09 11:34:59" itemprop="dateCreated datePublished" datetime="2022-06-09T11:34:59+08:00">2022-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 23:19:15" itemprop="dateModified" datetime="2022-07-13T23:19:15+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>22 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1><span id="quick-introduction-ofbatch-normalization">Quick Introduction of
Batch Normalization</span></h1>
<p>本篇是一个很快地介绍,Batch Normalization 这个技术</p>
<h3><span id="一-changinglandscape不变化的景观">一、Changing
Landscape（不变化的景观）</span></h3>
<p>之前才讲过说,我们能不能够直接改error surface 的 landscape,我们觉得说
error surface 如果很崎嶇的时候,它比较难
train,那我们能不能够直接把山剷平,让它变得比较好 train 呢？</p>
<p>==<strong>Batch Normalization</strong>==
就是其中一个,<strong>把山剷平的想法</strong>。我们一开始就跟大家讲说,不要小看
optimization 这个问题,有时候就算你的 error surface 是
convex的,它就是一个碗的形状,都不见得很好
train。假设你的两个参数啊,它们对 <strong>Loss
的斜率差别非常大</strong>,在 <span class="math inline">\(w_1\)</span>
这个方向上面,你的斜率变化很小,在 <span class="math inline">\(w_2\)</span> 这个方向上面斜率变化很大。</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616212321383.png" alt="image-20220616212321383">
<figcaption aria-hidden="true">image-20220616212321383</figcaption>
</figure>
<p>如果是<strong>固定的 learning
rate</strong>,你可能很难得到好的结果,所以我们才说你需要adaptive 的
learning rate、 Adam 等等比较进阶的 optimization
的方法,才能够得到好的结果</p>
<p>现在我们要从另外一个方向想,<strong>直接把难做的 error surface
把它改掉</strong>,看能不能够改得好做一点。在做这件事之前,也许我们第一个要问的问题就是,有这一种状况,$w_1
$ 跟 <span class="math inline">\(w_2\)</span>
它们的<strong>斜率差很多</strong>的这种状况,到底是从什麼地方来的</p>
<p>假设我现在有一个非常非常非常简单的 model,它的输入是 <span class="math inline">\(x_1\)</span> 跟 <span class="math inline">\(x_2\)</span>,它对应的参数就是 $ w_1 $ 跟 <span class="math inline">\(w_2\)</span>,它是一个 linear 的 model,没有
activation function</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616212341359.png" alt="image-20220616212341359"></p>
<p>$ w_1 $ 乘 <span class="math inline">\(x_1\)</span>,<span class="math inline">\(w_2\)</span> 乘 <span class="math inline">\(x_2\)</span> 加上 b 以后就得到 y,然后会计算 y 跟
<span class="math inline">\(\hat{y}\)</span> 之间的差距当做 e,把所有
training data e 加起来就是你的 Loss，然后去 minimize 你的
Loss，那什麼样的状况我们会產生像上面这样子,<strong>比较不好 train 的
error surface</strong> 呢？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616212421658.png" alt="image-20220616212421658" style="zoom:67%;"></p>
<p>当我们对 <strong>$ w_1 $ 有一个小小的改变</strong>,比如说加上 delta $
w_1 $ 的时候,那这个 L 也会有一个改变,那这个 $ w_1 $ 呢,是透过 $ w_1 $
改变的时候,你就改变了 y,y 改变的时候你就改变了
e,然后接下来就<strong>改变了 L</strong>。</p>
<p>那什麼时候 $ w_1 $ 的改变会对 L 的影响很小呢,也就是它在 error surface
上的斜率会很小呢？一个可能性是当你的 <strong>input
很小的时候</strong>,假设 <span class="math inline">\(x_1\)</span>
的值在不同的 training example 裡面,它的值都很小,那因為 <span class="math inline">\(x_1\)</span> 是直接乘上 $ w_1 $，如果 <span class="math inline">\(x_1\)</span> 的值都很小,$ w_1 $
有一个变化的时候,它得到的,它<strong>对 y 的影响也是小的</strong>,对 e
的影响也是小的,它对 L 的影响就会是小的。反之呢,如果今天是 <span class="math inline">\(x_2\)</span> 的话。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616212503953.png" alt="image-20220616212503953" style="zoom:67%;"></p>
<p>那假设 <strong><span class="math inline">\(x_2\)</span>
的值都很大</strong>,当你的 <span class="math inline">\(w_2\)</span>
有一个小小的变化的时候,虽然 <span class="math inline">\(w_2\)</span>
这个变化可能很小,但是因為它乘上了 <span class="math inline">\(x_2\)</span>,<span class="math inline">\(x_2\)</span> 的值很大,那 y 的变化就很大,那 e
的变化就很大,那 L 的变化就会很大,就会导致我们在 w
这个方向上,做变化的时候,我们把 w 改变一点点,那我们的 error surface
就会有很大的变化。</p>
<p>所以你发现说,既然在这个 linear 的 model 裡面,当我们 input 的
feature,<strong>每一个 dimension 的值,它的 scale
差距很大</strong>的时候,我们就可能產生像这样子的 error
surface,就可能產生<strong>不同方向,斜率非常不同,坡度非常不同的 error
surface</strong>。</p>
<p>所以怎麼办呢,我们有没有可能给feature 裡面<strong>不同的
dimension,让它有同样的数值的范围</strong>。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616212559236.png" alt="image-20220616212559236" style="zoom:67%;"></p>
<p>如果我们可以给不同的
dimension,同样的数值范围的话,那我们可能就可以製造比较好的 error
surface,让 training 变得比较容易一点</p>
<p>其实有很多不同的方法,这些不同的方法,往往就合起来统称為==Feature
Normalization==</p>
<h3><span id="feature-normalization">Feature Normalization</span></h3>
<p>以下所讲的方法只是Feature Normalization 的一种可能性,它<strong>并不是
Feature Normalization 的全部</strong>,假设 <span class="math inline">\(x^1\)</span> 到 <span class="math inline">\(x^R\)</span>,是我们所有的训练资料的 feature
vector</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221138101.png" alt="image-20220616221138101" style="zoom:67%;"></p>
<p>我们把所有训练资料的 feature vector ,统统都集合起来,那每一个 vector
,<span class="math inline">\(x_1\)</span> 裡面就 $x^1_1 $代表 <span class="math inline">\(x_1\)</span> 的第一个 element,$x^2_1 $,就代表
<span class="math inline">\(x_2\)</span> 的第一个 element,以此类推</p>
<p>那我们把<strong>不同笔资料即不同 feature vector,同一个
dimension</strong> 裡面的数值,把它取出来,然后去计算某一个 dimension 的
mean，它的 mean 呢 就是<span class="math inline">\(m_i\)</span>，我们计算第 i 个 dimension
的,standard deviation,我们用<span class="math inline">\(\sigma_i\)</span>来表示它</p>
<p>那接下来我们就可以做一种 normalization,那这种 normalization
其实叫做==<strong>标準化</strong>==,其实叫
==standardization==,不过我们这边呢,就等一下都统称 normalization 就好了
<span class="math display">\[
\tilde{x}^r_i ← \frac{x^r_i-m_i}{\sigma_i}
\]</span> 我们就是把这边的某一个数值x,减掉这一个 dimension 算出来的
mean,再除掉这个 dimension,算出来的 standard deviation,得到新的数值叫做
<span class="math inline">\(\tilde{x}\)</span></p>
<p>然后得到新的数值以后,<strong>再把新的数值把它塞回去</strong>,以下都用这个
tilde来代表有被 normalize 后的数值</p>
<p>那做完 normalize 以后有什麼好处呢？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221152221.png" alt="image-20220616221152221" style="zoom:67%;"></p>
<ul>
<li><p>做完 normalize 以后啊,这个 dimension 上面的数值就会平均是
0,然后它的 variance就会是 1,所以<strong>这一排数值的分布就都会在 0
上下</strong></p></li>
<li><p>对每一个 dimension都做一样的 normalization,就会发现所有 feature
不同 dimension 的数值都在 0 上下,那你可能就可以<strong>製造一个,比较好的
error surface</strong></p></li>
</ul>
<p>所以像这样子 Feature Normalization 的方式,往往对你的 training
有帮助,它可以让你在做 gradient descent 的时候,这个 gradient
descent,<strong>它的 Loss 收敛更快一点,可以让你的 gradient
descent,它的训练更顺利一点</strong>,这个是 Feature Normalization</p>
<h3><span id="considering-deep-learning">Considering Deep Learning</span></h3>
<p><span class="math inline">\(\tilde{x}\)</span> 代表 normalize 的
feature,把它丢到 deep network 裡面,去做接下来的计算和训练,所以把 <span class="math inline">\(x_1\)</span> tilde 通过第一个 layer 得到 <span class="math inline">\(z^1\)</span>,那你有可能通过 activation
function,不管是选 Sigmoid 或者 ReLU 都可以,然后再得到 <span class="math inline">\(a^1\)</span>,然后再通过下一层等等,那就看你有几层
network 你就做多少的运算</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221202386.png" alt="image-20220616221202386" style="zoom:67%;"></p>
<p>所以每一个 x 都做类似的事情,但是如果我们进一步来想的话,对 <span class="math inline">\(w_2\)</span> 来说</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221211793.png" alt="image-20220616221211793" style="zoom:67%;"></p>
<p>这边的 <span class="math inline">\(a^1\)</span> <span class="math inline">\(a^3\)</span> 这边的 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^3\)</span>,其实也是另外一种 input,如果这边 <span class="math inline">\(\tilde{x}\)</span>,虽然它已经做 normalize
了,但是通过 $ w_1 $ 以后它就<strong>没有做 normalize</strong>,如果 <span class="math inline">\(\tilde{x}\)</span> 通过 $ w_1 $ 得到是 <span class="math inline">\(z^1\)</span>,而 <span class="math inline">\(z^1\)</span> 不同的 dimension
间,它的数值的分布仍然有很大的差异的话,那我们要 train <span class="math inline">\(w_2\)</span> 第二层的参数,会不会也有困难呢</p>
<p>对 <span class="math inline">\(w_2\)</span> 来说,这边的 a 或这边的 z
其实也是一种 feature,我们应该要对这些 feature 也做 normalization</p>
<p>那如果你选择的是 Sigmoid,那可能比较推荐对 z 做 Feature
Normalization,因為Sigmoid 是一个 s 的形状,那它在 0
附近斜率比较大,所以如果你对 z 做 Feature Normalization,把所有的值都挪到
0 附近,那你到时候算 gradient 的时候,算出来的值会比较大</p>
<p>那不过因為你不见得是用 sigmoid ,所以你也不一定要把 Feature
Normalization放在 z
这个地方,如果是选别的,也许你选a也会有好的结果,也说不定，<strong>Ingeneral
而言,这个 normalization,要放在 activation function
之前,或之后都是可以的,在实作上,可能没有太大的差别</strong>,好
那我们这边呢,就是对 z 呢,做一下 Feature Normalization，</p>
<p>那怎麼对 z 做 Feature Normalization 呢</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221239128.png" alt="image-20220616221239128" style="zoom:67%;"></p>
<p>那你就把 z,想成是另外一种 feature ,我们这边有 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,我们就把 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span> 拿出来</p>
<ul>
<li><strong>算一下它的 mean</strong>，这边的 <span class="math inline">\(μ\)</span> 是一个 vector,我们就把 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,这三个 vector 呢,把它平均起来,得到
<span class="math inline">\(μ\)</span> 这个 vector</li>
<li><strong>算一个 standard deviation</strong>,这个 standard deviation
呢,这边这个成 <span class="math inline">\(\sigma\)</span>,它也代表了一个
vector,那这个 vector 怎麼算出来呢,你就把 <span class="math inline">\(z^i\)</span>减掉 <span class="math inline">\(μ\)</span>,然后取平方,这边的平方,这个 notation
有点 abuse 啊,这边的平方就是指,对每一个 element
都去做平方,然后再开根号,这边开根号指的是对每一个
element,向量裡面的每一个 element,都去做开根号,得到 <span class="math inline">\(\sigma\)</span>,反正你知道我的意思就好</li>
</ul>
<p>把这三个 vector,裡面的每一个 dimension,都去把它的 <span class="math inline">\(μ\)</span> 算出来,把它的 <span class="math inline">\(\sigma\)</span> 算出来,好
我这边呢,就不把那些箭头呢 画出来了,从 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,算出 <span class="math inline">\(μ\)</span>,算出 <span class="math inline">\(\sigma\)</span>。</p>
<p>接下来就把这边的每一个 z ,都去减掉 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,你把 <span class="math inline">\(z^i\)</span>减掉 <span class="math inline">\(μ\)</span>,除以 <span class="math inline">\(\sigma\)</span>,就得到 <span class="math inline">\(z^i\)</span>的 tilde。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221403703.png" alt="image-20220616221403703" style="zoom:50%;"></p>
<p>那这边的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,它都是<strong>向量</strong>,所以这边这个除的意思是<strong>element
wise 的相除</strong>,就是 <span class="math inline">\(z^i\)</span>减
<span class="math inline">\(μ\)</span>,它是一个向量,所以分子的地方是一个向量,分母的地方也是一个向量,把这个两个向量,它们对应的
element 的值相除,是我这边这个除号的意思,这边得到 Z 的 tilde。</p>
<p>所以我们就是把 <span class="math inline">\(z^1\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^1\)</span> tilde,同理 <span class="math inline">\(z^2\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^2\)</span> tilde,<span class="math inline">\(z^3\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^3\)</span> tilde,那就把这个 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,做 Feature Normalization,变成 <span class="math inline">\(z^1\)</span> tilde,<span class="math inline">\(z^2\)</span> tilde 跟 <span class="math inline">\(z^3\)</span> 的 tilde。</p>
<p>接下来就看你爱做什麼 就做什麼啦,通过 activation function,得到其他
vector,然后再通过,再去通过其他 layer 等等,这样就可以了,这样你就等於对
<span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,做了 Feature Normalization,变成 <span class="math inline">\(\tilde{z}^1\)</span> <span class="math inline">\(\tilde{z}^2\)</span> <span class="math inline">\(\tilde{z}^3\)</span> 。</p>
<p>在这边有一件有趣的事情,这边的 <span class="math inline">\(μ\)</span>
跟 <span class="math inline">\(\sigma\)</span>,它们其实都是根据 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span> 算出来的。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221515889.png" alt="image-20220616221515889" style="zoom: 67%;"></p>
<p>所以这边 <span class="math inline">\(z^1\)</span>
啊,它本来,如果我们没有做 Feature Normalization 的时候,你改变了 <span class="math inline">\(z^1\)</span> 的值,你会改变这边 a
的值,但是现在啊,当你改变 <span class="math inline">\(z^1\)</span>
的值的时候,<span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 也会跟著改变,<span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 改变以后,<span class="math inline">\(z^2\)</span> 的值 <span class="math inline">\(a^2\)</span> 的值,<span class="math inline">\(z^3\)</span> 的值 <span class="math inline">\(a^3\)</span> 的值,也会跟著改变。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221555203.png" alt="image-20220616221555203" style="zoom: 67%;"></p>
<p>所以<strong>之前</strong>,我们每一个 <span class="math inline">\(\tilde{x}_1\)</span> <span class="math inline">\(\tilde{x}_2\)</span> <span class="math inline">\(\tilde{x}_3\)</span>,它是<strong>独立分开处理的</strong>,但是我们在做
<strong>Feature Normalization 以后</strong>,这三个
example,它们变得<strong>彼此关联</strong>了。</p>
<p>我们这边 <span class="math inline">\(z^1\)</span> 只要有改变,接下来
<span class="math inline">\(z^2\)</span> <span class="math inline">\(a^2\)</span> <span class="math inline">\(z^3\)</span> <span class="math inline">\(a^3\)</span>,也都会跟著改变,所以这边啊,其实你要把,当你有做
Feature Normalization 的时候,你要把这一整个 process,就是有收集一堆
feature,把这堆 feature 算出 <span class="math inline">\(μ\)</span> 跟
<span class="math inline">\(\sigma\)</span> 这件事情,当做是 network
的一部分。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221637917.png" alt="image-20220616221637917" style="zoom:67%;"></p>
<p>也就是说,你现在有一个比较大的 network</p>
<ul>
<li>你之前的 network,都只吃一个 input,得到一个 output</li>
<li>现在你有一个比较大的 network,这个大的 network,它是吃一堆
input,用这堆 input 在这个 network 裡面,要算出 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,然后接下来產生一堆 output</li>
</ul>
<p>那这个地方比较抽象,只可会意 不可言传这样子</p>
<p>那这边就会有一个问题了,因為你的训练资料裡面的 data 非常多,现在一个
data set,benchmark corpus 都上百万笔资料， GPU 的
memory,根本没有办法,把它整个 data set 的 data 都 load 进去。</p>
<p><strong><font color="red"> 在实作的时候,你不会让这一个 network
考虑整个 training data 裡面的所有 example,你只会考虑一个 batch 裡面的
example</font></strong>,举例来说,你 batch 设 64,那你这个巨大的
network,就是把 64 笔 data 读进去,算这 64 笔 data 的 <span class="math inline">\(μ\)</span>,算这 64 笔 data 的 <span class="math inline">\(\sigma\)</span>,对这 64 笔 data 都去做
normalization</p>
<p>因為我们在实作的时候,我们只对一个 batch 裡面的 data,做
normalization,所以这招叫做 ==<strong>Batch Normalization</strong>==</p>
<p>那这个 Batch Normalization,显然有一个问题
就是,<strong>你一定要有一个够大的 batch,你才算得出 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span></strong>,假设你今天,你 batch size
设 1,那你就没有什麼 <span class="math inline">\(μ\)</span> 或 <span class="math inline">\(\sigma\)</span> 可以算</p>
<p>所以这个 Batch Normalization,是适用於 batch size 比较大的时候,因為
batch size 如果比较大,<strong>也许这个 batch size 裡面的
data,就足以表示,整个 corpus
的分布</strong>,那这个时候你就可以,把这个本来要对整个 corpus,做 Feature
Normalization 这件事情,改成只在一个 batch,做 Feature Normalization,作為
approximation。</p>
<p><strong>在做 Batch Normalization
的时候,往往还会有这样的设计你算出这个 <span class="math inline">\(\tilde{z}\)</span> 以后</strong></p>
<ul>
<li><strong><font color="red"> 接下来你会把这个 <span class="math inline">\(\tilde{z}\)</span>,再乘上另外一个向量叫做 <span class="math inline">\(γ\)</span>,这个 <span class="math inline">\(γ\)</span> 也是一个向量,所以你就是把 <span class="math inline">\(\tilde{z}\)</span> 跟 <span class="math inline">\(γ\)</span> 做 element wise 的相乘,把 z
这个向量裡面的 element,跟 <span class="math inline">\(γ\)</span>
这个向量裡面的 element,两两做相乘。</font></strong></li>
<li><strong><font color="red"> 再加上 <span class="math inline">\(β\)</span> 这个向量,得到 <span class="math inline">\(\hat{z}\)</span>。</font></strong></li>
</ul>
<p><strong>而 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span>,你要把它想成是 network
的参数,它是另外再被learn出来的,</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616222053826.png" alt="image-20220616222053826" style="zoom:50%;"></p>
<h4><span id="那為什麼要加上-β-跟-γ-呢">那為什麼要加上 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span> 呢？</span></h4>
<p>有人可能会觉得说,如果我们做 normalization 以后,那这边的 <span class="math inline">\(\tilde{z}\)</span>,它的平均就一定是
0,那也许,<strong>今天如果平均是 0 的话,就是给那 network
一些限制</strong>,那<strong>也许这个限制会带来什麼负面的影响</strong>,所以我们把
<span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span> 加回去。</p>
<p>然后让 network 呢,现在它的 hidden layer 的 output平均不是 0
的话,他就自己去learn这个 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span>,来调整一下输出的分布,来调整这个 <span class="math inline">\(\hat{z}\)</span> 的分布</p>
<p>但讲到这边又会有人问说,刚才不是说做 Batch Normalization
就是,為了要让每一个不同的 dimension,它的 range
都是一样吗,现在如果加去乘上 <span class="math inline">\(γ\)</span>,再加上 <span class="math inline">\(β\)</span>,把 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 加进去,</p>
<h4><span id="这样不会不同dimension-的分布它的-range-又都不一样了吗">这样不会不同
dimension 的分布,它的 range 又都不一样了吗？</span></h4>
<p>有可能,但是你实际上在训练的时候,这个 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 的初始值啊</p>
<ul>
<li><strong>你会把这个 <span class="math inline">\(γ\)</span> 的初始值
就都设為 1,所以 <span class="math inline">\(γ\)</span>
是一个裡面的值,一开始其实是一个裡面的值,全部都是 1 的向量</strong></li>
<li><strong>那 <span class="math inline">\(β\)</span>
是一个裡面的值,全部都是 0 的向量,所以 <span class="math inline">\(γ\)</span> 是一个 one vector,都是 1 的向量,<span class="math inline">\(β\)</span> 是一个 zero vector,裡面的值都是 0
的向量</strong></li>
</ul>
<p>所以让你的 network 在一开始训练的时候,每一个 dimension
的分布,是比较接近的,也许训练到后来,你已经训练够长的一段时间,已经找到一个比较好的
error surface,走到一个比较好的地方以后,那再把 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 慢慢地加进去,好所以加 Batch
Normalization,往往对你的训练是有帮助的。</p>
<h3><span id="testing">Testing</span></h3>
<h4><span id="这个batch-normalization-在-inference或是-testing的时候会有什麼样的问题呢">这个
Batch Normalization 在 inference,或是 testing
的时候,会有什麼样的问题呢？</span></h4>
<p>在 testing 的时候,如果 当然如果今天你是在做作业,我们一次会把所有的
testing 的资料给你,所以你确实也可以在 testing 的资料上面,製造一个一个
batch。</p>
<p><strong>但是假设你真的有系统上线,你是一个真正的线上的
application,你可以说,我今天一定要等 30,比如说你的 batch size 设
64,我一定要等 64
笔资料都进来,我才一次做运算吗,这显然是不行的。</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616222338443.png" alt="image-20220616222338443" style="zoom: 67%;"></p>
<p>但是在做 Batch Normalization 的时候,一个 <span class="math inline">\(\tilde{x}\)</span>,一个 normalization 过的 feature
进来,然后你有一个 z,你的 z 呢,要减掉 <span class="math inline">\(μ\)</span> 跟除 <span class="math inline">\(\sigma\)</span>,那这个 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,是<strong>用一个 batch
的资料算出来的</strong></p>
<h4><span id="但如果今天在testing-的时候根本就没有-batch那我们要怎麼算这个-μ跟怎麼算这个-sigma-呢">但如果今天在
testing 的时候,根本就没有 batch,那我们要怎麼算这个 <span class="math inline">\(μ\)</span>,跟怎麼算这个 <span class="math inline">\(\sigma\)</span> 呢？</span></h4>
<p>所以真正的,这个实作上的解法是这个样子的,如果你看那个 PyTorch
的话呢,Batch Normalization 在 testing
的时候,你并不需要做什麼特别的处理,PyTorch 帮你处理好了</p>
<p><strong><font color="red"> 在 training 的时候,如果你有在做 Batch
Normalization 的话,在 training 的时候,你每一个 batch 计算出来的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,他都会拿出来算 ==moving
average==</font></strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616222438574.png" alt="image-20220616222438574" style="zoom:67%;"></p>
<p>你每一次取一个 batch 出来的时候,你就会算一个 <span class="math inline">\(μ^1\)</span>,取第二个 batch 出来的时候,你就算个
<span class="math inline">\(μ^2\)</span>,一直到取第 t 个 batch
出来的时候,你就算一个 <span class="math inline">\(μ^t\)</span>
。接下来你会算一个 moving average,你会把你现在算出来的 <span class="math inline">\(μ\)</span> 的一个平均值,叫做 <span class="math inline">\(μ\)</span> bar,乘上某一个
factor,那这也是一个常数,这个这也是一个 constant,这也是一个那个 hyper
parameter,也是需要调的。</p>
<p>在 PyTorch 裡面,我没记错 他就设 0.1,我记得他 P 就设 0.1,好,然后加上 1
减 P,乘上 <span class="math inline">\(μ^t\)</span> ,然后来更新你的 <span class="math inline">\(μ\)</span> 的平均值,然后最后在 testing
的时候,你就不用算 batch 裡面的 <span class="math inline">\(μ\)</span> 跟
<span class="math inline">\(\sigma\)</span> 了。</p>
<p>因為 testing 的时候,在真正 application 上,也没有 batch
这个东西,你就直接拿 <span class="math inline">\(\barμ\)</span> 跟 <span class="math inline">\(\bar\sigma\)</span> ,也就是 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 在训练的时候,得到的 moving
average,<span class="math inline">\(\barμ\)</span> 跟 <span class="math inline">\(\bar\sigma\)</span> ,来取代这边的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,这个就是 Batch Normalization,在
testing 的时候的运作方式。</p>
<h3><span id="comparison">Comparison</span></h3>
<p>好 那这个是从 Batch
Normalization,原始的文件上面截出来的一个实验结果,那在原始的文件上还讲了很多其他的东西,举例来说,我们今天还没有讲的是<strong>==,Batch
Normalization 用在 CNN
上,要怎麼用呢==</strong>,那你自己去读一下原始的文献,裡面会告诉你说,Batch
Normalization 如果用在 CNN 上,应该要长什麼样子。</p>
<blockquote>
<p><strong>卷积层上的BN使用</strong>，其实也是使用了<strong>类似权值共享的策略</strong>，<strong>把一整张特征图当做一个神经元进行处理</strong>。</p>
<p>卷积神经网络经过卷积后得到的是一系列的特征图，如果min-batch
sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch
sizes，f为特征图个数，p、q分别为特征图的宽高。</p>
<p>在cnn中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch
Normalization，mini-batch size 的大小就是：m * p *
q，于是对于每个特征图都只有一对可学习参数：γ、β。</p>
<p>相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。</p>
<ul>
<li><strong>nb在每一个特征图上的所有点沿着一个batch的样本数据的方向对数据进行求和，求平均等处理，不考虑不同特征图的数据间的运算。</strong></li>
<li><strong>lrb在每一个特征图上沿着不同特征图的方向对数据进行求和，求平均等处理，不考虑不同输入样本数据间的运算。</strong></li>
</ul>
</blockquote>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616222534083.png" alt="image-20220616222534083">
<figcaption aria-hidden="true">image-20220616222534083</figcaption>
</figure>
<p>这个是原始文献上面截出来的一个数据</p>
<ul>
<li>横轴呢,代表的是训练的过程，纵轴代表的是 validation set 上面的
accuracy</li>
<li>那这个<strong>黑色</strong>的虚线是<strong>没有做 Batch
Normalization</strong> 的结果,它用的是 inception 的 network,就是某一种
network 架构啦,也是以 CNN 為基础的 network 架构</li>
<li>然后如果有做 Batch
Normalization,你会得到<strong>红色</strong>的这一条虚线,那你会发现说,红色这一条虚线,它<strong>训练的速度,显然比黑色的虚线还要快很多</strong>,虽然最后收敛的结果啊,就你只要给它足够的训练的时间,可能都跑到差不多的
accuracy,但是<strong>红色这一条虚线,可以在比较短的时间内,就跑到一样的
accuracy</strong>,那这边这个蓝色的菱形,代表说这几个点的那个 accuracy
是一样的</li>
<li><strong>粉红色</strong>的线是 sigmoid function,就 sigmoid function
一般的认知,我们虽然还没有讨论这件事啦,但一般都会选择 ReLu,而不是用
sigmoid function,因為 sigmoid function,它的 training
是比较困难的,但是这边想要强调的点是说,<strong>就算是 sigmoid
比较难搞的,加 Batch Normalization,还是 train 的起来</strong>,那这边没有
sigmoid,没有做 Batch Normalization
的结果,因為在这个实验上,作者有说,sigmoid 不加 Batch Normalization,根本连
train 都 train 不起来</li>
<li>蓝色的实线跟这个蓝色的虚线呢,是把 learning rate 设比较大一点,乘
5,就是 learning rate 变原来的 5 倍,然后乘 30,就是 learning rate 变原来的
30 倍,那因為<strong>如果你做 Batch Normalization 的话,那你的 error
surface 呢,会比较平滑
比较容易训练,所以你可以把你的比较不崎嶇,所以你就可以把你的 learning rate
呢,设大一点</strong></li>
</ul>
<h3><span id="internal-covariate-shift">Internal Covariate Shift?</span></h3>
<p><strong>好接下来的问题就是,Batch
Normalization,它為什麼会有帮助呢</strong>,在原始的 Batch
Normalization,那篇 paper 裡面,他提出来一个概念,叫做 ==internal covariate
shift==,==covariate
shift==(训练集和预测集样本分布不一致的问题就叫做“<em>covariate
shift</em>”现象) 这个词汇是原来就有的,internal covariate
shift,我认為是,Batch Normalization 的作者自己发明的。他认為说今天在
train network 的时候,会有以下这个问题,这个问题是这样。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616225043073.png" alt="image-20220616225043073" style="zoom: 67%;"></p>
<p>network 有很多层</p>
<ul>
<li><p>x 通过第一层以后 得到 a</p></li>
<li><p>a 通过第二层以后 得到 b</p></li>
<li><p>计算出 gradient 以后,把 A update 成 A′,把 B 这一层的参数 update
成 B′</p></li>
</ul>
<p>但是作者认為说,我们在计算 B,update 到 B′ 的 gradient
的时候,这个时候前一层的参数是 A 啊,或者是前一层的 output 是小 a 啊</p>
<p>那当前一层从 A 变成 A′ 的时候,它的 output 就从小 a 变成小 a′ 啊</p>
<p>但是我们计算这个 gradient 的时候,我们是根据这个 a 算出来的啊,所以这个
update 的方向,也许它<strong>适合用在 a 上,但不适合用在 a′
上面</strong></p>
<p>那如果说 Batch Normalization 的话,我们会让,因為我们每次都有做
normalization,我们就会让 a 跟 a′
呢,它的分布比较接近,也许这样就会对训练呢,有帮助。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616225149673.png" alt="image-20220616225149673" style="zoom:67%;"></p>
<p>但是有一篇 paper 叫做,How Does Batch Normalization,Help
Optimization,然后他就<strong>打脸了internal covariate shift
的这一个观点</strong>。</p>
<p>在这篇 paper 裡面,他从各式各样的面向来告诉你说,i<strong>nternal
covariate shift,首先它不一定是 training network 的时候的一个问题,然后
Batch Normalization,它会比较好,可能不见得是因為,它解决了 internal
covariate shift。</strong></p>
<p>那在这篇 paper
裡面呢,他做了很多很多的实验,比如说他比较了训练的时候,这个 a 的分布的变化
发现<strong>,不管有没有做 Batch
Normalization,它的变化都不大</strong>。</p>
<p>然后他又说,就算是变化很大,对 training
也没有太大的伤害,然后他又说,不管你是根据 a 算出来的 gradient,还是根据 a′
算出来的 gradient,方向居然都差不多。</p>
<p>所以他告诉你说,internal covariate shift,可能不是 training network
的时候,最主要的问题,它可能也不是,Batch Normalization
会好的一个的关键,那有关更多的实验,你就自己参见这篇文章。</p>
<h4><span id="為什麼-batchnormalization-会比较好呢">為什麼 Batch
Normalization 会比较好呢？</span></h4>
<p>那在这篇 How Does Batch Normalization,Help Optimization
这篇论文裡面,他从实验上,也从理论上,至少<strong>支持了 Batch
Normalization,可以改变 error surface,让 error surface
比较不崎嶇这个观点</strong>。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616225308853.png" alt="image-20220616225308853" style="zoom:67%;"></p>
<p>所以这个观点是有理论的支持,也有实验的佐证的,那在这篇文章裡面呢,作者还讲了一个非常有趣的话,他说他觉得啊,这个
Batch Normalization 的 positive impact。</p>
<p>因為他说,如果我们要让 network,这个 error surface
变得比较不崎嶇,<strong>其实不见得要做 Batch
Normalization,感觉有很多其他的方法,都可以让 error surface
变得不崎嶇</strong>,那他就试了一些其他的方法,发现说,跟 Batch
Normalization performance
也差不多,甚至还稍微好一点,所以他就讲了下面这句感嘆。</p>
<p>他觉得说,这个,positive impact of batchnorm on training,可能是
somewhat,<strong>serendipitous</strong>,什麼是 serendipitous
呢,这个字眼可能可以翻译成偶然的,但偶然并没有完全表达这个词汇的意思,这个词汇的意思是说,你发现了一个什麼意料之外的东西。</p>
<p>那这篇文章的作者也觉得,Batch Normalization
也像是盘尼西林一样,是一种偶然的发现,但无论如何,它是一个有用的方法。</p>
<h2><span id="to-learn-more">To learn more ……</span></h2>
<p>那其实 Batch Normalization,不是唯一的 normalization,normalization
的方法有一把啦,那这边就是列了几个比较知名的,</p>
<p>Batch Renormalization https://arxiv.org/abs/1702.03275 Layer
Normalization https://arxiv.org/abs/1607.06450 Instance Normalization
https://arxiv.org/abs/1607.08022 Group Normalization
https://arxiv.org/abs/1803.08494 Weight Normalization
https://arxiv.org/abs/1602.07868 Spectrum Normalization
https://arxiv.org/abs/1705.10941</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>lzy
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://powerlzy.github.io/posts/1HFEDWZ/" title="模型训练（5）Batch Normalization">https://powerlzy.github.io/posts/1HFEDWZ/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/1T7T14B/" rel="prev" title="模型训练（6）Local Minimum And Saddle Point">
                  <i class="fa fa-chevron-left"></i> 模型训练（6）Local Minimum And Saddle Point
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/N29PC4/" rel="next" title="模型训练（4）Batch and Momentum">
                  模型训练（4）Batch and Momentum <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
