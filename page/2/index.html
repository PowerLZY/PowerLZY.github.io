<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本博客主要用于记录个人学习笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/2/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="本博客主要用于记录个人学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta property="article:tag" content="Python、Machine Learing、CyberSecurity">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">相比到达的地方，同行的人更重要！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">本博客主要用于记录个人学习笔记</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-07 17:46:00" itemprop="dateModified" datetime="2023-03-07T17:46:00+08:00">2023-03-07</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="机器学习模型评价指标"><a href="#机器学习模型评价指标" class="headerlink" title="机器学习模型评价指标"></a>机器学习模型评价指标</h2><blockquote>
<p>  一文看懂机器学习指标：准确率、精准率、召回率、F1、ROC曲线、AUC曲线:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93107394">https://zhuanlan.zhihu.com/p/93107394</a></p>
<p>  <strong>机器学习-最全面的评价指标体系: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/359997979">https://zhuanlan.zhihu.com/p/359997979</a></strong></p>
<p>  <a target="_blank" rel="noopener" href="https://github.com/HaoMood/homepage/blob/master/files/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-03-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.pdf">机器学习工程师面试宝典-03-模型评估</a></p>
<p>  <strong><a target="_blank" rel="noopener" href="http://www.china-nb.cn/gongsidongtai/17-85.html">分类模型评估指标——准确率、精准率、召回率、F1、ROC曲线、AUC曲线</a></strong></p>
</blockquote>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/image-20220421165422230.png" alt="image-20220421165422230" style="zoom:50%;"></p>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/image-20220421165436795.png" alt="image-20220421165436795" style="zoom:50%;"></p>
<h3 id="一、二分类问题"><a href="#一、二分类问题" class="headerlink" title="一、二分类问题"></a>一、二分类问题</h3><blockquote>
<p>  <strong>阈值调节问题？</strong></p>
</blockquote>
<ul>
<li><strong>准确率 (Accuracy)</strong>：<strong>预测正确的概率</strong>  【<strong>(TP+TN)/(TP+TN+FP+FN)</strong>】</li>
<li><strong>精确率（查准率 Precision )：==预测为正的样本==中实际为正的样本的概率</strong> 【<strong>TP/(TP+FP)</strong>】</li>
<li>错误发现率（FDR）= 1 - 精确率 = ==预测为正的样本==中实际为负的样本的概率 【<strong>FP/(TP+FP)</strong>】</li>
<li><strong>召回率（查全率）- Recall</strong>：<strong>==实际为正的样本==中被预测为正样本的概率</strong>【<strong>TP/(TP+FN)</strong>】</li>
<li><strong>真正率（TPR） = 灵敏度（==召回率==） =</strong> <strong>TP/(TP+FN)</strong></li>
<li><strong>假正率（FPR） = 1- 特异度 =</strong> <strong>FP/(FP+TN)</strong></li>
<li><strong>F1=是准确率和召回率的==调和平均值== (2×Precision×Recall)/（Precision+Recall）</strong></li>
<li><strong>G-mean（GM）= 是准确率和召回率的==几何平均值==</strong> <img src="https://image.jiqizhixin.com/uploads/editor/c9841ee6-28df-4eb9-aace-8902a6e525a5/640.svg" alt="img"></li>
</ul>
<h3 id="1-1-F1-2×Precision×Recall-（Precision-Recall）"><a href="#1-1-F1-2×Precision×Recall-（Precision-Recall）" class="headerlink" title="1.1 F1=(2×Precision×Recall) /（Precision+Recall）"></a>1.1 <strong>F1=(2×Precision×Recall) /（Precision+Recall）</strong></h3><p>精确率（Precision）和召回率（Recall）之间的关系用图来表达，就是下面的PR曲线。可以发现他们俩的关系是「两难全」的关系。为了综合两者的表现，在两者之间找一个平衡点，就出现了一个 F1分数。</p>
<h4 id="F1-2×Precision×Recall-（Precision-Recall）"><a href="#F1-2×Precision×Recall-（Precision-Recall）" class="headerlink" title="F1=(2×Precision×Recall)  /（Precision+Recall）"></a><strong>F1=(2×Precision×Recall)  /（Precision+Recall）</strong></h4><p>P意义类似于每通过准确预测得到TP个正例需要TP+FP个预测类别为正例的样本。</p>
<p>R意义类似于每通过成功召回得到TP个正例需要TP+FN个真实类别为正例的样本。</p>
<p>F1度量了给定一批样本，对这一批样本进行预测与召回，最终得到的正例的多少。<strong>其中一半的正例是通过预测得到的，一半的正例是通过召回得到的。</strong></p>
<p>有一种把预测所需的预测类别为正例的样本和召回所需的真实类别为正例的样本看作原料，而我们的目标正例样本看作产品的感觉。<strong>所以也能解释为什么P跟R其中一者比较低的时候，F1会偏低。因为跟算术平均数不一样，两者不能互相替代，两部分各负责一半。那么加权调和平均Fbeta也可以很好的理解了。</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BF_%7B%5Cbeta%7D%7D%3D%5Cfrac%7B1%7D%7B1%2B%5Cbeta%5E%7B2%7D%7D%5Ccdot%5Cleft%28+%5Cfrac%7B1%7D%7BP%7D%2B+%5Cfrac%7B%5Cbeta%5E%7B2%7D%7D%7BR%7D%5Cright%29" alt="[公式]"></p>
<p>各自负责的比例不一样了。因此beta越大，Fbeta越着重考虑召回能力。</p>
<h3 id="1-2-ROC-AUC的概念"><a href="#1-2-ROC-AUC的概念" class="headerlink" title="1.2 ROC/AUC的概念"></a>1.2 ROC/AUC的概念</h3><p><strong>1. 灵敏度，特异度，真正率，假正率</strong></p>
<p>在正式介绍ROC/AUC之前，我们还要再介绍两个指标，<strong>这两个指标的选择也正是ROC和AUC可以无视样本不平衡的原因。</strong> 这两个指标分别是：<strong>灵敏度和（1-特异度），也叫做真正率（TPR）和假正率（FPR）</strong>。其实我们可以发现<strong>灵敏度和召回率是一模一样的，只是名字换了而已</strong>。由于我们比较关心正样本，所以需要查看有多少负样本被错误地预测为正样本，所以使用（1-特异度），而不是特异度。</p>
<p><strong>真正率（TPR） = 灵敏度（==召回率==） =</strong> <strong>TP/(TP+FN)</strong></p>
<p><strong>假正率（FPR） = 1- 特异度 =</strong> <strong>FP/(FP+TN)</strong></p>
<p>下面是真正率和假正率的示意，我们发现<strong>TPR和FPR分别是基于实际表现1和0出发的，也就是说它们分别在实际的正样本和负样本中来观察相关概率问题。</strong> </p>
<blockquote>
<p>  正因为如此，所以无论样本是否平衡，都不会被影响。还是拿之前的例子，总样本中，90%是正样本，10%是负样本。我们知道用准确率是有水分的，但是用TPR和FPR不一样。这里，TPR只关注90%正样本中有多少是被真正覆盖的，而与那10%毫无关系，同理，FPR只关注10%负样本中有多少是被错误覆盖的，也与那90%毫无关系，</p>
</blockquote>
<p><strong>如果我们从实际表现的各个结果角度出发，就可以避免样本不平衡的问题了，这也是为什么选用TPR和FPR作为ROC/AUC的指标的原因。</strong></p>
<h4 id="2-ROC（接受者操作特征曲线）"><a href="#2-ROC（接受者操作特征曲线）" class="headerlink" title="2. ROC（接受者操作特征曲线）"></a><strong>2. ROC（接受者操作特征曲线）</strong></h4><blockquote>
<p>  ROC（Receiver Operating Characteristic）曲线，又称接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力，ROC曲线是基于<strong>混淆矩阵</strong>得出的。</p>
</blockquote>
<p>ROC曲线中的主要两个指标就是<strong>真正率</strong>和<strong>假正率，</strong> 上面也解释了这么选择的好处所在。其中<strong>横坐标为假正率（FPR），纵坐标为真正率（TPR）</strong>，下面就是一个标准的ROC曲线图。</p>
<h4 id="AUC的缺陷？"><a href="#AUC的缺陷？" class="headerlink" title="AUC的缺陷？"></a>AUC的缺陷？</h4><p><strong>优点</strong>：目前普遍认为接收器工作特性曲线（ROC）曲线下的面积—AUC是评估分类模型准确性的标准方法。<strong>它避免了在阈值选择过程中假定的主观性</strong>，当连续的概率得到的分数被转换为二分类标签时，通过总结整体模型表现，其衡量模型区分正负样本的性能优于通过阈值来判断的其他方法（比如准确率、召回率等）。</p>
<ul>
<li><strong>忽略了预测的概率值和模型的拟合优度</strong></li>
<li><strong>AUC反应了太过笼统的信息。无法反应召回率、精确率等在实际业务中经常关心的指标</strong></li>
<li><font color="red"> **对FPR和TPR两种错误的代价同等看待**</font></li>
<li>它没有给出模型误差的空间分布信息</li>
<li>最重要的一点，AUC的misleading的问题</li>
</ul>
<p><strong>==auc仅反应模型的排序能力==，无法反应模型的拟合优度；auc很多时候无法直接反应细粒度的和业务目标更相关的metric信息，例如 top k的准确率，召回率等等（例如同auc的模型在不同的区间的预测能力是存在差别的）；</strong></p>
<h3 id="1-3、K-S曲线"><a href="#1-3、K-S曲线" class="headerlink" title="1.3、K-S曲线"></a>1.3、K-S曲线</h3><blockquote>
<p>  <strong>K-S曲线</strong>，又称作洛伦兹曲线。实际上，K-S曲线的数据来源以及本质和ROC曲线是一致的，只是ROC曲线是把真正率（ <img src="https://www.zhihu.com/equation?tex=TPR" alt="[公式]"> ）和假正率（ <img src="https://www.zhihu.com/equation?tex=FPR" alt="[公式]"> ）当作横纵轴，<strong>而K-S曲线是把真正率（ <img src="https://www.zhihu.com/equation?tex=TPR" alt="[公式]"> ）和假正率（ <img src="https://www.zhihu.com/equation?tex=FPR" alt="[公式]"> )都当作是纵轴，横轴则由选定的阈值来充当。从 </strong>K-S 曲线<strong>就能衍生出 <img src="https://www.zhihu.com/equation?tex=KS" alt="[公式]"> 值， <img src="https://www.zhihu.com/equation?tex=KS+%3D+max%28TPR+-+FPR%29" alt="[公式]"> ，即是两条曲线之间的最大间隔距离。</strong></p>
</blockquote>
<p><strong>K-S曲线的画法：</strong></p>
<ol>
<li><p><strong>排序：</strong>对于二元分类器来说，模型训练完成之后每个样本都会得到一个类概率值，把样本按这个类概率值从大到小进行排序；</p>
</li>
<li><p><strong>找阈值：</strong>取排序后前 <img src="https://www.zhihu.com/equation?tex=10%5C%25%5Ctimes+k%28k%3D1%2C2%2C3%2C...%2C9%29" alt="[公式]"> 处的值（概率值）作为阈值，分别计算出不同的 <img src="https://www.zhihu.com/equation?tex=TPR" alt="[公式]"> 和<img src="https://www.zhihu.com/equation?tex=FPR" alt="[公式]"> 值，以<img src="https://www.zhihu.com/equation?tex=10%5C%25%5Ctimes+k%28k%3D1%2C2%2C3%2C...%2C9%29" alt="[公式]">为横坐标，分别以<img src="https://www.zhihu.com/equation?tex=TPR" alt="[公式]"> 和<img src="https://www.zhihu.com/equation?tex=FPR" alt="[公式]"> 值为纵坐标，就可以画出两个曲线，这就是K-S曲线，类似于下图。</p>
</li>
<li><p><strong>KS值</strong>：</p>
<p>从 <strong>K-S 曲线</strong>就能衍生出 <img src="https://www.zhihu.com/equation?tex=KS" alt="[公式]"> 值， <img src="https://www.zhihu.com/equation?tex=KS+%3D+max%28TPR+-+FPR%29" alt="[公式]"> ，即是两条曲线之间的最大间隔距离。KS值越大表示模型 的区分能力越强。</p>
</li>
</ol>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/v2-f913b42cefcd32f9fdbfa027de2dfbc8_1440w.jpg" alt="img" style="zoom: 50%;"></p>
<h3 id="1-4-Lift曲线"><a href="#1-4-Lift曲线" class="headerlink" title="1.4 Lift曲线"></a>1.4 Lift曲线</h3><p><strong>Lift曲线它衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。实质上它强调的是投入与产出比</strong>。</p>
<p><strong>tip:</strong>理解<strong>Lift</strong>可以先看一下Quora上的一篇文章：<strong><a href="https://link.zhihu.com/?target=https%3A//www.quora.com/Whats-Lift-curve">What’s Lift curve?</a></strong></p>
<p><strong>Lift计算公式：</strong>先介绍几个相关的指标，以免混淆：</p>
<ul>
<li><strong>准确率（accuracy，ACC）</strong>：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=ACC%3D%5Cfrac%7BTP%2BTN%7D%7BFP%2BFN%2BTP%2BTN%7D%5C%5C" alt="[公式]"></p>
<ul>
<li><strong>正确率(Precision，PRE)，查准率</strong>：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=PRE+%3D+%5Cfrac%7BTP%7D%7BTP%2BFP%7D+%5C%5C" alt="[公式]"></p>
<ul>
<li><strong>真阳性率(True Positive Rate，TPR)，灵敏度(Sensitivity)，召回率(Recall)</strong>：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=TPR%3D%5Cfrac%7BTP%7D%7BTP%2BFN%7D+%5C%5C" alt="[公式]"></p>
<ul>
<li><strong>假阳性率(False Positice Rate，FPR)，误诊率( = 1 - 特异度)</strong>：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=FPR%3D%5Cfrac%7BFP%7D%7BFP%2BTN%7D%5C%5C" alt="[公式]"></p>
<p><strong>Lift计算公式：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=Lift%3D%5Cfrac%7B%5Cfrac%7BTP%7D%7BTP%2BFP%7D%7D%7B%5Cfrac%7BTP%2BFN%7D%7BTP%2BFP%2BTN%2BFN%7D%7D%3D%5Cfrac%7BPRE%7D%7B%E6%AD%A3%E4%BE%8B%E5%8D%A0%E6%AF%94%7D%5C%5C" alt="[公式]"></p>
<p>根据以上公式可知，<strong>Lift指标可以这样理解：</strong>在不使用模型的情况下，我们用先验概率估计正例的比例，即上式子分母部分，以此作为正例的命中率；利用模型后，我们不需要从整个样本中来挑选正例，只需要从我们预测为正例的那个样本的子集 <img src="https://www.zhihu.com/equation?tex=TP%2BFP" alt="[公式]"> 中挑选正例，这时正例的命中率为 <img src="https://www.zhihu.com/equation?tex=PRE" alt="[公式]"> ，后者除以前者即可得提升值<strong>Lift。</strong></p>
<h4 id="Lift曲线："><a href="#Lift曲线：" class="headerlink" title="Lift曲线："></a><strong>Lift曲线：</strong></h4><p>为了作出<strong>LIft</strong>曲线，首先引入 <img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]"> 的概念：</p>
<p><img src="https://www.zhihu.com/equation?tex=depth%3D%5Cfrac%7BTP%2BFP%7D%7BTP%2BFP%2BTN%2BFN%7D%5C%5C" alt="[公式]"></p>
<p><strong>从公式可以看出</strong>，<img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]">代表的是预测为正例的样本占整个样本的比例。</p>
<p>当阈值为0时，所有的样本都被预测为正例，因此 <img src="https://www.zhihu.com/equation?tex=depth%3D1" alt="[公式]"> ，于是 <img src="https://www.zhihu.com/equation?tex=Lift%3D1" alt="[公式]"> ，模型未起提升作用。随着阈值逐渐增大，被预测为正例的样本数逐渐减少，<img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]">减小，而较少的预测正例样本中的真实正例比例逐渐增大。当阈值增大至1时，没有样本被预测为正例，此时 <img src="https://www.zhihu.com/equation?tex=depth%3D0" alt="[公式]"> ，而 <img src="https://www.zhihu.com/equation?tex=Lift%3D0" alt="[公式]"> 。由此可见， <img src="https://www.zhihu.com/equation?tex=Lift" alt="[公式]"> 与<img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]">存在相反方向变化的关系。在此基础上作出 <img src="https://www.zhihu.com/equation?tex=Lift" alt="[公式]"> 图：</p>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/v2-4cfa1e77335b91d9a47acb7238383c1e_1440w.jpg" alt="img" style="zoom: 50%;"></p>
<p>一般要求，在尽量大的 <img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]"> 下得到尽量大的 <img src="https://www.zhihu.com/equation?tex=Lift" alt="[公式]">，所以 <img src="https://www.zhihu.com/equation?tex=Lift" alt="[公式]"> 曲线的右半部分应该尽量陡峭。</p>
<h3 id="1-5-P-R曲线"><a href="#1-5-P-R曲线" class="headerlink" title="1.5 P-R曲线"></a>1.5 <strong>P-R曲线</strong></h3><ul>
<li><p><strong>精确率（查准率）- Precision ：==预测为正的样本==中实际为正的样本的概率</strong> 【<strong>TP/(TP+FP)</strong>】</p>
</li>
<li><p><strong>召回率（查全率）- Recall</strong>：<strong>==实际为正的样本==中被预测为正样本的概率</strong>【<strong>TP/(TP+FN)</strong>】</p>
</li>
</ul>
<p>P-R曲线刻画<strong>查准率</strong>和<strong>查全率（召回率）</strong>之间的关系，查准率指的是在所有预测为正例的数据中，真正例所占的比例，查全率是指预测为真正例的数据占所有正例数据的比例。查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，查全率高时，查准率往往偏低。</p>
<p>在很多情况下，我们可以根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在后面的是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可计算当前的查全率和查准率，以查准率为y轴，以查全率为x轴，可以画出下面的P-R曲线。</p>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/v2-dc6abbb24e2dfbfefe4777408d2a8e5c_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p>如果一个学习器的P-R曲线被另一个学习器的P-R曲线完全包住，则可断言后者的性能优于前者，当然我们可以根据曲线下方的面积大小来进行比较，但更常用的是<strong>平衡点</strong>或者是F1值。</p>
<ul>
<li><strong>平衡点（BEP）</strong>是查准率=查全率时的取值，如果这个值较大，则说明学习器的性能较好。F1值越大，我们可以认为该学习器的性能较好。</li>
<li><font color="red"> **F1度量**：**BEP过于简单，这个平衡点是建立在”查准率=查全率“的前提下，无法满足实际不同场景的应用。**</font>

</li>
</ul>
<p>我们先来引入加权调和平均： <img src="https://www.zhihu.com/equation?tex=F_%5Cbeta" alt="[公式]">：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+%5Cfrac+%7B1%7D%7BF_%7B%5Cbeta%7D%7D%3D%5Cfrac+%7B1%7D%7B1%2B%7B%5Cbeta%7D%5E2%7D%28%5Cfrac%7B1%7D%7BP%7D%2B%5Cfrac%7B%5Cbeta%5E2%7D%7BR%7D%29++++%5Cquad+%E5%85%AC%E5%BC%8F%281%29" alt="[公式]"></p>
<p>加权调和平均与<strong>算术平均</strong> <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BP%2BR%7D%7B2%7D" alt="[公式]"> 和<strong>几何平均</strong> <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BP%2BR%7D" alt="[公式]"> 相比，<strong>调和平均更重视较小值（这可以从倒数上看出来）</strong>。当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3D1+" alt="[公式]"> ，即F1是基于查准率和查全率的调和平均定义的，F1的公式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+%5Cfrac+%7B1%7D%7BF_%7B1%7D%7D%3D%5Cfrac+%7B1%7D%7B2%7D%28%5Cfrac%7B1%7D%7BP%7D%2B%5Cfrac%7B1%7D%7BR%7D%29" alt="[公式]"></p>
<p>我们把公式求倒数，即可得：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+F1%3D%5Cfrac%7B2%2AP%2AR%7D%7BP%2BR%7D" alt="[公式]"></p>
<p>在一些应用中，对查准率和查全率的重视程度不同。例如在商品推荐中，为了尽可能少打扰用户，更希望推荐的内容确实是用户感兴趣的，此时查准率更重要；而在罪犯信息检索或者病人检查系统中，更希望尽可能少的漏判，此时查全率更重要。F1度量的一般形式是 <img src="https://www.zhihu.com/equation?tex=F_%7B%5Cbeta%7D" alt="[公式]"> ，能让我们自定义对查准率/查全率的不同偏好：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+F_%7B%5Cbeta%7D%3D%5Cfrac%7B%281%2B%5Cbeta%5E2%29%2AP%2AR%7D%7B%28%5Cbeta%5E2%2AP%29%2BR%7D" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3E0+" alt="[公式]"> 度量了查全率对查准率的相对重要性（不明白的同学可以回看公式1）， <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3D1" alt="[公式]"> 时退化为标准F1，<img src="https://www.zhihu.com/equation?tex=%5Cbeta%3E1+" alt="[公式]">==时查全率有更大影响； <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3C1" alt="[公式]"> 时，查准率有更大影响。==</p>
<h3 id="1-6-对数损失-Log-Loss"><a href="#1-6-对数损失-Log-Loss" class="headerlink" title="1.6 对数损失(Log Loss)"></a>1.6 <strong>对数损失(Log Loss)</strong></h3><p><strong>AUC ROC考虑用于确定模型性能的预测概率</strong>。然而，AUC ROC存在问题，它只考虑概率的顺序，因此<strong>没有考虑模型预测更可能为正样本的更高概率的能力(即考虑了大小，但没有考虑更高精度)</strong>。<strong>在这种情况下，我们可以使用对数损失，即每个实例的正例预测概率的对数的负平均值。</strong></p>
<p>对数损失（Logistic Loss，logloss）是对预测概率的似然估计，其标准形式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+logloss%3DlogP%28Y%7CX%29" alt="[公式]"></p>
<p>对数损失最小化本质是上利用样本中的已知分布，求解拟合这种分布的最佳模型参数，使这种分布出现概率最大。</p>
<p>对数损失对应的二分类的计算公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=logloss%3D-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_ilog%5Chat%7By_i%7D%2B%281-y_i%29log%281-%5Chat%7By_i%7D%29%29+%2C%5Cquad%5Cquad%5Cquad+y%5Cin%5B0%2C1%5D" alt="[公式]"></p>
<p>其中N为样本数， <img src="https://www.zhihu.com/equation?tex=%5Chat+y_i" alt="[公式]"> 为预测为1的概率。对数损失在多分类问题中也可以使用，其计算公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=logloss%3D-%5Cfrac%7B1%7D%7BN%7D%5Cfrac%7B1%7D%7BC%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Csum_%7Bj%3D1%7D%5E%7BC%7D%28y_%7Bij%7Dlog%5Chat%7By_%7Bij%7D%7D%29+%2C%5Cquad%5Cquad%5Cquad+y%5Cin%5B0%2C1%5D" alt="[公式]"></p>
<p>其中，N为样本数，C为类别数，logloss衡量的是预测概率分布和真实概率分布的差异性，取值越小越好。</p>
<h3 id="1-7-多分类"><a href="#1-7-多分类" class="headerlink" title="1.7 多分类"></a>1.7 多分类</h3><p>很多时候我们有多个<strong>二分类混淆矩阵</strong>，例如进行多次训练/测试，每次得到一个混淆矩阵；或是在多个数据集上进行训练/测试，希望估计算法的全局性能；或者是执行分类任务，每两两类别的组合都对应一个混淆矩阵；总之是在<strong>n个二分类混淆矩阵上综合考察查准率和查全率</strong>。</p>
<ul>
<li><strong>宏观</strong>：在各个混淆军阵上分别计算出查准率和查全率，记为(P1,R1)，(P2,R2),…(Pn,Rn)，在<strong>计算平均值</strong>，这样就得到“宏观查准率”(macro-P)，“宏观查全率”(macro-R)、“宏观F1”(macro-F1)：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+macro-P+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7DP_i" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+macro-R+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7DR_i" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+macro-F1%3D%5Cfrac%7B2%2Amacro-P%2Amacro-R%7D%7Bmacro-P%2Bmacro-R%7D" alt="[公式]"></p>
<ul>
<li><strong>微观</strong>：<strong>将个混淆矩阵对应的元素进行平均，得到TP、FP、TN、FN的平均值</strong>，分别记为 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTP%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BFP%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BFN%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTN%7D" alt="[公式]"> ，再基于这些平均值计算出“微观查准率”(micro-P)，“微观查全率”(micro-R)、“微观F1”(micro-F1)：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+micro-P%3D%5Cfrac%7B%5Coverline%7BTP%7D%7D%7B%5Coverline%7BTP%7D%2B%5Coverline%7BFP%7D%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+micro-R%3D%5Cfrac%7B%5Coverline%7BTP%7D%7D%7B%5Coverline%7BTP%7D%2B%5Coverline%7BFN%7D%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+micro-F1%3D%5Cfrac%7B2%2Amicro-P%2Amicro-R%7D%7Bmicro-P%2Bmicro-R%7D" alt="[公式]"></p>
<h2 id="二、回归问题评价指标"><a href="#二、回归问题评价指标" class="headerlink" title="二、回归问题评价指标"></a>二、回归问题评价指标</h2><blockquote>
<p>  <strong>均方差损失 Mean Squared Loss、平均绝对误差损失 Mean Absolute Error Loss、Huber Loss、分位数损失 Quantile Loss</strong></p>
</blockquote>
<p>机器学习中的监督学习本质上是给定一系列训练样本 <img src="https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29" alt="[公式]"> ，尝试学习 <img src="https://www.zhihu.com/equation?tex=x%5Crightarrow+y" alt="[公式]"> 的映射关系，使得给定一个 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，即便这个 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 不在训练样本中，也能够得到尽量接近真实 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 的输出 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="[公式]"> 。而损失函数（Loss Function）则是这个过程中关键的一个组成部分，用来<strong>衡量模型的输出</strong> <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="[公式]"> <strong>与真实的</strong> <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> <strong>之间的差距</strong>，给模型的优化指明方向。</p>
<h3 id="2-1-均方差损失-MSE、L2-loss"><a href="#2-1-均方差损失-MSE、L2-loss" class="headerlink" title="2.1 均方差损失 MSE、L2 loss"></a>2.1 均方差损失 MSE、L2 loss</h3><h5 id="基本形式与原理"><a href="#基本形式与原理" class="headerlink" title="基本形式与原理"></a><strong>基本形式与原理</strong></h5><p><strong>均方差Mean Squared Error (MSE)损失是机器学习、深度学习回归任务中最常用的一种损失函数</strong>，也称为 <strong>L2 Loss</strong>。其基本形式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7BMSE%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_i+-+%5Chat%7By_i%7D%29%5E2+%5C%5C" alt="[公式]"></p>
<p>从直觉上理解均方差损失，这个损失函数的最小值为 0（当预测等于真实值时），最大值为无穷大。下图是对于真实值 <img src="https://www.zhihu.com/equation?tex=y%3D0" alt="[公式]"> ，不同的预测值 <img src="https://www.zhihu.com/equation?tex=%5B-1.5%2C+1.5%5D" alt="[公式]"> 的均方差损失的变化图。横轴是不同的预测值，纵轴是均方差损失，可以看到随着预测与真实值绝对误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y-+%5Chat%7By%7D%5Crvert" alt="[公式]"> 的增加，均方差损失呈二次方地增加。</p>
<p><img src="https://pic1.zhimg.com/80/v2-f13a4355c21d16cad8b3f30e8a24b5cc_1440w.jpg" alt="img"></p>
<blockquote>
<h4 id="背后的假设"><a href="#背后的假设" class="headerlink" title="背后的假设"></a>背后的假设</h4><p>  <strong>【独立同分布-中心极限定理】</strong>：<br>  如果 <img src="https://www.zhihu.com/equation?tex=%5C%7BX_n%5C%7D" alt="[公式]"> 独立同分布，且 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb+EX%3D%5Cmu%2C%5Cquad+%5Cmathbb+D+X%3D%5Csigma%5E2%3E0" alt="[公式]"> ，则n足够大时 <img src="https://www.zhihu.com/equation?tex=%5Coverline+X_n" alt="[公式]"> 近似服从正态分布 <img src="https://www.zhihu.com/equation?tex=N%5Cleft%28%5Cmu%2C%5Cfrac%7B%5Csigma%5E2%7Dn%5Cright%29" alt="[公式]"> ，即</p>
<p>  <img src="https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Cto%5Cinfty%7DP%5Cleft%28%5Cfrac%7B%5Coverline+X_n-%5Cmu%7D%7B%5Csigma%2F%5Csqrt+n%7D%3Ca%5Cright%29%3D%5CPhi%28a%29%3D%5Cint_%7B-%5Cinfty%7D%5Ea%5Cfrac1%7B%5Csqrt%7B2%5Cpi%7D%7De%5E%7B-t%5E2%2F2%7Ddt%5C%5C" alt="[公式]"></p>
<p>  实际上在一定的假设下，我们可以使用最大化似然得到均方差损失的形式。假设<strong>模型预测与真实值之间的误差服从标准高斯分布</strong>（ <img src="https://www.zhihu.com/equation?tex=%5Cmu%3D0%2C+%5Csigma%3D1" alt="[公式]"> ），则给定一个 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 模型输出真实值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 的概率为</p>
<p>  <img src="https://www.zhihu.com/equation?tex=p%28y_i%7Cx_i%29+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cmathbb%7Bexp%7D%5Cleft+%28-%5Cfrac%7B%28y_i-%5Chat%7By_i%7D%29%5E2%7D%7B2%7D%5Cright+%29+%5C%5C" alt="[公式]"></p>
<p>  <strong>进一步我们假设数据集中 N 个样本点之间相互独立，则给定所有 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 输出所有真实值 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 的概率，即似然 Likelihood</strong>，为所有 <img src="https://www.zhihu.com/equation?tex=p%28y_i+%5Cvert+x_i%29" alt="[公式]"> 的累乘</p>
<p>  <img src="https://www.zhihu.com/equation?tex=L%28x%2C+y%29+%3D+%5Cprod_%7Bi%3D1%7D%5E%7BN%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cmathbb%7Bexp%7D%5Cleft+%28-%5Cfrac%7B%28y_i-%5Chat%7By_i%7D%29%5E2%7D%7B2%7D%5Cright%29+%5C%5C" alt="[公式]"></p>
<p>  通常为了计算方便，我们通常最大化对数似然 Log-Likelihood</p>
<p>  <img src="https://www.zhihu.com/equation?tex=LL%28x%2C+y%29%3D%5Cmathbb%7Blog%7D%28L%28x%2C+y%29%29%3D-%5Cfrac%7BN%7D%7B2%7D%5Cmathbb%7Blog%7D2%5Cpi+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%28y_i-%5Chat%7By_i%7D%29%5E2+%5C%5C" alt="[公式]"></p>
<p>  去掉与 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]"> 无关的第一项，然后转化为最小化负对数似然 Negative Log-Likelihood</p>
<p>  <img src="https://www.zhihu.com/equation?tex=NLL%28x%2C+y%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_i+-+%5Chat%7By_i%7D%29%5E2+%5C%5C" alt="[公式]"></p>
<p>  可以看到这个实际上就是均方差损失的形式。也就是说<strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的</strong>，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。</p>
</blockquote>
<h3 id="hulu-百面机器学习-——-平方根误差的”意外“"><a href="#hulu-百面机器学习-——-平方根误差的”意外“" class="headerlink" title=" hulu 百面机器学习 —— 平方根误差的”意外“"></a><strong><font color="red"> hulu 百面机器学习 —— 平方根误差的”意外“</font></strong></h3><h4 id="95-的时间区间效果很好，RMSE指标居高不下的原因？"><a href="#95-的时间区间效果很好，RMSE指标居高不下的原因？" class="headerlink" title="95%的时间区间效果很好，RMSE指标居高不下的原因？"></a>95%的时间区间效果很好，RMSE指标居高不下的原因？</h4><p><img src="https://www.zhihu.com/equation?tex=J_%7BMSE%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_i+-+%5Chat%7By_i%7D%29%5E2+%5C%5C" alt="[公式]"></p>
<p>一般情况下RSME能反应预测值与真实值的偏离程度，但是<strong>易受离群点</strong>的影响；</p>
<p><strong>解决方案</strong>：</p>
<ul>
<li>数据预处理将噪音去掉</li>
<li>将离群点的产生机制建模进去</li>
<li>更鲁棒的模型评估指标：<strong>平均绝对百分比误差</strong>（MAPE），<strong>分位数损失</strong></li>
</ul>
<h4 id="2-2-平均绝对误差-MAE"><a href="#2-2-平均绝对误差-MAE" class="headerlink" title="2.2 平均绝对误差 MAE"></a>2.2 <strong>平均绝对误差 MAE</strong></h4><p><strong>平均绝对误差 Mean Absolute Error (MAE）</strong> 是另一类常用的损失函数，也称为 <strong>L1 Loss</strong>。其基本形式如下</p>
<p><img src="https://www.zhihu.com/equation?tex=+J_%7BMAE%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft+%7C+y_i+-+%5Chat%7By_i%7D+%5Cright+%7C+%5C%5C" alt="[公式]"></p>
<p>同样的我们可以对这个损失函数进行可视化如下图，MAE 损失的最小值为 0（当预测等于真实值时），最大值为无穷大。可以看到随着预测与真实值绝对误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y-+%5Chat%7By%7D%5Crvert" alt="[公式]"> 的增加，MAE 损失呈线性增长。</p>
<p><img src="https://pic3.zhimg.com/80/v2-fd248542b6b5aa9fadcab44340045dee_1440w.jpg" alt="img"></p>
<blockquote>
<h4 id="背后的假设-1"><a href="#背后的假设-1" class="headerlink" title="背后的假设"></a>背后的假设</h4><p>  同样的我们可以在一定的假设下通过最大化似然得到 MAE 损失的形式，假设<strong>模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution</strong>（ <img src="https://www.zhihu.com/equation?tex=%5Cmu%3D0%2C+b%3D1" alt="[公式]"> ），则给定一个 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 模型输出真实值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 的概率为</p>
<p>  <img src="https://www.zhihu.com/equation?tex=p%28y_i%7Cx_i%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cmathbb%7Bexp%7D%28-%5Cleft+%7Cy_i-%5Chat%7By_i%7D%5Cright%7C%29+%5C%5C" alt="[公式]"></p>
<p>  与上面推导 MSE 时类似，我们可以得到的负对数似然实际上就是 MAE 损失的形式</p>
<p>  <img src="https://www.zhihu.com/equation?tex=L%28x%2C+y%29+%3D+%5Cprod_%7Bi%3D1%7D%5E%7BN%7D%5Cfrac%7B1%7D%7B2%7D%5Cmathbb%7Bexp%7D%28-%7Cy_i-%5Chat%7By_i%7D%7C%29%5C%5C+++LL%28x%2C+y%29+%3D+N%5Cln%7B%5Cfrac%7B1%7D%7B2%7D%7D+-+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%7Cy_i-%5Chat%7By_i%7D%7C+%5C%5C+++NLL%28x%2C+y%29+%3D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%7Cy_i-%5Chat%7By_i%7D%7C++%5C%5C" alt="[公式]"></p>
</blockquote>
<h3 id="2-3-MAE-与-MSE-区别"><a href="#2-3-MAE-与-MSE-区别" class="headerlink" title="2.3 MAE 与 MSE 区别"></a>2.3 MAE 与 MSE 区别</h3><p>MAE 和 MSE 作为损失函数的主要区别是：<strong>MSE 损失相比 MAE 通常可以更快地收敛，但 MAE 损失对于 outlier 更加健壮</strong>，即更加不易受到 outlier 影响。</p>
<ul>
<li><p><strong>MSE 通常比 MAE 可以更快地收敛</strong>。当使用梯度下降算法时，MSE 损失的梯度为 <img src="https://www.zhihu.com/equation?tex=-%5Chat%7By_i%7D" alt="[公式]"> ，而 MAE 损失的梯度为 <img src="https://www.zhihu.com/equation?tex=%5Cpm1" alt="[公式]"> ，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y_i-%5Chat%7By_i%7D+%5Crvert" alt="[公式]"> 很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的。当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了 MSE 在大部分时候比 MAE 收敛地更快。这个也是 MSE 更为流行的原因。</p>
</li>
<li><p><strong>MAE 对于异常值（outlier） 更加 robust</strong>。我们可以从两个角度来理解这一点：</p>
<ul>
<li>第一个角度是直观地理解，下图是 MAE 和 MSE 损失画到同一张图里面，由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失。<strong>因此当数据中出现一个误差非常大的 outlier 时，MSE 会产生一个非常大的损失，对模型的训练会产生较大的影响</strong>。<img src="https://pic2.zhimg.com/80/v2-c8edffe0406dafae41a042e412cd3251_1440w.jpg" alt="img"></li>
<li>第二个角度是从两个损失函数的假设出发，MSE 假设了误差服从高斯分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于 outlier 更加 robust。参考下图（来源：<a href="https://link.zhihu.com/?target=https%3A//www.cs.ubc.ca/~murphyk/MLbook/">Machine Learning: A Probabilistic Perspective</a> 2.4.3 The Laplace distribution Figure 2.8），当右图右侧出现了 outliers 时，拉普拉斯分布相比高斯分布受到的影响要小很多。因此以拉普拉斯分布为假设的 MAE 对 outlier 比高斯分布为假设的 MSE 更加 robust。<img src="https://pic1.zhimg.com/80/v2-93ad65845f5b0dc0327fde4ded661804_1440w.jpg" alt="img" style="zoom: 67%;"></li>
</ul>
</li>
</ul>
<h3 id="2-4-Huber-Loss"><a href="#2-4-Huber-Loss" class="headerlink" title="2.4 Huber Loss"></a>2.4 Huber Loss</h3><blockquote>
<ul>
<li>在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定</li>
<li>在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。</li>
</ul>
</blockquote>
<p>上文我们分别介绍了 MSE 和 MAE 损失以及各自的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Huber_loss">Huber Loss</a> 则是一种将 MSE 与 MAE 结合起来，取两者优点的损失函数，也被称作 Smooth Mean Absolute Error Loss 。其原理很简单，就是在误差接近 0 时使用 MSE，误差较大时使用 MAE，公式为</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7Bhuber%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%5Cmathbb%7BI%7D_%7B%7C+y_i+-+%5Chat%7By_i%7D%7C+%5Cleq+%5Cdelta%7D+%5Cfrac%7B%28y_i+-+%5Chat%7By_i%7D%29%5E2%7D%7B2%7D%2B+%5Cmathbb%7BI%7D_%7B%7C+y_i+-+%5Chat%7By_i%7D%7C+%3E+%5Cdelta%7D+%28%5Cdelta+%7Cy_i+-+%5Chat%7By_i%7D%7C+-+%5Cfrac%7B1%7D%7B2%7D%5Cdelta%5E2%29+%5C%5C" alt="[公式]"></p>
<p>上式中 <img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="[公式]"> 是 Huber Loss 的一个超参数，<img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="[公式]"> 的值是 MSE 和 MAE 两个损失连接的位置。上式等号右边第一项是 MSE 的部分，第二项是 MAE 部分，在 MAE 的部分公式为 <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%5Clvert+y_i+-+%5Chat%7By_i%7D%5Crvert+-+%5Cfrac%7B1%7D%7B2%7D%5Cdelta%5E2" alt="[公式]"> 是为了保证误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y+-+%5Chat%7By%7D%5Crvert%3D%5Cpm+%5Cdelta" alt="[公式]"> 时 MAE 和 MSE 的取值一致，进而保证 Huber Loss 损失连续可导。</p>
<p>下图是 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D1.0" alt="[公式]"> 时的 Huber Loss，可以看到在 <img src="https://www.zhihu.com/equation?tex=%5B-%5Cdelta%2C+%5Cdelta%5D" alt="[公式]"> 的区间内实际上就是 MSE 损失，在 <img src="https://www.zhihu.com/equation?tex=%28-%5Cinfty%2C+%5Cdelta%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%28%5Cdelta%2C+%5Cinfty%29" alt="[公式]"> 区间内为 MAE损失。</p>
<p><img src="https://pic4.zhimg.com/80/v2-b4260d38f70dd920fa46b8717596bda7_1440w.jpg" alt="img"></p>
<h3 id="2-5-分位数损失-Quantile-Loss"><a href="#2-5-分位数损失-Quantile-Loss" class="headerlink" title="2.5 分位数损失 Quantile Loss"></a>2.5 分位数损失 Quantile Loss</h3><blockquote>
<p>  <strong>MAE 中分别用不同的系数控制高估和低估的损失，进而实现分位数回归</strong></p>
</blockquote>
<p><strong>分位数回归 Quantile Regression 是一类在实际应用中非常有用的回归算法</strong>，通常的回归算法是拟合目标值的期望或者中位数，而分位数回归可以通过给定不同的分位点，<strong>拟合目标值的不同分位数</strong>。</p>
<p><img src="https://pic1.zhimg.com/80/v2-8eb8ecfcdd8031a16a471905217934a0_1440w.jpg" alt="img"></p>
<p>分位数回归是通过使用分位数损失 Quantile Loss 来实现这一点的，分位数损失形式如下，式中的 r 分位数系数。</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7Bquant%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Cmathbb%7BI%7D_%7B%5Chat%7By_i%7D%5Cgeq+y_i%7D%281-r%29%7Cy_i+-+%5Chat%7By_i%7D%7C+%2B+%5Cmathbb%7BI%7D_%7B%5Chat%7By_i%7D%3C+y_i%7Dr%7Cy_i-%5Chat%7By_i%7D%7C+%5C%5C" alt="[公式]"></p>
<p>我们如何理解这个损失函数呢？这个损失函数是一个分段的函数 ，将 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%5Cgeq+y_i" alt="[公式]"> （高估） 和 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%3C+y_i" alt="[公式]"> （低估） 两种情况分开来，并分别给予不同的系数。当 <img src="https://www.zhihu.com/equation?tex=r%3E0.5" alt="[公式]"> 时，低估的损失要比高估的损失更大，反过来当 <img src="https://www.zhihu.com/equation?tex=r+%3C+0.5" alt="[公式]"> 时，高估的损失比低估的损失大；分位数损失实现了<strong>分别用不同的系数控制高估和低估的损失，进而实现分位数回归</strong>。特别地，当 <img src="https://www.zhihu.com/equation?tex=r%3D0.5" alt="[公式]"> 时，分位数损失退化为 MAE 损失，从这里可以看出 MAE 损失实际上是分位数损失的一个特例 — 中位数回归。</p>
<p>下图是取不同的分位点 0.2、0.5、0.6 得到的三个不同的分位损失函数的可视化，可以看到 0.2 和 0.6 在高估和低估两种情况下损失是不同的，而 0.5 实际上就是 MAE。</p>
<p><img src="https://pic4.zhimg.com/80/v2-f8ed385f32a517c784bce841e6da1daf_1440w.jpg" alt="img"></p>
<h3 id="2-6-平均绝对百分误差-MAPE"><a href="#2-6-平均绝对百分误差-MAPE" class="headerlink" title="2.6  平均绝对百分误差 MAPE"></a>2.6  平均绝对百分误差 MAPE</h3><p>虽然平均绝对误差能够获得一个评价值，但是你并不知道这个值代表模型拟合是优还是劣，只有通过对比才能达到效果。当需要以相对的观点来衡量误差时，则使用MAPE。</p>
<p><strong>平均绝对百分误差</strong>（<strong>Mean Absolute Percentage Error，MAPE</strong>）是对 MAE 的一种改进，考虑了绝对误差相对真实值的比例。</p>
<ul>
<li><strong>优点</strong>：考虑了预测值与真实值的误差。考虑了误差与真实值之间的比例。</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=MAPE%3D%5Cfrac%7B100%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cleft%20%7C%20%20%5Cfrac%7By_%7Bi%7D-f%5Cleft%28x_%7Bi%7D%5Cright%29%7D%7By_%7Bi%7D%7D%20%5Cright%20%7C" alt="公式"></p>
<blockquote>
<p>  在某些场景下，如房价从 <img src="https://www.zhihu.com/equation?tex=5K" alt="公式"> 到 <img src="https://www.zhihu.com/equation?tex=50K" alt="公式"> 之间，<img src="https://www.zhihu.com/equation?tex=5K" alt="公式"> 预测成 <img src="https://www.zhihu.com/equation?tex=10K" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=50K" alt="公式"> 预测成 <img src="https://www.zhihu.com/equation?tex=45K" alt="公式"> 的差别是非常大的，而平均绝对百分误差考虑到了这点。</p>
</blockquote>
<h2 id="三、相似性度量指标"><a href="#三、相似性度量指标" class="headerlink" title="三、相似性度量指标"></a>三、相似性度量指标</h2><blockquote>
<p>  机器学习中的相似性度量方法 - 天下客的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/411876558">https://zhuanlan.zhihu.com/p/411876558</a></p>
</blockquote>
<p>描述样本之间相似度的方法有很多种，一般来说常用的有相关系数和欧式距离。本文对机器学习中常用的相似性度量方法进行了总结。<strong>在做分类时，常常需要估算不同样本之间的相似性度量（Similarity Measurement），</strong>这时通常采用的方法就是计算样本间的“距离”（distance）。采用什么样的方法计算距离是很讲究的，甚至关系到分类的正确与否。</p>
<ul>
<li><strong>欧式距离</strong>：k-means</li>
<li><strong>曼哈顿距离</strong>：</li>
<li><strong>切比雪夫距离</strong>：</li>
<li>闵可夫斯基距离</li>
<li>标准化欧氏距离</li>
<li>马氏距离</li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=夹角余弦&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;55493039&quot;}">夹角余弦</a></li>
<li><strong>汉明距离</strong>：simhash</li>
<li><strong>杰卡德距离&amp;杰卡德相似系数</strong>: <strong>杰卡德相似系数是衡量两个集合的相似度一种指标。</strong></li>
<li>相关系数&amp;相关距离</li>
<li>信息熵</li>
</ul>
<h2 id="四、推荐算法评价指标"><a href="#四、推荐算法评价指标" class="headerlink" title="四、推荐算法评价指标"></a>四、推荐算法评价指标</h2><ul>
<li>推荐算法评价指标 - 一干正事就犯困的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/359528909">https://zhuanlan.zhihu.com/p/359528909</a></li>
</ul>
<h4 id="4-1-AP"><a href="#4-1-AP" class="headerlink" title="4.1 AP"></a>4.1 AP</h4><p><code>AP</code> 衡量的是训练好的模型在每个类别上的好坏；</p>
<p><img src="https://pic2.zhimg.com/80/v2-e8656365e7eee25065d6bdfec33368e5_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>AP总结了一个精确召回曲线，作为在每个阈值处获得的精度的加权平均值，并且与以前的阈值相比，召回率的增加用作权重</strong>：</p>
<p><img src="/Users/apple/Library/Application Support/typora-user-images/image-20220711160205051.png" alt="image-20220711160205051" style="zoom:50%;"></p>
<p>其中和分别是第n个阈值[1]时的精度和召回率。此实现未进行插值，并且与使用梯形规则计算精确调用曲线下的面积有所不同，后者使用线性插值并且可能过于乐观。</p>
<h4 id="4-2-MAP"><a href="#4-2-MAP" class="headerlink" title="4.2 MAP"></a>4.2 MAP</h4><p><strong>MAP（Mean Average Precision）常用于排序任务，MAP的计算涉及另外两个指标：Precision和Recall</strong></p>
<ul>
<li><strong>Precision和Precision@k</strong></li>
</ul>
<p>推荐算法中的精度precision计算如下： </p>
<p><img src="https://www.zhihu.com/equation?tex=precision%3D%5Cfrac%7B%E7%AE%97%E6%B3%95%E7%BB%93%E6%9E%9C%E4%B8%AD%E7%9B%B8%E5%85%B3%E7%9A%84item%E6%95%B0%E9%87%8F%7D%7B%E6%8E%A8%E8%8D%90%E7%9A%84item%E6%80%BB%E6%95%B0%E9%87%8F%7D+%5C%5C" alt="[公式]"></p>
<p>可以看出Precision的计算没有考虑结果列表中item的顺序，Precision@k则通过切片的方式将顺序隐含在结果中。Precision@k表示列表前k项的Precision，随着k的变化，可以得到一系列precision值，用 <img src="https://www.zhihu.com/equation?tex=P%28k%29" alt="[公式]"> 表示。</p>
<ul>
<li><strong>Recall和Recall@k</strong></li>
</ul>
<p>推荐算法中的召回率recall计算如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=+recall%3D%5Cfrac%7B%E7%AE%97%E6%B3%95%E7%BB%93%E6%9E%9C%E4%B8%AD%E7%9B%B8%E5%85%B3%E7%9A%84item%E6%95%B0%E9%87%8F%7D%7B%E6%89%80%E6%9C%89%E7%9B%B8%E5%85%B3%E7%9A%84item%E6%95%B0%E9%87%8F%7D%5C%5C" alt="[公式]"></p>
<p>与Precision@k相似，recall@k表示结果列表前k项的recall，随着k的变化，可以得到一系列的recall值，用 <img src="https://www.zhihu.com/equation?tex=r%28k%29" alt="[公式]"> 表示。</p>
<ul>
<li><h5 id="AP-N"><a href="#AP-N" class="headerlink" title="AP@N"></a>AP@N</h5></li>
</ul>
<p>AP（Average Precision）平均精度的计算以Precision@k为基础，可以体现出结果列表中item顺序的重要性，其计算过程如下： </p>
<p><img src="https://www.zhihu.com/equation?tex=AP%40N%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum%5EN_%7Bk%3D1%7D%28P%28k%29%5Cquad+if%5C%2C+kth%5C%2C+item%5C%2C+is%5C%2C+relevant%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum%5EN_%7Bk%3D1%7DP%28k%29%5Ccdot+rel%28k%29+%5C%5C" alt="[公式]"></p>
<p>其中，N表示要求推荐的N个item，m表示所有相关的item总数， <img src="https://www.zhihu.com/equation?tex=rel%28k%29" alt="[公式]"> 表示第k个item是否相关，相关为1，反之为0</p>
<p><strong>AP@N的值越大，表示推荐列表中相关的item数量越多以及相关item的排名越靠前</strong></p>
<ul>
<li><h5 id="MAP-N"><a href="#MAP-N" class="headerlink" title="MAP@N"></a>MAP@N</h5></li>
</ul>
<p><strong>AP@N评价了算法对单个用户的性能，MAP@N则是算法对多个用户的平均值，是平均数的平均，其计算过程如下</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=MAP%40N%3D%5Cfrac%7B1%7D%7B%7CU%7C%7D%5Csum_%7Bu%3D1%7D%5E%7B%7CU%7C%7D%28AP%40N%29u%3D%5Cfrac%7B1%7D%7B%7CU%7C%7D%5Csum%7Bu%3D1%7D%5E%7B%7CU%7C%7D%28%5Cfrac%7B1%7D%7Bm%7D%5Csum%5EN_%7Bk%3D1%7DP_u%28k%29%5Ccdot+rel_u%28k%29%29+%5C%5C" alt="[公式]"></p>
<h2 id="五、聚类算法评价指标"><a href="#五、聚类算法评价指标" class="headerlink" title="五、聚类算法评价指标"></a>五、聚类算法评价指标</h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343667804">https://zhuanlan.zhihu.com/p/343667804</a></p>
<p>  十分钟掌握聚类算法的评估指标：<a target="_blank" rel="noopener" href="https://juejin.cn/post/6997913127572471821">https://juejin.cn/post/6997913127572471821</a></p>
</blockquote>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>如同之前介绍的其它算法模型一样，对于聚类来讲我们同样会通过一些评价指标来衡量聚类算法的优与劣。在聚类任务中，常见的评价指标有：<strong>纯度（Purity）</strong>、<strong>兰德系数（Rand Index, RI）</strong>、<strong>F值（F-score）</strong>和<strong>调整兰德系数（Adjusted Rand Index,ARI）</strong>。同时，这四种评价指标也是聚类相关论文中出现得最多的评价方法。下面，我们就来对这些算法一一进行介绍。</p>
<p><img src="https://pic3.zhimg.com/80/v2-e62c8b4b793c89b1cd70f2aaebf690c6_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>好的聚类算法，一般要求类簇具有：</p>
<ul>
<li><strong>簇内 (intra-cluster) 相似度高</strong></li>
<li><strong>簇间 (inter-cluster) 相似度底</strong></li>
</ul>
<p>一般来说，评估聚类质量有两个标准，内部评估评价指标和外部评估指标。</p>
<h3 id="【外部评估】"><a href="#【外部评估】" class="headerlink" title="【外部评估】"></a>【外部评估】</h3><h3 id="5-1聚类纯度-聚类的准确率"><a href="#5-1聚类纯度-聚类的准确率" class="headerlink" title="5.1聚类纯度 - 聚类的准确率"></a><strong>5.1聚类纯度</strong> - 聚类的准确率</h3><p>在聚类结果的评估标准中，一种最简单最直观的方法就是计算它的<strong>聚类纯度</strong>（purity），别看纯度听起来很陌生，但实际上和<strong>分类问题中的准确率有着异曲同工之妙</strong>。因为聚类纯度的总体思想也<strong>用聚类正确的样本数除以总的样本数，因此它也经常被称为聚类的准确率</strong>。只是对于聚类后的结果我们并不知道每个簇所对应的真实类别，因此需要取每种情况下的最大值。具体的，纯度的计算公式定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P%3D%28%5COmega%2C%5Cmathbb%7BC%7D%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bk%7D%5Cmax_%7Bj%7D%7C%5Comega_k%5Ccap+c_j%7C+%5Cend%7Baligned%7D%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%281%29+%5C%5C" alt="[公式]"></p>
<p>其中<img src="https://www.zhihu.com/equation?tex=N" alt="[公式]">表示总的样本数；<img src="https://www.zhihu.com/equation?tex=%5COmega%3D%5C%7B%5Comega_1%2C%5Comega_2%2C...%2C%5Comega_K%5C%7D" alt="[公式]">表示一个个聚类后的簇，而<img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BC%7D%3D%5C%7Bc_1%2C_2%2C...c_J%5C%7D" alt="[公式]">表示正确的类别；<img src="https://www.zhihu.com/equation?tex=%5Comega_k" alt="[公式]">表示聚类后第<img src="https://www.zhihu.com/equation?tex=k" alt="[公式]">个簇中的所有样本，<img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]">表示第<img src="https://www.zhihu.com/equation?tex=j" alt="[公式]">个类别中真实的样本。在这里<img src="https://www.zhihu.com/equation?tex=P" alt="[公式]">的取值范围为<img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[公式]">，越大表示聚类效果越好。</p>
<h3 id="5-2-兰德系数与F值-同簇混淆矩阵"><a href="#5-2-兰德系数与F值-同簇混淆矩阵" class="headerlink" title="5.2 兰德系数与F值  [同簇混淆矩阵]"></a><strong>5.2 兰德系数与F值</strong>  [同簇混淆矩阵]</h3><p>在介绍完了纯度这一评价指标后，我们再来看看兰德系数（Rand Index）和F值。虽然兰德系数听起来是一个陌生的名词，但它的计算过程却也与准确率的计算过程类似。同时，虽然这里也有一个叫做F值的指标，并且它的计算过程也和分类指标中的F值类似，但是两者却有着本质的差别。说了这么多，那这两个指标到底该怎么算呢？同分类问题中的混淆矩阵类似，这里我们也要先定义四种情况进行计数，然后再进行指标的计算。</p>
<p><strong>为了说明兰德系数背后的思想，我们还是以图1中的聚类结果为例进行说明（为了方便观察，我们再放一张图在这里）:</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-e62c8b4b793c89b1cd70f2aaebf690c6_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=TP" alt="[公式]">：表示两个<strong>同类样本点</strong>在<strong>同一个簇</strong>（布袋）中的情况数量；</li>
<li><img src="https://www.zhihu.com/equation?tex=FP" alt="[公式]">：表示两个<strong>非同类样本点</strong>在<strong>同一个簇</strong>中的情况数量；</li>
<li><img src="https://www.zhihu.com/equation?tex=TN" alt="[公式]">：表示两个<strong>非同类样本点</strong>分别在<strong>两个簇</strong>中的情况数量；</li>
<li><img src="https://www.zhihu.com/equation?tex=FN" alt="[公式]">：表示两个同类样本点分别在<strong>两个簇</strong>中的情况数量；</li>
</ul>
<p>由此，我们便能得到如下所示的对<strong>混淆矩阵（Pair Confusion Matrix）</strong>：</p>
<p><img src="https://pic3.zhimg.com/80/v2-a9e709a995b006be04d026aebc721c4e_1440w.png" alt="img" style="zoom:75%;"></p>
<p>有了上面各种情况的统计值，我们就可以定义出兰德系数和F值的计算公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=RI%3D%5Cfrac%7BTP%2BTN%7D%7BTP%2BFP%2BFN%2BTN%7D%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%283%29+%5C%5C" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+Precision%26%3D%5Cfrac%7BTP%7D%7BTP%2BFP%7D%5C%5C%5B2ex%5D+Recall%26%3D%5Cfrac%7BTP%7D%7BTP%2BFN%7D%5C%5C%5B2ex%5D+F_%7B%5Cbeta%7D%26%3D%281%2B%5Cbeta%5E2%29%5Cfrac%7BPrecision%5Ccdot+Recall%7D%7B%5Cbeta%5E2%5Ccdot+Precision%2BRecall%7D+%5Cend%7Baligned%7D%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%284%29+%5C%5C" alt="[公式]"></p>
<p>从上面的计算公式来看，<img src="https://www.zhihu.com/equation?tex=%283%29%284%29" alt="[公式]">从形式上看都非常像分类问题中的准确率与F值，但是有着本质的却别。同时，在这里<img src="https://www.zhihu.com/equation?tex=RI" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=F_%7B%5Cbeta%7D" alt="[公式]">的取值范围均为<img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[公式]">，越大表示聚类效果越好。</p>
<h4 id="5-3-调整兰德系数（Adjusted-Rand-index）【归一化】"><a href="#5-3-调整兰德系数（Adjusted-Rand-index）【归一化】" class="headerlink" title="5.3 调整兰德系数（Adjusted Rand index）【归一化】"></a>5.3 调整兰德系数（Adjusted Rand index）【归一化】</h4><p>对于随机结果，RI并不能保证分数接近零。<strong>为了实现“在聚类结果随机产生的情况下，指标应该接近零”</strong>，调整兰德系数（Adjusted rand index）被提出，它具有更高的区分度。</p>
<p>其公式为：</p>
<script type="math/tex; mode=display">
\mathrm{ARI}=\frac{\mathrm{RI}-E[\mathrm{RI}]}{\max (\mathrm{RI})-E[\mathrm{RI}]}</script><p>$A R$ 取值范围为 $[-1,1]$, 值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲, ARI衡量的是两个数据分布的吻合程度。</p>
<p>优点:</p>
<ul>
<li>对任意数量的聚类中心和样本数, 随机聚类的ARI都非常接近于 0 。</li>
<li>取值在 $[-1,1]$ 之间, 负数代表结果不好, 越接近于1越好。</li>
<li>对簇的结构不需作出任何假设：可以用于比较聚类算法。</li>
</ul>
<p>缺点:</p>
<ul>
<li>ARI 需要 ground truth classes 的相关知识, ARI需要真实标签, 而在实践中几乎不可用, 或者需要人工 标注者手动分配（如在监督学习环境中）。</li>
</ul>
<h3 id="5-4-标准化互信息（NMI-Normalized-Mutual-Information）"><a href="#5-4-标准化互信息（NMI-Normalized-Mutual-Information）" class="headerlink" title="5.4  标准化互信息（NMI, Normalized Mutual Information）"></a>5.4 <strong><font color="red"> 标准化互信息（NMI, Normalized Mutual Information）</font></strong></h3><p>互信息是用来衡量两个数据分布的吻合程度。它也是一有用的信息度量，它是指两个事件集合之间的相关性。互信息越大，词条和类别的相关程度也越大。</p>
<h3 id="【内部指标】"><a href="#【内部指标】" class="headerlink" title="【内部指标】"></a>【内部指标】</h3><p>内部评估指标主要基于数据集的集合结构信息从紧致性、分离性、连通性和重叠度等方面对聚类划分进行评价。即基于数据聚类自身进行评估的。</p>
<h3 id="5-5-轮廓系数（Silhouette-Coefficient）"><a href="#5-5-轮廓系数（Silhouette-Coefficient）" class="headerlink" title="5.5  轮廓系数（Silhouette Coefficient）"></a>5.5 <strong><font color="red"> 轮廓系数（Silhouette Coefficient）</font></strong></h3><p>轮廓系数适用于实际类别信息未知的情况。</p>
<p>对于单个样本，设<strong>a是与它同类别中其他样本的平均距离</strong>，<strong>b是与它距离最近不同类别中样本的平均距离</strong>，其轮廓系数为：</p>
<p>$s = \frac {b-a} {max(a, b)}$</p>
<p>对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。轮廓系数的取值范围是[-1,1]，同类别样本距离越相近，不同类别样本距离越远，值越大。当值为负数时，说明聚类效果很差。</p>
<h3 id="5-6-Calinski-Harabaz指数（Calinski-Harabaz-Index）"><a href="#5-6-Calinski-Harabaz指数（Calinski-Harabaz-Index）" class="headerlink" title="5.6 Calinski-Harabaz指数（Calinski-Harabaz Index）"></a>5.6 Calinski-Harabaz指数（Calinski-Harabaz Index）</h3><p>在真实的分群label不知道的情况下，Calinski-Harabasz可以作为评估模型的一个指标。</p>
<p>Calinski-Harabasz指数通过<strong>计算类中各点与类中心的距离平方和来度量类内的紧密度</strong>，通过<strong>==计算各类中心点与数据集中心点距离平方和来度量数据集的分离度==</strong>，CH指标<strong>由分离度与紧密度的比值得到</strong>。从而，CH越大代表着类自身越紧密，类与类之间越分散，即更优的聚类结果。</p>
<p><strong>优点</strong></p>
<ul>
<li>当簇的密集且分离较好时，分数更高。</li>
<li>得分计算很快，与轮廓系数的对比，最大的优势：快！相差几百倍！毫秒级。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>凸的簇的CH指数通常高于其他类型的簇。例如，通过 DBSCAN 获得基于密度的簇；所以，不适合基于密度的聚类算法（DBSCAN）。</li>
</ul>
<h3 id="5-7-戴维森堡丁指数（DBI-Davies-Bouldin-Index）"><a href="#5-7-戴维森堡丁指数（DBI-Davies-Bouldin-Index）" class="headerlink" title="5.7 戴维森堡丁指数（DBI, Davies-Bouldin Index）"></a>5.7 戴维森堡丁指数（DBI, Davies-Bouldin Index）</h3><p><strong>DB指数是计算任意两类别的类内距离平均距离之和除以两聚类中心距离求最大值</strong>。DB越小，意味着类内距 离越小同时类间距离越大。<strong>零是可能的最低值, 接近零的值表示更好的分区</strong>。</p>
<script type="math/tex; mode=display">
\begin{gathered}
R_{i j}=\frac{s_{i}+s_{j}}{d_{i j}} \\
D B=\frac{1}{k} \sum_{i=1}^{k} \max _{i \neq j} R_{i j}
\end{gathered}</script><p>其中, $s_{i}$ 表示簇的每个点与该簇的质心之间的平均距离, 也称为簇直径。 $d_{i j}$ 表示聚类和的质心之间的距 离。<br>算法生成的聚类结果越是朝着簇内距离最小（类内相似性最大）和笶间距离最大（类间相似性最小）变化， 那么Davies-Bouldin指数就会越小。<br><strong>缺点</strong>:</p>
<ul>
<li>因使用欧式距离, 所以对于环状分布聚类评测很差。</li>
</ul>
<h2 id="六、评分总结（sklearn）"><a href="#六、评分总结（sklearn）" class="headerlink" title="六、评分总结（sklearn）"></a>六、评分总结（sklearn）</h2><blockquote>
<p>  sklearn.metrics - 回归/分类模型的评估方法:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/408078074">https://zhuanlan.zhihu.com/p/408078074</a></p>
</blockquote>
<h3 id="6-1-分类模型"><a href="#6-1-分类模型" class="headerlink" title="6.1 分类模型"></a>6.1 分类模型</h3><h4 id="accuracy-score"><a href="#accuracy-score" class="headerlink" title="accuracy_score"></a><strong>accuracy_score</strong></h4><p><strong>分类准确率分数是指所有分类正确的百分比</strong>。分类准确率这一衡量分类器的标准比较容易理解，但是它不能告诉你响应值的潜在分布，并且它也不能告诉你分类器犯错的类型。所以在使用的时候，一般需要搭配matplotlib等数据可视化工具来观察预测的分类情况，与实际的结果做更加直观的比较。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score  </span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]  </span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]  </span><br><span class="line">accuracy_score(y_true, y_pred)  <span class="comment"># 默认normalization = True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.5</span></span><br><span class="line">accuracy_score(y_true, y_pred, normalize=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span></span><br></pre></td></tr></table></figure>
<h4 id="recall-score"><a href="#recall-score" class="headerlink" title="recall_score"></a><strong>recall_score</strong></h4><p>召回率 =<strong>提取出的正确信息条数 /样本中的信息条数</strong>。通俗地说，就是所有准确的条目有多少被检索出来了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">recall_score(y_true, y_pred, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>,average=<span class="string">&#x27;binary&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line">参数average : string, [<span class="literal">None</span>, ‘micro’, ‘macro’(default), ‘samples’, ‘weighted’]</span><br></pre></td></tr></table></figure>
<p>将一个二分类matrics拓展到多分类或多标签问题时，我们可以将数据看成多个二分类问题的集合，每个类都是一个二分类。接着，我们可以通过跨多个分类计算每个二分类metrics得分的均值，这在一些情况下很有用。你可以使用<strong>average参数</strong>来指定。 </p>
<ul>
<li>macro：计算二分类metrics的均值，为每个类给出相同权重的分值。</li>
<li>weighted:对于不均衡数量的类来说，计算二分类metrics的平均，通过在每个类的score上进行加权实现。 </li>
<li>micro：给出了每个样本类以及它对整个metrics的贡献的pair（sample-weight），而非对整个类的metrics求和，它会每个类的metrics上的权重及因子进行求和，来计算整个份额。</li>
<li>samples：应用在multilabel问题上。它不会计算每个类，相反，它会在评估数据中，通过计算真实类和预测类的差异的metrics，来求平均（sample_weight-weighted） </li>
<li>average：average=None将返回一个数组，它包含了每个类的得分.</li>
</ul>
<h4 id="roc-curve"><a href="#roc-curve" class="headerlink" title="roc_curve"></a><strong>roc_curve</strong></h4><p>ROC曲线指受试者工作特征曲线/接收器操作特性(receiver operating characteristic，ROC)曲线,是<strong>反映灵敏性和特效性连续变量的综合指标</strong>,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界值，从而计算出一系列敏感性和特异性。ROC曲线是根据一系列不同的二分类方式（分界值或决定阈），<strong>以真正例率（也就是灵敏度）（True Positive Rate,TPR）为纵坐标，假正例率（1-特效性）（False Positive Rate,FPR）为横坐标</strong>绘制的曲线。</p>
<p>通过ROC我们可以观察到模型正确识别的正例的比例与模型错误地把负例数据识别成正例的比例之间的权衡。TPR的增加以FPR的增加为代价。ROC曲线下的面积是模型准确率的度量，<strong>AUC</strong>（Area under roc curve）。</p>
<p><strong>TPR</strong> = TP /（TP + FN） （正样本<strong>预测数</strong> / 正样本<strong>实际数</strong>）</p>
<p><strong>FPR</strong> = FP /（FP + TN） （负样本<strong>预测数</strong> /负样本<strong>实际数</strong>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics  </span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])  </span><br><span class="line">scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])  </span><br><span class="line">fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=<span class="number">2</span>)  </span><br><span class="line">fpr  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.</span> ,  <span class="number">0.5</span>,  <span class="number">0.5</span>, <span class="number">1.</span> ])  </span><br><span class="line">tpr  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.5</span>,  <span class="number">0.5</span>,  <span class="number">1.</span> , <span class="number">1.</span> ])  </span><br><span class="line">thresholds  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.8</span> ,  <span class="number">0.4</span> ,  <span class="number">0.35</span>, <span class="number">0.1</span> ])  </span><br><span class="line"></span><br><span class="line"><span class="comment"># check auc score</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc   </span><br><span class="line">metrics.auc(fpr, tpr)   </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.75</span>   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以直接根据预测值+真实值来计算出auc值，略过roc的计算过程</span></span><br><span class="line">‘’‘</span><br><span class="line">sklearn.metrics.roc_auc_score(y_true, y_score, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line">average : string, [<span class="literal">None</span>, ‘micro’, ‘macro’(default), ‘samples’, ‘weighted’]</span><br><span class="line">’‘’</span><br><span class="line"><span class="comment"># 真实值（必须是二值）、预测值（可以是0/1,也可以是proba值）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score  </span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])  </span><br><span class="line">y_scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])  </span><br><span class="line">roc_auc_score(y_true, y_scores)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.75</span>  </span><br></pre></td></tr></table></figure>
<h4 id="confusion-metric"><a href="#confusion-metric" class="headerlink" title="confusion metric"></a><strong>confusion metric</strong></h4><p>混淆矩阵（confusion matrix），又称为可能性表格或是错误矩阵。它是一种特定的矩阵用来呈现算法性能的可视化效果。其每一列代表预测值，每一行代表的是实际的类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matric(y_true, y_pred, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">&#x27;binary&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h4 id="precision-score"><a href="#precision-score" class="headerlink" title="precision_score"></a><strong>precision_score</strong></h4><p>计算精确度——precision <img src="https://www.zhihu.com/equation?tex=%3DTP%2F%28TP%2FFP%29" alt="[公式]"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">precision_score(y_true, y_pred, labels=None, pos_label=1, average=&#x27;binary&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic2.zhimg.com/v2-a3b6092e30d2eab7d2372007aec15105_r.jpg" alt="preview"></p>
<h2 id="评价指标Q-amp-A"><a href="#评价指标Q-amp-A" class="headerlink" title="评价指标Q&amp;A"></a>评价指标Q&amp;A</h2><h4 id="精度指标存在的问题？"><a href="#精度指标存在的问题？" class="headerlink" title="精度指标存在的问题？"></a><strong>精度指标存在的问题</strong>？</h4><ul>
<li>有倾向性的问题。比如，判断空中的飞行物是导弹还是其他飞行物，很显然为了减少损失，我们更倾向于相信是导弹而采用相应的防护措施。此时判断为导弹实际上是其他飞行物与判断为其他飞行物实际上是导弹这两种情况的重要性是不一样的；</li>
<li>样本类别数量严重不均衡的情况。比如银行客户样本中好客户990个，坏客户10个。如果一个模型直接把所有客户都判断为好客户，得到精度为99%，但这显然是没有意义的。</li>
</ul>
<h4 id="为什么-ROC-和-AUC-都能应用于非均衡的分类问题？"><a href="#为什么-ROC-和-AUC-都能应用于非均衡的分类问题？" class="headerlink" title="为什么 ROC 和 AUC 都能应用于非均衡的分类问题？"></a><strong>为什么 ROC 和 AUC 都能应用于非均衡的分类问题？</strong></h4><p><strong>ROC曲线只与横坐标 (FPR) 和 纵坐标 (TPR) 有关系</strong> 。我们可以发现TPR只是正样本中预测正确的概率，而FPR只是负样本中预测错误的概率，和正负样本的比例没有关系。因此 ROC 的值与实际的正负样本比例无关，因此既可以用于均衡问题，也可以用于非均衡问题。而 AUC 的几何意义为ROC曲线下的面积，因此也和实际的正负样本比例无关。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8818%EF%BC%89TF-IDF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8818%EF%BC%89TF-IDF/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:06:42" itemprop="dateCreated datePublished" datetime="2022-03-16T21:06:42+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-19 22:50:55" itemprop="dateModified" datetime="2022-07-19T22:50:55+08:00">2022-07-19</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/u010417185/article/details/87905899">https://blog.csdn.net/u010417185/article/details/87905899</a></p>
</blockquote>
<p><strong>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)</strong>是一种用于资讯检索与资讯探勘的常用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加权技术&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;97273457&quot;}">加权技术</a>。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p>
<p>上述引用总结就是, <strong>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。</strong>这也就是TF-IDF的含义。</p>
<h4 id="1-1-TF"><a href="#1-1-TF" class="headerlink" title="1.1 TF"></a><strong>1.1 TF</strong></h4><p><strong>TF(Term Frequency, ==词频==)</strong>表示词条在文本中出现的频率，这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。TF用公式表示如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=TF_%7Bi%2Cj%7D%3D%5Cfrac%7Bn_%7Bi%2Cj%7D%7D%7B%5Csum_%7Bk%7D%7Bn_%7Bk%2Cj%7D%7D%7D%5Ctag%7B1%7D+%5C%5C" alt="[公式]"></p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=n_%7Bi%2Cj%7D" alt="[公式]"> 表示词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 在文档 <img src="https://www.zhihu.com/equation?tex=d_j" alt="[公式]"> 中出现的次数，<img src="https://www.zhihu.com/equation?tex=TF_%7Bi%2Cj%7D" alt="[公式]"> 就是表示词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 在文档 <img src="https://www.zhihu.com/equation?tex=d_j" alt="[公式]"> 中出现的频率。</p>
<p>但是，需要注意， 一些<strong>通用的词语对于主题并没有太大的作用</strong>， <strong>反倒是一些出现频率较少的词才能够表达文章的主题</strong>， 所以单纯使用是TF不合适的。权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章的主题的作用很大，这些词的权重应该设计的较大。IDF就是在完成这样的工作。</p>
<h4 id="1-2-IDF"><a href="#1-2-IDF" class="headerlink" title="1.2 IDF"></a><strong>1.2 IDF</strong></h4><p><strong>IDF(Inverse Document Frequency, ==逆文件频率==)</strong>表示关键词的普遍程度。如果包含词条 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 的文档越少， <strong>IDF</strong>越大，则说明该词条具有很好的类别区分能力。某一特定词语的<strong>IDF</strong>，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到:</p>
<p><img src="https://www.zhihu.com/equation?tex=IDF_i%3D%5Clog%5Cfrac%7B%5Cleft%7CD+%5Cright%7C%7D%7B1%2B%5Cleft%7Cj%3A+t_i+%5Cin+d_j%5Cright%7C%7D%5Ctag%7B2%7D+%5C%5C" alt="[公式]"></p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=%5Cleft%7CD+%5Cright%7C" alt="[公式]"> 表示所有<strong>文档的数量</strong>，<img src="https://www.zhihu.com/equation?tex=%5Cleft%7Cj%3A+t_i+%5Cin+d_j%5Cright%7C" alt="[公式]"> 表示包<strong>含词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 的文档数量</strong>，为什么这里要加 1 呢？主要是<strong>防止包含词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 的数量为 0 从而导致运算出错的现象发生</strong>。</p>
<p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于<strong>过滤掉常见的词语，保留重要的词语</strong>，表达为</p>
<p><img src="https://www.zhihu.com/equation?tex=TF+%5Ctext%7B-%7DIDF%3D+TF+%5Ccdot+IDF%5Ctag%7B3%7D+%5C%5C" alt="[公式]"></p>
<p>==<strong>最后</strong>在计算完文档中每个字符的tfidf之后，对其进行归一化，将值保留在0-1之间，并保存成稀疏矩阵。==</p>
<h2 id="TF-IDF-Q-amp-A"><a href="#TF-IDF-Q-amp-A" class="headerlink" title="TF-IDF Q&amp;A"></a>TF-IDF Q&amp;A</h2><h3 id="1、究竟应该是对整个语料库进行tf-idf呢？还是先对训练集进行tf-idf，然后再对xtest进行tf-idf呢？两者有什么区别？"><a href="#1、究竟应该是对整个语料库进行tf-idf呢？还是先对训练集进行tf-idf，然后再对xtest进行tf-idf呢？两者有什么区别？" class="headerlink" title="1、究竟应该是对整个语料库进行tf-idf呢？还是先对训练集进行tf-idf，然后再对xtest进行tf-idf呢？两者有什么区别？"></a><strong>1、究竟应该是对整个语料库进行tf-idf呢？还是先对训练集进行tf-idf，然后再对xtest进行tf-idf呢？两者有什么区别？</strong></h3><blockquote>
<h4 id="fit"><a href="#fit" class="headerlink" title="fit"></a>fit</h4><p>  学习输入的数据有多少个不同的单词，以及每个单词的idf</p>
<h4 id="transform-训练集"><a href="#transform-训练集" class="headerlink" title="transform 训练集"></a>transform 训练集</h4><p>  返回我们一个document-term matrix.</p>
<h4 id="transform-测试集"><a href="#transform-测试集" class="headerlink" title="transform 测试集"></a>transform 测试集</h4></blockquote>
<p>transform的过程也很让人好奇。要知道，他是将测试集的数据中的文档数量纳入进来，重新计算每个词的idf呢，还是<strong>直接用训练集学习到的idf去计算测试集里面每一个tf-idf</strong>呢？</p>
<p><strong>如果纳入了测试集新词，就等于预先知道测试集中有什么词，影响了idf的权重。这样预知未来的行为，会导致算法丧失了泛化性。</strong></p>
<h3 id="2、TF-IDF-模型加载太慢"><a href="#2、TF-IDF-模型加载太慢" class="headerlink" title="2、TF-IDF 模型加载太慢"></a>2、TF-IDF 模型加载太慢</h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/">https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> idfs <span class="keyword">import</span> idfs <span class="comment"># numpy array with our pre-computed idfs</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># subclass TfidfVectorizer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyVectorizer</span>(<span class="title class_ inherited__">TfidfVectorizer</span>):</span><br><span class="line">    <span class="comment"># plug our pre-computed IDFs</span></span><br><span class="line">    TfidfVectorizer.idf_ = idfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiate vectorizer</span></span><br><span class="line">vectorizer = MyVectorizer(lowercase = <span class="literal">False</span>,</span><br><span class="line">                          min_df = <span class="number">2</span>,</span><br><span class="line">                          norm = <span class="string">&#x27;l2&#x27;</span>,</span><br><span class="line">                          smooth_idf = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plug _tfidf._idf_diag</span></span><br><span class="line">vectorizer._tfidf._idf_diag = sp.spdiags(idfs,</span><br><span class="line">                                         diags = <span class="number">0</span>,</span><br><span class="line">                                         m = <span class="built_in">len</span>(idfs),</span><br><span class="line">                                         n = <span class="built_in">len</span>(idfs))</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8813%EF%BC%89EM%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8813%EF%BC%89EM%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 17:16:27" itemprop="dateCreated datePublished" datetime="2022-03-16T17:16:27+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-07 19:17:59" itemprop="dateModified" datetime="2022-07-07T19:17:59+08:00">2022-07-07</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="EM——期望最大-概率模型"><a href="#EM——期望最大-概率模型" class="headerlink" title="EM——期望最大 [概率模型]"></a>EM——期望最大 [概率模型]</h1><blockquote>
<p>  <strong>EM 算法通过引入隐含变量，使用 MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM 算法首先会固定其中的第一个参数，然后使用 MLE 计算第二个变量值；接着通过固定第二个变量，再使用 MLE 估测第一个变量值，依次迭代，直至收敛到局部最优解。</strong></p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/27976634">怎么通俗易懂地解释 EM 算法并且举个例子?</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/zouxy09/article/details/8537620">从最大似然到 EM 算法浅解</a></li>
<li><h5 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/39490840">EM算法</a></h5></li>
</ol>
</blockquote>
<p><strong><font color="red"> EM 算法，全称 Expectation Maximization Algorithm。期望最大算法是一种迭代算法，用于含有隐变量（Hidden Variable）的概率参数模型的最大似然估计或极大后验概率估计。</font></strong></p>
<p>本文思路大致如下：先简要介绍其思想，然后举两个例子帮助大家理解，有了感性的认识后再进行严格的数学公式推导。</p>
<h2 id="1-思想"><a href="#1-思想" class="headerlink" title="1. 思想"></a>1. 思想</h2><p>EM 算法的核心思想非常简单，分为两步：<strong>Expection-Step</strong> 和 <strong>Maximization-Step</strong>。<strong>E-Step 主要通过观察数据和现有模型来估计参数</strong>，然后用这个估计的参数值来计算似然函数的期望值；而 M-Step 是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后<strong>似然函数都会增加</strong>，所以函数最终会收敛。</p>
<p><img src="https://www.zhihu.com/equation?tex=EM" alt="[公式]"> <strong>算法一句话总结就是</strong>： <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步固定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 优化 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步固定 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 优化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 。</p>
<h3 id="2-例子"><a href="#2-例子" class="headerlink" title="2 例子"></a>2 例子</h3><h4 id="2-1-例子-A"><a href="#2-1-例子-A" class="headerlink" title="2.1 例子 A"></a>2.1 例子 A</h4><p>假设有两枚硬币 A 和 B，他们的随机抛掷的结果如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-4e19d89b47e21cf284644b0576e9af0f_1440w.jpg" alt="img"></p>
<p>我们很容易估计出两枚硬币抛出正面的概率：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_A+%3D+24%2F30+%3D0.8+%5C%5C%5Ctheta_B+%3D+9%2F20+%3D0.45++%5C%5C" alt="[公式]"></p>
<p>现在我们加入<strong>隐变量</strong>，抹去每轮投掷的硬币标记：</p>
<p><img src="https://pic1.zhimg.com/80/v2-caa896173185a8f527c037c122122258_1440w.jpg" alt="img"></p>
<p>碰到这种情况，我们该如何估计 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值？</p>
<p>我们多了一个隐变量 <img src="https://www.zhihu.com/equation?tex=Z%3D%28z_1%2C+z_2%2C+z_3%2C+z_4%2C+z_5%29" alt="[公式]"> ，代表每一轮所使用的硬币，我们需要知道每一轮抛掷所使用的硬币这样才能估计 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值，但是估计隐变量 Z 我们又需要知道 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值，才能用极大似然估计法去估计出 Z。这就陷入了一个鸡生蛋和蛋生鸡的问题。</p>
<p>其解决方法就是先<strong>随机初始化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"></strong> ，然后用去估计 Z， 然后基于 Z 按照最大似然概率去估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> ，循环至收敛。</p>
<h4 id="2-1-2-计算"><a href="#2-1-2-计算" class="headerlink" title="2.1.2 计算"></a><strong>2.1.2 计算</strong></h4><p>随机初始化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A%3D0.6" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B%3D0.5" alt="[公式]"></p>
<p>对于第一轮来说，如果是硬币 A，得出的 5 正 5 反的概率为： <img src="https://www.zhihu.com/equation?tex=0.6%5E5%2A0.4%5E5" alt="[公式]"> ；如果是硬币 B，得出的 5 正 5 反的概率为： <img src="https://www.zhihu.com/equation?tex=0.5%5E5%2A0.5%5E5" alt="[公式]"> 。我们可以算出使用是硬币 A 和硬币 B 的概率分别为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P_A%3D%5Cfrac%7B0.6%5E5+%2A+0.4%5E5%7D%7B%280.6%5E5+%2A+0.4%5E5%29+%2B+%280.5%5E5+%2A+0.5%5E5%29%7D+%3D+0.45%5C%5C+P_B%3D%5Cfrac%7B0.5%5E5+%2A+0.5%5E5%7D%7B%280.6%5E5+%2A+0.4%5E5%29+%2B+%280.5%5E5+%2A+0.5%5E5%29%7D+%3D+0.55+%5C%5C" alt="[公式]"></p>
<p><img src="https://pic4.zhimg.com/80/v2-b325de65a5bcac196fc0939f346410d7_1440w.jpg" alt="img"></p>
<p>从期望的角度来看，对于第一轮抛掷，使用硬币 A 的概率是 0.45，使用硬币 B 的概率是 0.55。同理其他轮。这一步我们实际上是<strong>估计出了 Z 的概率分布</strong>，这部就是 <strong>E-Step</strong>。</p>
<p>结合硬币 A 的概率和上一张投掷结果，我们利用期望可以求出硬币 A 和硬币 B 的贡献。以第二轮硬币 A 为例子，计算方式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=H%3A+0.80%2A9+%3D7.2+%5C%5C+T%3A+0.80%2A1%3D0.8+%5C%5C" alt="[公式]"></p>
<p>于是我们可以得到：</p>
<p><img src="https://pic1.zhimg.com/80/v2-9b6e8c50c0761c6ac19909c26e0a71d4_1440w.jpg" alt="img"></p>
<p>然后用极大似然估计来估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_A+%3D+%5Cfrac%7B21.3%7D%7B21.3%2B8.6%7D+%3D+0.71+%5C%5C+%5Ctheta_B+%3D+%5Cfrac%7B11.7%7D%7B11.7+%2B+8.4%7D+%3D+0.58+%5C%5C" alt="[公式]"></p>
<p>这步就对应了 M-Step，重新估计出了参数值。如此反复迭代，我们就可以算出最终的参数值。</p>
<p>上述讲解对应下图：</p>
<p><img src="https://pic3.zhimg.com/v2-6cac968d6500cbca58fc90347c288466_r.jpg" alt="preview" style="zoom:50%;"></p>
<h4 id="2-2-例子-B"><a href="#2-2-例子-B" class="headerlink" title="2.2 例子 B"></a>2.2 例子 B</h4><p>如果说例子 A 需要计算你可能没那么直观，那就举更一个简单的例子：</p>
<p>现在一个班里有 50 个男生和 50 个女生，且男女生分开。我们假定男生的身高服从正态分布： <img src="https://www.zhihu.com/equation?tex=N%28%5Cmu_1%2C+%5Csigma%5E2_1+%29" alt="[公式]"> ，女生的身高则服从另一个正态分布： <img src="https://www.zhihu.com/equation?tex=N%28%5Cmu_2%2C+%5Csigma%5E2_2+%29" alt="[公式]"> 。这时候我们可以用极大似然法（MLE），分别通过这 50 个男生和 50 个女生的样本来估计这两个正态分布的参数。</p>
<p>但现在我们让情况复杂一点，就是这 50 个男生和 50 个女生混在一起了。我们拥有 100 个人的身高数据，却不知道这 100 个人每一个是男生还是女生。</p>
<p>这时候情况就有点尴尬，因为通常来说，我们只有知道了精确的男女身高的正态分布参数我们才能知道每一个人更有可能是男生还是女生。但从另一方面去考量，我们只有知道了每个人是男生还是女生才能尽可能准确地估计男女各自身高的正态分布的参数。</p>
<p>这个时候有人就想到我们必须从某一点开始，并用迭代的办法去解决这个问题：<strong>==我们先设定男生身高和女生身高分布的几个参数（初始值），然后根据这些参数去判断每一个样本（人）是男生还是女生，之后根据标注后的样本再反过来重新估计参数。之后再多次重复这个过程，直至稳定。这个算法也就是 EM 算法。==</strong></p>
<h3 id="3-推导"><a href="#3-推导" class="headerlink" title="3. 推导"></a>3. 推导</h3><p>给定数据集，假设样本间相互独立，我们想要拟合模型 <img src="https://www.zhihu.com/equation?tex=p%28x%3B%5Ctheta%29" alt="[公式]"> 到数据的参数。根据分布我们可以得到如下<strong>似然函数</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+L%28%5Ctheta%29+%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dlog+p%28x_i%3B%5Ctheta%29++%5C%5C+%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dlog+%5Csum_%7Bz%7Dp%28x_i%2C+z%3B%5Ctheta%29+%5Cend%7Baligned%7D+%5C%5C" alt="[公式]"></p>
<p>第一步是<strong>对极大似然函数取对数</strong>，第二步是对每个样本的每个可能的类别 z 求<strong>联合概率分布之和</strong>。如果这个 z 是已知的数，那么使用极大似然法会很容易。但如果 z 是隐变量，我们就需要用 EM 算法来求。<strong>事实上，隐变量估计问题也可以通过梯度下降等优化算法，但事实由于求和项将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而 EM 算法则可看作一种非梯度优化方法。</strong></p>
<h4 id="3-1-求解含有隐变量的概率模型"><a href="#3-1-求解含有隐变量的概率模型" class="headerlink" title="3.1 求解含有隐变量的概率模型"></a>3.1 求解含有隐变量的概率模型</h4><p><strong>为了求解含有隐变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 的概率模型</strong> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D%5Chat%7B%5Ctheta%7D%3D%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%5Cend%7Baligned%7D" alt="[公式]"> <strong>需要一些特殊的技巧</strong>，通过引入隐变量 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%28i%29%7D" alt="[公式]"> 的概率分布为 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> ，<strong>==因为 <img src="https://www.zhihu.com/equation?tex=%5Clog+%28x%29" alt="[公式]"> 是凹函数故结合凹函数形式下的詹森不等式进行放缩处理==</strong><br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7D+Q_i%28z%5E%7B%28i%29%7D%29%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C+%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Cmathbb%7BE%7D%28%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5C%5C+%26%5Cge%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Cmathbb%7BE%7D%5B%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5D%5C%5C+%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>其中由概率分布的充要条件 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%3D1%E3%80%81Q_i%28z%5E%7B%28i%29%7D%29%5Cge0" alt="[公式]"> 可看成下述关于 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 函数分布列的形式：</p>
<p><img src="https://pic2.zhimg.com/v2-cb7ddb5cdc34761ec70d63c97189b102_720w.jpg?source=d16d100b" alt="img" style="zoom:50%;"></p>
<p><strong>这个过程可以看作是对 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 求了下界</strong>，假设 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 已经给定那么 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的值就取决于 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%29" alt="[公式]"> 了，因此可以通过调整这两个概率使下界不断上升，以逼近 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的真实值，当不等式变成等式时说明调整后的概率能够等价于 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> ，所以必须找到使得等式成立的条件，即寻找<br> <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5B%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5D%3D%5Clog+%5Cmathbb%7BE%7D%5B%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5D%5C%5C" alt="[公式]"><br>由期望得性质可知当<br> <img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%3DC%2C%5C+%5C+%5C+%5C+%5C+C%5Cin%5Cmathbb%7BR%7D+%5C+%5C+%5C+%5C+%5C+%28%2A%29%5C%5C" alt="[公式]"><br>等式成立，对上述等式进行变形处理可得<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DCQ_i%28z%5E%7B%28i%29%7D%29%5C%5C+%26%5CLeftrightarrow+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DC%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%3DC%5C%5C+%26%5CLeftrightarrow+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DC+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28%2A%2A%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>把 <img src="https://www.zhihu.com/equation?tex=%28%2A%2A%29" alt="[公式]"> 式带入 <img src="https://www.zhihu.com/equation?tex=%28%2A%29" alt="[公式]"> 化简可知<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+Q_i%28z%5E%7B%28i%29%7D%29%26%3D%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7B%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%5C%5C+%26%3D%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7Bp%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%5C%5C+%26%3Dp%28z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>至此，可以推出<strong>在固定参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 后</strong>， <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 的<strong>计算公式就是后验概率</strong>，解决了 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 如何选择得问题。这一步称为 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步，建立 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 得下界；接下来得 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步，就是在给定 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 后，调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 去极大化 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的下界即<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+p%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Cleft%5B%5Clog+p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29-%5Clog+Q_i%28z%5E%7B%28i%29%7D%29%5Cright%5D%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog+p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>因此EM算法的迭代形式为：</p>
<p><img src="https://pic2.zhimg.com/80/v2-8a4f41596e78bfeb1b4044212b259524_1440w.jpg?source=d16d100b" alt="img" style="zoom:50%;"></p>
<p><img src="https://pic3.zhimg.com/80/v2-2f7fc5ca144d2f85f14d46e88055dd86_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>这张图的意思就是：<strong>首先我们固定</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>使下界</strong> <img src="https://www.zhihu.com/equation?tex=J%28z%2CQ%29" alt="[公式]"> <strong>上升至与</strong> <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="[公式]"> <strong>在此点</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>处相等（绿色曲线到蓝色曲线），然后固定</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>使下界</strong> <img src="https://www.zhihu.com/equation?tex=J%28z%2CQ%29" alt="[公式]"> <strong>达到最大值（</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta_t" alt="[公式]"> <strong>到</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D" alt="[公式]"> <strong>），然后再固定</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>，一直到收敛到似然函数</strong> <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="[公式]"> <strong>的最大值处的</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 。</p>
<p><strong><font color="red"> EM 算法通过引入隐含变量，使用 MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM 算法首先会固定其中的第一个参数，然后使用 MLE 计算第二个变量值；接着通过固定第二个变量，再使用 MLE 估测第一个变量值，依次迭代，直至收敛到局部最优解。</font></strong></p>
<h4 id="3-2-EM算法的收敛性"><a href="#3-2-EM算法的收敛性" class="headerlink" title="3.2 EM算法的收敛性"></a>3.2 EM算法的收敛性</h4><p>不妨假设 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 是EM算法第 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 次迭代和第 <img src="https://www.zhihu.com/equation?tex=k%2B1" alt="[公式]"> 次迭代的结果，要确保 <img src="https://www.zhihu.com/equation?tex=EM" alt="[公式]"> 算法收敛那么等价于证明 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%5Cle%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29" alt="[公式]"> 也就是说极大似然估计单调增加，那么算法最终会迭代到极大似然估计的最大值。在选定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 后可以得到 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步 <img src="https://www.zhihu.com/equation?tex=Q_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%3Dp%28z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> ，这一步保证了在给定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 时，詹森不等式中的等式成立即<br> <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C" alt="[公式]"><br>然后再进行 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步，固定 <img src="https://www.zhihu.com/equation?tex=Q_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 并将 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 视作变量，对上式 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> 求导后得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 因此有如下式子成立<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28a%29%5C%5C+%26%5Cle+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28b%29%5C%5C+%26%5Cle%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28c%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>首先 <img src="https://www.zhihu.com/equation?tex=%28a%29" alt="[公式]"> 式是前面 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步所保证詹森不等式中的等式成立的条件， <img src="https://www.zhihu.com/equation?tex=%28a%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步的定义，<img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28c%29" alt="[公式]">对任意参数都成立，而其等式的条件是固定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 并调整好 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 时成立，<img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28c%29" alt="[公式]">只是固定 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> ，在得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 时，只是最大化 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> ，也就是 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29" alt="[公式]"> 的一个下界而没有使等式成立。</p>
<h3 id="4-另一种理解"><a href="#4-另一种理解" class="headerlink" title="4. 另一种理解"></a>4. 另一种理解</h3><p>坐标上升法（Coordinate ascent）：</p>
<p><img src="https://pic4.zhimg.com/80/v2-b28bfe68513ff86d9643fec10786b827_1440w.jpg" alt="img"></p>
<p>途中直线为迭代优化路径，因为每次只优化一个变量，所以可以看到它没走一步都是平行与坐标轴的。</p>
<p>EM 算法类似于坐标上升法，E 步：固定参数，优化 Q；M 步：固定 Q，优化参数。交替将极值推向最大。</p>
<h4 id="5-应用"><a href="#5-应用" class="headerlink" title="5. 应用"></a>5. 应用</h4><p>EM 的应用有很多，比如、混合高斯模型、聚类、HMM 等等。其中 <strong>EM 在 K-means 中的用处</strong>，我将在介绍 K-means 中的给出。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8817%EF%BC%89KNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8817%EF%BC%89KNN/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:48" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:48+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-10 19:48:26" itemprop="dateModified" datetime="2022-07-10T19:48:26+08:00">2022-07-10</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-什么是KNN【KD树-SIFT-BBF算法】"><a href="#1-什么是KNN【KD树-SIFT-BBF算法】" class="headerlink" title="1. 什么是KNN【KD树 + SIFT+BBF算法】"></a>1. 什么是KNN【KD树 + SIFT+BBF算法】</h2><blockquote>
<p>  KNN与KD树：<a target="_blank" rel="noopener" href="https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272">https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272</a></p>
<p>  【数学】kd 树算法之详细篇 - 椰了的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23966698">https://zhuanlan.zhihu.com/p/23966698</a></p>
<p>  <strong>KNN是生成式模型还是判别式的</strong>，为什么？ - 风控算法小白的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/475072467/answer/2027766449">https://www.zhihu.com/question/475072467/answer/2027766449</a></p>
</blockquote>
<h3 id="1-1-KNN的通俗解释"><a href="#1-1-KNN的通俗解释" class="headerlink" title="1.1 KNN的通俗解释"></a>1.1 KNN的通俗解释</h3><p>何谓K近邻算法，即K-Nearest Neighbor algorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。</p>
<p>用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，<strong>在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。</strong></p>
<p>​                                                                     <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067"><img src="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067" alt="img"></a></p>
<p>​                                                                <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67"><img src="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67" alt="img"></a></p>
<p>如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），KNN就是解决这个问题的。</p>
<p>如果<strong>K=3</strong>，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>红色</strong>的三角形一类。</p>
<p>如果<strong>K=5</strong>，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>蓝色</strong>的正方形一类。</p>
<p><strong>于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</strong></p>
<h3 id="1-2-近邻的距离度量"><a href="#1-2-近邻的距离度量" class="headerlink" title="1.2 近邻的距离度量"></a>1.2 近邻的距离度量</h3><p>我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。</p>
<p><strong>有哪些距离度量的表示法</strong>(普及知识点，可以跳过)：</p>
<ol>
<li><p><strong>==欧氏距离==</strong>，最常见的两点之间或多点之间的距离表示法，又称之为<strong>欧几里得度量</strong>，它定义于欧几里得空间中，如点 x = (x1,…,xn) 和 y = (y1,…,yn) 之间的距离为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0699ac91c0bb89297e4cf418ad7d46cac175652d1a1de30fccc66a16a0eef254/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6428782c79293d2535437371727425374228785f312d795f3129253545322b28785f322d795f3229253545322b2e2e2e2b28785f6e2d795f6e29253545322537443d2535437371727425374225354373756d5f253742693d312537442535452537426e25374428785f692d795f692925354532253744"><img src="https://camo.githubusercontent.com/0699ac91c0bb89297e4cf418ad7d46cac175652d1a1de30fccc66a16a0eef254/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6428782c79293d2535437371727425374228785f312d795f3129253545322b28785f322d795f3229253545322b2e2e2e2b28785f6e2d795f6e29253545322537443d2535437371727425374225354373756d5f253742693d312537442535452537426e25374428785f692d795f692925354532253744" alt="img"></a></p>
<ul>
<li><p>二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b21b48fd9bce0b9f47d16d8e5505a6b600a02e1032632df73a77d5ba43012e2a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f322925354532253744"><img src="https://camo.githubusercontent.com/b21b48fd9bce0b9f47d16d8e5505a6b600a02e1032632df73a77d5ba43012e2a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f322925354532253744" alt="img"></a></p>
</li>
<li><p>三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ebf7fbc4dd6e6eb0d80576e7e5cd19d38ea7a8b9cd801680fc9b39f242367515/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f3229253545322b287a5f312d7a5f322925354532253744"><img src="https://camo.githubusercontent.com/ebf7fbc4dd6e6eb0d80576e7e5cd19d38ea7a8b9cd801680fc9b39f242367515/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f3229253545322b287a5f312d7a5f322925354532253744" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/62f0123f5d52ed280b23c7972c60f635d52a02d0b4958bd5bf0a7c61328f3225/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374225354373756d5f2537426b3d312537442535452537426e25374428785f253742316b2537442d785f253742326b2537442925354532253744"><img src="https://camo.githubusercontent.com/62f0123f5d52ed280b23c7972c60f635d52a02d0b4958bd5bf0a7c61328f3225/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374225354373756d5f2537426b3d312537442535452537426e25374428785f253742316b2537442d785f253742326b2537442925354532253744" alt="img"></a></p>
<p>也可以用表示成向量运算的形式：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8e7528374c22f456904f868c6619a94ba0e51180896315eb3b019f9d7b358742/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228612d622928612d622925354554253744"><img src="https://camo.githubusercontent.com/8e7528374c22f456904f868c6619a94ba0e51180896315eb3b019f9d7b358742/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228612d622928612d622925354554253744" alt="img"></a></p>
</li>
</ul>
</li>
<li><p><strong>曼哈顿距离</strong>，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在<strong>==欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和==</strong>。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为： <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/eeacd6c593ac3eadb18eebfdaf9e27626a54b8f8bdb9f53bc11a90713e8b0bf8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253743785f312d785f322537432b253743795f312d795f32253743"><img src="https://camo.githubusercontent.com/eeacd6c593ac3eadb18eebfdaf9e27626a54b8f8bdb9f53bc11a90713e8b0bf8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253743785f312d785f322537432b253743795f312d795f32253743" alt="img"></a>，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。</p>
<p>通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，此即曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。</p>
<ul>
<li><p>二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/c914feff78b9ccb3f920a67c8dba69b110d641abde5f2533f184a656780cb94d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d253743785f312d785f322537432b253743795f312d795f32253743"><img src="https://camo.githubusercontent.com/c914feff78b9ccb3f920a67c8dba69b110d641abde5f2533f184a656780cb94d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d253743785f312d785f322537432b253743795f312d795f32253743" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/eb4a219d2fbe50979339a7e1a83464ec27957562f106da7a3b08da8cc3394795/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d25354373756d5f2537426b3d312537442535452537426e253744253743785f253742316b2537442d785f253742326b253744253743"><img src="https://camo.githubusercontent.com/eb4a219d2fbe50979339a7e1a83464ec27957562f106da7a3b08da8cc3394795/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d25354373756d5f2537426b3d312537442535452537426e253744253743785f253742316b2537442d785f253742326b253744253743" alt="img"></a></p>
</li>
</ul>
</li>
<li><p><strong>切比雪夫距离</strong>，若二个向量或二个点p 、and q，其座标分别为Pi及qi，则两者之间的切比雪夫距离定义如下：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/3a305bb0cf8cdcd6a77f9e2e2cc1085542ae46f8c70c5b43c67be9728d3d99d6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686562797368657625374428702c71293d6d61785f6928253743705f692d715f6925374329"><img src="https://camo.githubusercontent.com/3a305bb0cf8cdcd6a77f9e2e2cc1085542ae46f8c70c5b43c67be9728d3d99d6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686562797368657625374428702c71293d6d61785f6928253743705f692d715f6925374329" alt="img"></a></p>
<p>这也等于以下Lp度量的极值： <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8fe6945bfb5795e12cfe28b72758c59a598e0c5a89b1ffb874f01cc8ff77a7ba/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d392d32345f32322d31392d34312e706e67"><img src="https://camo.githubusercontent.com/8fe6945bfb5795e12cfe28b72758c59a598e0c5a89b1ffb874f01cc8ff77a7ba/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d392d32345f32322d31392d34312e706e67" alt="img"></a>，因此切比雪夫距离也称为L∞度量。</p>
<p>以数学的观点来看，切比雪夫距离是由一致范数（uniform norm）（或称为上确界范数）所衍生的度量，也是超凸度量（injective metric space）的一种。</p>
<p>在平面几何中，若二点p及q的直角坐标系坐标为(x1,y1)及(x2,y2)，则切比雪夫距离为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2c68232279b3d8c857a5ebf5bcb91f6792abeb47daedeb22214391611680dd97/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686573732537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329"><img src="https://camo.githubusercontent.com/2c68232279b3d8c857a5ebf5bcb91f6792abeb47daedeb22214391611680dd97/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686573732537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329" alt="img"></a></p>
<p><strong>玩过国际象棋的朋友或许知道，国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？。你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。</strong></p>
<ul>
<li><p>二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离 ：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/9387d181258b2a722bcabf891beb2d616a2ec297f35bade87679099641f27f09/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329"><img src="https://camo.githubusercontent.com/9387d181258b2a722bcabf891beb2d616a2ec297f35bade87679099641f27f09/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ea4a630eab19efb4c0270f3808dde70fec7bb5f21d37f76ab016316277fbb730/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d61785f6928253743785f25374231692537442d785f253742326925374425374329"><img src="https://camo.githubusercontent.com/ea4a630eab19efb4c0270f3808dde70fec7bb5f21d37f76ab016316277fbb730/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d61785f6928253743785f25374231692537442d785f253742326925374425374329" alt="img"></a></p>
</li>
</ul>
</li>
</ol>
<p><strong>==简单说来，各种“距离”的应用场景简单概括为：==</strong></p>
<ul>
<li><strong>空间：欧氏距离</strong>，</li>
<li><strong>路径：曼哈顿距离，国际象棋国王：切比雪夫距离</strong>，</li>
<li>以上三种的统一形式:闵可夫斯基距离，</li>
<li>加权：标准化欧氏距离，</li>
<li>排除量纲和依存：马氏距离，</li>
<li>向量差距：夹角余弦，</li>
<li><strong>编码差别：汉明距离</strong>，</li>
<li>集合近似度：杰卡德类似系数与距离，</li>
<li>相关：相关系数与相关距离。</li>
</ul>
<h3 id="1-3-K值选择"><a href="#1-3-K值选择" class="headerlink" title="1.3 K值选择"></a>1.3 K值选择</h3><ol>
<li>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></li>
<li>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且<strong>K值的增大就意味着整体的模型变得简单。</strong></li>
<li>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</li>
</ol>
<p>在实际应用中，K值一般取一个比较小的数值，<strong>例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</strong></p>
<h3 id="1-4-KNN最近邻分类算法的过程"><a href="#1-4-KNN最近邻分类算法的过程" class="headerlink" title="1.4 KNN最近邻分类算法的过程"></a>1.4 KNN最近邻分类算法的过程</h3><ol>
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前 k 个最小距离的样本；</li>
<li>根据这 k 个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<h2 id="关于KNN的一些问题"><a href="#关于KNN的一些问题" class="headerlink" title="关于KNN的一些问题"></a>关于KNN的一些问题</h2><ol>
<li><p>在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用<strong>曼哈顿距离</strong>？</p>
<p><strong>答：</strong>我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向的运动。</p>
</li>
<li><p>KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?</p>
<p>答：极大的节约了时间成本．点线距离如果 &gt;　最小点，无需回溯上一层，如果&lt;,则再上一层寻找。</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8812%EF%BC%89%E9%99%8D%E7%BB%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8812%EF%BC%89%E9%99%8D%E7%BB%B4/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-29 16:31:00" itemprop="dateModified" datetime="2023-01-29T16:31:00+08:00">2023-01-29</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><blockquote>
<ul>
<li>数据降维算法: <a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1194552337170214912">https://www.zhihu.com/column/c_1194552337170214912</a></li>
</ul>
</blockquote>
<p><img src="https://pic2.zhimg.com/v2-e47296e78fff3d97eea11d0657ddcb81_1440w.jpg?source=172ae18b" alt="【机器学习】降维——PCA（非常详细）" style="zoom:51%;"></p>
<h2 id="一、PCA"><a href="#一、PCA" class="headerlink" title="一、PCA"></a>一、PCA</h2><blockquote>
<p>  <strong><font color="red"> 降维问题的优化目标：将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。</font></strong></p>
<p>  要找的 <strong>==P 是能让原始协方差矩阵对角化的 P==</strong>。换句话说，优化目标变成了<strong>寻找一个矩阵 P，满足</strong> <img src="https://www.zhihu.com/equation?tex=PCP%5ET" alt="[公式]"> <strong>是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件</strong>。</p>
</blockquote>
<p><strong>PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量</strong>。PCA 的数学推导可以从<strong>==最大可分型==</strong>和<strong>最近重构性</strong>两方面进行，前者的优化条件为划分后方差最大，后者的优化条件为点到划分平面距离最小，这里我将从最大可分性的角度进行证明。</p>
<h3 id="1-向量表示与基变换"><a href="#1-向量表示与基变换" class="headerlink" title="1. 向量表示与基变换"></a>1. 向量表示与基变换</h3><p>我们先来介绍些线性代数的基本知识。</p>
<h3 id="1-1-内积"><a href="#1-1-内积" class="headerlink" title="1.1 内积"></a>1.1 内积</h3><p><strong>两个向量的 A 和 B 内积</strong>我们知道形式是这样的：</p>
<p><img src="https://www.zhihu.com/equation?tex=%28a_1%2Ca_2%2C%5Ccdots%2Ca_n%29%5Ccdot+%28b_1%2Cb_2%2C%5Ccdots%2Cb_n%29%5E%5Cmathsf%7BT%7D%3Da_1b_1%2Ba_2b_2%2B%5Ccdots%2Ba_nb_n+%5C%5C" alt="[公式]"></p>
<p>内积运算将两个向量映射为实数，其计算方式非常容易理解，但我们无法看出其物理含义。接下来我们从几何角度来分析，为了简单起见，我们假设 A 和 B 均为二维向量，则：</p>
<p><img src="https://www.zhihu.com/equation?tex=A%3D%28x_1%2Cy_1%29%EF%BC%8CB%3D%28x_2%2Cy_2%29+%5C+A+%5Ccdot+B+%3D+%7CA%7C%7CB%7Ccos%28%5Calpha%29+%5C%5C" alt="[公式]"></p>
<p>其几何表示见下图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-cf4c0041c8459d2894b9a57d8f679a0a_1440w.jpg" alt="img"></p>
<p>我们看出 A 与 B 的内积等于 <strong>A 到 B 的投影长度乘以 B 的模</strong>。</p>
<p>如果假设 B 的模为 1，即让 <img src="https://www.zhihu.com/equation?tex=%7CB%7C%3D1" alt="[公式]"> ，那么就变成了：</p>
<p><img src="https://www.zhihu.com/equation?tex=A%5Ccdot+B%3D%7CA%7Ccos%28a%29+%5C%5C" alt="[公式]"></p>
<p>也就是说，<strong>A 与 B 的内积值等于 A 向 B 所在直线投影的标量大小。</strong></p>
<h3 id="1-2-基"><a href="#1-2-基" class="headerlink" title="1.2 基"></a>1.2 基</h3><p>在我们常说的坐标系种，向量 (3,2) 其实隐式引入了一个定义：以 x 轴和 y 轴上正方向长度为 1 的向量为标准。向量 (3,2) 实际是说在 x 轴投影为 3 而 y 轴的投影为 2。<strong>注意投影是一个标量，所以可以为负。</strong></p>
<p>所以，对于向量 (3, 2) 来说，如果我们想求它在 <img src="https://www.zhihu.com/equation?tex=%281%2C0%29%2C%280%2C1%29" alt="[公式]"> 这组基下的坐标的话，分别内积即可。当然，内积完了还是 (3, 2)。</p>
<p>所以，我们大致可以得到一个结论，我们<strong>要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了</strong>。为了方便求坐标，我们希望这组基向量模长为 1。因为向量的内积运算，当模长为 1 时，内积可以直接表示投影。然后还需要这组基是线性无关的，我们一般用正交基，非正交的基也是可以的，不过正交基有较好的性质。</p>
<h3 id="1-3-基变换的矩阵表示"><a href="#1-3-基变换的矩阵表示" class="headerlink" title="1.3 基变换的矩阵表示"></a>1.3 基变换的矩阵表示</h3><p>这里我们先做一个练习：对于向量 (3,2) 这个点来说，在 <img src="https://www.zhihu.com/equation?tex=%28%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%2C%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%28-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%2C+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 这组基下的坐标是多少？</p>
<p>我们拿 (3,2) 分别与之内积，得到 <img src="https://www.zhihu.com/equation?tex=%28%5Cfrac%7B5%7D%7B%5Csqrt%7B2%7D%7D%2C-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 这个新坐标。</p>
<p>我们可以用矩阵相乘的形式简洁的表示这个变换：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D++1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+3+%5C%5C+2+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+5%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D++%5Cend%7Bpmatrix%7D++%5C%5C" alt="[公式]"></p>
<p>左边矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。推广一下，如果我们有 m 个二维向量，只要将二维向量按列排成一个两行 m 列矩阵，然后用“基矩阵”乘以这个矩阵就可以得到了所有这些向量在新基下的值。例如对于数据点 <img src="https://www.zhihu.com/equation?tex=%281%2C1%29%EF%BC%8C%282%2C2%29%EF%BC%8C%283%2C3%29" alt="[公式]"> 来说，想变换到刚才那组基上，则可以这样表示：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+1+%26+2+%26+3+%5C%5C+1+%26+2+%26+3+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+2%2F%5Csqrt%7B2%7D+%26+4%2F%5Csqrt%7B2%7D+%26+6%2F%5Csqrt%7B2%7D+%5C%5C+0+%26+0+%26+0+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>我们可以把它写成通用的表示形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+p_1+%5C%5C+p_2+%5C%5C+%5Cvdots+%5C%5C+p_R+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+a_1+%26+a_2+%26+%5Ccdots+%26+a_M+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+p_1a_1+%26+p_1a_2+%26+%5Ccdots+%26+p_1a_M+%5C%5C+p_2a_1+%26+p_2a_2+%26+%5Ccdots+%26+p_2a_M+%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C+p_Ra_1+%26+p_Ra_2+%26+%5Ccdots+%26+p_Ra_M+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 是一个行向量，表示第 i 个基， <img src="https://www.zhihu.com/equation?tex=a_j" alt="[公式]"> 是一个列向量，表示第 j 个原始数据记录。实际上也就是做了一个向量矩阵化的操作。</p>
<p>==上述分析给矩阵相乘找到了一种物理解释：<strong>两个矩阵相乘的意义是将右边矩阵中的每一列向量</strong> <img src="https://www.zhihu.com/equation?tex=a_i" alt="[公式]"> <strong>变换到左边矩阵中以每一行行向量为基所表示的空间中去。</strong>也就是说一个矩阵可以表示一种线性变换。==</p>
<h3 id="2-最大可分性"><a href="#2-最大可分性" class="headerlink" title="2. 最大可分性"></a>2. 最大可分性</h3><p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，<strong>如果基的数量少于向量本身的维数，则可以达到降维的效果</strong>。</p>
<p><strong>但是我们还没回答一个最关键的问题：如何选择基才是最优的。或者说，如果我们有一组 N 维向量，现在要将其降到 K 维（K 小于 N），那么我们应该如何选择 K 个基才能最大程度保留原有的信息？</strong></p>
<p>一种直观的看法是：<strong><font color="red"> 希望投影后的投影值尽可能分散，因为如果重叠就会有样本消失。当然这个也可以从熵的角度进行理解，熵越大所含信息越多。</font></strong></p>
<h4 id="2-1-方差"><a href="#2-1-方差" class="headerlink" title="2.1 方差"></a>2.1 方差</h4><p>我们知道数值的分散程度，可以用数学上的方差来表述。<strong>一个变量的方差可以看做是每个元素与变量均值的差的平方和的均值</strong>，即：</p>
<p><img src="https://www.zhihu.com/equation?tex=Var%28a%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu%29%5E2%7D+%5C%5C" alt="[公式]"></p>
<p><strong>为了方便处理，我们将每个变量的均值都化为 0</strong> ，因此方差可以直接用每个元素的平方和除以元素个数表示：</p>
<p><img src="https://www.zhihu.com/equation?tex=Var%28a%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%5C%5C" alt="[公式]"></p>
<p>于是上面的问题被形式化表述为：<strong>寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</strong></p>
<h4 id="2-2-协方差"><a href="#2-2-协方差" class="headerlink" title="2.2 协方差"></a>2.2 协方差</h4><p>在一维空间中我们可以用方差来表示数据的分散程度。而对于高维数据，我们用协方差进行约束，<strong>协方差可以表示两个变量的相关性</strong>。<strong><font color="red"> 为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性</font></strong>，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。</p>
<p>协方差公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm-1%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu_a%29%28b_i-%5Cmu_b%29%7D+%5C%5C" alt="[公式]"></p>
<p>由于均值为 0，所以我们的协方差公式可以表示为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C" alt="[公式]"></p>
<p>当样本数较大时，不必在意其是 m 还是 m-1，为了方便计算，我们分母取 m。</p>
<p><strong><font color="red"> 协方差为 0 时，表示两个变量完全不相关</font></strong>。为了让协方差为 0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。</p>
<p>（<strong>补充</strong>：协方差为 0 时，两个变量只是线性不相关。完全独立是有问题的，才疏学浅，还望见谅。）</p>
<p><strong><font color="red"> 至此，我们得到了降维问题的优化目标：将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。</font></strong></p>
<h4 id="2-3-协方差矩阵"><a href="#2-3-协方差矩阵" class="headerlink" title="2.3 协方差矩阵"></a>2.3 协方差矩阵</h4><p>针对我们给出的优化目标，接下来我们将从数学的角度来给出优化目标。我们看到，最终要达到的目的与<strong>变量内方差及变量间协方差</strong>有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们有：</p>
<p>假设我们只有 a 和 b 两个变量，那么我们将它们按行组成矩阵 X：</p>
<p><img src="https://www.zhihu.com/equation?tex=X%3D%5Cbegin%7Bpmatrix%7D++a_1+%26+a_2+%26+%5Ccdots+%26+a_m+%5C%5C+b_1+%26+b_2+%26+%5Ccdots+%26+b_m++%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>然后：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D%3D+%5Cbegin%7Bpmatrix%7D++%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Bb_i%5E2%7D++%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D++Cov%28a%2Ca%29+%26+Cov%28a%2Cb%29+%5C%5C++Cov%28b%2Ca%29+%26+Cov%28b%2Cb%29+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>我们可以看到这个矩阵对角线上的分别是两个变量的方差，而其它元素是 a 和 b 的协方差。两者被统一到了一个矩阵里。</p>
<p><strong>设我们有 m 个 n 维数据记录，将其排列成矩阵</strong> <img src="https://www.zhihu.com/equation?tex=X_%7Bn%2Cm%7D" alt="[公式]"> <strong>，设</strong> <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5ET" alt="[公式]"> <strong>，则 C 是一个对称矩阵，其对角线分别对应各个变量的方差，而第 i 行 j 列和 j 行 i 列元素相同，表示 i 和 j 两个变量的协方差</strong>。</p>
<h4 id="2-4-矩阵对角化"><a href="#2-4-矩阵对角化" class="headerlink" title="2.4 矩阵对角化"></a>2.4 矩阵对角化</h4><p>根据我们的优化条件，<strong>我们需要将除对角线外的其它元素化为 0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大）</strong>，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系。</p>
<p>设原始数据矩阵 X 对应的协方差矩阵为 C，而 P 是一组基按行组成的矩阵，设 Y=PX，则 Y 为 X 对 P 做基变换后的数据。设 Y 的协方差矩阵为 D，我们推导一下 D 与 C 的关系：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++D+%26+%3D++%5Cfrac%7B1%7D%7Bm%7DYY%5ET+%5C%5C++%26+%3D+%5Cfrac%7B1%7D%7Bm%7D%28PX%29%28PX%29%5ET+%5C%5C+%26+%3D+%5Cfrac%7B1%7D%7Bm%7DPXX%5ETP%5ET+%5C%5C++%26+%3D+P%28%5Cfrac%7B1%7D%7Bm%7DXX%5ET%29P%5ET+%5C%5C++%26+%3D+PCP%5ET++%5Cend%7Baligned%7D++%5C%5C" alt="[公式]"></p>
<p>这样我们就看清楚了，我们要找的 <strong>==P 是能让原始协方差矩阵对角化的 P==</strong>。换句话说，优化目标变成了<strong>寻找一个矩阵 P，满足</strong> <img src="https://www.zhihu.com/equation?tex=PCP%5ET" alt="[公式]"> <strong>是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件</strong>。</p>
<p>至此，我们离 PCA 还有仅一步之遥，我们还需要完成对角化。</p>
<p><strong>由上文知道，协方差矩阵 C 是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质：</strong></p>
<ol>
<li><strong>实对称矩阵不同特征值对应的特征向量必然正交</strong>。</li>
<li><strong>设特征向量 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> 重数为 r，则必然存在 r 个线性无关的特征向量对应于 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> ，因此可以将这 r 个特征向量单位正交化。</strong></li>
</ol>
<p><strong>由上面两条可知，一个 n 行 n 列的实对称矩阵一定可以找到 n 个单位正交特征向量，设这 n 个特征向量为 <img src="https://www.zhihu.com/equation?tex=e_1%2Ce_2%2C%5Ccdots%2Ce_n" alt="[公式]"> ，我们将其按列组成矩阵： <img src="https://www.zhihu.com/equation?tex=E%3D%28e_1+%2C+e_2+%2C+%5Ccdots+%2C+e_n+%29" alt="[公式]"> 。</strong></p>
<p>则对协方差矩阵 C 有如下结论：</p>
<p><img src="https://www.zhihu.com/equation?tex=E%5ETCE%3D%5CLambda%3D%5Cbegin%7Bpmatrix%7D+%5Clambda_1+%26+%26+%26+%5C%5C+%26+%5Clambda_2+%26+%26+%5C%5C+%26+%26+%5Cddots+%26+%5C%5C+%26+%26+%26+%5Clambda_n+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。到这里，我们发现我们已经找到了需要的矩阵 P： <img src="https://www.zhihu.com/equation?tex=P%3DE%5E%5Cmathsf%7BT%7D" alt="[公式]"> 。</p>
<p><strong>P 是协方差矩阵的特征向量单位化后按行排列出的矩阵</strong>，其中每一行都是 C 的一个特征向量。如果设 P 按照 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 中特征值的从大到小，将特征向量从上到下排列，则用 P 的前 K 行组成的矩阵乘以原始数据矩阵 X，就得到了我们需要的降维后的数据矩阵 Y。</p>
<blockquote>
<p>  <strong>拉格朗日乘子法证明</strong>:<strong>方差就是协方差矩阵的特征值</strong></p>
</blockquote>
<h4 id="2-5-最近重构性-思路"><a href="#2-5-最近重构性-思路" class="headerlink" title="2.5 最近重构性-思路"></a>2.5 最近重构性-思路</h4><p>以上的证明思路主要是基于最大可分性的思想，<strong>通过一条直线使得样本点投影到该直线上的方差最大</strong>。除此之外，我们还可以<strong>将其转换为线型回归问题，其目标是求解一个线性函数使得对应直线能够更好地拟合样本点集合</strong>。这就<strong>使得我们的优化目标从方差最大转化为平方误差最小</strong>，因为映射距离越短，丢失的信息也会越小。区别于最大可分性，这是从最近重构性的角度进行论证。</p>
<h3 id="3-求解步骤"><a href="#3-求解步骤" class="headerlink" title="==3. 求解步骤=="></a>==3. 求解步骤==</h3><h4 id="总结一下-PCA-的算法步骤：设有-m-条-n-维数据。"><a href="#总结一下-PCA-的算法步骤：设有-m-条-n-维数据。" class="headerlink" title="总结一下 PCA 的算法步骤：设有 m 条 n 维数据。"></a>总结一下 PCA 的算法步骤：<strong>设有 m 条 n 维数据。</strong></h4><ol>
<li><strong>将原始数据按列组成 n 行 m 列矩阵 X；</strong></li>
<li><strong>将 X 的每一行进行==零均值化==，即减去这一行的均值</strong>；【<strong>零均值化</strong>】【<strong>方差、协方差好计算</strong>】</li>
<li><strong>==求出协方差矩阵==</strong> <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D" alt="[公式]"> ；</li>
<li><strong>求出协方差矩阵的特征值及对应的特征向量</strong>；</li>
<li><strong>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P</strong>；</li>
<li><img src="https://www.zhihu.com/equation?tex=Y%3DPX" alt="[公式]"> <strong>即为降维到 k 维后的数据</strong>。</li>
</ol>
<h4 id="4-性质【维度灾难、降噪、过拟合、特征独立】"><a href="#4-性质【维度灾难、降噪、过拟合、特征独立】" class="headerlink" title="4. 性质【维度灾难、降噪、过拟合、特征独立】"></a>4. 性质【维度灾难、降噪、过拟合、特征独立】</h4><ol>
<li><strong>==缓解维度灾难==</strong>：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；</li>
<li><strong>==降噪==</strong>：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；</li>
<li><strong>==过拟合==</strong>：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；</li>
<li><strong>==特征独立==</strong>：PCA 不仅将数据压缩到低维，它也使得<strong>降维之后的数据各特征相互独立</strong>；</li>
</ol>
<h3 id="5-细节"><a href="#5-细节" class="headerlink" title="5. 细节"></a>5. 细节</h3><h4 id="5-1-零均值化"><a href="#5-1-零均值化" class="headerlink" title="5.1 零均值化"></a>5.1 零均值化</h4><p>当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。==而<strong>对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来</strong>，不能使用验证集或者测试集的中心向量。==</p>
<p>其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。</p>
<p>另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现 Variance Shift 的问题。</p>
<h4 id="5-2-SVD-的对比"><a href="#5-2-SVD-的对比" class="headerlink" title="==5.2 SVD 的对比=="></a>==5.2 SVD 的对比==</h4><p>这是两个不同的数学定义。我们先给结论：<strong>特征值和特征向量是针对方阵</strong>才有的，而<strong>对任意形状的矩阵都可以做奇异值分解</strong>。</p>
<p><strong>PCA</strong>：<strong>方阵的特征值分解</strong>，对于一个方阵 A。其中，Q 是这个矩阵 A 的特征向量组成的矩阵， <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列)。也就是说矩阵 A 的信息可以由其特征值和特征向量表示。</p>
<p><strong>SVD</strong>：<strong>矩阵的奇异值分解其实就是对于矩阵 A 的协方差矩阵 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 做特征值分解推导出来的</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=A_%7Bm%2Cn%7D%3DU_%7Bm%2Cm%7D%5CLambda_%7Bm%2Cn%7DV%5ET_%7Bn%2Cn%7D+%5Capprox+U_%7Bm%2Ck%7D%5CLambda_%7Bk%2Ck%7DV%5ET_%7Bk%2Cn%7D+%5C%5C" alt="[公式]"></p>
<p>其中：U V 都是正交矩阵，有 <img src="https://www.zhihu.com/equation?tex=U%5ETU%3DI_m%2C+V%5ETV%3DI_n" alt="[公式]"> 。这里的约等于是因为 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 中有 n 个奇异值，但是由于排在后面的很多接近 0，所以我们可以仅保留比较大的 k 个奇异值。</p>
<p><img src="https://www.zhihu.com/equation?tex=A%5ETA%3D%28U+%5CLambda+V%5ET%29%5ETU+%5CLambda+V%5ET+%3DV+%5CLambda%5ET+U%5ETU+%5CLambda+V%5ET++%3D+V%5CLambda%5E2+V%5ET+%5C%5C+AA%5ET%3DU+%5CLambda+V%5ET%28U+%5CLambda+V%5ET%29%5ET+%3DU+%5CLambda+V%5ETV+%5CLambda%5ET+U%5ET+%3D+U%5CLambda%5E2+U%5ET++%5C%5C" alt="[公式]"></p>
<p>所以，V U 两个矩阵分别是 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 的特征向量，中间的矩阵对角线的元素是 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 的特征值。我们也很容易看出 A 的奇异值和 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 的特征值之间的关系。</p>
<p>PCA 需要对协方差矩阵 <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5ET" alt="[公式]"> 。进行特征值分解； SVD 也是对 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 进行特征值分解。如果取 <img src="https://www.zhihu.com/equation?tex=A%3D%5Cfrac%7BX%5ET%7D%7B%5Csqrt%7Bm%7D%7D" alt="[公式]"> 则两者基本等价。所以 PCA 问题可以转换成 SVD 求解。</p>
<p><strong>==而实际上 Sklearn 的 PCA 就是用 SVD 进行求解的==</strong>，原因有以下几点：</p>
<ol>
<li>当样本维度很高时，协方差矩阵计算太慢；</li>
<li>方阵特征值分解计算效率不高；</li>
<li><strong>==SVD 除了特征值分解这种求解方式外，还有更高效更准确的迭代求解方式，避免了 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 的计算；==</strong></li>
<li><strong>其实 PCA 与 SVD 的右奇异向量的压缩效果相同</strong>。</li>
</ol>
<blockquote>
<ol>
<li>《机器学习》周志华</li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.codinglabs.org/articles/pca-tutorial.html">PCA 的数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm">Singular Value Decomposition (SVD) tutorial</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">机器学习中的数学（4）——线性判别分析（LDA）, 主成分分析（PCA）</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//my.oschina.net/findbill/blog/535044">从SVD到PCA——奇妙的数学游戏</a></li>
<li>scikit-learn：降维算法PCA和SVD <a target="_blank" rel="noopener" href="https://blog.csdn.net/HHG20171226/article/details/102981822">https://blog.csdn.net/HHG20171226/article/details/102981822</a></li>
</ol>
</blockquote>
<h2 id="二、线性判别分析（LDA）【监督】"><a href="#二、线性判别分析（LDA）【监督】" class="headerlink" title="二、线性判别分析（LDA）【监督】"></a>二、线性判别分析（LDA）【监督】</h2><blockquote>
<p>  ==<strong>“投影后类内方差最小，类间方差最大”</strong>==</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/liuweiyuxiang/article/details/78874106">https://blog.csdn.net/liuweiyuxiang/article/details/78874106</a></li>
</ul>
</blockquote>
<h3 id="2-1-概念"><a href="#2-1-概念" class="headerlink" title="2.1 概念"></a>2.1 概念</h3><p><strong>线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。</strong></p>
<p><strong>LDA分类思想简单总结如下：</strong></p>
<ol>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。</li>
<li>对于训练数据，设法将多维数据投影到一条直线上，<strong>同类数据的投影点尽可能接近，异类数据点尽可能远离</strong>。</li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</li>
</ol>
<p><strong><font color="red"> 如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</font></strong></p>
<p>假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="image-20220526135646769.png" alt="image-20220526135646769"></p>
<p> 从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。 以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3 id="2-2-原理"><a href="#2-2-原理" class="headerlink" title="2.2 原理"></a>2.2 原理</h3><p> LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(<a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Linear_classifier">Linear Classifier</a>)：因为LDA是一种线性分类器。对于<strong>K-分类的一个分类问题，会有K个线性函数</strong>：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455464493.png" alt="image"></p>
<p>当满足条件：对于所有的j，都有Yk &gt; Yj,的时候，我们就说x属于类别k。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的就是所属的分类了。</p>
<p>上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455475507.gif" alt="clip_image002" style="zoom:67%;"></p>
<p>红色的方形的点为0类的原始点、蓝色的方形点为1类的原始点，经过原点的那条线就是投影的直线，从图上可以清楚的看到，红色的点和蓝色的点被<strong>原点</strong>明显的分开了，这个数据只是随便画的，如果在高维的情况下，看起来会更好一点。下面我来推导一下二分类LDA问题的公式：假设用来区分二分类的直线（投影函数)为：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455471885.png" alt="image"></p>
<p> <strong>LDA分类的一个目标是使得==不同类别==之间的距离越远越好，==同一类别==之中的距离越近越好</strong>，所以我们需要定义几个关键的值。</p>
<ul>
<li><p><strong>==类别i的原始中心点为==</strong>：（Di表示属于类别i的点)</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455478264.png" alt="image"></p>
</li>
<li><p>类别i投影后的中心点为：</p>
</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455488231.png" alt="image"></p>
<ul>
<li><strong>==衡量类别i投影后，类别点之间的分散程度（方差）为==</strong>：</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455487326.png" alt="image"></p>
<ul>
<li><strong>==最终我们可以得到一个下面的公式，表示LDA投影到w后的损失函数==</strong>：</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455484785.png" alt="image"></p>
<p>我们<strong>分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。</strong>分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。</p>
<p> 我们定义一个<strong>投影前的==各类别分散程度的矩阵==</strong>，这个矩阵看起来有一点麻烦，其实意思是，如果某一个分类的<strong>输入点集Di里面的点距离这个分类的中心店mi越近</strong>，则Si里面元素的值就越小，如果分类的点都紧紧地围绕着mi，则Si里面的元素值越更接近0.</p>
<script type="math/tex; mode=display">
S_{i}=\sum_{x \in D_{i}}\left(x-m_{i}\right)\left(x-m_{i}\right)^{T}</script><p>带入 $\mathrm{Si}$, 将 $\mathrm{J}(\mathrm{w})$ 分母化为:</p>
<p>$\tilde{s}_{i}=\sum_{x \in D_{i}}\left(w^{T} x-w^{T} m_{i}\right)^{2}=\sum_{x \in D_{i}} w^{T}\left(x-m_{i}\right)\left(x-m_{i}\right)^{T} w=w^{T} S_{i} w$</p>
<script type="math/tex; mode=display">{\tilde{S_{1}}}^{2}+{\tilde{S_{2}}}^{2}=w^{T}\left(S_{1}+S_{2}\right) w=w^{T} S_{w} w</script><p>同样的将 $\mathrm{J}(\mathrm{w})$ 分子化为:</p>
<script type="math/tex; mode=display">
\left|\widetilde{m_{1}}-\widetilde{m_{2}}\right|^{2}=w^{T}\left(m_{1}-m_{2}\right)\left(m_{1}-m_{2}\right)^{T} w=w^{T} S_{B} w</script><p>这样<strong>损失函数</strong>可以化成下面的形式:</p>
<script type="math/tex; mode=display">
J(w)=\frac{w^{T} S_{B} w}{w^{T} S_{w} w}</script><p>这样就可以用最喜欢的<strong>==拉格朗日乘子法==</strong>了, 但是还有一个问题, 如果分子、分母是都可以取任意值的, 那就会 使得有无穷解, 我们将分母限制为长度为 1, 并作为拉格朗日乘子法的限制条件, 带入得到:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&c(w)=w^{T} S_{B} w-\lambda\left(w^{T} S_{w} w-1\right) \\
&\Rightarrow \frac{d c}{d w}=2 S_{B} w-2 \lambda S_{w} w=0 \\
&\Rightarrow S_{B} w=\lambda S_{w} w
\end{aligned}</script><p><strong>==这样的式子就是一个求特征值的问题了。==</strong><br>对于 $N(N&gt;2)$ 分类的问题, 我就直接写出下面的结论了:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&S_{W}=\sum_{i=1}^{c} S_{i} \\
&S_{B}=\sum_{i=1}^{c} n_{i}\left(m_{i}-m\right)\left(m_{i}-m\right)^{T} \\
&S_{B} w_{i}=\lambda S_{w} w_{i}
\end{aligned}</script><p>这同样是一个求特征值的问题，我们求出的第i大的特征向量，就是对应的Wi了。</p>
<blockquote>
<p>  这里想多谈谈特征值，特征值在纯数学、量子力学、固体力学、计算机等等领域都有广泛的应用，特征值表示的是矩阵的性质，当我们取到矩阵的前N个最大的特征值的时候，我们可以说提取到的矩阵主要的成分（这个和之后的PCA相关，但是不是完全一样的概念）。在机器学习领域，不少的地方都要用到特征值的计算，比如说图像识别、pagerank、LDA、还有之后将会提到的PCA等等。</p>
</blockquote>
<p><strong>优缺点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>优缺点</th>
<th>简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>1. 可以使用类别的先验知识； 2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr>
<td>缺点</td>
<td>1. LDA不适合对非高斯分布样本进行降维； 2. <strong>LDA降维最多降到分类数k-1维</strong>； 3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好； 4. LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="三、t-SNE-高维数据可视化"><a href="#三、t-SNE-高维数据可视化" class="headerlink" title="三、t-SNE 高维数据可视化"></a>三、t-SNE 高维数据可视化</h2><blockquote>
<p>  高维数据可视化之t-SNE算法🌈:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/57937096">https://zhuanlan.zhihu.com/p/57937096</a></p>
</blockquote>
<p><strong>T-SNE算法是用于可视化的算法中效果最好的算法之一</strong>，相信大家也对T-SNE算法略有耳闻，本文参考T-SNE作者<strong>Laurens van der Maaten</strong>给出的源代码自己实现T-SNE算法代码，以此来加深对T-SNE的理解。先简单介绍一下T-SNE算法，T-SNE将数据点变换映射到概率分布上。</p>
<h4 id="3-1-t-SNE数据算法的目的"><a href="#3-1-t-SNE数据算法的目的" class="headerlink" title="3.1 t-SNE数据算法的目的"></a>3.1 t-SNE数据算法的目的</h4><p><strong>主要是将数据从高维数据转到低维数据，并在低维空间里也保持其在高维空间里所携带的信息（比如高维空间里有的清晰的分布特征，转到低维度时也依然存在）。</strong></p>
<p><strong>==t-SNE将欧氏距离距离转换为条件概率，来表达点与点之间的相似度，再优化两个分布之间的距离-KL散度，从而保证点与点之间的分布概率不变。==</strong></p>
<h4 id="3-2-SNE原理"><a href="#3-2-SNE原理" class="headerlink" title="3.2 SNE原理"></a>3.2 SNE原理</h4><p>$S N E$ 是<strong>通过仿射变换将数据点映射到相应概率分布上</strong>, 主要包括下面两个步骤:</p>
<ol>
<li>通过在高维空间中构建数据点之间的概率分布 $P$, 使得相似的数据点有更高的概率被选择, 而 不相似的数据点有较低的概率被选择;</li>
<li>然后在低维空间里重构这些点的概率分布 $Q$, 使得这两个概率分布尽可能相似。</li>
</ol>
<p>令输入空间是 $X \in \mathbb{R}^{n}$, 输出空间为 $Y \in \mathbb{R}^{t}(t \ll n)$ 。不妨假设含有 $m$ 个样本数据 $\left\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\right\}$, 其中 $x^{(i)} \in X$, 降维后的数据为 $\left\{y^{(1)}, y^{(2)}, \cdots, y^{(m)}\right\}, y^{(i)} \in Y$ 。 $S N E$ 是<strong>先将欧几里得距离转化为条件概率来表达点与点之间的相似度</strong>, 即首先是计算条件概 率 $p_{j \mid i}$, 其正比于 $x^{(i)}$ 和 $x^{(j)}$ 之间的相似度, $p_{j \mid i}$ 的计算公式为:</p>
<script type="math/tex; mode=display">
p_{j \mid i}=\frac{\exp \left(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^{2}}{2 \sigma_{i}^{2}}\right)}{\sum_{k \neq i} \exp \left(-\frac{\left\|x^{(i)}-x^{(k)}\right\|^{2}}{2 \sigma_{i}^{2}}\right)}</script><p>在这里引入了一个参数 $\sigma_{i}$, 对于不同的数据点 $x^{(i)}$ 取值亦不相同, 因为我们关注的是不同数据 点两两之间的相似度, 故可设置 $p_{i \mid i}=0$ 。对于低维度下的数据点 $y^{(i)}$, 通过条件概率 $q_{j \mid i}$ 来 刻画 $y^{(i)}$ 与 $y^{(j)}$ 之间的相似度, $q_{j \mid i}$ 的计算公式为:</p>
<script type="math/tex; mode=display">
q_{j \mid i}=\frac{\exp \left(-\left\|y^{(i)}-y^{(j)}\right\|^{2}\right)}{\sum_{k \neq i} \exp \left(-\left\|y^{(i)}-y^{(k)}\right\|^{2}\right)}</script><p>同理, 设置 $q_{i \mid i}=0$ 。<br>如果降维的效果比较好, 局部特征保留完整, 那么有 $p_{i \mid j}=q_{i \mid j}$ 成立, 因此通过优化两个分布之 间的 <strong>$K L$ 散度构造出的损失函数为</strong>:</p>
<script type="math/tex; mode=display">
C\left(y^{(i)}\right)=\sum_{i} K L\left(P_{i} \| Q_{i}\right)=\sum_{i} \sum_{j} p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}}</script><p>这里的 $P_{i}$ 表示在给定高维数据点 $x^{(i)}$ 时, 其他所有数据点的条件概率分布; $Q_{i}$ 则表示在给定 低维数据点 $y^{(i)}$ 时, 其他所有数据点的条件概率分布。从损失函数可以看出, 当 $p_{j \mid i}$ 较大 $q_{j \mid i}$ 较小时, 惩罚较高; 而 $p_{j \mid i}$ 较小 $q_{j \mid i}$ 较大时, 惩罚较低。换句话说就是高维空间中两个数据点距 离较近时, 若映射到低维空间后距离较远, 那么将得到一个很高的惩罚; 反之, 高维空间中两个数 据点距离较远时, 若映射到低维空间距离较近, 将得到一个很低的惩罚值。也就是说, <strong>$S N E$ 的 损失函数更关注于局部特征, 而忽视了全局结构</strong>。</p>
<h4 id="3-3-目标函数求解"><a href="#3-3-目标函数求解" class="headerlink" title="3.3 目标函数求解"></a>3.3 目标函数求解</h4><h4 id="3-4-对称性-SNE"><a href="#3-4-对称性-SNE" class="headerlink" title="3.4 对称性-SNE"></a>3.4 对称性-SNE</h4><p><strong>优化 <img src="https://www.zhihu.com/equation?tex=KL%28P%5CVert+Q%29" alt="[公式]"> 的一种替换思路是使用联合概率分布来替换条件概率分布</strong>，即 <img src="https://www.zhihu.com/equation?tex=P" alt="[公式]"> 是高维空间里数据点的联合概率分布， <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 是低维空间里数据点的联合概率分布，此时的损失函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=C%28y%5E%7B%28i%29%7D%29%3DKL%28P%5CVert+Q%29%3D%5Csum%5Climits_i%5Csum%5Climits_jp_%7Bij%7D%5Clog%5Cdfrac%7Bp_%7Bij%7D%7D%7Bq_%7Bij%7D%7D%5C%5C" alt="[公式]"></p>
<p>同样的 <img src="https://www.zhihu.com/equation?tex=p_%7Bii%7D%3Dq_%7Bii%7D%3D0" alt="[公式]"> ，这种改进下的 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 称为对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> ，因为它的先验假设为对 <img src="https://www.zhihu.com/equation?tex=%5Cforall+i" alt="[公式]"> 有 <img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3Dp_%7Bji%7D%2Cq_%7Bij%7D%3Dq_%7Bji%7D" alt="[公式]"> 成立，故概率分布可以改写成：</p>
<p><img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3D%5Cdfrac%7Bexp%28-%5Cfrac%7B%5CVert+x%5E%7B%28i%29%7D-x%5E%7B%28j%29%7D%5CVert%5E2%7D%7B2%5Csigma%5E2%7D%29%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7Dexp%28-%5Cfrac%7B%5CVert+x%5E%7B%28k%29%7D-x%5E%7B%28l%29%7D+%5CVert%5E2%7D%7B2%5Csigma%5E2%7D%29%7D%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+q_%7Bij%7D%3D%5Cdfrac%7Bexp%28-%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7Dexp%28-%5CVert+y%5E%7B%28k%29%7D-y%5E%7B%28l%29%7D+%5CVert%5E2%29%7D%5C%5C" alt="[公式]"></p>
<p>这种改进方法使得表达式简洁很多，但是容易受到异常点数据的影响，为了解决这个问题通过对联合概率分布定义修正为： <img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3D%5Cfrac%7Bp_%7Bj%7Ci%7D%2Bp_%7Bi%7Cj%7D%7D%7B2%7D" alt="[公式]"> ，这保证了 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_jp_%7Bij%7D+%5Cgt+%5Cfrac%7B1%7D%7B2m%7D" alt="[公式]"> ，使得每个点对于损失函数都会有贡献。对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 最大的优点是简化了梯度计算，梯度公式改写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpartial+C%28y%5E%7B%28i%29%7D%29%7D%7B%5Cpartial+y%5E%7B%28i%29%7D%7D%3D4%5Csum%5Climits_j%28p_%7Bij%7D-q_%7Bij%7D%29%28y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D%29%5C%5C" alt="[公式]"></p>
<p>研究表明，对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 的效果差不多，有时甚至更好一点。</p>
<h4 id="3-5-t-SNE"><a href="#3-5-t-SNE" class="headerlink" title="3.5 t-SNE"></a>3.5 t-SNE</h4><p><img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7DSNE" alt="[公式]"> 在对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 的改进是，首先<strong>通过在高维空间中使用高斯分布将距离转换为概率分布，然后在低维空间中，使用更加偏重长尾分布的方式来将距离转换为概率分布</strong>，使得高维度空间中的中低等距离在映射后能够有一个较大的距离。</p>
<p><img src="https://pic4.zhimg.com/80/v2-928a3ada308128f26b719d510a728fbb_1440w.jpg" alt="img"></p>
<p>从图中可以看到，在没有异常点时， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布与高斯分布的拟合结果基本一致。而在第二张图中，出现了部分异常点，由于高斯分布的尾部较低，对异常点比较敏感，为了照顾这些异常点，高斯分布的拟合结果偏离了大多数样本所在位置，方差也较大。<strong>相比之下， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布的尾部较高，对异常点不敏感，保证了其鲁棒性，因此拟合结果更为合理，较好的捕获了数据的全局特征。</strong></p>
<p>使用 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布替换高斯分布之后 <img src="https://www.zhihu.com/equation?tex=q_%7Bij+%7D" alt="[公式]"> 的变化如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=q_%7Bij%7D%3D%5Cdfrac%7B%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7D%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%7D%5C%5C" alt="[公式]"></p>
<p>此外，随着自由度的逐渐增大， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布的密度函数逐渐接近标准正态分布，因此在计算梯度方面会简单很多，优化后的梯度公式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpartial+C%28y%5E%7B%28i%29%7D%29%7D%7B%5Cpartial+y%5E%7B%28i%29%7D%7D%3D4%5Csum%5Climits_%7Bj%7D%28p_%7Bij%7D-q_%7Bij%7D%29%28y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D%29%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%5C%5C" alt="[公式]"></p>
<p>总的来说， <img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7DSNE" alt="[公式]"> 的梯度更新具有以下两个优势：</p>
<ul>
<li><strong>对于低维空间中不相似的数据点，用一个较小的距离会产生较大的梯度让这些数据点排斥开来</strong>；</li>
<li><strong>这种排斥又不会无限大，因此避免了不相似的数据点距离太远</strong>。</li>
</ul>
<h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p><img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7D+SNE" alt="[公式]"> 算法其实就是在 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 算法的基础上增加了两个改进：</p>
<ul>
<li>把 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 修正为对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> ，提高了计算效率，效果稍有提升；</li>
<li>在低维空间中采用了 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布替换原来的高斯分布，解决了高维空间映射到低维空间所产生的拥挤问题，优化了 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 过于关注局部特征而忽略全局特征的问题 。</li>
</ul>
<h2 id="四、AutoEncoder"><a href="#四、AutoEncoder" class="headerlink" title="四、AutoEncoder"></a>四、AutoEncoder</h2><blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80377698">【全】一文带你了解自编码器（<em>AutoEncoder</em>）</a></li>
</ul>
</blockquote>
<p>理解为：（下图）高维数据（左测蓝色）通过某种网络变成低位数据（中间红色）后，又经过某种网络变回高维数据（右侧蓝色）。数据经过该模型前后没有变化，而中间的低维数据完全具有输入输出的高维数据的全部信息，所以可以用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=低维数据&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;157482881&quot;}">低维数据</a>代表高维数据。</p>
<p>之所以叫AutoEncoder，而不叫AutoEncoderDecoder，是因为训练好之后只有encoder部分有用，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=decoder&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;157482881&quot;}">decoder</a>部分就不用了。</p>
<p><img src="https://pic2.zhimg.com/v2-47f6429e5ffb379205ba0bcb0db399d1_b.jpg" alt="img"></p>
<p>进入深度学习的思路之后，编码的网络是开放的，可以自由设计的。一个思路是端到端，将网络的输出设为你任务要的结果（如类别、序列等），<strong>过程中的某层嵌入都可以作为降维的低维结果</strong>。当然，这种低维结果其实是模型的副产品，因为任务已经解决。比如bert模型得到（中文的）字嵌入。</p>
<h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><ul>
<li>能够学习到非线性特性</li>
<li>降低数据维度</li>
</ul>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul>
<li>训练的<strong>计算成本高</strong></li>
<li><strong>可解释性较差</strong></li>
<li>背后的数学知识复杂</li>
<li>容易产生<strong>过度拟合</strong>的问题，尽管可以通过引入正则化策略缓解</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:58:04" itemprop="dateCreated datePublished" datetime="2022-03-15T22:58:04+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-18 15:57:02" itemprop="dateModified" datetime="2023-03-18T15:57:02+08:00">2023-03-18</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="机器学习优化方法-Optimization"><a href="#机器学习优化方法-Optimization" class="headerlink" title="机器学习优化方法 (Optimization)"></a>机器学习优化方法 (Optimization)</h2><blockquote>
<p>  机器学习与优化基础（Machine Learning and Optimization）:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/169835477">https://zhuanlan.zhihu.com/p/169835477</a></p>
<p>  最优化方法复习笔记：<a target="_blank" rel="noopener" href="https://github.com/LSTM-Kirigaya/OptimizeNote">https://github.com/LSTM-Kirigaya/OptimizeNote</a></p>
<p>  <strong>FreeWill</strong>：<a target="_blank" rel="noopener" href="https://plushunter.github.io/2017/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8825%EF%BC%89%EF%BC%9A%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/">机器学习算法系列（25）：最速下降法、牛顿法、拟牛顿法</a></p>
</blockquote>
<h3 id="一、什么是凸优化"><a href="#一、什么是凸优化" class="headerlink" title="一、什么是凸优化"></a>一、什么是凸优化</h3><p><strong>凸函数</strong>的严格定义为，函数L(·) 是凸函数当且仅当对定义域中的任意两点x，y和任意实数λ∈[0,1]总有：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d50ff64d3e458ed576babbb0c25c3f88b9f791b9f3df7435102dc77545df5147/687474703a2f2f7778342e73696e61696d672e636e2f6d773639302f30303633304465666c7931673565357769736474756a333064343031696a72612e6a7067"><img src="https://camo.githubusercontent.com/d50ff64d3e458ed576babbb0c25c3f88b9f791b9f3df7435102dc77545df5147/687474703a2f2f7778342e73696e61696d672e636e2f6d773639302f30303633304465666c7931673565357769736474756a333064343031696a72612e6a7067" alt="img"></a></p>
<p>该不等式的一个直观解释是，凸函数曲面上任意两点连接而成的线段，其上的任 意一点都不会处于该函数曲面的下方，如下图所示所示。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/4fbcceabdb43cc783d5adc0fb906cda38aeddd8790ce66b2d8e01b46b98cf467/687474703a2f2f7778342e73696e61696d672e636e2f6d773639302f30303633304465666c793167356366706d7336776f6a3330653130343977657a2e6a7067"><img src="https://camo.githubusercontent.com/4fbcceabdb43cc783d5adc0fb906cda38aeddd8790ce66b2d8e01b46b98cf467/687474703a2f2f7778342e73696e61696d672e636e2f6d773639302f30303633304465666c793167356366706d7336776f6a3330653130343977657a2e6a7067" alt="img"></a></p>
<p>凸优化问题的例子包括支持向量机、线性回归等 线性模型，非凸优化问题的例子包括低秩模型（如矩阵分解）、深度神经网络模型等。</p>
<h3 id="二、正则化项"><a href="#二、正则化项" class="headerlink" title="二、正则化项"></a>二、正则化项</h3><p>使用正则化项，也就是给loss function加上一个参数项，正则化项有<strong>L1正则化、L2正则化、ElasticNet</strong>。加入这个正则化项好处：</p>
<ul>
<li>控制参数幅度，不让模型“无法无天”。</li>
<li>限制参数搜索空间</li>
<li>解决欠拟合与过拟合的问题。</li>
</ul>
<p>详细请参考之前的文章：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine Learning/Liner Regression">线性回归—第5点</a></p>
<h3 id="三、常见的几种最优化方法"><a href="#三、常见的几种最优化方法" class="headerlink" title="==三、常见的几种最优化方法=="></a>==三、常见的几种最优化方法==</h3><ol>
<li><p><strong>梯度下降法</strong></p>
<p>梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当<strong>目标函数是凸函数时，梯度下降法的解是全局解</strong>。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的搜索迭代示意图如下图所示：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/bc08cfbd80c5299970c01cde376a8855269944fca897fb91eb9ff3d5076383b1/68747470733a2f2f696d61676573323031372e636e626c6f67732e636f6d2f626c6f672f313032323835362f3230313730392f313032323835362d32303137303931363230313933323733352d3234333634363139392e706e67"><img src="https://camo.githubusercontent.com/bc08cfbd80c5299970c01cde376a8855269944fca897fb91eb9ff3d5076383b1/68747470733a2f2f696d61676573323031372e636e626c6f67732e636f6d2f626c6f672f313032323835362f3230313730392f313032323835362d32303137303931363230313933323733352d3234333634363139392e706e67" alt="img"></a></p>
<p>缺点：靠近极小值时收敛速度减慢；直线搜索时可能会产生一些问题；可能会“之字形”地下降。</p>
</li>
<li><p><strong>牛顿法</strong></p>
<p>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。具体步骤：</p>
<ul>
<li><p>首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f ‘ (x0)（这里f ‘ 表示函数 f 的导数）。</p>
</li>
<li><p>然后我们计算穿过点(x0, f (x0)) 并且斜率为f ‘(x0)的直线和 x 轴的交点的x坐标，也就是求如下方程的解：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/03d83e51c9348519324fd1b60c8aa7e5497e5cbb592c43e93f7d11fbac453681/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f782a662535452537422725374428785f30292b6628785f30292d785f302a662535452537422725374428785f30293d30"><img src="https://camo.githubusercontent.com/03d83e51c9348519324fd1b60c8aa7e5497e5cbb592c43e93f7d11fbac453681/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f782a662535452537422725374428785f30292b6628785f30292d785f302a662535452537422725374428785f30293d30" alt="img"></a></p>
</li>
<li><p>我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f (x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。</p>
</li>
</ul>
<p>由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。牛顿法搜索动态示例图：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/7de2bde3d0eb6ad59fb428c06b904198c2cac6bd7e31f788f73b6bbbc4980709/68747470733a2f2f696d61676573323031372e636e626c6f67732e636f6d2f626c6f672f313032323835362f3230313730392f313032323835362d32303137303931363230323731393037382d313538383434363737352e676966"><img src="https://camo.githubusercontent.com/7de2bde3d0eb6ad59fb428c06b904198c2cac6bd7e31f788f73b6bbbc4980709/68747470733a2f2f696d61676573323031372e636e626c6f67732e636f6d2f626c6f672f313032323835362f3230313730392f313032323835362d32303137303931363230323731393037382d313538383434363737352e676966" alt="img"></a></p>
<p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。<strong>缺点：</strong></p>
<ul>
<li>牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</li>
<li>在高维情况下这个矩阵非常大，计算和存储都是问题。</li>
<li>在小批量的情况下，牛顿法对于二阶导数的估计噪声太大。</li>
<li>目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。</li>
</ul>
</li>
<li><p><strong>拟牛顿法</strong></p>
<p>拟牛顿法是求解非线性优化问题最有效的方法之一，<strong>本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</strong>拟牛顿法和梯度下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于梯度下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>
</li>
<li><p><strong>共轭梯度法</strong></p>
<p>共轭梯度法是介于梯度下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p>
<p>具体的实现步骤请参加wiki百科<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB">共轭梯度法</a>。下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/6b3d0a2120d546206dbf7ae0e114fe52d4c0fd4e4634c5c90520c2c63b3f6286/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673563683072327034386a3330387a30616c6d79342e6a7067"><img src="https://camo.githubusercontent.com/6b3d0a2120d546206dbf7ae0e114fe52d4c0fd4e4634c5c90520c2c63b3f6286/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673563683072327034386a3330387a30616c6d79342e6a7067" alt="img"></a></p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%887%EF%BC%89%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%887%EF%BC%89%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:43:32" itemprop="dateCreated datePublished" datetime="2022-03-15T22:43:32+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-29 16:20:32" itemprop="dateModified" datetime="2023-01-29T16:20:32+08:00">2023-01-29</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/dev/modules/naive_bayes.html#naive-bayes">https://scikit-learn.org/dev/modules/naive_bayes.html#naive-bayes</a></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://plushunter.github.io/2017/02/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">FREE WILL 机器学习算法系列（10）：朴素贝叶斯</a></p>
</li>
<li><h5 id="最大似然估计、最大后验估计、贝叶斯估计的对比"><a href="#最大似然估计、最大后验估计、贝叶斯估计的对比" class="headerlink" title="最大似然估计、最大后验估计、贝叶斯估计的对比"></a><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jiangxinyang/p/9378535.html">最大似然估计、最大后验估计、贝叶斯估计的对比</a></h5></li>
</ul>
<h3 id="一、朴素贝叶斯的学习与分类"><a href="#一、朴素贝叶斯的学习与分类" class="headerlink" title="一、朴素贝叶斯的学习与分类"></a>一、朴素贝叶斯的学习与分类</h3><blockquote>
<p>  朴素贝叶斯（Naive Bayes）是基于<strong>贝叶斯定理</strong>与<strong>特征条件假设</strong>的<strong>分类</strong>方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯是<strong>选出各个分类类别后验概率最大</strong>的作为最终分类。</p>
<ul>
<li><strong>优点</strong>：对小规模的数据表现很好，适合多分类任务，<strong>适合增量式训练</strong>。</li>
<li><strong>缺点</strong>：对输入数据的表达形式很敏感<strong>（离散、连续，值极大极小之类的）</strong>。</li>
</ul>
</blockquote>
<h4 id="1-1贝叶斯定理"><a href="#1-1贝叶斯定理" class="headerlink" title="1.1贝叶斯定理"></a>1.1贝叶斯定理</h4><p><strong>条件概率:</strong></p>
<p>$P(A|B)$  表示事件B已经发生的前提下，事件B已经发生的前提下，事件A发生的概率，叫做事件 $B$<br>发生下事件 $A$ 的条件概率。其基本求解公式为</p>
<p><img src="image-20220325172121116.png" alt="image-20220325172121116" style="zoom:50%;"></p>
<p><strong>贝叶斯定理</strong>便是基于<strong>条件概率</strong>，通过$P(A|B)$来求$P(B|A)$：【通过<strong>先验概率</strong>计算<strong>后验概率</strong>】</p>
<p><img src="image-20220325172351101.png" alt="image-20220325172351101" style="zoom:50%;"></p>
<p>顺便提一下，上式中的分母，可以根据<strong>全概率公式</strong>分解为：</p>
<p><img src="image-20220325172306652.png" alt="image-20220325172306652" style="zoom:50%;"></p>
<h4 id="1-2-特征条件独立假设"><a href="#1-2-特征条件独立假设" class="headerlink" title="1.2 特征条件独立假设"></a>1.2 特征条件独立假设</h4><p>这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件<strong>独立假设</strong>。给定训练数据集$(X,Y)$，其中每个样本$X$都包括 $n$ 维特征，即$x=(x1,x2,···,xn)$，类标记集合含有$K$种类别，即$y=(y1,y2,···,yk)$.</p>
<p>如果现在来了一个新样本 $x$ 我们要怎么判断它的类别?从概率的角度来看，这个问题就是给定$x$，它属于哪个类别的概率更大。那么问题就转化为求解 $P(y1|x),P(y2|x),P(yk|x)P(y1|x),P(y2|x),P(yk|x)$ 中最大的那个，即求<strong>后验概率最大</strong>的输出：==$arg max_{y_k}P(y_k|x)$==</p>
<p><img src="image-20220325203314162.png" alt="image-20220325203314162" style="zoom:50%;"></p>
<p>根据<strong>全概率公式</strong>，可以进一步分解上式中的分母：</p>
<p><img src="image-20220325203335825.png" alt="image-20220325203335825" style="zoom:50%;"></p>
<ul>
<li><p><img src="image-20220325205350669.png" alt="image-20220325205350669" style="zoom:50%;">：<strong>先验概率</strong> 【训练集计算】</p>
</li>
<li><p><img src="image-20220325205444531.png" alt="image-20220325205444531" style="zoom:50%;">：<strong>条件概率</strong>，它的参数规模是<strong>指数</strong>数量级别的。假设第i维特征xi可取值的个数有Si个，类别取值个数为k个，那么参数个数为$k∏S_j$。</p>
</li>
<li><p><strong>独立性的假设</strong>：通俗地讲就是说假设各个维度的特征互相独立，这样<strong>参数规模</strong>就降到了$∑S_ik$, 【积-&gt;和】</p>
<p><img src="image-20220325211846297.png" alt="image-20220325211846297" style="zoom:50%;"></p>
</li>
<li><p>代入公式1得出：</p>
<p><img src="image-20220325213643901.png" alt="image-20220325213643901" style="zoom:50%;"></p>
</li>
<li><p>于是朴素贝叶斯分类器可表示为：</p>
<p><img src="image-20220325213705861.png" alt="image-20220325213705861" style="zoom:50%;"></p>
</li>
<li><p>由于分母值都是一样的：<strong>==极大后验概率估计==</strong></p>
</li>
</ul>
<p><img src="image-20220325213802190.png" alt="image-20220325213802190" style="zoom:50%;"></p>
<h4 id="1-3-朴素贝叶斯法的参数估计【求解】"><a href="#1-3-朴素贝叶斯法的参数估计【求解】" class="headerlink" title="1.3 朴素贝叶斯法的参数估计【求解】"></a>1.3 朴素贝叶斯法的参数估计【求解】</h4><p>朴素贝叶斯要学习的东西就是：<img src="image-20220325215221520.png" alt="image-20220325215221520" style="zoom:50%;"> 和 <img src="image-20220325215238733.png" alt="image-20220325215238733" style="zoom:50%;">【极大似然函数 + 拉格朗日乘数法】</p>
<ul>
<li><p><strong>先验概率</strong>$P(Y=ck)$的极大似然估计是, <strong>样本在$c_k$出现的次数除以样本容量</strong>：</p>
<p><img src="image-20220325215849572.png" alt="image-20220325215849572" style="zoom:50%;"></p>
</li>
<li><p>$设第 j 个特征x(j)可能取值的集合为a_{j1},a_{j2},···,a_{jl}, 条件概率P(X_j=a_{jl} |Y=ck)的极大似然估计是$：</p>
<p><img src="image-20220325221320810.png" alt="image-20220325221320810" style="zoom:50%;"></p>
</li>
</ul>
<h4 id="1-4-贝叶斯估计【缺失值处理】【拉普拉斯平滑】"><a href="#1-4-贝叶斯估计【缺失值处理】【拉普拉斯平滑】" class="headerlink" title="1.4 贝叶斯估计【缺失值处理】【拉普拉斯平滑】"></a>1.4 贝叶斯估计【缺失值处理】【拉普拉斯平滑】</h4><p><strong>先验概率</strong>的贝叶斯估计：</p>
<p><img src="image-20220325222259483.png" alt="image-20220325222259483" style="zoom:50%;"></p>
<p><strong>条件概率</strong>的贝叶斯估计：【<strong>离散型</strong>】</p>
<p><img src="image-20220325222226643.png" alt="image-20220325222226643" style="zoom:50%;"></p>
<h4 id="1-5-朴素贝叶斯有什么优缺点？"><a href="#1-5-朴素贝叶斯有什么优缺点？" class="headerlink" title=" 1.5 朴素贝叶斯有什么优缺点？"></a><strong><font color="red"> 1.5 朴素贝叶斯有什么优缺点？</font></strong></h4><h5 id="优点：【数学理论、缺失异常不敏感、快、增量式训练】"><a href="#优点：【数学理论、缺失异常不敏感、快、增量式训练】" class="headerlink" title="优点：【数学理论、缺失异常不敏感、快、增量式训练】"></a>优点：【数学理论、缺失异常不敏感、快、增量式训练】</h5><ul>
<li>朴素贝叶斯模型<strong>发源于古典数学理论</strong>，有稳定的分类效率。</li>
<li><strong>对缺失数据和异常数据不太敏感</strong>，算法也比较简单，常用于文本分类。</li>
<li><strong>分类准确度高，速度快</strong>。</li>
<li><strong>对小规模的数据表现很好，能处理多分类任务，适合增量式训练，当数据量超出内存时，我们可以一批批的去增量训练</strong>(朴素贝叶斯在训练过程中只需要计算各个类的概率和各个属性的类条件概率，这些概率值可以快速地根据增量数据进行更新，无需重新全量计算)。</li>
</ul>
<h5 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h5><ul>
<li><strong>对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）</strong>。</li>
<li><strong>对训练数据的依赖性很强</strong>，如果训练数据误差较大，那么预测出来的效果就会不佳。</li>
<li>理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。 但是在实际中，因为朴素贝叶斯“朴素，”的特点，<strong>导致在属性个数比较多或者属性之间相关性较大时，分类效果不好。</strong>而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。</li>
<li>需要知道<strong>先验概率</strong>，且先验概率很多时候是基于假设或者已有的训练数据所得的，这在某些时候可能会因为假设先验概率的原因出现分类决策上的错误。</li>
</ul>
<h2 id="二、高斯贝叶斯模型"><a href="#二、高斯贝叶斯模型" class="headerlink" title="二、高斯贝叶斯模型"></a>二、高斯贝叶斯模型</h2><blockquote>
<p>  classifier = naive_bayes.MultinomialNB()</p>
</blockquote>
<h4 id="2-1-朴素贝叶斯-连续型数据处理"><a href="#2-1-朴素贝叶斯-连续型数据处理" class="headerlink" title="2.1 朴素贝叶斯(连续型数据处理)"></a>2.1 朴素贝叶斯(连续型数据处理)</h4><ul>
<li>每一个连续的<strong>数据离散化</strong>，然后用相应的离散区间替换连续数值。这种方法对于划分离散区间的粒度要求较高，不能太细，也不能太粗。</li>
<li>假设<strong>连续数据服从某个概率分布</strong>，<strong>使用训练数据估计分布参数</strong>，通常我们用<strong>高斯分布</strong>来表示<strong>连续数据的类条件概率分布</strong>。</li>
</ul>
<p><strong>==GaussianNB 的条件概率密度计算：其中均值和方差可以通过极大似然估计得出。==</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
&p\left(X^{(j)}=a_{j l} \mid y=c_{k}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{j k}} e^{-\frac{\left(a_{j l}-\mu_{j k}\right)^{2}}{2 \sigma_{j k}^{2}}}
\end{aligned}</script><h2 id="三、贝叶斯网络"><a href="#三、贝叶斯网络" class="headerlink" title="三、贝叶斯网络"></a>三、贝叶斯网络</h2><h4 id="3-1-概率图模型"><a href="#3-1-概率图模型" class="headerlink" title="3.1 概率图模型"></a>3.1 概率图模型</h4><p>概率图模型分为<strong>贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）</strong>两大类。贝叶斯网络可以用一个有向图结构表示，马尔可夫网络可以表示成一个无向图的网络结构。更详细地说，<strong>概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型</strong>等，在机器学习的诸多场景中都有着广泛的应用。</p>
<ul>
<li><strong>贝叶斯网络</strong> — 结点与结点之间是以有向箭头相连接，代表是这个结点会影响下一个结点</li>
<li><strong>马尔可夫网络</strong> — 结点与结点之间是以无向箭头相连接，代表是结点与结点之间会相互影响</li>
</ul>
<h2 id="四、最大似然估计、最大后验估计、贝叶斯估计的对比"><a href="#四、最大似然估计、最大后验估计、贝叶斯估计的对比" class="headerlink" title="四、最大似然估计、最大后验估计、贝叶斯估计的对比"></a>四、最大似然估计、最大后验估计、贝叶斯估计的对比</h2><h4 id="4-1-贝叶斯公式"><a href="#4-1-贝叶斯公式" class="headerlink" title="4.1 贝叶斯公式"></a>4.1 <strong>贝叶斯公式</strong></h4><p>这三种方法都和贝叶斯公式有关，所以我们先来了解下贝叶斯公式：</p>
<script type="math/tex; mode=display">
p(\theta \mid X)=\frac{p(X \mid \theta) p(\theta)}{p(X)}</script><p>每一项的表示如下:</p>
<script type="math/tex; mode=display">
\text { posterior }=\frac{\text { likehood } * \text { prior }}{\text { evidence }}</script><ul>
<li>posterior: 通过样本X得到参数 $\theta$ 的概率, 也就是后验概率。</li>
<li>likehood: 通过参数 $\theta$ 得到样本X的概率, 似然函数, 通常就是我们的数据集的表现。</li>
<li>prior: 参数 $\theta$ 的先验概率, 一般是根据人的先验知识来得出的。</li>
</ul>
<h4 id="4-2-极大似然估计-MLE"><a href="#4-2-极大似然估计-MLE" class="headerlink" title="4.2 极大似然估计 (MLE)"></a>4.2 极大似然估计 (MLE)</h4><p>极大似然估计的核心思想是: 认为当前发生的事件是概率最大的事件。<strong>因此就可以给定的数据集, 使得该数据集发生的概率最大来求得模型中的参数</strong>。似然函数如下:</p>
<script type="math/tex; mode=display">
p(X \mid \theta)=\prod_{x 1}^{x n} p(x i \mid \theta)</script><p>为了便于计算, 我们对似然函数两边取对数, 生成新的对数似然函数（因为对数函数是单调增函数, 因此求似然函数最大化就可 以转换成对数似然函数最大化）：</p>
<script type="math/tex; mode=display">
p(X \mid \theta)=\prod_{x 1}^{x n} p(x i \mid \theta)=\sum_{x 1}^{x n} \log p(x i \mid \theta)</script><p>求对数似然函数最大化, 可以通过导数为 0 来求解。<strong><font color="red"> 极大似然估计只关注当前的样本, 也就是只关注当前发生的事情, 不考虑事情的先验情况</font></strong>。由于计算简单, 而且不需要关注先验 知识, 因此在机器学习中的应用非常广, 最常见的就是逻辑回归。</p>
<h4 id="4-3-最大后验估计-MAP"><a href="#4-3-最大后验估计-MAP" class="headerlink" title="4.3 最大后验估计 (MAP)"></a>4.3 最大后验估计 (MAP)</h4><p>和最大似然估计不同的是, 最大后验估计中引入了<strong>先验概率</strong>（先验分布属于贝叶斯学派引入的, 像L1, L2正则化就是对参数引入 了拉普拉斯先验分布和高斯先验分布）, 而且最大后验估计要求的是 $p(\theta \mid X)$<br>最大后验估计可以写成下面的形式:</p>
<script type="math/tex; mode=display">
\operatorname{argmaxp}(\theta \mid X)=\operatorname{argmax} \frac{p(X \mid \theta) p(\theta)}{p(X)}=\operatorname{argmaxp}(X \mid \theta) p(\theta)=\operatorname{argmax}\left(\prod_{x 1}^{x n} p(x i \mid \theta)\right) p(\theta)</script><p>在求最大后验概率时, 可以忽略分母 $p(x)$, 因为该值不影响对 $\theta$ 的估计。同样为了便于计算, 对两边取对数, 后验概率最大化就变成了:</p>
<script type="math/tex; mode=display">
\operatorname{argmax}\left(\sum_{x 1}^{x n} \operatorname{logp}(x i \mid \theta)+\log p(\theta)\right)</script><p><strong><font color="red"> 最大后验估计不只是关注当前的样本的情况，还关注已经发生过的先验知识。在朴素贝叶斯中会有最大后验概率的应用，但并没有用上最大后验估计来求参数（因为朴素贝叶斯中的θ其实就是分类的类别）。</font></strong></p>
<p><strong>最大后验估计和最大似然估计的区别：</strong>最大后验估计允许我们把先验知识加入到估计模型中，<strong>这在样本很少的时候是很有用的（因此朴素贝叶斯在较少的样本下就能有很好的表现）</strong>，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如beta分布的α，β，我们还可以调节把估计的结果“拉”向先验的幅度，α，β越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。</p>
<h2 id="朴素贝叶斯-Q-amp-A"><a href="#朴素贝叶斯-Q-amp-A" class="headerlink" title="朴素贝叶斯 Q&amp;A"></a>朴素贝叶斯 Q&amp;A</h2><blockquote>
<ul>
<li>朴素贝叶斯分类器原理以及公式，出现估计概率值为 0 怎么处理（拉普拉斯平滑），缺点；</li>
<li>解释贝叶斯公式和朴素贝叶斯分类。</li>
<li>贝叶斯分类，这是一类分类方法，主要代表是朴素贝叶斯，朴素贝叶斯的原理，重点在假设各个属性类条件独立。然后能根据贝叶斯公式具体推导。考察给你一个问题，如何利用朴素贝叶斯分类去分类，比如：给你一个人的特征，判断是男是女，比如身高，体重，头发长度等特征的的数据，那么你要能推到这个过程。给出最后的分类器公式。</li>
<li>那你说说贝叶斯怎么分类啊？<strong>比如说看看今天天气怎么样？</strong>我：blabla，，，利用天气的历史数据，可以知道天气类型的先验分布，以及每种类型下特征数据（比如天气数据的特征：温度啊，湿度啊）的条件分布，这样我们根据贝叶斯公式就能求得天气类型的后验分布了。。。。面试官：en（估计也比较满意吧）<strong>那你了解关于求解模型的优化方法吗？一般用什么优化方法来解？</strong></li>
<li>贝叶斯分类器的优化和特殊情况的处理</li>
</ul>
</blockquote>
<h3 id="1、朴素贝叶斯、SVM和LR的区别？"><a href="#1、朴素贝叶斯、SVM和LR的区别？" class="headerlink" title=" 1、朴素贝叶斯、SVM和LR的区别？"></a><strong><font color="red"> 1、朴素贝叶斯、SVM和LR的区别？</font></strong></h3><p><strong>朴素贝叶斯是生成模型</strong>，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解P(Y|X)。</p>
<p><strong>LR是判别模型</strong>，根据极大化对数似然函数直接求出条件概率P(Y|X)；朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求；<strong>朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>SVM</th>
<th>LR</th>
<th>朴素贝叶斯</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong></td>
<td><strong>想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面</strong>。</td>
<td>使用线性回归模型的预测值逼近分类任务真实标记的对数几率。</td>
<td>基于<strong>贝叶斯定理</strong>与<strong>特征条件假设</strong>的<strong>分类</strong>方法。选出各个分类类别后验概率最大的作为最终分类。</td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td>判别模型、<strong>非概率方法</strong>；</td>
<td><strong>概率方法</strong>；需要对$p(y</td>
<td>x)$进行假设，具有概率意义。</td>
<td>生成模型</td>
</tr>
<tr>
<td><strong>经验损失函数</strong></td>
<td><strong>合页损失函数</strong>；有一段平的零区域、使得SVM的对偶性有稀疏性。</td>
<td><strong>交叉熵损失函数</strong></td>
<td><strong>后验概率最大</strong></td>
</tr>
<tr>
<td><strong>训练样本</strong></td>
<td><strong>支持向量</strong>（少数样本），SVM的参数和假设函数只和支持向量有关。</td>
<td>全样本</td>
<td>全样本</td>
</tr>
<tr>
<td><strong>优化方法</strong></td>
<td>次梯度下降和坐标梯度下降 【<strong>SMO算法</strong>】</td>
<td><strong>梯度下降</strong></td>
<td>无</td>
</tr>
<tr>
<td>多分类</td>
<td><strong>多分类SVM</strong></td>
<td><strong>Softmax回归</strong></td>
<td>后验概率最大</td>
</tr>
<tr>
<td><strong>敏感程度</strong></td>
<td><strong>SVM考虑分类边界线附近的样本</strong>（决定分类超平面的样本）。在支持向量外添加或减少任何样本点对分类决策面没有任何影响；【不敏感】</td>
<td><strong>LR受所有数据点的影响</strong>。直接依赖数据分布，每个样本点都会影响决策面的结果。如果训练数据不同类别严重不平衡。【敏感】</td>
<td><strong>特征值是基于频数进行统计的。</strong>一个值的异常（变成了别的数），<strong>只是贝叶斯公式里的计算概率的分子或者分母发生微小的变化，整体结果影响不大</strong>，不敏感【概率排序】</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2、朴素贝叶斯“朴素”在哪里？"><a href="#2、朴素贝叶斯“朴素”在哪里？" class="headerlink" title="2、朴素贝叶斯“朴素”在哪里？"></a>2、<strong>朴素贝叶斯“朴素”在哪里？</strong></h3><p>简单来说：它假定<strong>所有的特征在数据集中的作用是同样重要和独立的</strong>，正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。</p>
<p>利用贝叶斯定理求解联合概率P(XY)时，需要计算条件概率P(X|Y)。在计算P(X|Y)时，朴素贝叶斯做了一个很强的条件独立假设（当Y确定时，X的各个分量取值之间相互独立），即P(X1=x1,X2=x2,…Xj=xj|Y=yk) = P(X1=x1|Y=yk)P(X2=x2|Y=yk)…*P(Xj=xj|Y=yk)。 多个特征全是独立的，需要分别相乘。</p>
<h3 id="3、在估计条件概率P-X-Y-时出现概率为0的情况怎么办？"><a href="#3、在估计条件概率P-X-Y-时出现概率为0的情况怎么办？" class="headerlink" title="3、在估计条件概率P(X|Y)时出现概率为0的情况怎么办？"></a>3、<strong>在估计条件概率P(X|Y)时出现概率为0的情况怎么办？</strong></h3><p><strong>拉普拉斯平滑法</strong>是朴素贝叶斯中处理零概率问题的一种修正方式。在进行分类的时候，可能会出现某个属性在训练集中没有与某个类同时出现过的情况，如果直接基于朴素贝叶斯分类器的表达式进行计算的话就会出现<strong>零概率现象</strong>。</p>
<p>为了避免其他属性所携带的信息被训练集中未出现过的属性值“抹去”，所以才使用拉普拉斯估计器进行修正。具体的方法是：<strong>在分子上加1,对于先验概率，在分母上加上训练集中label的类别数；对于特征i 在label下的条件概率，则在分母上加上第i个属性可能的取值数（特征 i 的unique()）</strong></p>
<p><strong>先验概率</strong>的贝叶斯估计：</p>
<p><img src="image-20230129161943432.png" alt="image-20230129161943432" style="zoom:50%;"></p>
<p><strong>条件概率</strong>的贝叶斯估计：【<strong>离散型</strong>】</p>
<p><img src="image-20230129162002522.png" alt="image-20230129162002522" style="zoom:50%;"></p>
<h3 id="4、先验概率和后验概率都是？"><a href="#4、先验概率和后验概率都是？" class="headerlink" title="4、先验概率和后验概率都是？"></a>4、<strong>先验概率和后验概率都是？</strong></h3><p><strong>先验概率是指根据以往经验和分析得到的概率</strong>,如全概率公式,它往往作为”由因求果”问题中的”因”出现.<strong>后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</strong></p>
<p><strong>先验概率和后验概率是相对的。</strong>如果以后还有新的信息引入，更新了现在所谓的后验概率，得到了新的概率值，那么这个新的概率值被称为后验概率。</p>
<h3 id="5、朴素贝叶斯算法的前提假设是什么？"><a href="#5、朴素贝叶斯算法的前提假设是什么？" class="headerlink" title="5、朴素贝叶斯算法的前提假设是什么？"></a>5、<strong>朴素贝叶斯算法的前提假设是什么？</strong></h3><ol>
<li>特征之间相互独立</li>
<li>每个特征同等重要</li>
</ol>
<h3 id="6、面试的时候怎么标准回答朴素贝叶斯呢？"><a href="#6、面试的时候怎么标准回答朴素贝叶斯呢？" class="headerlink" title="6、面试的时候怎么标准回答朴素贝叶斯呢？"></a>6、<strong>面试的时候怎么标准回答朴素贝叶斯呢？</strong></h3><p>首先朴素贝斯是一个<strong>生成模型（很重要）</strong>，其次它通过学习已知样本，计算出联合概率，再求条件概率。</p>
<h4 id="生成模式和判别模式的区别-常见-："><a href="#生成模式和判别模式的区别-常见-：" class="headerlink" title="生成模式和判别模式的区别(常见)："></a><strong>生成模式和判别模式的区别(常见)：</strong></h4><p><strong>生成模式</strong>：由数据学得<strong>联合概率分布，求出条件概率分布P(Y|X)的预测模型</strong>；<strong>比较在乎数据是怎么生成的</strong>；常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机。</p>
<p><strong>判别模式</strong>：由数据学得<strong>决策函数或条件概率分布作为预测模型</strong>，<strong>要关注在数据的差异分布上，而不是生成</strong>；常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场。</p>
<h3 id="7、为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果-【排序能力】"><a href="#7、为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果-【排序能力】" class="headerlink" title="7、为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?【排序能力】"></a>7、<strong>为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?</strong>【排序能力】</h3><p>首先独立性假设在实际中不存在，确实会导致朴素贝叶斯不如一些其他算法，但是就算法本身而言，朴素贝叶斯也会有不错的分类效果，原因是：</p>
<ul>
<li><strong>分类问题看中的是类别的条件概率的排序</strong>，而不是具体的概率值，所以这里面对精准概率值的计算是有一定的容错的。</li>
<li>如果特征属性之间的依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。</li>
</ul>
<h3 id="8、朴素贝叶斯中概率计算的下溢问题如何解决？"><a href="#8、朴素贝叶斯中概率计算的下溢问题如何解决？" class="headerlink" title=" 8、朴素贝叶斯中概率计算的下溢问题如何解决？"></a><strong><font color="red"> 8、朴素贝叶斯中概率计算的下溢问题如何解决？</font></strong></h3><p><strong>在朴素贝叶斯的计算过程中，需要对特定分类中各个特征出现的概率进行连乘</strong>，小数相乘，越乘越小，这样就造成下溢出。在程序中，在相应小数位置进行四舍五入，计算结果可能就变成0了。</p>
<p>为了解决这个问题，<strong>对乘积结果取自然对数</strong>。将小数的乘法操作转化为取对数后的加法操作，规避了变为0的风险同时并不影响分类结果。</p>
<h3 id="9、朴素贝叶斯分类器对异常值和缺失值敏感吗？"><a href="#9、朴素贝叶斯分类器对异常值和缺失值敏感吗？" class="headerlink" title="9、朴素贝叶斯分类器对异常值和缺失值敏感吗？"></a>9、<strong>朴素贝叶斯分类器对异常值和缺失值敏感吗？</strong></h3><p>回想朴素贝叶斯的计算过程，它在推理的时候，输入的某个特征组合，<strong>他们的特征值在训练的时候在贝叶斯公式中都是基于频数进行统计的。</strong>所以一个值的异常（变成了别的数），<strong>只是贝叶斯公式里的计算概率的分子或者分母发生微小的变化，整体结果影响不大</strong>，就算微微影响最终概率值的获得，由于<strong>分类问题只关注概率的排序而不关注概率的值，所以影响不大</strong>，保留异常值还可以提高模型的泛化性能。</p>
<p>缺失值也是一样，如果一个数据实例缺失了一个属性的数值，在建模的时将被忽略，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。</p>
<h3 id="10、朴素贝叶斯中有没有超参数可以调？"><a href="#10、朴素贝叶斯中有没有超参数可以调？" class="headerlink" title="10、朴素贝叶斯中有没有超参数可以调？"></a>10、<strong>朴素贝叶斯中有没有超参数可以调？</strong></h3><p><strong>朴素贝叶斯是没有超参数可以调的，所以它不需要调参</strong>，朴素贝叶斯是根据训练集进行分类，分类出来的结果基本上就是确定了的，拉普拉斯估计器不是朴素贝叶斯中的参数，不能通过拉普拉斯估计器来对朴素贝叶斯调参。</p>
<h3 id="11、朴素贝叶斯有哪三个模型？"><a href="#11、朴素贝叶斯有哪三个模型？" class="headerlink" title="11、朴素贝叶斯有哪三个模型？"></a>11、<strong>朴素贝叶斯有哪三个模型？</strong></h3><ul>
<li><strong>多项式模型对应于离散变量</strong>，其中离散变量指的是category型变量，也就是类别变量，比如性别；连续变量一般是数字型变量，比如年龄，身高，体重。</li>
<li><strong>高斯模型 对应于连续变量</strong>（每一维服从正态分布）</li>
<li><strong>伯努利模型</strong> <strong>对应于文本分类</strong> （特征只能是0或者1）</li>
</ul>
<h3 id="12、朴素贝叶斯为什么适合增量计算？"><a href="#12、朴素贝叶斯为什么适合增量计算？" class="headerlink" title=" 12、朴素贝叶斯为什么适合增量计算？"></a><strong><font color="red"> 12、朴素贝叶斯为什么适合增量计算？</font></strong></h3><p>朴素贝叶斯在训练过程中实际上需要<strong>计算出各个类别的概率和各个特征的条件概率</strong>，这些概率以频数统计比值（对于多项式模型而言）的形式产生概率值，<strong>可以快速根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算。</strong></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89SVM*/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89SVM*/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:38:05" itemprop="dateCreated datePublished" datetime="2022-03-15T22:38:05+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-08 17:20:17" itemprop="dateModified" datetime="2023-03-08T17:20:17+08:00">2023-03-08</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>26 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机 SVM"></a>支持向量机 SVM</h2><p><strong>以几何的角度，在丰富的数据理论的基础上，简化了通常的分类和回归问题。</strong></p>
<h2 id><a href="#" class="headerlink" title></a><img src="https://pic1.zhimg.com/v2-e833772fe2044ad9c353fb0173bd0b79_1440w.jpg?source=172ae18b" alt="【机器学习】支持向量机 SVM（非常详细）" style="zoom:51%;"></h2><p>SVM 是一个非常优雅的算法，具有完善的数学理论，虽然如今工业界用到的不多，但还是决定花点时间去写篇文章整理一下。</p>
<h2 id="1-支持向量"><a href="#1-支持向量" class="headerlink" title="1. 支持向量"></a>1. 支持向量</h2><blockquote>
<p>  <strong>本质：SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</strong>为了对数据中的噪声有一定的容忍能力。</p>
<p>  <strong>几何意义</strong>：找到一个超平面将特征空间的正负样本分开，最大分隔（对噪音有一定的容忍能力）</p>
<p>  <strong>间隔表示</strong>：划分超平面到属于不同标记的最近样本的距离之和</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52168498">https://zhuanlan.zhihu.com/p/52168498</a></p>
<p><strong>KKT条件</strong>：<strong>判断不等式约束问题是否为最优解的必要条件</strong></p>
</li>
</ul>
</blockquote>
<h3 id="1-1-线性可分"><a href="#1-1-线性可分" class="headerlink" title="1.1 线性可分"></a>1.1 线性可分</h3><p>首先我们先来了解下什么是线性可分。</p>
<p><img src="https://pic4.zhimg.com/80/v2-a75409cca671ad0819cd28ff9f40a01b_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>在二维空间上，两类点被一条直线完全分开叫做线性可分。</p>
<p>严格的数学定义是：</p>
<p><img src="https://www.zhihu.com/equation?tex=D_0" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 是 n 维欧氏空间中的两个点集。如果存在 n 维向量 w 和实数 b，使得所有属于 <img src="https://www.zhihu.com/equation?tex=D_0" alt="[公式]"> 的点 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 都有 <img src="https://www.zhihu.com/equation?tex=wx_i+%2B+b+%3E+0" alt="[公式]"> ，而对于所有属于 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 的点 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 则有 <img src="https://www.zhihu.com/equation?tex=wx_j+%2B+b+%3C+0" alt="[公式]"> ，则我们称 <img src="https://www.zhihu.com/equation?tex=D_0" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 线性可分。</p>
<h3 id="1-2-最大间隔超平面"><a href="#1-2-最大间隔超平面" class="headerlink" title="1.2 最大间隔超平面"></a>1.2 最大间隔超平面</h3><p>从二维扩展到多维空间中时，将 <img src="https://www.zhihu.com/equation?tex=D_0" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 完全正确地划分开的 <img src="https://www.zhihu.com/equation?tex=wx%2Bb%3D0" alt="[公式]"> 就成了一个超平面。</p>
<p>为了使这个超平面更具鲁棒性，我们会去找最佳超平面，以最大间隔把两类样本分开的超平面，也称之为最大间隔超平面。</p>
<ul>
<li>两类样本分别分割在该超平面的两侧；</li>
<li><strong>两侧距离超平面最近的样本点到超平面的距离被最大化了。</strong>【附近点】</li>
</ul>
<h3 id="1-3-支持向量-【距离超平面最近的点】"><a href="#1-3-支持向量-【距离超平面最近的点】" class="headerlink" title="1.3 支持向量 【距离超平面最近的点】"></a>1.3 支持向量 【距离超平面最近的点】</h3><p><img src="https://pic4.zhimg.com/80/v2-0f1ccaf844905148b7e75cab0d0ee2e3_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>样本中距离超平面最近的一些点，这些点叫做支持向量。</p>
<h3 id="1-4-SVM-最优化问题"><a href="#1-4-SVM-最优化问题" class="headerlink" title="1.4 SVM 最优化问题"></a>1.4 SVM 最优化问题</h3><p><strong>==SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面==</strong>。任意超平面可以用下面这个线性方程来描述：</p>
<p>​                                 <img src="https://www.zhihu.com/equation?tex=w%5ETx%2Bb%3D0+%5C%5C" alt="[公式]"></p>
<p>二维空间点 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="[公式]"> 到直线 <img src="https://www.zhihu.com/equation?tex=Ax%2BBy%2BC%3D0" alt="[公式]"> 的距离公式是：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%7CAx%2BBy%2BC%7C%7D%7B%5Csqrt%7BA%5E2%2BB%5E2%7D%7D+%5C%5C" alt="[公式]"></p>
<p><strong>扩展到 n 维空间后，点 <img src="https://www.zhihu.com/equation?tex=x%3D%28x_1%2Cx_2%E2%80%A6x_n%29" alt="[公式]"> 到直线 <img src="https://www.zhihu.com/equation?tex=w%5ETx%2Bb%3D0" alt="[公式]"> 的距离为</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%7Cw%5ETx%2Bb%7C%7D%7B%7C%7Cw%7C%7C%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C%3D%5Csqrt%7Bw_1%5E2%2B%E2%80%A6w_n%5E2%7D" alt="[公式]"> 。</p>
<p>如图所示，根据支持向量的定义我们知道，支持向量到超平面的距离为 d，其他点到超平面的距离大于 d。</p>
<p><img src="https://pic4.zhimg.com/80/v2-1510522df255bd2987bf9ce8541f45af_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>于是我们有这样的一个公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+%5Cfrac%7Bw%5ETx%2Bb%7D%7B%7C%7Cw%7C%7C%7D+%26%5Cgeq+d+%5Cquad++y%3D1+%5C%5C+%5Cfrac%7Bw%5ETx%2Bb%7D%7B%7C%7Cw%7C%7C%7D+%26%5Cleq+-d++%5Cquad+y%3D-1++%5Cend%7Baligned%7D+%5Cright.+%5C%5C" alt="[公式]"></p>
<p>将两个方程合并，我们可以简写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=y%28w%5ETx%2Bb%29+%5Cgeq+1+%5C%5C" alt="[公式]"></p>
<p>至此我们就可以得到最大间隔超平面的上下两个超平面：</p>
<p><img src="image-20220409203050568.png" alt="image-20220409203050568" style="zoom:50%;"></p>
<p><strong>间隔</strong>：<strong>训练集中离划分超平面最近的样本到划分超平面距离的两倍</strong>。有了间隔的定义，划分超平面“离正负样本都比较远”这一目标可以等价描述为正负样本里划分超平面的距离尽可能远。即<strong>让离划分超平面最近的样本到划分超平面距离尽可能远</strong>。==优化目标==：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max _{\boldsymbol{w}, b} \gamma &=\max _{\boldsymbol{w}, b}\left(2 \min _{i} \frac{1}{\|\boldsymbol{w}\|}\left|\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right|\right) \\
&=\max _{\boldsymbol{w}, b} \min _{i} \frac{2}{\|\boldsymbol{w}\|}\left|\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right|
\end{aligned}</script><p><strong>简化过程</strong>：</p>
<ul>
<li><p><strong>缩放</strong>：为了简化优化问题, 我们可以通过调整 $(\boldsymbol{w}, b)$ 使得：</p>
<script type="math/tex; mode=display">
\min _{i}\left|\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right|=1 .</script></li>
<li><p><strong>标签替换绝对值</strong>：</p>
<script type="math/tex; mode=display">
s.t. \min _{i} y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right)=1</script></li>
<li><p><strong>简化约束条件</strong>【<strong>反正法</strong>】</p>
</li>
</ul>
<h4 id="硬间隔线性SVM的基本型："><a href="#硬间隔线性SVM的基本型：" class="headerlink" title=" 硬间隔线性SVM的基本型："></a><strong><font color="red"> 硬间隔线性SVM的基本型：</font></strong></h4><script type="math/tex; mode=display">
\begin{array}{ll}
\min _{\boldsymbol{w}, b} & \frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w} \\
\text { s.t. } & y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m
\end{array}</script><p><img src="image-20220423163130805.png" alt="image-20220423163130805" style="zoom:50%;"></p>
<p><strong>二次规划是指目标函数是==二次函数==，约束是==线性不等式约束==的一类优化问题</strong> + 凸函数</p>
<h2 id="2-硬间隔线性SVM对偶型"><a href="#2-硬间隔线性SVM对偶型" class="headerlink" title="2. 硬间隔线性SVM对偶型"></a>2. 硬间隔线性SVM对偶型</h2><blockquote>
<p>  本科高等数学学的<strong>拉格朗日程数法</strong>是<strong>等式约束优化问题</strong>：</p>
<p>  <img src="https://www.zhihu.com/equation?tex=%5Cmin+f%28x_%7B1%7D+%2Cx_%7B2%7D+%2C...%2Cx_%7Bn%7D+%29+%5C%5C+s.t.+%5Cquad+h_%7Bk%7D+%28x_%7B1%7D+%2Cx_%7B2%7D+%2C...%2Cx_%7Bn%7D+%29%3D0+%5Cquad+k+%3D1%2C2%2C...%2Cl%5C%5C" alt="[公式]"></p>
<p>  我们令 <img src="https://www.zhihu.com/equation?tex=L%28x%2C%5Clambda+%29+%3D+f%28x%29+%2B+%5Csum%5Climits_%7Bk+%3D+1%7D%5El+%5Clambda+_k+h_k+%28x%29" alt="[公式]"> ，函数 <img src="https://www.zhihu.com/equation?tex=L%28x%2Cy%29" alt="[公式]"> 称为 Lagrange 函数，参数 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> 称为 Lagrange 乘子<strong>==没有非负要求。==</strong></p>
<p>  利用必要条件找到可能的极值点：</p>
<p>  <img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+x_i%7D+%3D+0+%5Cquad+i%3D1%2C2%2C...%2Cn+%5C%5C+%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+%5Clambda_k%7D+%3D+0+%5Cquad+k%3D1%2C2%2C...%2Cl++%5Cend%7Baligned%7D+%5Cright.+%5C%5C" alt="[公式]"></p>
<p>  具体是否为极值点需根据问题本身的具体情况检验。这个方程组称为等式约束的极值必要条件。</p>
<p>  等式约束下的 Lagrange 乘数法引入了 <img src="https://www.zhihu.com/equation?tex=l" alt="[公式]"> 个 Lagrange 乘子，我们将 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bk%7D" alt="[公式]"> 一视同仁，把 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bk%7D+" alt="[公式]"> 也看作优化变量，共有 <img src="https://www.zhihu.com/equation?tex=%28n%2Bl%29" alt="[公式]"> 个优化变量。</p>
</blockquote>
<h5 id="（1）写成约束优化问题的基本型"><a href="#（1）写成约束优化问题的基本型" class="headerlink" title="（1）写成约束优化问题的基本型"></a>（1）写成约束优化问题的基本型</h5><script type="math/tex; mode=display">
\begin{array}{ll}
\min _{\boldsymbol{w}, b} & \frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w} \\
\text { s.t. } & 1-y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right) \leq 0, \quad i=1,2, \ldots, m
\end{array}</script><h5 id="（2）-构建基本型的拉格朗日函数"><a href="#（2）-构建基本型的拉格朗日函数" class="headerlink" title="（2） 构建基本型的拉格朗日函数"></a>（2） 构建基本型的拉格朗日函数</h5><script type="math/tex; mode=display">
\mathcal{L}(\boldsymbol{w}, b, \boldsymbol{\alpha}):=\frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right)\right)</script><h5 id="（3）交换min-max顺序"><a href="#（3）交换min-max顺序" class="headerlink" title="（3）交换min, max顺序"></a>（3）交换min, max顺序</h5><blockquote>
<p>  ==<strong>解得最优解 $\boldsymbol{u}^{\star}$ 。这样两层优化问题将变为一层最大化（max）问题, 问题难度大大降低, 称为对偶问题 (Dual Problem) :</strong>==【<strong>对偶问题是原问题的下界</strong>】</p>
<ul>
<li><script type="math/tex; mode=display">
\max _{\boldsymbol{\alpha}, \boldsymbol{\beta}} \min _{\boldsymbol{u}} \mathcal{L}(\boldsymbol{u}, \boldsymbol{\alpha}, \boldsymbol{\beta}) \leq \min _{\boldsymbol{u}} \max _{\boldsymbol{\alpha}, \boldsymbol{\beta}} \mathcal{L}(\boldsymbol{u}, \boldsymbol{\alpha}, \boldsymbol{\beta})</script></li>
<li><p>硬间隔线性SVM满足<strong>Slater条件</strong>， <strong>因此原问题和对偶问题等价</strong></p>
</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\begin{array}{cl}
\max _{\boldsymbol{\alpha}} \min _{\boldsymbol{w}, b} & \frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right)\right) \\
\text { s.t. } & \alpha_{i} \geq 0, \quad i=1,2, \ldots, m .
\end{array}</script><p>首先计算 $\boldsymbol{w}$ 的最优值, 令 $\frac{\partial \mathcal{L}}{\partial \boldsymbol{w}}=\mathbf{0}$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{w}} &=\frac{\partial}{\partial \boldsymbol{w}}\left(\frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right)\right)\right) \\
&=\boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}\left(-y_{i} \boldsymbol{x}_{i}\right) \\
&=\boldsymbol{w}-\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \\
&=\mathbf{0}
\end{aligned}</script><p>==可以解得最优值==$\boldsymbol{w}$</p>
<script type="math/tex; mode=display">
\boldsymbol{w}^{\star}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}</script><p>然后计算 $b$ 的最优值, 令 $\frac{\partial \mathcal{L}}{\partial b}=0$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial b} &=\frac{\partial}{\partial b}\left(\frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right)\right)\right) \\
&=\sum_{i=1}^{m} \alpha_{i}\left(-y_{i}\right) \\
&=-\sum_{i=1}^{m} \alpha_{i} y_{i} \\
&=0
\end{aligned}</script><p>==可以得到一个等式== $b^{\star}$ </p>
<script type="math/tex; mode=display">
\sum_{i=1}^{m} \alpha_{i} y_{i}=0</script><p>注意到这里并没有给出最优值 $b^{\star}$ 应该是多少, 而是一个等式, 该等式是一个约束项, 而最优值通过后面的 <strong>KKT 条件</strong>的互补松弛可以计算得到。</p>
<h4 id="硬性间隔线性SVM的对偶型："><a href="#硬性间隔线性SVM的对偶型：" class="headerlink" title=" 硬性间隔线性SVM的对偶型："></a><strong><font color="red"> 硬性间隔线性SVM的对偶型：</font></strong></h4><script type="math/tex; mode=display">
\begin{array}{ll}
\min _{\boldsymbol{\alpha}} & \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\top} \boldsymbol{x}_{j}-\sum_{i=1}^{m} \alpha_{i} \\
\text { s.t. } & \alpha_{i} \geq 0, \quad i=1,2, \ldots, m \\
& \sum_{i=1}^{m} \alpha_{i} y_{i}=0
\end{array}</script><h5 id="（4）利用KKT条件得到主问题的最优解"><a href="#（4）利用KKT条件得到主问题的最优解" class="headerlink" title="==（4）利用KKT条件得到主问题的最优解=="></a>==（4）利用KKT条件得到主问题的最优解==</h5><p><strong>KKT 条件是指优化问题在最优处（包括基本型的最优值，对偶问题的最优值）必须满足的条件</strong>。</p>
<p>线性支持向量机的 <strong>KKT 条件</strong>:</p>
<ul>
<li><strong>主问题可行</strong>: $g_{i}\left(\boldsymbol{u}^{\star}\right)=1-y_{i}\left(\boldsymbol{w}^{\star \top} \boldsymbol{x}_{i}+b^{\star}\right) \leq 0$ ；</li>
<li><strong>对偶问题可行</strong>: $\alpha_{i}^{\star} \geq 0$;</li>
<li><strong>主变量最优</strong>: $\boldsymbol{w}^{\star}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}, \sum_{i=1}^{m} \alpha_{i} y_{i}=0$;</li>
<li><strong><font color="red"> 互补松弛: $\alpha_{i}^{\star} g_{i}\left(\boldsymbol{u}^{\star}\right)=\alpha_{i}^{\star}\left(1-y_{i}\left(\boldsymbol{w}^{\star \top} \boldsymbol{x}_{i}+b^{\star}\right)\right)=0$ ；</font></strong></li>
</ul>
<p><strong>根据 KKT 条件中的 $\alpha_{i}^{\star} \geq 0$, 我们可以根据 $\alpha_{i}^{\star}$ 的取值将训练集 $D$ 中所有的样本分成两类</strong>, 如 图 17 所示。</p>
<ul>
<li>如果 $\alpha_{i}^{\star}&gt;0$, <strong>对应的样本称为支持向量 (Support Vector)</strong>, 根据 $\alpha_{i}^{\star}\left(1-y_{i}\left(\boldsymbol{w}^{\star \top} \boldsymbol{x}_{i}+b^{\star}\right)\right)=0$ , 那么一定有 $y_{i}\left(\boldsymbol{w}^{\star \top} \boldsymbol{x}_{i}+b^{\star}\right)=1$, 该样本是距离划分超平面最近的样本, 位于最大间隔边界 （见第 $2.3$ 节）;</li>
<li>如果 $\alpha_{i}^{\star}=0$, 对应的样本不是非支持向量, 那么有 $y_{i}\left(\boldsymbol{w}^{\star \top} \boldsymbol{x}_{i}+b^{\star}\right) \geq 1$, 该样本不一定是距离 划分超平面最近的样本, <strong>位于最大间隔边界或之外</strong>。</li>
</ul>
<p><img src="image-20220409212350705.png" alt="image-20220409212350705" style="zoom:50%;"></p>
<p><strong>结论</strong>：</p>
<ul>
<li><strong><font color="red"> 参数 w， b 仅由支持向量决定，与训练集的其他样本无关；</font></strong></li>
<li><strong><font color="red"> 对偶性是非参数模型，预测阶段不仅需要$\alpha_{i}$参数，还支持向量；</font></strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&h(\boldsymbol{x}):=\operatorname{sign}\left(\boldsymbol{w}^{\star \top} \boldsymbol{x}+b^{\star}\right) \quad \text { （硬间隔线性 SVM 的基本型的假设函数） }\\
&=\operatorname{sign}\left(\sum_{i \in S V} \alpha_{i}^{\star} y_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{x}+b^{\star}\right) \text {.(硬间隔线性 SVM 的对偶型的假设函数） }
\end{aligned}</script><h3 id="3、SVM优化方法"><a href="#3、SVM优化方法" class="headerlink" title="3、SVM优化方法"></a>3、SVM优化方法</h3><h5 id="SMO算法求解"><a href="#SMO算法求解" class="headerlink" title="==SMO算法求解=="></a>==SMO算法求解==</h5><p>我们可以看出来这是一个<strong>二次规划问题</strong>，问题规模正比于训练样本数，我们常用 SMO(Sequential Minimal Optimization) 算法求解。</p>
<p><strong>==SMO(Sequential Minimal Optimization)，序列最小优化算法【基于坐标下降算法】，其核心思想非常简单：每次只优化一个参数，其他参数先固定住，仅求当前这个优化参数的极值。我们来看一下 SMO 算法在 SVM 中的应用。==</strong></p>
<p>我们刚说了 SMO 算法每次只优化一个参数，但我们的优化目标有约束条件： <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bn%7D%5Clambda_iy_i+%3D+0" alt="[公式]"> ，没法一次只变动一个参数。所以我们选择了一次选择两个参数。具体步骤为：</p>
<ol>
<li>选择两个需要更新的参数 <img src="https://www.zhihu.com/equation?tex=%5Clambda_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Clambda_j" alt="[公式]"> ，固定其他参数。于是我们有以下约束：</li>
</ol>
<p>这样约束就变成了：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Clambda_i+y_i%2B%5Clambda_j+y_j+%3D+c+%5Cquad+%5Clambda_i+%5Cgeq+0%2C%5Clambda_j+%5Cgeq+0+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=c%3D-%5Csum%5Climits_%7Bk+%5Cne+i%2Cj%7D%5Clambda_ky_k" alt="[公式]"> ，由此可以得出 <img src="https://www.zhihu.com/equation?tex=%5Clambda_j%3D%5Cfrac%7Bc-%5Clambda_iy_i%7D%7By_j%7D" alt="[公式]"> ，也就是说我们可以用 <img src="https://www.zhihu.com/equation?tex=%5Clambda_i" alt="[公式]"> 的表达式代替 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bj%7D" alt="[公式]"> 。这样就相当于把目标问题转化成了仅有一个约束条件的最优化问题，仅有的约束是 <img src="https://www.zhihu.com/equation?tex=%5Clambda_i+%5Cgeq+0" alt="[公式]"> 。</p>
<ol>
<li><p>对于仅有一个约束条件的最优化问题，我们完全可以在 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bi%7D" alt="[公式]"> 上对优化目标求偏导，令导数为零，从而求出变量值 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bi_%7Bnew%7D%7D" alt="[公式]"> ，然后根据 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bi_%7Bnew%7D%7D" alt="[公式]"> 求出 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bj_%7Bnew%7D%7D" alt="[公式]"> 。</p>
</li>
<li><p>多次迭代直至收敛。</p>
</li>
</ol>
<p>通过 SMO 求得最优解 <img src="https://www.zhihu.com/equation?tex=%5Clambda%5E%2A" alt="[公式]"> 。</p>
<h3 id="4-软间隔线性SVM"><a href="#4-软间隔线性SVM" class="headerlink" title="4. 软间隔线性SVM"></a>4. 软间隔线性SVM</h3><p>在实际应用中，完全线性可分的样本是很少的，如果遇到了不能够完全线性可分的样本，我们应该怎么办？比如下面这个：</p>
<p><img src="https://pic1.zhimg.com/80/v2-ec8ef880d21fe3479e6bcdcd16d7e050_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>于是我们就有了软间隔，相比于硬间隔的苛刻条件，我们允许个别样本点出现在间隔带里面，比如：</p>
<p><img src="https://pic3.zhimg.com/80/v2-834c7a5f310e187b448831676b7eeeee_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>我们允许部分样本点不满足约束条件：</p>
<p><img src="https://www.zhihu.com/equation?tex=1-y_i%28w%5ETx_i+%2B+b%29+%5Cleq+0+%5C%5C" alt="[公式]"></p>
<p>为了度量这个间隔软到何种程度，我们为每个样本引入一个松弛变量 <img src="https://www.zhihu.com/equation?tex=%5Cxi_%7Bi%7D" alt="[公式]"> ，令 <img src="https://www.zhihu.com/equation?tex=%5Cxi_%7Bi%7D+%5Cgeq+0" alt="[公式]"> ，且 <img src="https://www.zhihu.com/equation?tex=1+-+y_i%28w%5ETx_i+%2B+b%29-%5Cxi_i+%5Cleq+0" alt="[公式]"> 。对应如下图所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-8e3d96fd9f9cad298628c7e2c4c8a8b8_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p><strong>这边要注意一个问题，在间隔内的那部分样本点是不是支持向量？</strong></p>
<p>我们可以由求参数 w 的那个式子可看出，只要 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bi%7D+%3E+0" alt="[公式]"> 的点都能够影响我们的超平面，因此都是支持向量。</p>
<p><strong>==硬间隔线性SVM的基本型：==</strong>【凸二次规划问题， 具有全局最小值】</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{\boldsymbol{w}, b} & \frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w} \\
\text { s.t. } & y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m
\end{array}</script><p>约束中要求所有的样本都满足 $y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right) \geq 1$, 也就是让所有的样本都满足 $y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right)&gt;0$。<br>现在我们想对<strong>该约束进行一点放松</strong>, 我们希望在优化间隔的同时, 允许分类错误的样本出现, 但这类样本应尽可能少:</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{\boldsymbol{w}, b} & \frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w}+C \sum_{i=1}^{m} \mathbb{I}\left(y_{i} \neq \operatorname{sign}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right)\right) \\
\text { s.t. } & y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right) \geq 1, \quad \text { 若 } y_{i}=\operatorname{sign}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right) .
\end{array}</script><p>其中, 优化目标的第一项 $\frac{1}{2} \boldsymbol{w}^{\top} \boldsymbol{w}$ 源自硬间隔核化 SVM 的基本型, 即优化间隔。优化目标的第二项 中的 $\mathbb{I}(\cdot)$ 是指示函数, 函数的参数通常是一个条件, 如果条件为真（True）, 则指示函数值为 1 ; 如果条件为假 (False), 则指示函数值为 0 。<br>$\sum_{i=1}^{m} \mathbb{I}\left(y_{i} \neq \operatorname{sign}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right)\right)$ 的含义是统计训练集 $D$ 中所有<strong>预测错误的样本总数</strong>。因此, 公式 162 的目标函数是同时优化间隔和最小化训练集预测错误的样本总数, <strong>==$C$ 是个可调节的超参数, 用于权衡优化间隔和出现少量分类错误的样本这两个目标。==</strong></p>
<p>但是, 由于<strong>==指示函数 $I(\cdot)$ 不是连续函数, 更不是凸函数, 使得优化问题不再是二次规划问题==</strong>, 求解起来十分困难, 所以我们需要对其进行简化。另外<strong>==指示函数没有区分预测错误的不同程度==</strong>,因此, 我们<strong>引入松他变量</strong> (Slack Variable) $\xi_{i} \in \mathbb{R}$, 用于<strong>度量训练集 $D$ 中第 $i$ 个样本违背约束的程度</strong>。当第 $i$ 个样本<strong>==违背约束的程度==</strong>越大, 松弛变量 $\xi_{i}$ 的值越大</p>
<script type="math/tex; mode=display">
\xi_{i}:= \begin{cases}0 & \text { 若 } y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right) \geq 1 ; \\ 1-y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right) & \text { 否则. }\end{cases}</script><p>基于以上定义, 松弛变量 $\xi_{i}$ 的取值有以下四种情况, 如图 27 所示, 注意图 27 只是示意图, 用于理 解概念, 不表示用图中数据训练得到的分类边界一定是这样:</p>
<ul>
<li>当 $\xi_{i}=0$ 时, 训练集 $D$ 中第 $i$ 个样本分类正确 $h\left(\boldsymbol{x}_{i}\right)=y_{i}$, 且满足大间隔约束 $y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b\right) \geq 1$;</li>
<li>当 $0&lt;\xi_{i}&lt;1$ 时, 训练集 $D$ 中第 $i$ 个样本分类正确 $h\left(\boldsymbol{x}_{i}\right)=y_{i}$, 但是不满足大间隔约束;</li>
<li>当 $\xi_{i}=1$ 时, 训练集 $D$ 中第 $i$ 个样本恰好位于划分超平面 $\boldsymbol{w}^{\top} \boldsymbol{\phi}\left(\boldsymbol{x}_{i}\right)+b=0$ 上, 且不满足大间 隔约束;</li>
<li>当 $\xi&gt;0$ 时, 训练集 $D$ 中第 $i$ 个样本分类错误 $h\left(\boldsymbol{x}_{i}\right) \neq y_{i}$, 且不满足大间隔约束。</li>
</ul>
<p><img src="image-20220409230106609.png" alt="image-20220409230106609" style="zoom:50%;"></p>
<h4 id="软间隔核化SVM基本型-：（-合页损失函数-）"><a href="#软间隔核化SVM基本型-：（-合页损失函数-）" class="headerlink" title="==软间隔核化SVM基本型==：（==合页损失函数==）"></a><strong>==软间隔核化SVM基本型==：</strong>（<strong>==合页损失函数==</strong>）</h4><p><img src="image-20220401163522598.png" alt="image-20220401163522598" style="zoom:50%;"></p>
<p><img src="image-20220401164527480.png" alt="image-20220401164527480" style="zoom:50%;"></p>
<p><strong>变为：</strong></p>
<p><img src="image-20220401164354384.png" alt="image-20220401164354384" style="zoom:50%;"></p>
<p>令<img src="image-20220401164559373.png" alt="image-20220401164559373" style="zoom:50%;">：<strong>==合页损失==</strong></p>
<p><img src="image-20220401164608500.png" alt="image-20220401164608500" style="zoom:50%;"></p>
<blockquote>
<h5 id="回顾：对数几率回归："><a href="#回顾：对数几率回归：" class="headerlink" title="回顾：对数几率回归："></a>回顾：对数几率回归：</h5><p>  <img src="image-20220401164913393.png" alt="image-20220401164913393" style="zoom:50%;"></p>
</blockquote>
<p>==<strong>软间隔核化SVM对偶性：</strong>== <strong>软间隔的对偶性是在硬间隔的对偶性对拉格朗日参数添加一个上界</strong>。</p>
<p><img src="image-20220401161707141.png" alt="image-20220401161707141" style="zoom:50%;"></p>
<h3 id="5-核函数【对偶性】"><a href="#5-核函数【对偶性】" class="headerlink" title="5. 核函数【对偶性】"></a>5. 核函数【对偶性】</h3><h4 id="5-1-线性不可分"><a href="#5-1-线性不可分" class="headerlink" title="5.1 线性不可分"></a>5.1 线性不可分</h4><blockquote>
<p>  <strong>==对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。==</strong></p>
</blockquote>
<p>我们刚刚讨论的<strong>硬间隔</strong>和<strong>软间隔</strong>都是在说样本的完全线性可分或者大部分样本点的线性可分。</p>
<p>但我们可能会碰到的一种情况是样本点不是线性可分的，比如：</p>
<p><img src="https://pic1.zhimg.com/80/v2-a17603302d58b3747118084aa25fe758_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>这种情况的解决方法就是：将<strong>==二维线性不可分样本映射到高维空间中，让样本点在高维空间线性可分==</strong>，比如：</p>
<p><img src="https://pic1.zhimg.com/80/v2-9758d49e634c15a3e684ab84bad913ec_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。</p>
<p>我们用 x 表示原来的样本点，用 <img src="https://www.zhihu.com/equation?tex=%5Cphi%28x%29" alt="[公式]"> 表示 x 映射到特征新的特征空间后到新向量。那么分割超平面可以表示为： <img src="https://www.zhihu.com/equation?tex=f%28x%29%3Dw+%5Cphi%28x%29%2Bb" alt="[公式]"> 。</p>
<p>对于非线性 SVM 的对偶问题就变成了：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmin%5Climits_%7B%5Clambda%7D+%5B%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Clambda_i+%5Clambda_j+y_i+y_j+%28%5Cphi%28x_i%29+%5Ccdot+%5Cphi%28x_j%29%29-%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Clambda_i%5D+%5C%5C+s.t.++%5Cquad+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Clambda_iy_i+%3D+0%2C+%5Cquad+%5Clambda_i+%5Cgeq+0%2C+%5Cquad+C-%5Clambda_i-%5Cmu_i%3D0+%5C%5C" alt="[公式]"></p>
<p>可以看到与线性 SVM 唯一的不同就是：之前的 <img src="https://www.zhihu.com/equation?tex=%28x_i+%5Ccdot+x_j%29" alt="[公式]"> 变成了 <img src="https://www.zhihu.com/equation?tex=%28%5Cphi%28x_i%29+%5Ccdot+%5Cphi%28x_j%29%29" alt="[公式]"> 。</p>
<h3 id="5-2-核函数的作用"><a href="#5-2-核函数的作用" class="headerlink" title="5.2 核函数的作用"></a>5.2 核函数的作用</h3><p>我们不禁有个疑问：只是做个内积运算，为什么要有核函数的呢？</p>
<p>这是因为<strong>低维空间映射到高维空间后维度可能会很大，如果将全部样本的点乘全部计算好，这样的计算量太大</strong>了。</p>
<p>但如果我们有这样的一==核函数== <img src="https://www.zhihu.com/equation?tex=k%28x%2Cy%29+%3D+%28%5Cphi%28x%29%2C%5Cphi%28y%29%29" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 在特征空间的内积等于它们在原始样本空间中通过函数 <img src="https://www.zhihu.com/equation?tex=k%28+x%2C+y%29" alt="[公式]"> 计算的结果，我们就不需要计算高维甚至无穷维空间的内积了。</p>
<p>举个例子：假设我们有一个<strong>==多项式核函数==</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=k%28x%2Cy%29%3D%28x+%5Ccdot+y+%2B+1%29%5E2+%5C%5C" alt="[公式]"></p>
<p>带进样本点的后：</p>
<p><img src="https://www.zhihu.com/equation?tex=k%28x%2Cy%29+%3D+%28%5Csum_%7Bi%3D1%7D%5En%28x_i+%5Ccdot+y_i%29+%2B+1%29%5E2+%5C%5C" alt="[公式]"></p>
<p>而它的展开项是：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5Enx_i%5E2y_i%5E2%2B%5Csum_%7Bi%3D2%7D%5En%5Csum_%7Bj%3D1%7D%5E%7Bi-1%7D%28%5Csqrt2x_ix_j%29%28%5Csqrt2y_iy_j%29%2B%5Csum_%7Bi%3D1%7D%7Bn%7D%28%5Csqrt2x_i%29%28%5Csqrt2y_i%29%2B1+%5C%5C" alt="[公式]"></p>
<p>如果没有核函数，我们则需要把向量映射成：</p>
<p><img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%28x_1%5E2%2C...%2Cx_n%5E2%2C...%5Csqrt2x_1%2C...%2C%5Csqrt2x_n%2C1%29+%5C%5C" alt="[公式]"></p>
<p>然后在进行内积计算，才能与多项式核函数达到相同的效果。</p>
<p>可见核函数的引入一方面减少了我们计算量，另一方面也减少了我们存储数据的内存使用量。</p>
<h3 id="5-3-常见核函数"><a href="#5-3-常见核函数" class="headerlink" title="5.3 常见核函数"></a>5.3 常见核函数</h3><p>我们常用核函数有：</p>
<p><strong>线性核函数</strong>【无映射】</p>
<p><img src="https://www.zhihu.com/equation?tex=k%28x_i%2Cx_j%29+%3D+x_i%5ETx_j+%5C%5C" alt="[公式]"></p>
<ul>
<li>优点：有加速算法库、没有特征映射、过拟合风险低</li>
<li>缺点：只能处理线性</li>
</ul>
<p><strong>多项式核函数</strong>【映射，超参数】</p>
<p><img src="https://www.zhihu.com/equation?tex=+k%28x_i%2Cx_j%29+%3D+%28x_i%5ETx_j%29%5Ed%5C%5C" alt="[公式]"></p>
<p><strong>高斯核函数</strong>【映射，超参数】</p>
<p><img src="https://www.zhihu.com/equation?tex=k%28x_i%2Cx_j%29+%3D+exp%28-%5Cfrac%7B%7C%7Cx_i-x_j%7C%7C%7D%7B2%5Cdelta%5E2%7D%29+%5C%5C" alt="[公式]"></p>
<ul>
<li><strong>表示能力强，但容易过拟合</strong></li>
<li><strong>高斯核没有多项核不稳定的问题</strong></li>
<li><strong>只有一个超参数</strong></li>
</ul>
<h3 id="5-4-如何选择核函数？"><a href="#5-4-如何选择核函数？" class="headerlink" title="==5.4 如何选择核函数？=="></a>==5.4 <strong>如何选择核函数？</strong>==</h3><blockquote>
<p>  <strong>其他核函数：拉普拉斯、sigmod、卡方、直方图交叉</strong></p>
<p>  可自定义和组合核函数</p>
</blockquote>
<ul>
<li>如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；</li>
<li>如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；</li>
<li>如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。</li>
</ul>
<p><img src="image-20220331203905843.png" alt="image-20220331203905843" style="zoom:50%;"></p>
<h3 id="5-5-核方法"><a href="#5-5-核方法" class="headerlink" title="5.5 核方法"></a>5.5 核方法</h3><h4 id="核化LR-非线性"><a href="#核化LR-非线性" class="headerlink" title="核化LR [非线性]"></a>核化LR [非线性]</h4><p>正类: y = +1 负类 y= -1</p>
<p><img src="image-20220331205845613.png" alt="image-20220331205845613" style="zoom:50%;"></p>
<p><img src="image-20220331211740625.png" alt="image-20220331211740625" style="zoom:50%;"></p>
<p><img src="image-20220331211757367.png" alt="image-20220331211757367" style="zoom:50%;"></p>
<p><strong>梯度下降求解：</strong></p>
<p>核化LR的参数通常都非0，并且几乎用到所有训练的样本，预测效率比较低。</p>
<p><img src="image-20220331212433609.png" alt="image-20220331212433609" style="zoom:50%;"></p>
<h3 id="5-6-支持向量回归-SVR-？核化岭回归？"><a href="#5-6-支持向量回归-SVR-？核化岭回归？" class="headerlink" title="5.6 支持向量回归 SVR = ？核化岭回归？"></a>5.6 支持向量回归 SVR = ？核化岭回归？</h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/ch18328071580/article/details/94168411">https://blog.csdn.net/ch18328071580/article/details/94168411</a></p>
<p>  <strong>支持向量在隔代之外</strong></p>
</blockquote>
<h4 id="SVR与一般线性回归的区别"><a href="#SVR与一般线性回归的区别" class="headerlink" title="SVR与一般线性回归的区别"></a>SVR与一般线性回归的区别</h4><div class="table-container">
<table>
<thead>
<tr>
<th>SVR</th>
<th>线性回归</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据在间隔带内则不计算损失，<strong>当且仅当f(x)与y之间的差距的绝对值大于ϵ才计算损失</strong></td>
<td>只要f(x)与y不相等时，就计算损失</td>
</tr>
<tr>
<td><strong>通过最大化间隔带的宽度与最小化总损失</strong>来优化模型</td>
<td>通过梯度下降之后求均值来优化模型</td>
</tr>
</tbody>
</table>
</div>
<p><strong>岭回归：</strong><img src="image-20220401144206172.png" alt="image-20220401144206172" style="zoom:50%;"></p>
<h5 id="支持向量回归：-我们假设f-x-与y之间最多有一定的偏差，大于偏差才计数损失"><a href="#支持向量回归：-我们假设f-x-与y之间最多有一定的偏差，大于偏差才计数损失" class="headerlink" title="支持向量回归：==我们假设f(x)与y之间最多有一定的偏差，大于偏差才计数损失=="></a>支持向量回归：==我们假设f(x)与y之间最多有一定的偏差，大于偏差才计数损失==</h5><script type="math/tex; mode=display">
\min _{w, b} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} l_{\epsilon}\left(f\left(x_{i}\right), y_{i}\right)</script><p>其中C为正则化常数, $l_{\epsilon}$ 是图中所示的 $\epsilon$-不敏感损失 ( $\epsilon$-insensitive loss)函数:</p>
<script type="math/tex; mode=display">
l_{\epsilon}(\mathrm{z})= \begin{cases}0, & \text { if }|z| \leq \epsilon \\ |z|-\epsilon, & \text { otherwise }\end{cases}</script><p>引入松弛变量 $\xi_{i}$ 和 $\left(\xi_{i}\right)$, 可将式重写为:</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{w, b, \xi_{i}, \xi_{i}} & \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}, \widehat{\xi}_{i}\right) \\
\text { s.t. } & f\left(x_{i}\right)-y_{i} \leq \epsilon+\xi_{i} \\
& y_{i}-f\left(x_{i}\right) \leq \epsilon+\widehat{\xi}_{i} \\
& \xi_{i} \geq 0, \hat{\xi}_{i} \geq 0, i=1,2, \ldots m
\end{array}</script><p>引入拉格朗日乘子 $\mu_{i}$,</p>
<p>$L(w, b, \alpha, \hat{\alpha}, \xi, \hat{\xi}, \mu, \hat{\mu})$<br>$=\frac{1}{2}|w|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\widehat{\xi}_{i}\right)-\sum_{i=1}^{m} \xi_{i} \mu_{i}-\sum_{i=1}^{m} \widehat{\xi}_{i} \widehat{\mu_{i}}$<br>$+\sum_{i=1}^{m} \alpha_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)+\sum_{i=1}^{m} \widehat{\alpha_{i}}\left(y_{i}-f\left(x_{i}\right)-\epsilon-\widehat{\xi}_{i}\right)$</p>
<p>再令 $L(w, b, a, \hat{a}, \xi, \hat{\xi}, \mu, \mu)$ 对 $w, b, \xi_{i}, \hat{\xi}_{i}$ 的偏导为零可得:</p>
<script type="math/tex; mode=display">
w=\sum_{i=1}^{m}\left(\widehat{\alpha_{i}}-\alpha_{i}\right) x_{i}</script><p>上述过程中需满足KKT条件, 即要求:</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{c}
\alpha_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)=0 \\
\widehat{\alpha_{i}}\left(y_{i}-f\left(x_{i}\right)-\epsilon-\widehat{\xi}_{i}\right)=0 \\
\alpha_{i} \widehat{\alpha_{i}}=0, \xi_{i} \widehat{\xi}_{i}=0 \\
\left(C-\alpha_{i}\right) \xi_{i}=0,\left(C-\widehat{\alpha_{i}}\right) \widehat{\xi}_{i}=0 .
\end{array}\right.</script><h3 id="5-7-多分类-SVM"><a href="#5-7-多分类-SVM" class="headerlink" title="5.7 多分类 SVM"></a>5.7 多分类 SVM</h3><h4 id="5-7-1-多分类问题"><a href="#5-7-1-多分类问题" class="headerlink" title="5.7.1 多分类问题"></a>5.7.1 多分类问题</h4><ul>
<li>多分类问题拆解成若干个二分类问题，对于每个二分类训练一个分类器。<ul>
<li><strong>one vs one 拆解</strong>：K(K-1)/2 个分类器。</li>
<li><strong>one vs Rest 拆解</strong>：K个分类器</li>
</ul>
</li>
</ul>
<p><img src="image-20220401192909167.png" alt="image-20220401192909167" style="zoom:50%;"></p>
<ul>
<li><p><strong>根据模型特点设计：多分类线性SVM</strong></p>
<ul>
<li><p><strong>层次支持向量机</strong></p>
</li>
<li><p><strong>回顾二分类</strong>；</p>
<p><img src="image-20220401193645250.png" alt="image-20220401193645250" style="zoom:50%;"></p>
</li>
<li><p><strong>多分类线性SVM</strong>：</p>
<p><img src="image-20220401194059380.png" alt="image-20220401194059380" style="zoom:50%;"></p>
</li>
</ul>
</li>
</ul>
<h2 id="6-优缺点"><a href="#6-优缺点" class="headerlink" title="6. 优缺点"></a>6. 优缺点</h2><h3 id="6-1-优点"><a href="#6-1-优点" class="headerlink" title="6.1 优点"></a>6.1 优点</h3><ul>
<li>有严格的<strong>==数学理论支持==</strong>，<strong>==可解释性强==</strong>，<strong>==不依靠统计方法==</strong>，从而<strong>简化了通常的分类和回归问题</strong>；</li>
<li>能找出对任务至关重要的<strong>==关键样本==</strong>（即：<strong>支持向量</strong>）；</li>
<li>采用<strong>==核技巧==</strong>之后，<strong>==可以处理非线性分类/回归任务==</strong>；</li>
<li><strong>==最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。==</strong></li>
</ul>
<h3 id="6-2-缺点"><a href="#6-2-缺点" class="headerlink" title="6.2 缺点"></a>6.2 缺点</h3><ul>
<li><strong>训练时间长</strong>：当采用 SMO 算法时，由于每次都需要挑选一对参数，因此时间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28N%5E2%29" alt="[公式]"> ，其中 N 为训练样本的数量；</li>
<li><strong>存储空间大</strong>：当采用核技巧时，如果需要存储核矩阵，则空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28N%5E2%29" alt="[公式]"> ；</li>
<li><strong>预测时间长</strong>：模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。</li>
</ul>
<p><strong>==因此支持向量机目前只适合小批量样本的任务，无法适应百万甚至上亿样本的任务。==</strong></p>
<h2 id="SVM-Q-amp-A"><a href="#SVM-Q-amp-A" class="headerlink" title="SVM Q&amp;A"></a>SVM Q&amp;A</h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93715996">https://zhuanlan.zhihu.com/p/93715996</a></p>
</blockquote>
<h3 id="1、原理："><a href="#1、原理：" class="headerlink" title="1、原理："></a>1、原理：</h3><ul>
<li>简单介绍SVM（详细原理）：从分类平面，到求两类间的最大间隔，到转化为求间隔分之一，等优化问题，然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题， 最后再利用SMO（序列最小优化）来解决这个对偶问题。<strong>svm里面的超参数c有啥用：软间隔SVM去权衡优化目标和少量分错样本的目标。</strong></li>
<li><p>SVM的推导，解释原问题和对偶问题，<strong>SVM原问题和对偶问题的关系</strong>，<strong>KKT限制条件</strong>，<strong>KKT条件用哪些</strong>，完整描述；软间隔问题，解释支持向量、核函数（哪个地方引入、画图解释高维映射，高斯核可以升到多少维，如何选择核函数），引入拉格朗日的优化方法的原因，最大的特点，损失函数解释</p>
<ul>
<li><strong>KKT限制</strong>：主问题可行、对偶问题可行、主变量最优、<strong>互补松弛</strong></li>
</ul>
</li>
<li><p><strong>为什么要把原问题转换为对偶问题？</strong></p>
<ul>
<li>因为原问题是凸二次规划问题，转换为对偶问题更加高效。为什么求解对偶问题更加高效？因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0.alpha系数有多少个？样本点的个数</li>
</ul>
</li>
</ul>
<h3 id="2、SVM与LR最大区别，LR和SVM对于outlier的敏感程度分析，逻辑回归与SVM的区别？"><a href="#2、SVM与LR最大区别，LR和SVM对于outlier的敏感程度分析，逻辑回归与SVM的区别？" class="headerlink" title="2、SVM与LR最大区别，LR和SVM对于outlier的敏感程度分析，逻辑回归与SVM的区别？"></a>2、SVM与LR最大区别，LR和SVM对于outlier的敏感程度分析，逻辑回归与SVM的区别？</h3><h3 id="3、SVM如何解决-多分类问题-、可以做-回归-吗，怎么做？"><a href="#3、SVM如何解决-多分类问题-、可以做-回归-吗，怎么做？" class="headerlink" title="3、SVM如何解决==多分类问题==、可以做==回归==吗，怎么做？"></a>3、SVM如何解决==多分类问题==、可以做==回归==吗，怎么做？</h3><h3 id="4、机器学习有很多关于核函数的说法，核函数的定义和作用是什么？"><a href="#4、机器学习有很多关于核函数的说法，核函数的定义和作用是什么？" class="headerlink" title="4、机器学习有很多关于核函数的说法，核函数的定义和作用是什么？"></a>4、机器学习有很多关于核函数的说法，核函数的定义和作用是什么？</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/24627666">https://www.zhihu.com/question/24627666</a></p>
<h3 id="5、Linear-SVM-和-LR-有什么异同？"><a href="#5、Linear-SVM-和-LR-有什么异同？" class="headerlink" title="5、Linear SVM 和 LR 有什么异同？"></a>5、Linear SVM 和 LR 有什么异同？</h3><h4 id="SVM和LR相同点："><a href="#SVM和LR相同点：" class="headerlink" title="SVM和LR相同点："></a>SVM和LR相同点：</h4><ul>
<li>SVM和LR都属于机器学习的监督学习中的<strong>判别式模型</strong>（判别式模型对$p(y|x)$进行建模或直接基于x预测y，生成模型：$p(x|y)$和$p(y)$进行建模,预测后验概率）。</li>
<li>SVM和LR都是线性二分类模型，<strong>分类边界为一个超平面</strong>。</li>
<li>线性SVM和对数几率回归都可以基于表示定理和<strong>核技巧处理非线性可分问题</strong>。</li>
<li><strong>SVM的基本型和对数几率函数都属于参数模型。SVM的对偶性和核化对数几率回归都属于非参数模型</strong>。</li>
<li>SVM和LR优化目标都表示成：经验风险+结构风险（正则项）的形式；均是0，1损失的替代函数。风险结构都是L2正则化。</li>
<li><strong>SVM和LR都是凸优化问题，都能收敛到全局最优解。</strong></li>
<li>SVM和对数几率函数的优化目标相似，性能相当。</li>
<li><strong>SVM多分类：多分类SVM；LR多分类：Softmax回归。</strong></li>
</ul>
<h3 id="SVM和LR的区别："><a href="#SVM和LR的区别：" class="headerlink" title=" SVM和LR的区别："></a><strong><font color="red"> SVM和LR的区别：</font></strong></h3><blockquote>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>SVM</th>
<th>LR</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong></td>
<td><strong>SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面</strong>。</td>
<td><strong>逻辑回归</strong>是使用线性回归模型的预测值逼近分类任务真实标记的对数几率。</td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td><strong>非概率方法</strong>；</td>
<td><strong>概率方法</strong>；需要对$p(y</td>
<td>x)$进行假设，具有概率意义。</td>
</tr>
<tr>
<td><strong>经验损失函数</strong></td>
<td><strong>合页损失函数</strong>；有一段平的零区域、使得SVM的对偶性有稀疏性。</td>
<td><strong>交叉熵损失函数</strong></td>
</tr>
<tr>
<td><strong>训练样本</strong></td>
<td>支持向量（少数样本），SVM的参数和假设函数只和支持向量有关。</td>
<td>全样本</td>
</tr>
<tr>
<td><strong>优化方法</strong></td>
<td>次梯度下降和坐标梯度下降 【<strong>SMO算法</strong>】</td>
<td><strong>梯度下降</strong></td>
</tr>
<tr>
<td>多分类</td>
<td><strong>多分类SVM</strong></td>
<td><strong>Softmax回归</strong></td>
</tr>
<tr>
<td><strong>敏感程度</strong></td>
<td>SVM考虑分类边界线附近的样本（决定分类超平面的样本）。在支持向量外添加或减少任何样本点对分类决策面没有任何影响；</td>
<td>LR受所有数据点的影响。直接依赖数据分布，每个样本点都会影响决策面的结果。如果训练数据不同类别严重不平衡。</td>
</tr>
</tbody>
</table>
</div>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26768865">https://www.zhihu.com/question/26768865</a></p>
<h3 id="6、支持向量机-SVM-是否适合大规模数据？【速度】"><a href="#6、支持向量机-SVM-是否适合大规模数据？【速度】" class="headerlink" title="6、支持向量机(SVM)是否适合大规模数据？【速度】"></a>6、支持向量机(SVM)是否适合大规模数据？【速度】</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/19591450">https://www.zhihu.com/question/19591450</a></p>
<h3 id="7、SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？"><a href="#7、SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？" class="headerlink" title="7、SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？"></a>7、SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/30123068">https://www.zhihu.com/question/30123068</a></p>
<h3 id="8、各种机器学习的应用场景分别是什么？例如，k近邻-贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型。"><a href="#8、各种机器学习的应用场景分别是什么？例如，k近邻-贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型。" class="headerlink" title="8、各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型。"></a>8、各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型。</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26726794">https://www.zhihu.com/question/26726794</a></p>
<h3 id="9、SVM与感知器的联系和优缺点比较"><a href="#9、SVM与感知器的联系和优缺点比较" class="headerlink" title="==9、SVM与感知器的联系和优缺点比较=="></a>==9、SVM与感知器的联系和优缺点比较==</h3><p><strong>感知机用误分类样本点的几何距离之和</strong>来表示模型的损失函数，用梯度下降算法优化，直至没有误分类点。</p>
<script type="math/tex; mode=display">
L(w, b)=-\sum_{x^{(i)} \in M} y^{(i)}\left(w^{T} x^{(i)}+b\right)</script><h4 id="感知机与SVM区别："><a href="#感知机与SVM区别：" class="headerlink" title="==感知机与SVM区别：=="></a>==感知机与SVM区别：==</h4><p><strong>SVM可以视为对感知器的二阶改进</strong>：第一阶改进是加入了 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="[公式]"> 获得hinge loss，从而具备了产生大间隔的潜质；第二阶改进是加入了权向量的L2正则化项，从而避免产生无意义的大函数间隔，而是产生大的几何间隔。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>感知机</th>
<th>SVM</th>
</tr>
</thead>
<tbody>
<tr>
<td>思想</td>
<td>分离超平面基于误分类的损失函数<img src="https://www.zhihu.com/equation?tex=%5Cmin_%7Bw%2Cb%7D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En+max%280%2C+-y_i%28w%5ETx_i%2Bb%29%29%5C%5C" alt="[公式]"></td>
<td><img src="https://www.zhihu.com/equation?tex=%5Cmin_%7Bw%2Cb%7D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En+max%280%2C+1-y_i%28w%5ETx_i%2Bb%29%29+%2B+%5Calpha+%7C%7Cw%7C%7C_2%5E2+%5C%5C" alt="[公式]" style="zoom:150%;"></td>
</tr>
<tr>
<td>超平面</td>
<td>因采用的初值不同而得到不同的超平面</td>
<td>让离划分超平面最近的样本到划分超平面距离尽可能远</td>
</tr>
<tr>
<td>关键样本</td>
<td>每步的分错样本</td>
<td><strong>支持向量</strong></td>
</tr>
<tr>
<td>非线性问题</td>
<td>无</td>
<td>核化</td>
</tr>
</tbody>
</table>
</div>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8814%EF%BC%89%E8%81%9A%E7%B1%BB*-Kmeans/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8814%EF%BC%89%E8%81%9A%E7%B1%BB*-Kmeans/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-10 22:41:00" itemprop="dateModified" datetime="2022-07-10T22:41:00+08:00">2022-07-10</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="聚类算法-【无监督】"><a href="#聚类算法-【无监督】" class="headerlink" title="聚类算法 【无监督】"></a>聚类算法 【无监督】</h2><blockquote>
<p>  常用聚类算法 - 小胡子的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104355127">https://zhuanlan.zhihu.com/p/104355127</a></p>
<p>  <strong>K-means, K-medians, K-mediods and K-centers</strong> - 仲基的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/398600714">https://zhuanlan.zhihu.com/p/398600714</a></p>
</blockquote>
<p>什么是聚类算法？聚类是一种机器学习技术，它涉及到数据点的分组。给定一组数据点，我们可以使用聚类算法将每个数据点划分为一个特定的组。理论上，同一组中的数据点应该具有相似的属性和/或特征，而不同组中的数据点应该具有高度不同的属性和/或特征。<strong>聚类是一种无监督学习的方法</strong>，是许多领域中常用的统计数据分析技术。</p>
<p><strong>聚类算法主要包括以下五类：</strong></p>
<ul>
<li><strong>基于分层的聚类（hierarchical methods）</strong></li>
</ul>
<p>这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。<strong>代表算法有：<em>BIRCH算法</em>（1996）、<em>CURE算法</em>、CHAMELEON算法等。</strong></p>
<blockquote>
<p>  层次聚类通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层，树的顶层是一个聚类的根节点。</p>
<p>  <strong>最小距离的层次聚类算法</strong>通过自下而上合并创建聚类树，合并算法通过计算两类数据点间的欧式距离来计算不同类别数据点间的相似度，对所有数据点中最为相似的两个数据点进行组合，组合后，最小距离（Single Linkage）的计算方法是将两个组合数据点中距离最近的两个数据点间的距离作为这两个组合数据点的距离。并反复迭代这一过程。</p>
</blockquote>
<ul>
<li><strong>基于划分的聚类（partitioning methods）</strong></li>
</ul>
<p>给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：<strong><em>==K-means算法==</em>、<em>K-medoids算法</em>、<em>CLARANS算法</em></strong></p>
<ul>
<li><strong>基于密度的聚类（density-based methods）</strong></li>
</ul>
<p>基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有<strong>：<em>==DBSCAN（Density-Based Spatial Clustering of Applic with Noise）==算法（1996）</em>、<em>OPTICS（Ordering Points to Identify Clustering Structure）算法（1999）</em>、<em>DENCLUE算法（1998）</em>、<em>WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据）</em></strong></p>
<ul>
<li><strong>基于网格的聚类（grid-based methods）</strong></li>
</ul>
<p>这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：<strong><em>STING（Statistical Information Grid）</em>、<em>CLIQUE（Clustering In Quest）算法（1998）</em>、<em>WaveCluster算法</em>。</strong>其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。</p>
<ul>
<li><strong>基于模型的聚类（model-based methods）</strong></li>
</ul>
<p>基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。</p>
<h2 id="一、K-means-【基于划分】-K值的选择？"><a href="#一、K-means-【基于划分】-K值的选择？" class="headerlink" title="一、K-means 【基于划分】[==K值的选择？==]"></a>一、K-means 【基于划分】[==K值的选择？==]</h2><p><img src="https://pic1.zhimg.com/v2-e7195b6620e2e6ec743fb77702b1d3ff_1440w.jpg?source=172ae18b" alt="【机器学习】K-means（非常详细）" style="zoom:51%;"></p>
<blockquote>
<p>  K-means 聚类的迭代算法实际上是 EM 算法。EM 算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。在 K-means 中的隐变量是每个类别所属类别。</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/20463356">K-means 笔记（三）数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//sofasofa.io/forum_main_post.php%3Fpostid%3D1000282">K-means 怎么选 K?</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161733843">K-Means：隐变量、聚类、EM</a></li>
</ol>
</blockquote>
<p>k近邻法中，当<strong>训练集</strong>、<strong>距离度量</strong>、<strong>K值</strong>以及<strong>分类决策规则</strong>确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。</p>
<p><strong>K-均值是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为:</strong></p>
<ul>
<li>首先选择𝐾个<strong>随机</strong>的点，称为<strong>聚类中心</strong>（cluster centroids）；</li>
<li>对于数据集中的每一个数据，按照<strong>距离𝐾个中心点的距离</strong>，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</li>
<li>计算每一个组的平均值，将该组所<strong>关联的中心点移动到平均值</strong>的位置。</li>
<li>重复步骤，直至中心点不再变化。</li>
</ul>
<p>K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用 K-均值算法将数据分为三类，用于帮助确定将要生产的 T-恤衫的三种尺寸。</p>
<p><img src="https://camo.githubusercontent.com/86b1cfa2d801f27862bcc8cab59f04401e01defd5248311eb77ec75a02286c5f/687474703a2f2f7778332e73696e61696d672e636e2f6d773639302f30303633304465666c79316735623734367a776a676a33306668306337676e6a2e6a7067" alt="img" style="zoom: 50%;"></p>
<h3 id="1-1-损失函数"><a href="#1-1-损失函数" class="headerlink" title="1.1 损失函数"></a>1.1 损失函数</h3><p><strong>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和</strong>，因此 <strong>K-均值的代价函数（又称==畸变函数 Distortion function）==为</strong>：</p>
<p><img src="/Users/apple/Library/Application Support/typora-user-images/image-20220707191400653.png" alt="image-20220707191400653" style="zoom:50%;"></p>
<p>其中 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/c0d78bacde143a412a432d0f2c8d1a746fc34802fec6dfd079a07708e30a98f4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f25374263253545253742286929253744253744"><img src="https://camo.githubusercontent.com/c0d78bacde143a412a432d0f2c8d1a746fc34802fec6dfd079a07708e30a98f4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f25374263253545253742286929253744253744" alt="img"></a>代表与 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b14a6424262c05d4fb45a600d8fb5b4881cee4801e8699dcc0c58ef503164f94/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253545253742286929253744"><img src="https://camo.githubusercontent.com/b14a6424262c05d4fb45a600d8fb5b4881cee4801e8699dcc0c58ef503164f94/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253545253742286929253744" alt="img"></a>最近的聚类中心点。 我们的的优化目标便是找出使得代价函数最小的 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/c48ec5ec6d35ce0993ee193bc86c0d4eddb3dbac31e30fa1f38fbf56b1401083/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f632535452537422831292537442c632535452537422832292537442c2e2e2e2c63253545253742286d29253744"><img src="https://camo.githubusercontent.com/c48ec5ec6d35ce0993ee193bc86c0d4eddb3dbac31e30fa1f38fbf56b1401083/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f632535452537422831292537442c632535452537422832292537442c2e2e2e2c63253545253742286d29253744" alt="img"></a>和 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d287d301a22d8bb905fe7e82874a282adff45c972a158b7a2cb91f0354a4ae84/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f312c755f322c2e2e2e2c755f6b"><img src="https://camo.githubusercontent.com/d287d301a22d8bb905fe7e82874a282adff45c972a158b7a2cb91f0354a4ae84/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f312c755f322c2e2e2e2c755f6b" alt="img"></a>。</p>
<h3 id="1-2-k值的选择-【肘部法则】"><a href="#1-2-k值的选择-【肘部法则】" class="headerlink" title="1.2 k值的选择 【肘部法则】"></a>1.2 k值的选择 【肘部法则】</h3><p>在运行 K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：</p>
<ol>
<li>我们应该选择𝐾 &lt; 𝑚，即聚类中心点的个数要小于所有训练集实例的数量。</li>
<li>随机选择𝐾个训练实例，然后令𝐾个聚类中心分别与这𝐾个训练实例相等K-均值的一个问题在于，它有可能会<strong>停留在一个局部最小值</strong>处，而这取决于初始化的情况。</li>
</ol>
<p>为了解决这个问题，我们通常需要多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行 K-均值的结果，选择代价函数最小的结果。这种方法在𝐾较小的时候（2—10）还是可行的，<strong>但是如果𝐾较大，这么做也可能不会有明显地改善。</strong></p>
<p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么。有一个可能会谈及的方法叫作<strong>“肘部法则”</strong>。关 于“肘部法则”，我们所需要做的是改变𝐾值，也就是聚类类别数目的总数。我们用一个聚类来运行 K 均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后<strong>计算成本函数或者计算畸变函数</strong>𝐽。𝐾代表聚类数字。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8888198abee1b3069d27a6bff19f1830a94ac4141ebe7dd300f8ee4000262c6d/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562377062616561766a3330716f3063777463392e6a7067"><img src="https://camo.githubusercontent.com/8888198abee1b3069d27a6bff19f1830a94ac4141ebe7dd300f8ee4000262c6d/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562377062616561766a3330716f3063777463392e6a7067" alt="img"></a></p>
<p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。你会发现这种模式，它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用 3 个聚类来进行聚类是正确的，<strong>这是因为那个点是曲线的肘点，畸变值下降得很快，𝐾 = 3之后就下降得很慢，那么我们就选𝐾 = 3。</strong>当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p>
<h3 id="1-3-KNN与K-means区别？"><a href="#1-3-KNN与K-means区别？" class="headerlink" title="1.3 KNN与K-means区别？"></a>1.3 KNN与K-means区别？</h3><p>K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。</p>
<h4 id="区别："><a href="#区别：" class="headerlink" title="区别："></a>区别：</h4><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>KNN</th>
<th>K-Means</th>
</tr>
</thead>
<tbody>
<tr>
<td>类别</td>
<td>1.KNN是<strong>分类</strong>算法 2.属于<strong>监督学习</strong> 3.训练数据集是带label的数据</td>
<td>1.K-Means是<strong>聚类</strong>算法 2.属于<strong>非监督学习</strong> 3.训练数据集是无label的数据，是杂乱无章的，经过聚类后变得有序，先无序，后有序。</td>
</tr>
<tr>
<td></td>
<td>没有明显的前期训练过程，属于memory based learning</td>
<td>有明显的前期训练过程</td>
</tr>
<tr>
<td>k值的含义</td>
<td>K的含义：一个样本x，对它进行分类，就从训练数据集中，<strong>在x附近找离它最近的K个数据点</strong>，这K个数据点，类别c占的个数最多，就把x的label设为c。</td>
<td>K的含义：<strong>K是人工固定好的数字，假设数据集合可以分为K个蔟</strong>，那么就利用训练数据来训练出这K个分类。</td>
</tr>
</tbody>
</table>
</div>
<h4 id="相似点："><a href="#相似点：" class="headerlink" title="相似点："></a><strong>相似点</strong>：</h4><p>都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法思想。</p>
<h3 id="1-4-K-Means优缺点及改进"><a href="#1-4-K-Means优缺点及改进" class="headerlink" title="1.4 K-Means优缺点及改进"></a>1.4 K-Means优缺点及改进</h3><p>k-means：在大数据的条件下，<strong>会耗费大量的时间和内存</strong>。 优化k-means的建议：</p>
<ol>
<li><p>减少聚类的数目K。因为，每个样本都要跟类中心计算距离。</p>
</li>
<li><p>减少样本的特征维度。比如说，<strong>通过PCA等进行降维</strong>。</p>
</li>
<li><p>考察其他的聚类算法，通过选取toy数据，去测试不同聚类算法的性能。</p>
</li>
<li><p><strong>hadoop集群</strong>，K-means算法是很容易进行并行计算的。</p>
</li>
<li><p>算法可能找到局部最优的聚类，而不是全局最优的聚类。使用改进的二分k-means算法。</p>
<p>二分k-means算法：首先将整个数据集看成一个簇，然后进行一次k-means（k=2）算法将该簇一分为二，并计算每个簇的误差平方和，选择平方和最大的簇迭代上述过程再次一分为二，直至簇数达到用户指定的k为止，此时可以达到的全局最优。</p>
</li>
</ol>
<h3 id="二、K-means的调优与改进"><a href="#二、K-means的调优与改进" class="headerlink" title="二、K - means的调优与改进"></a>二、K - means的调优与改进</h3><p>针对 K-means 算法的缺点，我们可以有很多种调优方式：如<strong>数据预处理</strong>（去除异常点），<strong>合理选择 K 值</strong>，<strong>高维映射</strong>等。以下将简单介绍：</p>
<h3 id="2-1-数据预处理"><a href="#2-1-数据预处理" class="headerlink" title="2.1 数据预处理"></a>2.1 数据预处理</h3><p>K-means 的本质是基于欧式距离的数据划分算法，均值和方差大的维度将对数据的聚类产生决定性影响。所以<strong>未做归一化处理和统一单位的数据是无法直接参与运算和比较</strong>的。常见的数据预处理方式有：<strong>数据归一化，数据标准化</strong>。</p>
<p>此外，离群点或者噪声数据会对均值产生较大的影响，导致中心偏移，因此我们还需要对数据进行异常点检测。</p>
<h3 id="2-2-合理选择-K-值"><a href="#2-2-合理选择-K-值" class="headerlink" title="2.2 合理选择 K 值"></a>2.2 合理选择 K 值</h3><p>K 值的选取对 K-means 影响很大，这也是 K-means 最大的缺点，常见的选取 K 值的方法有：<strong>手肘法、Gap statistic 方法</strong>。</p>
<p><strong>【手肘法】</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-5ca4a5fe0b06b25a2b97262abb401a16_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>当 K &lt; 3 时，曲线急速下降；当 K &gt; 3 时，曲线趋于平稳，通过手肘法我们认为拐点 3 为 K 的最佳值。</p>
<p>==【<strong>Gap statistic</strong>】==</p>
<p><img src="https://www.zhihu.com/equation?tex=Gap%28K%29%3D%5Ctext%7BE%7D%28%5Clog+D_k%29-%5Clog+D_k+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=D_k" alt="[公式]"> 为损失函数，这里 <img src="https://www.zhihu.com/equation?tex=E%28logD_k%29" alt="[公式]"> 指的是 <img src="https://www.zhihu.com/equation?tex=logD_k" alt="[公式]"> 的期望。这个数值通常通过<strong>蒙特卡洛模拟</strong>产生，我们在样本里所在的区域中按照<strong>均匀分布随机产生和原始样本数一样多的随机样本</strong>，并对这个<strong>随机样本做 K-Means</strong>，从而得到一个 <img src="https://www.zhihu.com/equation?tex=D_k+" alt="[公式]"> 。如此往复多次，通常 20 次，我们可以得到 20 个 <img src="https://www.zhihu.com/equation?tex=logD_k" alt="[公式]"> 。对这 20 个数值求平均值，就得到了 <img src="https://www.zhihu.com/equation?tex=E%28logD_k%29" alt="[公式]">  的近似值。最终可以计算 Gap Statisitc。而 Gap statistic 取得最大值所对应的 K 就是最佳的 K。</p>
<p><img src="https://pic3.zhimg.com/80/v2-9a39a8dad143e5dd52a506d83c2cbb36_1440w.jpg" alt="img"></p>
<p>由图可见，当 K=3 时，Gap(K) 取值最大，所以最佳的簇数是 K=3。</p>
<p>Github 上一个项目叫 <a href="https://link.zhihu.com/?target=https%3A//github.com/milesgranger/gap_statistic">gap_statistic</a> ，可以更方便的获取建议的类簇个数。</p>
<h3 id="2-3-采用核函数"><a href="#2-3-采用核函数" class="headerlink" title="2.3 采用核函数"></a>2.3 采用核函数</h3><p><strong>基于欧式距离的 K-means 假设了了各个数据簇的数据具有一样的的先验概率并呈现球形分布</strong>，但这种分布在实际生活中并不常见。面对非凸的数据分布形状时我们可以引入核函数来优化，这时算法又称为核 K-means 算法，是核聚类方法的一种。<strong>核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。</strong>非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。</p>
<h3 id="2-4-K-means"><a href="#2-4-K-means" class="headerlink" title="==2.4 K-means++=="></a>==2.4 K-means++==</h3><blockquote>
<p>  <strong>K-means++ 就是选择离已选中心点最远的点</strong>。这也比较符合常理，聚类中心当然是互相离得越远越好。</p>
</blockquote>
<p>我们知道初始值的选取对结果的影响很大，对初始值选择的改进是很重要的一部分。在所有的改进算法中，K-means++ 最有名。</p>
<p>K-means++ 算法步骤如下所示：</p>
<ol>
<li>随机选取一个中心点 <img src="https://www.zhihu.com/equation?tex=a_1" alt="[公式]"> ；</li>
<li>计算数据到之前 n 个聚类中心最远的距离 <img src="https://www.zhihu.com/equation?tex=D%28x%29" alt="[公式]"> ，并以一定概率 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BD%28x%29%5E2%7D%7B%5Csum%7BD%28x%29%5E2%7D%7D" alt="[公式]"> 选择新中心点 <img src="https://www.zhihu.com/equation?tex=a_i" alt="[公式]"> ；</li>
<li>重复第二步。</li>
</ol>
<p>简单的来说，就是 <strong>K-means++ 就是选择离已选中心点最远的点</strong>。这也比较符合常理，聚类中心当然是互相离得越远越好。</p>
<p>但是这个算法的缺点在于，难以并行化。所以 k-means II 改变取样策略，并非按照 k-means++ 那样每次遍历只取样一个样本，而是每次遍历取样 k 个，重复该取样过程 <img src="https://www.zhihu.com/equation?tex=log%28n+%29" alt="[公式]"> 次，则得到 <img src="https://www.zhihu.com/equation?tex=klog%28n%29" alt="[公式]"> 个样本点组成的集合，然后从这些点中选取 k 个。当然一般也不需要 <img src="https://www.zhihu.com/equation?tex=log%28n%29" alt="[公式]"> 次取样，5 次即可。</p>
<h3 id="2-5-ISODATA"><a href="#2-5-ISODATA" class="headerlink" title="2.5 ISODATA"></a>2.5 ISODATA</h3><p>ISODATA 的全称是<strong>迭代自组织数据分析法</strong>。它解决了 K 的值需要预先人为的确定这一缺点。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出 K 的大小。ISODATA 就是针对这个问题进行了改进，它的思想也很直观：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别。</p>
<h3 id="三、-收敛证明【EM算法】"><a href="#三、-收敛证明【EM算法】" class="headerlink" title="==三、 收敛证明【EM算法】=="></a>==三、 收敛证明【EM算法】==</h3><p>我们先来看一下 K-means 算法的步骤：先随机选择初始节点，然后计算每个样本所属类别，然后通过类别再跟新初始化节点。这个过程有没有想到之前介绍的 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/78311644">EM 算法</a> 。</p>
<p>我们需要知道的是 K-means 聚类的迭代算法实际上是 EM 算法。EM 算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。在 <strong>==K-means 中的隐变量是每个样本所属类别==</strong>。</p>
<p>K-means 算法迭代步骤中的每次确认中心点以后重新进行标记对应 EM 算法中的 <strong>E 步</strong>：<strong>求当前参数条件下的 Expectation</strong>。而根据标记重新求中心点 对应 EM 算法中的 <strong>M 步</strong>：<strong>求似然函数最大化时（损失函数最小时）对应的参数 。</strong></p>
<p>首先我们看一下损失函数的形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=J%3D%5Csum_%7Bi%3D1%7D%5E%7BC%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BN%7D%7Br_%7Bij%7D%5Ccdot+%7B%5Cnu%28x_j%2C%7B%5Cmu%7D_i%29%7D%7D%7D+%5C%5C" alt="[公式]"></p>
<p>其中：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cnu%28%7Bx_j%7D%2C%5Cmu_i%29%3D%7B%7C%7Cx_j-%7B%5Cmu%7D_i%7C%7C%7D%5E%7B2%7D+%2C%5Cquad++%7Br%7D_%7Bnk%7D%3D%5Cleft+%5C%7B+%5Cbegin%7Baligned%7D+%261+%5Cquad+if+%5C%3B+x_n+%5Cin+k+%5C%5C+%260+%5Cquad+else+%5Cend%7Baligned%7D+%5Cright.+%5C%5C" alt="[公式]"></p>
<p>为了求极值，我们令损失函数求偏导数且等于 0：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%7BJ%7D%7D%7B%5Cpartial+%5Cmu_k%7D%3D2+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7D%28x_i-%7B%5Cmu%7D_%7Bk%7D%29%7D%3D0+%5C%5C" alt="[公式]"></p>
<p>k 是指第 k 个中心点，于是我们有：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu_k%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7Dx_i%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7D%7D%7D+%5C%5C" alt="[公式]"></p>
<p>可以看出，新的中心点就是所有该类的<strong>质心</strong>。</p>
<p><strong>EM 算法的缺点就是，容易陷入局部极小值，这也是 K-means 有时会得到局部最优解的原因。</strong></p>
<h3 id="四、高斯混合模型-GMM"><a href="#四、高斯混合模型-GMM" class="headerlink" title="四、高斯混合模型(GMM)"></a>四、高斯混合模型(GMM)</h3><h3 id="4-1-GMM的思想"><a href="#4-1-GMM的思想" class="headerlink" title="4.1 GMM的思想"></a>4.1 GMM的思想</h3><p>高斯混合模型（Gaussian Mixed Model，GMM）也是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。<strong>高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的</strong>，当前<strong>数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。</strong></p>
<p>第一张图是一个数据分布的样例，如果只用一个高斯分布来拟合图中的数据，图 中所示的椭圆即为高斯分布的二倍标准差所对应的椭圆。直观来说，图中的数据 明显分为两簇，因此只用一个高斯分布来拟和是不太合理的，需要推广到用多个 高斯分布的叠加来对数据进行拟合。第二张图是用两个高斯分布的叠加来拟合得到的结果。<strong>这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。</strong>理论上，高斯混合模型可以拟合出任意类型的分布。</p>
<p>高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来 的。在该假设下，每个单独的分模型都是标准高斯模型，其均值 $u_i$ 和方差 $\sum_i$ 是待估计的参数。此外，每个分模型都还有一个参数 $\pi_i$，可以理解为权重或生成数据的概 率。高斯混合模型的公式为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/4055f35cd39fe2e095b0fc70f8cccb1e3cab924a2ebd6bc56ca7bdcf026c79ec/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f702878293d25354373756d5f253742693d312537442535452537426b25374425354370695f694e2878253743755f692c25354373756d5f6929"><img src="https://camo.githubusercontent.com/4055f35cd39fe2e095b0fc70f8cccb1e3cab924a2ebd6bc56ca7bdcf026c79ec/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f702878293d25354373756d5f253742693d312537442535452537426b25374425354370695f694e2878253743755f692c25354373756d5f6929" alt="img"></a></p>
<p>通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列 数据点，给出一个类别的数量K后，希望求得最佳的K个高斯分模型。因此，<strong>高斯混合模型的计算，便成了最佳的均值μ，方差Σ、权重π的寻找</strong>，这类问题通常通过最大似然估计来求解。遗憾的是，此问题中直接使用最大似然估计，得到的是一个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。</p>
<p><strong>在这种情况下，可以用EM算法。 </strong>EM算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。具体到高 斯混合模型的求解，EM算法的迭代过程如下。</p>
<p>首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。</p>
<ul>
<li>E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。</li>
<li>M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。</li>
</ul>
<blockquote>
<p>  高斯混合模型是一个生成式模型。可以这样理解数据的生成过程，假设一个最简单的情况，即只有两个一维标准高斯分布的分模型<em>N</em>(0,1)和<em>N</em>(5,1)，其权重分别为0.7和0.3。那么，在生成第一个数据点时，先按照权重的比例，随机选择一个分布，比如选择第一个高斯分布，接着从<em>N</em>(0,1)中生成一个点，如−0.5，便是第一个数据点。在生成第二个数据点时，随机选择到第二个高斯分布<em>N</em>(5,1)，生成了第二个点4.7。如此循环执行，便生成出了所有的数据点。</p>
</blockquote>
<p>也就是说，我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个 数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不 变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。</p>
<h3 id="4-2-GMM与K-Means相比"><a href="#4-2-GMM与K-Means相比" class="headerlink" title="4.2 GMM与K-Means相比"></a>4.2 GMM与K-Means相比</h3><p>高斯混合模型与K均值算法的相同点是：</p>
<ul>
<li><strong>都需要指定K值</strong>；</li>
<li><strong>都是使用EM算法来求解</strong>；</li>
<li>都往往只能收敛于局部最优。</li>
</ul>
<p>而它相比于K 均值算法的优点是，可以给出一个样本属于某类的<strong>概率</strong>是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；<strong>并且可以用于生成新的样本点</strong>。</p>
<h3 id="五、聚类算法如何评估"><a href="#五、聚类算法如何评估" class="headerlink" title="五、聚类算法如何评估"></a>五、聚类算法如何评估</h3><p>由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。例 如，K均值聚类可以用<strong>误差平方</strong>和来评估，但是基于密度的数据簇可能不是球形， 误差平方和则会失效。在许多情况下，判断聚类算法结果的好坏强烈依赖于主观解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。</p>
<p>聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结 果的质量。这一过程又分为三个子任务。</p>
<ol>
<li><p><strong>估计聚类趋势。</strong></p>
<p>这一步骤是检测数据分布中是否存在非随机的簇结构。如果数据是基本随机 的，那么聚类的结果也是毫无意义的。我们可以观察聚类误差是否随聚类类别数 量的增加而单调变化，如果数据是基本随机的，即不存在非随机簇结构，那么聚 类误差随聚类类别数量增加而变化的幅度应该较不显著，并且也找不到一个合适 的K对应数据的真实簇数。</p>
</li>
<li><p><strong>判定数据簇数。</strong></p>
<p>确定聚类趋势之后，我们需要找到与真实数据分布最为吻合的簇数，据此判定聚类结果的质量。数据簇数的判定方法有很多，例如<strong>手肘法</strong>和<strong>Gap Statistic</strong>方 法。需要说明的是，用于评估的最佳数据簇数可能与程序输出的簇数是不同的。 例如，有些聚类算法可以自动地确定数据的簇数，但可能与我们通过其他方法确定的最优数据簇数有所差别。</p>
</li>
<li><p><strong>测定聚类质量。</strong></p>
<p>在无监督的情况下，我们可以通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。定义评估指标可以展现面试者实际解决和分析问题的能力。事实上测量指标可以有很多种，以下列出了几种常用的度量指标，更多的指标可以阅读相关文献。</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8815%EF%BC%89%E8%81%9A%E7%B1%BB*-DBSCAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="本博客主要用于记录个人学习笔记">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8815%EF%BC%89%E8%81%9A%E7%B1%BB*-DBSCAN/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-14 22:05:42" itemprop="dateModified" datetime="2023-03-14T22:05:42+08:00">2023-03-14</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="二、DBSCAN算法【基于密度】"><a href="#二、DBSCAN算法【基于密度】" class="headerlink" title="二、DBSCAN算法【基于密度】"></a>二、DBSCAN算法【基于密度】</h2><blockquote>
<p>  （3）聚类算法之DBSCAN算法 - GISer.Wang的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77043965">https://zhuanlan.zhihu.com/p/77043965</a></p>
</blockquote>
<p>密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。其代表算法为<strong>DBSCAN算法</strong>和<strong>密度最大值</strong>算法。</p>
<h3 id="2-1-DBSCAN算法原理"><a href="#2-1-DBSCAN算法原理" class="headerlink" title="2.1 DBSCAN算法原理"></a>2.1 DBSCAN算法原理</h3><p><strong><font color="red"> DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。</font></strong>与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并<strong>可在有“噪声”的数据中发现任意形状的聚类</strong>。</p>
<h3 id="2-2-若干概念"><a href="#2-2-若干概念" class="headerlink" title="2.2 若干概念"></a>2.2 若干概念</h3><p><strong>DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数 <img src="https://www.zhihu.com/equation?tex=%28%CF%B5%2C+MinPts%29" alt="[公式]"> 用来描述邻域的样本分布紧密程度</strong>。其中， <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 描述了某一数据点的<strong>邻域距离阈值（半径）</strong>， <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 描述了数据点<strong>半径为</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> <strong>的邻域</strong>中数据点个数的最小个数。下面是与密度聚类相关的定义（假设我的样本集是 <img src="https://www.zhihu.com/equation?tex=D%3D%5C%7Bx_1%2Cx_2%2C...%2Cx_m%5C%7D" alt="[公式]"> )：</p>
<ul>
<li><p><strong>对象的ε领域</strong>：给定对象在半径<strong>ε</strong>内的区域；对于 <img src="https://www.zhihu.com/equation?tex=x_j%E2%88%88D" alt="[公式]"> ，其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域包含样本集 <img src="https://www.zhihu.com/equation?tex=D" alt="[公式]"> 中与 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 的距离不大于 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 的子样本集。即 <img src="https://www.zhihu.com/equation?tex=N_%CF%B5%28x_j%29%3D%5C%7Bx_i%E2%88%88D%7Cdistance%28x_i%2Cx_j%29%E2%89%A4%CF%B5%5C%7D" alt="[公式]"> , 这个子样本集的个数记为 <img src="https://www.zhihu.com/equation?tex=%7CN_%CF%B5%28x_j%29%7C" alt="[公式]"> 。 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域是一个集合</p>
</li>
<li><p><strong>核心对象</strong>：对于任一样本 <img src="https://www.zhihu.com/equation?tex=x_j%E2%88%88D" alt="[公式]"> ，如果其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域对应的 <img src="https://www.zhihu.com/equation?tex=N_%CF%B5%28x_j%29" alt="[公式]"> 至少包含 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 个样本，即如果 <img src="https://www.zhihu.com/equation?tex=+%7CN_%CF%B5%28x_j%29%7C%E2%89%A5MinPts" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 是核心对象。</p>
</li>
<li><p><strong>直接密度可达</strong>：如果 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 位于 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域中，且 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 是核心对象，则称 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 密度直达。反之不一定成立，即此时不能说 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 密度直达, 除非 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 也是核心对象，<strong>即密度直达不满足对称性</strong>。如图ε=1,m=5，q是一个核心对象，从对象q出发到对象p是<strong>直接密度可达</strong>的。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afb2323b988730.jpg" alt="2019-05-18-061126.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>密度可达</strong>：对于 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> ,如果存在样本样本序列 <img src="https://www.zhihu.com/equation?tex=p_1%2Cp_2%2C...%2Cp_T" alt="[公式]"> ,满足 <img src="https://www.zhihu.com/equation?tex=p1%3Dx_i%2Cp_T%3Dx_j" alt="[公式]"> , 且 <img src="https://www.zhihu.com/equation?tex=p_%7Bt%2B1%7D" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=p_t" alt="[公式]"> 密度直达，则称 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本 <img src="https://www.zhihu.com/equation?tex=p_1%2Cp_2%2C...%2Cp_%7BT%E2%88%921%7D" alt="[公式]"><strong>均为核心对象</strong>，因为只有核心对象才能使其他样本密度直达。<strong>密度可达也不满足对称性</strong>，这个可以由密度直达的不对称性得出。</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afbfa514735525.jpg" alt="2019-05-18-061154.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>密度相连</strong>：对于 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> ,如果存在核心对象样本 <img src="https://www.zhihu.com/equation?tex=x_k" alt="[公式]"> ，使<strong><img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 均由 <img src="https://www.zhihu.com/equation?tex=x_k" alt="[公式]"> 密度可达</strong>，则称 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 密度相连。<strong>密度相连关系满足对称性</strong>。</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afd32fc7340149.jpg" alt="2019-05-18-061202.jpg" style="zoom:50%;"></p>
<ul>
<li><p><strong>==簇：一个基于密度的簇是最大的密度相连对象的集合。==</strong></p>
</li>
<li><p><strong>噪声</strong>：不包含在任何簇中的对象称为噪声。</p>
</li>
</ul>
<p>从下图可以很容易看出理解上述定义，图中 <img src="https://www.zhihu.com/equation?tex=MinPts%3D5" alt="[公式]"> ，红色的点都是核心对象，因为其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域至少有 <img src="https://www.zhihu.com/equation?tex=5" alt="[公式]"> 个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的圆内，如果不在圆内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列，此序列是一个簇集。在这些密度可达的样本序列的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域内所有的样本相互都是密度相连的 <strong>(注意，此图中有两个簇集)</strong>。</p>
<p><img src="https://pic2.zhimg.com/80/v2-7d15fc871942e0287be42a12d6d615dd_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h3 id="2-3-DBSCAN密度聚类思想"><a href="#2-3-DBSCAN密度聚类思想" class="headerlink" title="2.3 DBSCAN密度聚类思想"></a>2.3 DBSCAN密度聚类思想</h3><p><strong>DBSCAN的聚类定义很简单</strong>： <strong>由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。（注意是密度相连的集合）</strong>，簇里面可以有一个或者多个核心对象。<strong>如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> <strong>-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -<strong>邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达</strong>。这些核心对象的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域里所有的样本的集合组成的一个DBSCAN聚类簇。</p>
<p>那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够<strong>密度可达</strong>的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找<strong>密度可达</strong>的样本集合，这样就得到另一个聚类簇 <strong>（这样的得到都肯定是密度相连的）</strong>。一直运行到<strong>所有核心对象都有类别为止。</strong></p>
<p>基本上这就是DBSCAN算法的主要内容了，是不是很简单？<strong>但是我们还是有三个问题没有考虑。</strong></p>
<ul>
<li><strong>异常点问题：</strong>一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。</li>
<li><strong>距离度量问题</strong>：<strong><font color="red"> 即如何计算某样本和核心对象样本的距离</font></strong>。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量<strong>样本距离，比如欧式距离、曼哈顿距离</strong>等。</li>
<li><strong>数据点优先级分配问题</strong>：例如某些样本可能到两个核心对象的距离都小于 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，<strong>此时 DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说DBSCAN的算法不是完全稳定的算法。</strong></li>
</ul>
<h3 id="2-4-算法步骤"><a href="#2-4-算法步骤" class="headerlink" title="2.4 算法步骤"></a>2.4 算法步骤</h3><p><strong>输入：样本集 <img src="https://www.zhihu.com/equation?tex=D%3D%5C%7Bx_1%2Cx_2%2C...%2Cx_m%5C%7D" alt="[公式]"> ，邻域参数 <img src="https://www.zhihu.com/equation?tex=%28%CF%B5%2CMinPts%29" alt="[公式]"></strong></p>
<ol>
<li>初始化核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9%3D%E2%88%85%2C" alt="[公式]"> 初始化类别 <img src="https://www.zhihu.com/equation?tex=k%3D0" alt="[公式]"></li>
<li>遍历 <img src="https://www.zhihu.com/equation?tex=D" alt="[公式]"> 的元素，如果是核心对象，则将其加入到核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中</li>
<li>如果核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中元素都已经被<strong>访问</strong>，<strong>则算法结束</strong>，<strong>否则转入步骤4</strong>.</li>
<li>在核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中，随机选择一个<strong>未访问</strong>的核心对象 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> ，首先将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 标记为<strong>已访问</strong>，然后将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 标记类别 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> ，最后将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域中<strong>未访问</strong>的数据，存放到种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds" alt="[公式]"> 中。</li>
<li>如果种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds%3D%E2%88%85" alt="[公式]"> ，则当前聚类簇 <img src="https://www.zhihu.com/equation?tex=C_k" alt="[公式]"> 生成完毕, 且 <img src="https://www.zhihu.com/equation?tex=k%3Dk%2B1" alt="[公式]"> ，<strong>跳转到3</strong>。否则，从种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds" alt="[公式]"> 中挑选一个种子点 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> ，首先将其标记为已访问、标记类别 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> ，然后判断 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> 是否为核心对象，如果是将 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> 中<strong>未访问</strong>的种子点加入到种子集合中，<strong>跳转到5</strong>。</li>
</ol>
<p><strong>从上述算法可知：</strong></p>
<ul>
<li><strong>每个簇至少包含一个核心对象</strong>；</li>
<li>非核心对象可以是簇的一部分，构成了簇的边缘（edge）；</li>
<li>包含过少对象的簇被认为是噪声；</li>
</ul>
<h3 id="2-5-总结"><a href="#2-5-总结" class="headerlink" title="2.5 总结"></a>2.5 总结</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li><strong>可以对任意形状的稠密数据集进行聚类</strong>，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li><strong>可以在聚类的同时发现异常点</strong>，对数据集中的异常点不敏感。</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li><strong>不能处理密度差异过大（密度不均匀）的聚类</strong>：如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
<li>如果样本集较大时，聚类收敛时间较长;<strong>此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进；</strong></li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂，<strong>主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响</strong>。【OPTICS算法】</li>
<li><strong>边界点不完全确定性</strong></li>
</ol>
<h3 id="2-6-OPTICS算法"><a href="#2-6-OPTICS算法" class="headerlink" title="2.6 OPTICS算法:"></a>2.6 OPTICS算法:</h3><p><strong>OPTICS主要针对输入参数$ϵ$过敏感做的改进</strong>，OPTICS和DBSCNA的输入参数一样（ <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> ），虽然OPTICS算法中也需要两个输入参数，但该算法对 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 输入不敏感（一般将 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 固定为无穷大），同时该算法中并不显式的生成数据聚类，只是对数据集合中的对象进行排序，得到一个有序的对象列表，通过该有序列表，可以得到一个决策图，通过决策图可以不同 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 参数的数据集中检测簇集，即：<strong>先通过固定的 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 和无穷大的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 得到有序列表，然后得到决策图，通过决策图可以知道当 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 取特定值时（比如 <img src="https://www.zhihu.com/equation?tex=%CF%B5%3D3" alt="[公式]"> )数据的聚类情况。</strong></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
