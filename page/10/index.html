<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/10/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/10/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/10/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">257</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">52</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1HFEDWZ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1HFEDWZ/" class="post-title-link" itemprop="url">模型训练（5）Batch Normalization</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-09 11:34:59" itemprop="dateCreated datePublished" datetime="2022-06-09T11:34:59+08:00">2022-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-05-01 18:27:22" itemprop="dateModified" datetime="2023-05-01T18:27:22+08:00">2023-05-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>22 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="李宏毅课程笔记quickintroduction-of-batch-normalization">李宏毅课程笔记：Quick
Introduction of Batch Normalization</span></h3>
<p>本篇是一个很快地介绍,Batch Normalization 这个技术</p>
<h3><span id="一-changinglandscape不变化的景观">一、Changing
Landscape（不变化的景观）</span></h3>
<p>之前才讲过说,我们能不能够直接改error surface 的 landscape,我们觉得说
error surface 如果很崎嶇的时候,它比较难
train,那我们能不能够直接把山剷平,让它变得比较好 train 呢？</p>
<p><strong><font color="red">Batch Normalization</font></strong>
就是其中一个,<strong>把山剷平的想法</strong>。我们一开始就跟大家讲说,不要小看
optimization 这个问题,有时候就算你的 error surface 是
convex的,它就是一个碗的形状,都不见得很好
train。假设你的两个参数啊,它们对 <strong>Loss
的斜率差别非常大</strong>,在 <span class="math inline">\(w_1\)</span>
这个方向上面,你的斜率变化很小,在 <span class="math inline">\(w_2\)</span> 这个方向上面斜率变化很大。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011440832.png" alt="image-20220616212321383">
<figcaption aria-hidden="true">image-20220616212321383</figcaption>
</figure>
<p>如果是<strong>固定的 learning
rate</strong>,你可能很难得到好的结果,所以我们才说你需要adaptive 的
learning rate、 Adam 等等比较进阶的 optimization
的方法,才能够得到好的结果。</p>
<p>现在我们要从另外一个方向想,<strong>直接把难做的 error surface
把它改掉</strong>,看能不能够改得好做一点。在做这件事之前,也许我们第一个要问的问题就是,有这一种状况,$w_1
$ 跟 <span class="math inline">\(w_2\)</span>
它们的<strong>斜率差很多</strong>的这种状况,到底是从什麼地方来的。</p>
<p>假设我现在有一个非常非常非常简单的 model,它的输入是 <span class="math inline">\(x_1\)</span> 跟 <span class="math inline">\(x_2\)</span>,它对应的参数就是 $ w_1 $ 跟 <span class="math inline">\(w_2\)</span>,它是一个 linear 的 model,没有
activation function。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011440514.png" alt="image-20220616212341359" style="zoom:67%;"></p>
<p>$ w_1 $ 乘 <span class="math inline">\(x_1\)</span>,<span class="math inline">\(w_2\)</span> 乘 <span class="math inline">\(x_2\)</span> 加上 b 以后就得到 y,然后会计算 y 跟
<span class="math inline">\(\hat{y}\)</span> 之间的差距当做 e,把所有
training data e 加起来就是你的 Loss，然后去 minimize 你的
Loss，那什麼样的状况我们会產生像上面这样子,<strong>比较不好 train 的
error surface</strong> 呢？</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011440611.png" alt="image-20220616212421658" style="zoom:67%;"></p>
<p>当我们对 <strong>$ w_1 $ 有一个小小的改变</strong>,比如说加上 delta $
w_1 $ 的时候,那这个 L 也会有一个改变,那这个 $ w_1 $ 呢,是透过 $ w_1 $
改变的时候,你就改变了 y,y 改变的时候你就改变了
e,然后接下来就<strong>改变了 L</strong>。</p>
<p>那什麼时候 $ w_1 $ 的改变会对 L 的影响很小呢,也就是它在 error surface
上的斜率会很小呢？一个可能性是当你的 <strong>input
很小的时候</strong>,假设 <span class="math inline">\(x_1\)</span>
的值在不同的 training example 裡面,它的值都很小,那因為 <span class="math inline">\(x_1\)</span> 是直接乘上 $ w_1 $，如果 <span class="math inline">\(x_1\)</span> 的值都很小,$ w_1 $
有一个变化的时候,它得到的,它<strong>对 y 的影响也是小的</strong>,对 e
的影响也是小的,它对 L 的影响就会是小的。反之呢,如果今天是 <span class="math inline">\(x_2\)</span> 的话。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011440761.png" alt="image-20220616212503953" style="zoom:67%;"></p>
<p>那假设 <strong><span class="math inline">\(x_2\)</span>
的值都很大</strong>,当你的 <span class="math inline">\(w_2\)</span>
有一个小小的变化的时候,虽然 <span class="math inline">\(w_2\)</span>
这个变化可能很小,但是因為它乘上了 <span class="math inline">\(x_2\)</span>,<span class="math inline">\(x_2\)</span> 的值很大,那 y 的变化就很大,那 e
的变化就很大,那 L 的变化就会很大,就会导致我们在 w
这个方向上,做变化的时候,我们把 w 改变一点点,那我们的 error surface
就会有很大的变化。</p>
<p>所以你发现说,既然在这个 linear 的 model 裡面,当我们 input 的
feature,<strong>每一个 dimension 的值,它的 scale
差距很大</strong>的时候,我们就可能產生像这样子的 error
surface,就可能產生<strong>不同方向,斜率非常不同,坡度非常不同的 error
surface</strong>。</p>
<p>所以怎麼办呢,我们有没有可能给feature 裡面<strong>不同的
dimension,让它有同样的数值的范围</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011441571.png" alt="image-20220616212559236" style="zoom:67%;"></p>
<p>如果我们可以给不同的
dimension,同样的数值范围的话,那我们可能就可以製造比较好的 error
surface,让 training 变得比较容易一点</p>
<p>其实有很多不同的方法,这些不同的方法,往往就合起来统称為Feature
Normalization</p>
<h3><span id="二-feature-normalization">二、Feature Normalization</span></h3>
<p>以下所讲的方法只是Feature Normalization 的一种可能性,它<strong>并不是
Feature Normalization 的全部</strong>,假设 <span class="math inline">\(x^1\)</span> 到 <span class="math inline">\(x^R\)</span>,是我们所有的训练资料的 feature
vector</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011441532.png" alt="image-20220616221138101" style="zoom:67%;"></p>
<p>我们把所有训练资料的 feature vector ,统统都集合起来,那每一个 vector
,<span class="math inline">\(x_1\)</span> 裡面就 $x^1_1 $代表 <span class="math inline">\(x_1\)</span> 的第一个 element,$x^2_1 $,就代表
<span class="math inline">\(x_2\)</span> 的第一个 element,以此类推</p>
<p>那我们把<strong>不同笔资料即不同 feature vector,同一个
dimension</strong> 裡面的数值,把它取出来,然后去计算某一个 dimension 的
mean，它的 mean 呢 就是<span class="math inline">\(m_i\)</span>，我们计算第 i 个 dimension
的,standard deviation,我们用<span class="math inline">\(\sigma_i\)</span>来表示它</p>
<p>那接下来我们就可以做一种 normalization,那这种 normalization
其实叫做<strong>标準化</strong>,其实叫
standardization,不过我们这边呢,就等一下都统称 normalization 就好了 <span class="math display">\[
\tilde{x}^r_i ← \frac{x^r_i-m_i}{\sigma_i}
\]</span> 我们就是把这边的某一个数值x,减掉这一个 dimension 算出来的
mean,再除掉这个 dimension,算出来的 standard deviation,得到新的数值叫做
<span class="math inline">\(\tilde{x}\)</span></p>
<p>然后得到新的数值以后,<strong>再把新的数值把它塞回去</strong>,以下都用这个
tilde来代表有被 normalize 后的数值</p>
<p>那做完 normalize 以后有什麼好处呢？</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011441247.png" alt="image-20220616221152221" style="zoom:67%;"></p>
<ul>
<li><p>做完 normalize 以后啊,这个 dimension 上面的数值就会平均是
0,然后它的 variance就会是 1,所以<strong>这一排数值的分布就都会在 0
上下</strong></p></li>
<li><p>对每一个 dimension都做一样的 normalization,就会发现所有 feature
不同 dimension 的数值都在 0 上下,那你可能就可以<strong>製造一个,比较好的
error surface</strong></p></li>
</ul>
<p>所以像这样子 Feature Normalization 的方式,往往对你的 training
有帮助,它可以让你在做 gradient descent 的时候,这个 gradient
descent,<strong>它的 Loss 收敛更快一点,可以让你的 gradient
descent,它的训练更顺利一点</strong>,这个是 Feature Normalization</p>
<h3><span id="三-considering-deep-learning">三、Considering Deep Learning</span></h3>
<p><span class="math inline">\(\tilde{x}\)</span> 代表 normalize 的
feature,把它丢到 deep network 裡面,去做接下来的计算和训练,所以把 <span class="math inline">\(x_1\)</span> tilde 通过第一个 layer 得到 <span class="math inline">\(z^1\)</span>,那你有可能通过 activation
function,不管是选 Sigmoid 或者 ReLU 都可以,然后再得到 <span class="math inline">\(a^1\)</span>,然后再通过下一层等等,那就看你有几层
network 你就做多少的运算</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011441807.png" alt="image-20220616221202386" style="zoom:67%;"></p>
<p>所以每一个 x 都做类似的事情,但是如果我们进一步来想的话,对 <span class="math inline">\(w_2\)</span> 来说</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011441112.png" alt="image-20220616221211793" style="zoom:67%;"></p>
<p>这边的 <span class="math inline">\(a^1\)</span> <span class="math inline">\(a^3\)</span> 这边的 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^3\)</span>,其实也是另外一种 input,如果这边 <span class="math inline">\(\tilde{x}\)</span>,虽然它已经做 normalize
了,但是通过 $ w_1 $ 以后它就<strong>没有做 normalize</strong>,如果 <span class="math inline">\(\tilde{x}\)</span> 通过 $ w_1 $ 得到是 <span class="math inline">\(z^1\)</span>,而 <span class="math inline">\(z^1\)</span> 不同的 dimension
间,它的数值的分布仍然有很大的差异的话,那我们要 train <span class="math inline">\(w_2\)</span> 第二层的参数,会不会也有困难呢</p>
<p>对 <span class="math inline">\(w_2\)</span> 来说,这边的 a 或这边的 z
其实也是一种 feature,我们应该要对这些 feature 也做 normalization</p>
<p>那如果你选择的是 Sigmoid,那可能比较推荐对 z 做 Feature
Normalization,因為Sigmoid 是一个 s 的形状,那它在 0
附近斜率比较大,所以如果你对 z 做 Feature Normalization,把所有的值都挪到
0 附近,那你到时候算 gradient 的时候,算出来的值会比较大</p>
<p>那不过因為你不见得是用 sigmoid ,所以你也不一定要把 Feature
Normalization放在 z
这个地方,如果是选别的,也许你选a也会有好的结果,也说不定，<strong>Ingeneral
而言,这个 normalization,要放在 activation function
之前,或之后都是可以的,在实作上,可能没有太大的差别</strong>,好
那我们这边呢,就是对 z 呢,做一下 Feature Normalization，</p>
<p>那怎麼对 z 做 Feature Normalization 呢</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011441486.png" alt="image-20220616221239128" style="zoom:67%;"></p>
<p>那你就把 z,想成是另外一种 feature ,我们这边有 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,我们就把 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span> 拿出来</p>
<ul>
<li><strong>算一下它的 mean</strong>，这边的 <span class="math inline">\(μ\)</span> 是一个 vector,我们就把 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,这三个 vector 呢,把它平均起来,得到
<span class="math inline">\(μ\)</span> 这个 vector</li>
<li><strong>算一个 standard deviation</strong>,这个 standard deviation
呢,这边这个成 <span class="math inline">\(\sigma\)</span>,它也代表了一个
vector,那这个 vector 怎麼算出来呢,你就把 <span class="math inline">\(z^i\)</span>减掉 <span class="math inline">\(μ\)</span>,然后取平方,这边的平方,这个 notation
有点 abuse 啊,这边的平方就是指,对每一个 element
都去做平方,然后再开根号,这边开根号指的是对每一个
element,向量裡面的每一个 element,都去做开根号,得到 <span class="math inline">\(\sigma\)</span>,反正你知道我的意思就好</li>
</ul>
<p>把这三个 vector,裡面的每一个 dimension,都去把它的 <span class="math inline">\(μ\)</span> 算出来,把它的 <span class="math inline">\(\sigma\)</span> 算出来,好
我这边呢,就不把那些箭头呢 画出来了,从 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,算出 <span class="math inline">\(μ\)</span>,算出 <span class="math inline">\(\sigma\)</span>。</p>
<p>接下来就把这边的每一个 z ,都去减掉 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,你把 <span class="math inline">\(z^i\)</span>减掉 <span class="math inline">\(μ\)</span>,除以 <span class="math inline">\(\sigma\)</span>,就得到 <span class="math inline">\(z^i\)</span>的 tilde。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011441288.png" alt="image-20220616221403703" style="zoom:50%;"></p>
<p>那这边的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,它都是<strong>向量</strong>,所以这边这个除的意思是<strong>element
wise 的相除</strong>,就是 <span class="math inline">\(z^i\)</span>减
<span class="math inline">\(μ\)</span>,它是一个向量,所以分子的地方是一个向量,分母的地方也是一个向量,把这个两个向量,它们对应的
element 的值相除,是我这边这个除号的意思,这边得到 Z 的 tilde。</p>
<p>所以我们就是把 <span class="math inline">\(z^1\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^1\)</span> tilde,同理 <span class="math inline">\(z^2\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^2\)</span> tilde,<span class="math inline">\(z^3\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^3\)</span> tilde,那就把这个 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,做 Feature Normalization,变成 <span class="math inline">\(z^1\)</span> tilde,<span class="math inline">\(z^2\)</span> tilde 跟 <span class="math inline">\(z^3\)</span> 的 tilde。</p>
<p>接下来就看你爱做什麼 就做什麼啦,通过 activation function,得到其他
vector,然后再通过,再去通过其他 layer 等等,这样就可以了,这样你就等於对
<span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,做了 Feature Normalization,变成 <span class="math inline">\(\tilde{z}^1\)</span> <span class="math inline">\(\tilde{z}^2\)</span> <span class="math inline">\(\tilde{z}^3\)</span> 。</p>
<p>在这边有一件有趣的事情,这边的 <span class="math inline">\(μ\)</span>
跟 <span class="math inline">\(\sigma\)</span>,它们其实都是根据 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span> 算出来的。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011442381.png" alt="image-20220616221515889" style="zoom: 67%;"></p>
<p>所以这边 <span class="math inline">\(z^1\)</span>
啊,它本来,如果我们没有做 Feature Normalization 的时候,你改变了 <span class="math inline">\(z^1\)</span> 的值,你会改变这边 a
的值,但是现在啊,当你改变 <span class="math inline">\(z^1\)</span>
的值的时候,<span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 也会跟著改变,<span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 改变以后,<span class="math inline">\(z^2\)</span> 的值 <span class="math inline">\(a^2\)</span> 的值,<span class="math inline">\(z^3\)</span> 的值 <span class="math inline">\(a^3\)</span> 的值,也会跟著改变。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011442075.png" alt="image-20220616221555203" style="zoom: 67%;"></p>
<p>所以<strong>之前</strong>,我们每一个 <span class="math inline">\(\tilde{x}_1\)</span> <span class="math inline">\(\tilde{x}_2\)</span> <span class="math inline">\(\tilde{x}_3\)</span>,它是<strong>独立分开处理的</strong>,但是我们在做
<strong>Feature Normalization 以后</strong>,这三个
example,它们变得<strong>彼此关联</strong>了。</p>
<p>我们这边 <span class="math inline">\(z^1\)</span> 只要有改变,接下来
<span class="math inline">\(z^2\)</span> <span class="math inline">\(a^2\)</span> <span class="math inline">\(z^3\)</span> <span class="math inline">\(a^3\)</span>,也都会跟著改变,所以这边啊,其实你要把,当你有做
Feature Normalization 的时候,你要把这一整个 process,就是有收集一堆
feature,把这堆 feature 算出 <span class="math inline">\(μ\)</span> 跟
<span class="math inline">\(\sigma\)</span> 这件事情,当做是 network
的一部分。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011442417.png" alt="image-20220616221637917" style="zoom:67%;"></p>
<p>也就是说,你现在有一个比较大的 network</p>
<ul>
<li>你之前的 network,都只吃一个 input,得到一个 output</li>
<li>现在你有一个比较大的 network,这个大的 network,它是吃一堆
input,用这堆 input 在这个 network 裡面,要算出 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,然后接下来產生一堆 output</li>
</ul>
<p>那这个地方比较抽象,只可会意 不可言传这样子</p>
<p>那这边就会有一个问题了,因為你的训练资料裡面的 data 非常多,现在一个
data set,benchmark corpus 都上百万笔资料， GPU 的
memory,根本没有办法,把它整个 data set 的 data 都 load 进去。</p>
<p><strong><font color="red"> 在实作的时候,你不会让这一个 network
考虑整个 training data 裡面的所有 example,你只会考虑一个 batch 裡面的
example</font></strong>,举例来说,你 batch 设 64,那你这个巨大的
network,就是把 64 笔 data 读进去,算这 64 笔 data 的 <span class="math inline">\(μ\)</span>,算这 64 笔 data 的 <span class="math inline">\(\sigma\)</span>,对这 64 笔 data 都去做
normalization</p>
<p>因為我们在实作的时候,我们只对一个 batch 裡面的 data,做
normalization,所以这招叫做 <strong>Batch Normalization</strong></p>
<p>那这个 Batch Normalization,显然有一个问题
就是,<strong>你一定要有一个够大的 batch,你才算得出 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span></strong>,假设你今天,你 batch size
设 1,那你就没有什麼 <span class="math inline">\(μ\)</span> 或 <span class="math inline">\(\sigma\)</span> 可以算</p>
<p>所以这个 Batch Normalization,是适用於 batch size 比较大的时候,因為
batch size 如果比较大,<strong>也许这个 batch size 裡面的
data,就足以表示,整个 corpus
的分布</strong>,那这个时候你就可以,把这个本来要对整个 corpus,做 Feature
Normalization 这件事情,改成只在一个 batch,做 Feature Normalization,作為
approximation。</p>
<p><strong>在做 Batch Normalization
的时候,往往还会有这样的设计你算出这个 <span class="math inline">\(\tilde{z}\)</span> 以后</strong></p>
<ul>
<li><strong><font color="red"> 接下来你会把这个 <span class="math inline">\(\tilde{z}\)</span>,再乘上另外一个向量叫做 <span class="math inline">\(γ\)</span>,这个 <span class="math inline">\(γ\)</span> 也是一个向量,所以你就是把 <span class="math inline">\(\tilde{z}\)</span> 跟 <span class="math inline">\(γ\)</span> 做 element wise 的相乘,把 z
这个向量裡面的 element,跟 <span class="math inline">\(γ\)</span>
这个向量裡面的 element,两两做相乘。</font></strong></li>
<li><strong><font color="red"> 再加上 <span class="math inline">\(β\)</span> 这个向量,得到 <span class="math inline">\(\hat{z}\)</span>。</font></strong></li>
</ul>
<p><strong>而 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span>,你要把它想成是 network
的参数,它是另外再被learn出来的,</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011442134.png" alt="image-20220616222053826" style="zoom:50%;"></p>
<h5><span id="那為什麼要加上-β-跟-γ-呢">那為什麼要加上 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span> 呢？</span></h5>
<p>有人可能会觉得说,如果我们做 normalization 以后,那这边的 <span class="math inline">\(\tilde{z}\)</span>,它的平均就一定是
0,那也许,<strong>今天如果平均是 0 的话,就是给那 network
一些限制</strong>,那<strong>也许这个限制会带来什麼负面的影响</strong>,所以我们把
<span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span> 加回去。</p>
<p>然后让 network 呢,现在它的 hidden layer 的 output平均不是 0
的话,他就自己去learn这个 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span>,来调整一下输出的分布,来调整这个 <span class="math inline">\(\hat{z}\)</span> 的分布</p>
<p>但讲到这边又会有人问说,刚才不是说做 Batch Normalization
就是,為了要让每一个不同的 dimension,它的 range
都是一样吗,现在如果加去乘上 <span class="math inline">\(γ\)</span>,再加上 <span class="math inline">\(β\)</span>,把 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 加进去,</p>
<h5><span id="这样不会不同dimension-的分布它的-range-又都不一样了吗">这样不会不同
dimension 的分布,它的 range 又都不一样了吗？</span></h5>
<p>有可能,但是你实际上在训练的时候,这个 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 的初始值啊</p>
<ul>
<li><strong>你会把这个 <span class="math inline">\(γ\)</span> 的初始值
就都设為 1,所以 <span class="math inline">\(γ\)</span>
是一个裡面的值,一开始其实是一个裡面的值,全部都是 1 的向量</strong></li>
<li><strong>那 <span class="math inline">\(β\)</span>
是一个裡面的值,全部都是 0 的向量,所以 <span class="math inline">\(γ\)</span> 是一个 one vector,都是 1 的向量,<span class="math inline">\(β\)</span> 是一个 zero vector,裡面的值都是 0
的向量</strong></li>
</ul>
<p>所以让你的 network 在一开始训练的时候,每一个 dimension
的分布,是比较接近的,也许训练到后来,你已经训练够长的一段时间,已经找到一个比较好的
error surface,走到一个比较好的地方以后,那再把 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 慢慢地加进去,好所以加 Batch
Normalization,往往对你的训练是有帮助的。</p>
<h3><span id="四-testing">四、Testing</span></h3>
<h5><span id="这个batch-normalization-在-inference或是-testing的时候会有什麼样的问题呢">这个
Batch Normalization 在 inference,或是 testing
的时候,会有什麼样的问题呢？</span></h5>
<p>在 testing 的时候,如果 当然如果今天你是在做作业,我们一次会把所有的
testing 的资料给你,所以你确实也可以在 testing 的资料上面,製造一个一个
batch。</p>
<p><strong>但是假设你真的有系统上线,你是一个真正的线上的
application,你可以说,我今天一定要等 30,比如说你的 batch size 设
64,我一定要等 64
笔资料都进来,我才一次做运算吗,这显然是不行的。</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011442171.png" alt="image-20220616222338443" style="zoom: 67%;"></p>
<p>但是在做 Batch Normalization 的时候,一个 <span class="math inline">\(\tilde{x}\)</span>,一个 normalization 过的 feature
进来,然后你有一个 z,你的 z 呢,要减掉 <span class="math inline">\(μ\)</span> 跟除 <span class="math inline">\(\sigma\)</span>,那这个 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,是<strong>用一个 batch
的资料算出来的</strong></p>
<h5><span id="但如果今天在testing-的时候根本就没有-batch那我们要怎麼算这个-μ跟怎麼算这个-sigma-呢">但如果今天在
testing 的时候,根本就没有 batch,那我们要怎麼算这个 <span class="math inline">\(μ\)</span>,跟怎麼算这个 <span class="math inline">\(\sigma\)</span> 呢？</span></h5>
<p>所以真正的,这个实作上的解法是这个样子的,如果你看那个 PyTorch
的话呢,Batch Normalization 在 testing
的时候,你并不需要做什麼特别的处理,PyTorch 帮你处理好了</p>
<p><strong><font color="red"> 在 training 的时候,如果你有在做 Batch
Normalization 的话,在 training 的时候,你每一个 batch 计算出来的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,他都会拿出来算 moving
average</font></strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011442960.png" alt="image-20220616222438574" style="zoom:67%;"></p>
<p>你每一次取一个 batch 出来的时候,你就会算一个 <span class="math inline">\(μ^1\)</span>,取第二个 batch 出来的时候,你就算个
<span class="math inline">\(μ^2\)</span>,一直到取第 t 个 batch
出来的时候,你就算一个 <span class="math inline">\(μ^t\)</span>
。接下来你会算一个 moving average,你会把你现在算出来的 <span class="math inline">\(μ\)</span> 的一个平均值,叫做 <span class="math inline">\(μ\)</span> bar,乘上某一个
factor,那这也是一个常数,这个这也是一个 constant,这也是一个那个 hyper
parameter,也是需要调的。</p>
<p>在 PyTorch 裡面,我没记错 他就设 0.1,我记得他 P 就设 0.1,好,然后加上 1
减 P,乘上 <span class="math inline">\(μ^t\)</span> ,然后来更新你的 <span class="math inline">\(μ\)</span> 的平均值,然后最后在 testing
的时候,你就不用算 batch 裡面的 <span class="math inline">\(μ\)</span> 跟
<span class="math inline">\(\sigma\)</span> 了。</p>
<p>因為 testing 的时候,在真正 application 上,也没有 batch
这个东西,你就直接拿 <span class="math inline">\(\barμ\)</span> 跟 <span class="math inline">\(\bar\sigma\)</span> ,也就是 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 在训练的时候,得到的 moving
average,<span class="math inline">\(\barμ\)</span> 跟 <span class="math inline">\(\bar\sigma\)</span> ,来取代这边的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,这个就是 Batch Normalization,在
testing 的时候的运作方式。</p>
<h3><span id="五-comparison">五、Comparison</span></h3>
<p>好 那这个是从 Batch
Normalization,原始的文件上面截出来的一个实验结果,那在原始的文件上还讲了很多其他的东西,举例来说,我们今天还没有讲的是<strong>,Batch
Normalization 用在 CNN
上,要怎麼用呢</strong>,那你自己去读一下原始的文献,裡面会告诉你说,Batch
Normalization 如果用在 CNN 上,应该要长什麼样子。</p>
<blockquote>
<p><strong>卷积层上的BN使用</strong>，其实也是使用了<strong>类似权值共享的策略</strong>，<strong>把一整张特征图当做一个神经元进行处理</strong>。</p>
<p>卷积神经网络经过卷积后得到的是一系列的特征图，如果min-batch
sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch
sizes，f为特征图个数，p、q分别为特征图的宽高。</p>
<p>在cnn中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch
Normalization，mini-batch size 的大小就是：m * p *
q，于是对于每个特征图都只有一对可学习参数：γ、β。</p>
<p>相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。</p>
<ul>
<li><strong>nb在每一个特征图上的所有点沿着一个batch的样本数据的方向对数据进行求和，求平均等处理，不考虑不同特征图的数据间的运算。</strong></li>
<li><strong>lrb在每一个特征图上沿着不同特征图的方向对数据进行求和，求平均等处理，不考虑不同输入样本数据间的运算。</strong></li>
</ul>
</blockquote>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011442587.png" alt="image-20220616222534083">
<figcaption aria-hidden="true">image-20220616222534083</figcaption>
</figure>
<p>这个是原始文献上面截出来的一个数据</p>
<ul>
<li>横轴呢,代表的是训练的过程，纵轴代表的是 validation set 上面的
accuracy</li>
<li>那这个<strong>黑色</strong>的虚线是<strong>没有做 Batch
Normalization</strong> 的结果,它用的是 inception 的 network,就是某一种
network 架构啦,也是以 CNN 為基础的 network 架构</li>
<li>然后如果有做 Batch
Normalization,你会得到<strong>红色</strong>的这一条虚线,那你会发现说,红色这一条虚线,它<strong>训练的速度,显然比黑色的虚线还要快很多</strong>,虽然最后收敛的结果啊,就你只要给它足够的训练的时间,可能都跑到差不多的
accuracy,但是<strong>红色这一条虚线,可以在比较短的时间内,就跑到一样的
accuracy</strong>,那这边这个蓝色的菱形,代表说这几个点的那个 accuracy
是一样的</li>
<li><strong>粉红色</strong>的线是 sigmoid function,就 sigmoid function
一般的认知,我们虽然还没有讨论这件事啦,但一般都会选择 ReLu,而不是用
sigmoid function,因為 sigmoid function,它的 training
是比较困难的,但是这边想要强调的点是说,<strong>就算是 sigmoid
比较难搞的,加 Batch Normalization,还是 train 的起来</strong>,那这边没有
sigmoid,没有做 Batch Normalization
的结果,因為在这个实验上,作者有说,sigmoid 不加 Batch Normalization,根本连
train 都 train 不起来</li>
<li>蓝色的实线跟这个蓝色的虚线呢,是把 learning rate 设比较大一点,乘
5,就是 learning rate 变原来的 5 倍,然后乘 30,就是 learning rate 变原来的
30 倍,那因為<strong>如果你做 Batch Normalization 的话,那你的 error
surface 呢,会比较平滑
比较容易训练,所以你可以把你的比较不崎嶇,所以你就可以把你的 learning rate
呢,设大一点</strong></li>
</ul>
<h3><span id="六-internal-covariate-shift">六、Internal Covariate Shift?</span></h3>
<p><strong>好接下来的问题就是,Batch
Normalization,它為什麼会有帮助呢</strong>,在原始的 Batch
Normalization,那篇 paper 裡面,他提出来一个概念,叫做<strong>internal
covariate shift,covariate
shift</strong>(训练集和预测集样本分布不一致的问题就叫做“<em>covariate
shift</em>”现象) 这个词汇是原来就有的,internal covariate
shift,我认為是,Batch Normalization 的作者自己发明的。他认為说今天在
train network 的时候,会有以下这个问题,这个问题是这样。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011443501.png" alt="image-20220616225043073" style="zoom: 67%;"></p>
<p>network 有很多层</p>
<ul>
<li><p>x 通过第一层以后 得到 a</p></li>
<li><p>a 通过第二层以后 得到 b</p></li>
<li><p>计算出 gradient 以后,把 A update 成 A′,把 B 这一层的参数 update
成 B′</p></li>
</ul>
<p>但是作者认為说,我们在计算 B,update 到 B′ 的 gradient
的时候,这个时候前一层的参数是 A 啊,或者是前一层的 output 是小 a 啊</p>
<p>那当前一层从 A 变成 A′ 的时候,它的 output 就从小 a 变成小 a′ 啊</p>
<p>但是我们计算这个 gradient 的时候,我们是根据这个 a 算出来的啊,所以这个
update 的方向,也许它<strong>适合用在 a 上,但不适合用在 a′
上面</strong></p>
<p>那如果说 Batch Normalization 的话,我们会让,因為我们每次都有做
normalization,我们就会让 a 跟 a′
呢,它的分布比较接近,也许这样就会对训练呢,有帮助。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011443393.png" alt="image-20220616225149673" style="zoom:67%;"></p>
<p>但是有一篇 paper 叫做,How Does Batch Normalization,Help
Optimization,然后他就<strong>打脸了internal covariate shift
的这一个观点</strong>。</p>
<p>在这篇 paper 裡面,他从各式各样的面向来告诉你说,i<strong>nternal
covariate shift,首先它不一定是 training network 的时候的一个问题,然后
Batch Normalization,它会比较好,可能不见得是因為,它解决了 internal
covariate shift。</strong></p>
<p>那在这篇 paper
裡面呢,他做了很多很多的实验,比如说他比较了训练的时候,这个 a 的分布的变化
发现<strong>,不管有没有做 Batch
Normalization,它的变化都不大</strong>。</p>
<p>然后他又说,就算是变化很大,对 training
也没有太大的伤害,然后他又说,不管你是根据 a 算出来的 gradient,还是根据 a′
算出来的 gradient,方向居然都差不多。</p>
<p>所以他告诉你说,internal covariate shift,可能不是 training network
的时候,最主要的问题,它可能也不是,Batch Normalization
会好的一个的关键,那有关更多的实验,你就自己参见这篇文章。</p>
<h5><span id="為什麼-batchnormalization-会比较好呢">為什麼 Batch
Normalization 会比较好呢？</span></h5>
<p>那在这篇 How Does Batch Normalization,Help Optimization
这篇论文裡面,他从实验上,也从理论上,至少<strong>支持了 Batch
Normalization,可以改变 error surface,让 error surface
比较不崎嶇这个观点</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011443110.png" alt="image-20220616225308853" style="zoom:67%;"></p>
<p>所以这个观点是有理论的支持,也有实验的佐证的,那在这篇文章裡面呢,作者还讲了一个非常有趣的话,他说他觉得啊,这个
Batch Normalization 的 positive impact。</p>
<p>因為他说,如果我们要让 network,这个 error surface
变得比较不崎嶇,<strong>其实不见得要做 Batch
Normalization,感觉有很多其他的方法,都可以让 error surface
变得不崎嶇</strong>,那他就试了一些其他的方法,发现说,跟 Batch
Normalization performance
也差不多,甚至还稍微好一点,所以他就讲了下面这句感嘆。</p>
<p>他觉得说,这个,positive impact of batchnorm on training,可能是
somewhat,<strong>serendipitous</strong>,什麼是 serendipitous
呢,这个字眼可能可以翻译成偶然的,但偶然并没有完全表达这个词汇的意思,这个词汇的意思是说,你发现了一个什麼意料之外的东西。</p>
<p>那这篇文章的作者也觉得,Batch Normalization
也像是盘尼西林一样,是一种偶然的发现,但无论如何,它是一个有用的方法。</p>
<h3><span id="to-learn-more">To learn more ……</span></h3>
<p>那其实 Batch Normalization,不是唯一的 normalization,normalization
的方法有一把啦,那这边就是列了几个比较知名的,</p>
<p>Batch Renormalization https://arxiv.org/abs/1702.03275 Layer
Normalization https://arxiv.org/abs/1607.06450 Instance Normalization
https://arxiv.org/abs/1607.08022 Group Normalization
https://arxiv.org/abs/1803.08494 Weight Normalization
https://arxiv.org/abs/1602.07868 Spectrum Normalization
https://arxiv.org/abs/1705.10941</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1T7T14B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1T7T14B/" class="post-title-link" itemprop="url">模型训练（6）Local Minimum And Saddle Point</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-09 11:34:59" itemprop="dateCreated datePublished" datetime="2022-06-09T11:34:59+08:00">2022-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-05-01 18:29:53" itemprop="dateModified" datetime="2023-05-01T18:29:53+08:00">2023-05-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>19 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="李宏毅课程笔记whengradient-is-small">李宏毅课程笔记：When
gradient is small</span></h3>
<h3><span id="一-critical-point">一、Critical Point</span></h3>
<h4><span id="11-training-fails-because">1.1 Training Fails because</span></h4>
<p>现在我们要讲的是Optimization的部分,所以我们要讲的东西基本上跟Overfitting没有什么太大的关联,我们只讨论Optimization的时候,怎么把gradient
descent做得更好,那为什么Optimization会失败呢？</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011444482.png" alt="image-20220616161429257" style="zoom:67%;"></p>
<p>你常常在做Optimization的时候,你会发现,<strong>随著你的参数不断的update,你的training的loss不会再下降</strong>,但是你对这个loss仍然不满意,就像我刚才说的,你可以把deep的network,跟linear的model,或比较shallow
network
比较,发现说它没有做得更好,所以你觉得deepnetwork,没有发挥它完整的力量,所以Optimization显然是有问题的。</p>
<p><strong>但有时候你会甚至发现,一开始你的model就train不起来,一开始你不管怎么update你的参数,你的loss通通都掉不下去,那这个时候到底发生了什么事情呢？</strong></p>
<p>过去常见的一个猜想,是因为我们现在走到了一个地方,<strong>这个地方参数对loss的微分为零</strong>,当你的参数对loss微分为零的时候,gradient
descent就没有办法再update参数了,这个时候training就停下来了,loss当然就不会再下降了。</p>
<p>讲到gradient为零的时候,大家通常脑海中最先浮现的,可能就是<strong>local
minima</strong>,所以常有人说做deep learning,用gradient
descent会卡在local minima,然后所以gradient descent不work,所以deep
learning不work。</p>
<p><strong>但是如果有一天你要写,跟deep
learning相关paper的时候,你千万不要讲卡在local
minima这种事情,别人会觉得你非常没有水准,为什么？</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011444929.png" alt="image-20220626143535169"></p>
<p>因为<strong>不是只有local
minima的gradient是零</strong>,还有其他可能会让gradient是零,比如说
<strong>saddle point</strong>,所谓的saddle
point,其实就是gradient是零,但是不是local minima,也不是local
maxima的地方,像在右边这个例子里面
红色的这个点,它在左右这个方向是比较高的,前后这个方向是比较低的,它就像是一个马鞍的形状,所以叫做saddle
point,那中文就翻成<strong>鞍点</strong>。</p>
<p>像saddle point这种地方,它也是gradient为零,但它不是local
minima,那像这种gradient为零的点,统称为critical
point,所以<strong>你可以说你的loss,没有办法再下降,也许是因为卡在了critical
point,但你不能说是卡在local minima,因为saddle
point也是微分为零的点</strong></p>
<p>但是今天如果你发现你的gradient,真的很靠近零,卡在了某个critical
point,我们有没有办法知道,到底是local minima,还是saddle
point？其实是有办法的</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011444794.png" alt="image-20220616161554342" style="zoom:50%;"></p>
<p><strong>为什么我们想要知道到底是卡在local minima,还是卡在saddle
point呢</strong></p>
<ul>
<li>因为如果是<strong>卡在local
minima,那可能就没有路可以走了</strong>,因为四周都比较高,你现在所在的位置已经是最低的点,loss最低的点了,往四周走
loss都会比较高,你会不知道怎么走到其他的地方去</li>
<li>但saddle point就比较没有这个问题,如果你今天是<strong>卡在saddle
point的话,saddle
point旁边还是有路可以走的,</strong>还是有路可以让你的loss更低的,你只要逃离saddle
point,你就有可能让你的loss更低</li>
</ul>
<p><strong>所以鉴别今天我们走到,critical point的时候,到底是local
minima,还是saddle
point,是一个值得去探讨的问题,那怎么知道今天一个critical
point,到底是属于local minima,还是saddle point呢？</strong></p>
<h4><span id="12-warning-of-math">1.2 Warning of Math</span></h4>
<p>这边需要用到一点数学,以下这段其实没有很难的数学,就只是微积分跟线性代数,但如果你没有听懂的话,以下这段skip掉是没有关系的，那怎么知道说一个点,到底是local
minima,还是saddle point呢？</p>
<p>你要知道我们loss function的形状,可是我们怎么知道,loss
function的形状呢,network本身很复杂,用复杂network算出来的loss
function,显然也很复杂,我们怎么知道loss
function,长什么样子,虽然我们没有办法完整知道,整个loss function的样子</p>
<h5><span id="taylerseries-approximation"><strong><font color="red"> Tayler
Series Approximation</font></strong></span></h5>
<p>但是如果给定某一组参数,比如说蓝色的这个<span class="math inline">\(θ&#39;\)</span>,在<span class="math inline">\(θ&#39;\)</span>附近的loss
function,是有办法被写出来的,它写出来就像是这个样子：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011444883.png" alt="image-20220626143850955" style="zoom:67%;"></p>
<p>所以这个<span class="math inline">\(L(θ)\)</span>完整的样子写不出来,但是它在<span class="math inline">\(θ&#39;\)</span>附近,你可以用这个式子来表示它,这个式子是,Tayler
Series
Appoximation泰勒级数展开,这个假设你在微积分的时候,已经学过了,所以我就不会细讲这一串是怎么来的,但我们就只讲一下它的概念,这一串里面包含什么东西呢?</p>
<ul>
<li><p>第一项是<span class="math inline">\(L(θ&#39;)\)</span>,就告诉我们说,当<span class="math inline">\(θ\)</span>跟<span class="math inline">\(θ&#39;\)</span>很近的时候,<span class="math inline">\(L(θ)\)</span>应该跟<span class="math inline">\(L(θ&#39;)\)</span>还蛮靠近的</p></li>
<li><p>第二项是<span class="math inline">\((θ-θ&#39;)^Tg\)</span></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011444585.png" alt="image-20220626143927098" style="zoom:67%;"></p>
<p><strong><span class="math inline">\(g\)</span>是一个向量,这个g就是我们的gradient</strong>,我们用绿色的这个g来代表gradient,这个<strong>gradient会来弥补,<span class="math inline">\(θ&#39;\)</span>跟<span class="math inline">\(θ\)</span>之间的差距</strong>,我们虽然刚才说<span class="math inline">\(θ&#39;\)</span>跟<span class="math inline">\(θ\)</span>,它们应该很接近,但是中间还是有一些差距的,那这个差距,第一项我们用这个gradient,来表示他们之间的差距,有时候gradient会写成<span class="math inline">\(∇L(θ&#39;)\)</span>,这个地方的<span class="math inline">\(g\)</span>是一个向量,<strong>它的第i个component,就是θ的第i个component对L的微分</strong>,光是看g还是没有办法,完整的描述L(θ),你还要看第三项</p></li>
<li><p>第三项跟Hessian有关,这边有一个$H $</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011444461.png" alt="image-20220626143956011" style="zoom:67%;"></p>
<p>这个<span class="math inline">\(H\)</span>叫做Hessian,它是一个矩阵,这个第三项是,再<span class="math inline">\((θ-θ&#39;)^TH(θ-θ&#39;)\)</span>,所以第三项会再补足,再加上gradient以后,与真正的L(θ)之间的差距.<strong>H里面放的是L的二次微分</strong>,<strong>它第i个row,第j个column的值,就是把θ的第i个component,对L作微分,再把θ的第j个component,对L作微分,再把θ的第i个component,对L作微分,做两次微分以后的结果</strong>
就是这个<span class="math inline">\(H_i{_j}\)</span></p></li>
</ul>
<p>如果这边你觉得有点听不太懂的话,也没有关系,反正你就记得这个<span class="math inline">\(L(θ)\)</span>,这个loss function,这个error
surface在<span class="math inline">\(θ&#39;\)</span>附近,可以写成这个样子,这个式子跟两个东西有关系,<strong>跟gradient有关系,跟hessian有关系,gradient就是一次微分,hessian就是里面有二次微分的项目</strong></p>
<h5><span id="hession">Hession</span></h5>
<p><strong>那如果我们今天走到了一个critical
point,意味著gradient为零,也就是绿色的这一项完全都不见了</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011444340.png" alt="image-20220626144155017" style="zoom:50%;"></p>
<p><span class="math inline">\(g\)</span><strong>是一个zero
vector,绿色的这一项完全都不见了</strong>,只剩下红色的这一项,所以当在critical
point的时候,这个loss function,它可以被近似为<span class="math inline">\(L(θ&#39;)\)</span>,加上红色的这一项。我们可以<strong>根据红色的这一项来判断</strong>,在<span class="math inline">\(θ&#39;\)</span>附近的error
surface,到底长什么样子。知道error surface长什么样子,我就可以判断。</p>
<h5><span id="判断-θ39它是一个local-minima还是一个saddlepoint"><strong><font color="red">判断 <span class="math inline">\(θ&#39;\)</span>它是一个local minima,还是一个saddle
point</font>。</strong></span></h5>
<p>我们可以靠这一项来了解,这个error
surface的地貌,大概长什么样子,知道它地貌长什么样子,我们就可以知道说,现在是在什么样的状态,这个是Hessian。</p>
<p>那我们就来看一下怎么根据Hessian,怎么根据红色的这一项,来判断θ'附近的地貌。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011444553.png" alt="image-20220626144347777" style="zoom:67%;"></p>
<p>我们现在为了等一下符号方便起见,我们<strong>把<span class="math inline">\((θ-θ&#39;)\)</span>用<span class="math inline">\(v\)</span>这个向量来表示</strong></p>
<ul>
<li>如果今天对任何可能的<span class="math inline">\(v\)</span><strong>,<span class="math inline">\(v^THv\)</span>都大于零</strong>,也就是说
现在θ不管代任何值,v可以是任何的v,也就是θ可以是任何值,不管θ代任何值,<strong>红色框框里面通通都大于零</strong>,那意味著说
<span class="math inline">\(L(θ)&gt;L(θ&#39;)\)</span>。<span class="math inline">\(L(θ)\)</span>不管代多少 只要在<span class="math inline">\(θ&#39;\)</span>附近,<span class="math inline">\(L(θ)\)</span>都大于<span class="math inline">\(L(θ&#39;)\)</span>,<strong>代表<span class="math inline">\(L(θ&#39;)\)</span>是附近的一个最低点,所以它是local
minima</strong></li>
<li>如果今天反过来说,对所有的<span class="math inline">\(v\)</span>而言,<strong><span class="math inline">\(v^THv\)</span>都小于零,也就是红色框框里面永远都小于零</strong>,也就是说<span class="math inline">\(θ\)</span>不管代什么值,红色框框里面都小于零,意味著说<span class="math inline">\(L(θ)&lt;L(θ&#39;)\)</span>,<strong>代表<span class="math inline">\(L(θ&#39;)\)</span>是附近最高的一个点,所以它是local
maxima</strong></li>
<li>第三个可能是假设,<strong><span class="math inline">\(v^THv\)</span>,有时候大于零
有时候小于零</strong>,你代不同的v进去
代不同的θ进去,红色这个框框里面有时候大于零,有时候小于零,意味著说在θ'附近,有时候L(θ)&gt;L(θ')
有时候L(θ)&lt;L(θ'),在L(θ')附近,有些地方高
有些地方低,这意味著什么,<strong>这意味著这是一个saddle
point</strong></li>
</ul>
<p>但是你这边是说我们要代所有的<span class="math inline">\(v\)</span>,去看<span class="math inline">\(v^THv\)</span>是大于零,还是小于零.我们怎么有可能把所有的v,都拿来试试看呢,所以有一个更简便的方法,去确认说这一个条件或这一个条件,会不会发生.</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445282.png" alt="image-20220626144425820" style="zoom:67%;"></p>
<p><strong><font color="red">
这个就直接告诉你结论,线性代数理论上是有教过这件事情的,如果今天对所有的v而言,<span class="math inline">\(v^THv\)</span>都大于零,那这种矩阵叫做positive
definite 正定矩阵,positive definite的矩阵,它所有的eigen
value特征值都是正的</font></strong></p>
<p>所以如果你今天算出一个hessian,你不需要把它跟所有的v都乘看看,你只要去直接看这个H的eigen
value,如果你发现：</p>
<ul>
<li><strong>所有eigen
value都是正的</strong>,那就代表说这个条件成立,就<span class="math inline">\(v^THv\)</span>,会大于零,也就代表说是一个local
minima。所以你从hessian metric可以看出,它是不是local
minima,你只要算出hessian metric算完以后,看它的eigen
value发现都是正的,它就是local minima。</li>
<li>那反过来说也是一样,如果今天在这个状况,对所有的v而言,<span class="math inline">\(v^THv\)</span>小于零,那H是negative
definite,那就代表所有<strong>eigen
value都是负的</strong>,就保证他是local maxima</li>
<li><strong>那如果eigen value有正有负</strong>,那就代表是saddle
point,</li>
</ul>
<p>那假设在这里你没有听得很懂的话,你就可以记得结论,<strong>你只要算出一个东西,这个东西的名字叫做hessian,它是一个矩阵,这个矩阵如果它所有的eigen
value,都是正的,那就代表我们现在在local
minima,如果它有正有负,就代表在saddle point。</strong></p>
<p>那如果刚才讲的,你觉得你没有听得很懂的话,我们这边举一个例子：</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445797.png" alt="image-20220626145009069">
<figcaption aria-hidden="true">image-20220626145009069</figcaption>
</figure>
<p>我们现在有一个史上最废的network,输入一个x,它只有一个neuron，乘上<span class="math inline">\(w₁\)</span>,而且这个neuron,还没有activation
function,所以x乘上<span class="math inline">\(w₁\)</span>以后
之后就输出,然后再乘上<span class="math inline">\(w₂\)</span>
然后就再输出,就得到最终的数据就是y.总之这个function非常的简单 <span class="math display">\[
y= w₁×w₂×x
\]</span> 我们有一个史上最废的training set,这个data
set说,我们只有一笔data,这笔data是x,是1的时候,它的level是1
所以输入1进去,你希望最终的输出跟1越接近越好</p>
<p>而这个史上最废的training,它的error
surface,也是有办法直接画出来的,因为反正只有两个参数 w₁
w₂,连bias都没有,假设没有bias,只有w₁跟w₂两个参数,这个network只有两个参数
w₁跟w₂,那我们可以穷举所有w₁跟w₂的数值,算出所有w₁
w₂数值所代来的loss,然后就画出error surface 长这个样</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445240.png" alt="image-20220626145041635">
<figcaption aria-hidden="true">image-20220626145041635</figcaption>
</figure>
<p>四个角落loss是高的,好 那这个图上你可以看出来说,有一些critical
point,这个黑点点的地方(0,0),<strong>原点的地方是critical
point</strong>,然后事实上,<strong>右上三个黑点也是一排critical
point,左下三个点也是一排critical
point</strong>。如果你更进一步要分析,他们是saddle point,还是local
minima的话,那圆心这个地方,<strong>原点这个地方 它是saddle
point</strong>,为什么它是saddle point呢？你往左上这个方向走
loss会变大,往右下这个方向走 loss会变大,往左下这个方向走
loss会变小,往右下这个方向走 loss会变小,它是一个saddle point。</p>
<p>而这两群critical point,它们都是local
minima,所以这个山沟里面,有一排local minima,这一排山沟里面有一排local
minima,然后在原点的地方,有一个saddle point,这个是我们把error
surface,暴力所有的参数,得到的loss
function以后,得到的loss的值以后,画出error
surface,可以得到这样的结论。</p>
<p>现在假设如果不暴力所有可能的loss,如果要直接算说一个点,是local
minima,还是saddle point的话 怎么算呢</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445178.png" alt="image-20220626145125121" style="zoom: 67%;"></p>
<p>我们可以把loss的function写出来,这个loss的function 这个L是 <span class="math display">\[
L=(\hat{y}-w_1 w_2 x)^2
\]</span> 正确答案 ŷ减掉model的输出,也就是w₁ w₂x,这边取square
error,这边<strong>只有一笔data,所以就不会summation over所有的training
data</strong>,因为反正只有一笔data,x代1
ŷ代1,我刚才说过只有一笔训练资料最废的,所以只有一笔训练资料,所以loss
function就是<span class="math inline">\(L=(\hat{y}-w_1 w_2
x)^2\)</span>,那你可以把这一个loss
function,它的gradient求出来,w₁对L的微分,w₂对L的微分写出来是这个样子
<span class="math display">\[
\frac{∂L}{∂w_1 }=2(1-w_1 w_2 )(-w_2 )
\]</span></p>
<p><span class="math display">\[
\frac{∂L}{∂w_2 }=2(1-w_1 w_2 )(-w_1 )
\]</span></p>
<p>​ 这个东西 <span class="math display">\[
\begin{bmatrix}
\frac{∂L}{∂w_1 }\\\
\frac{∂L}{∂w_2 }
\end{bmatrix}
\]</span>
就是所谓的g,所谓的gradient,什么时候gradient会零呢,什么时候会到一个critical
point呢?</p>
<p>举例来说 如果w₁=0 w₂=0,就在圆心这个地方,如果w₁代0 w₂代0,w₁对L的微分
w₂对L的微分,算出来就都是零
就都是零,这个时候我们就知道说,原点就是一个critical
point,但<strong>它是local maxima,它是local maxima,local
minima,还是saddle point呢,那你就要看hessian才能够知道了</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445367.png" alt="image-20220626145206428" style="zoom:67%;"></p>
<p>当然 我们刚才已经暴力所有可能的w₁
w₂了,所以你已经知道说,它显然是一个saddle
point,但是现在假设还没有暴力所有可能的loss,所以我们要看看能不能够用H,用Hessian看出它是什么样的critical
point,那怎么算出这个H呢？</p>
<p><strong>H它是一个矩阵,这个矩阵里面元素就是L的二次微分</strong>,所以这个矩阵里面第一个row,第一个coloumn的位置,就是w₁对L微分两次,第一个row
第二个coloumn的位置,就是先用w₂对L作微分,再用w₁对L作微分,然后这边就是w₁对L作微分,w₂对L作微分,然后w₂对L微分两次,这四个值组合起来,就是我们的hessian,那这个hessian的值是多少呢</p>
<p>这个hessian的式子,我都已经把它写出来了,你只要把w₁=0 w₂=0代进去,代进去
你就得到在原点的地方,hessian是这样的一个矩阵 <span class="math display">\[
\begin{bmatrix}
{0}&amp;-2\\\
{-2}&amp;0
\end{bmatrix}
\]</span> 这个hessian告诉我们,它是local minima,还是saddle
point呢,那你就要看这个矩阵的eigen value,算一下发现,这个矩阵有两个eigen
value,2跟-2 <strong>eigen value有正有负,代表saddle point</strong></p>
<p>所以我们现在就是用一个例子,跟你操作一下
告诉你说,你怎么从hessian看出一个点,它一个critical point 它是saddle
point,还是local minima</p>
<h4><span id="13-dont-afraid-of-saddlepoint">1.3 Don't afraid of saddle
point</span></h4>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445833.png" alt="image-20220626145319827" style="zoom:67%;"></p>
<p>如果今天你卡的地方是saddle
point,也许你就不用那么害怕了,因为如果你今天你发现,你停下来的时候,是因为saddle
point 停下来了,那其实就有机会可以放心了。</p>
<p>因为H它不只可以帮助我们判断,现在是不是在一个saddle
point,它还指出了我们参数,可以update的方向,就之前我们参数update的时候,都是看gradient
看g,但是我们走到某个地方以后,发现g变成0了 不能再看g了,g不见了
gradient没有了<strong>,但如果是一个saddle
point的话,还可以再看H,怎么再看H呢,H怎么告诉我们,怎么update参数呢</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445962.png" alt="image-20220626145640713" style="zoom:67%;"></p>
<p>我们这边假设<span class="math inline">\(\mu\)</span>是H的eigenvector特征向量,然后<span class="math inline">\(λ\)</span>是u的eigen
value特征值。如果我们把这边的<span class="math inline">\(v\)</span>换成<span class="math inline">\(\mu\)</span>的话,我们把<span class="math inline">\(\mu\)</span>乘在H的左边,跟H的右边,也就是<span class="math inline">\(\mu^TH\mu\)</span>, <span class="math inline">\(H\mu\)</span>会得到<span class="math inline">\(λ\mu\)</span>，因为<span class="math inline">\(\mu\)</span>是一个eigen vector。H乘上eigen
vector特征向量会得到特征向量λ eigen value乘上eigen vector即<span class="math inline">\(λ\mu\)</span></p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445930.png" alt="image-20220626145734569">
<figcaption aria-hidden="true">image-20220626145734569</figcaption>
</figure>
<p>所以我们在这边得到uᵀ乘上λu,然后再整理一下,把uᵀ跟u乘起来,得到‖u‖²,所以得到λ‖u‖²</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445674.png" alt="image-20220626145742180">
<figcaption aria-hidden="true">image-20220626145742180</figcaption>
</figure>
<p>假设我们这边v,代的是一个eigen vector,我们这边θ减θ',放的是一个eigen
vector的话,会发现说我们这个红色的项里面,其实就是λ‖u‖²</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445829.png" alt="image-20220626145756118" style="zoom:67%;"></p>
<p>那今天如果λ<strong>小于零</strong>,eigen
value小于零的话,那λ‖u‖²就会小于零,因为‖u‖²一定是正的,所以eigen
value是负的,那这一整项就会是<strong>负的</strong>,也就是u的transpose乘上H乘上u,它是负的,也就是<strong>红色这个框里是负的</strong>。所以这意思是说假设<span class="math inline">\(θ-θ&#39;=\mu\)</span>,那这一项<span class="math inline">\((θ-θ&#39;)^TH(θ-θ&#39;)\)</span>就是负的,也就是<span class="math inline">\(L(θ)&lt;L(θ&#39;)\)</span>。也就是说假设<span class="math inline">\(θ-θ&#39;=\mu\)</span>,也就是,<strong>你在θ'的位置加上u,沿著u的方向做update得到θ,你就可以让loss变小</strong>。</p>
<p>因为根据这个式子,你只要θ减θ'等于u,loss就会变小,所以你今天只要让θ等于θ'加u,你就可以让loss变小,你只要沿著u,也就是eigen
vector的方向,去更新你的参数 去改变你的参数,你就可以让loss变小了</p>
<p><strong><font color="red"> 所以虽然在critical
point没有gradient,如果我们今天是在一个saddle
point,你也不一定要惊慌,你只要找出负的eigen value,再找出它对应的eigen
vector,用这个eigen
vector去加θ',就可以找到一个新的点,这个点的loss比原来还要低。</font></strong></p>
<h5><span id="举具体的例子">举具体的例子：</span></h5>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011445872.png" alt="image-20220626145931496" style="zoom: 67%;"></p>
<p>刚才我们已经发现,原点是一个critical
point,它的Hessian长这个样,那我现在发现说,这个Hessian有一个负的eigen
value,这个eigen value等于-2,那它对应的eigen
vector,它有很多个,其实是无穷多个对应的eigen
vector,我们就取一个出来,我们取<span class="math inline">\(\begin{bmatrix}{1} \\\
{1}\end{bmatrix}\)</span>是它对应的一个eigen
vector,那我们其实只要顺著这个u的方向,顺著<span class="math inline">\(\begin{bmatrix}{1} \\\
{1}\end{bmatrix}\)</span>这个vector的方向,去更新我们的参数,就可以找到一个,比saddle
point的loss还要更低的点。</p>
<p>如果以今天这个例子来看的话,你的saddle
point在(0,0)这个地方,你在这个地方会没有gradient,Hessian的eigen
vector告诉我们,只要往<span class="math inline">\(\begin{bmatrix}{1} \\\
{1}\end{bmatrix}\)</span>的方向更新,你就可以让loss变得更小,也就是说你可以逃离你的saddle
point,然后让你的loss变小,所以从这个角度来看,似乎saddle
point并没有那么可怕。如果你今天在training的时候,你的gradient你的训练停下来,你的gradient变成零,你的训练停下来,是因为saddle
point的话,那似乎还有解。</p>
<p><strong>但是当然实际上,在实际的implementation里面,你几乎不会真的把Hessian算出来</strong>,这个要是二次微分,要计算这个矩阵的computation,需要的运算量非常非常的大,更遑论你还要把它的eigen
value,跟 eigen
vector找出来,所以在实作上,你几乎没有看到,有人用这一个方法来逃离saddle
point。</p>
<p><strong>等一下我们会讲其他,也有机会逃离saddle
point的方法,他们的运算量都比要算这个H,还要小很多</strong>,那今天之所以我们把,这个saddle
point跟 eigen vector,跟Hessian的eigen
vector拿出来讲,是想要告诉你说,如果是卡在saddle
point,也许没有那么可怕,最糟的状况下你还有这一招,可以告诉你要往哪一个方向走.</p>
<h4><span id="14-saddle-point-vs-localminima">1.4 Saddle Point v.s. Local
Minima</span></h4>
<p>讲到这边你就会有一个问题了,这个问题是,那到底<strong>saddle
point跟local minima,谁比较常见呢</strong>,我们说,saddle
point其实并没有很可怕,那如果我们今天,常遇到的是saddle
point,比较少遇到local minima,那就太好了,那到底saddle point跟local
minima,哪一个比较常见呢?</p>
<p>总之这个<strong>从三维的空间来看,是没有路可以走的东西,在高维的空间中是有路可以走的,error
surface会不会也一样呢？</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011446813.png" alt="image-20220626150135469" style="zoom:67%;"></p>
<p>而经验上,如果你自己做一些实验的话,也支持这个假说</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011446029.png" alt="image-20220626150207285" style="zoom: 67%;"></p>
<p>这边是训练某一个network的结果,每一个点代表,训练那个network训练完之后,把它的Hessian拿出来进行计算,所以这边的每一个点,都代表一个network,就我们训练某一个network,然后把它训练训练,训练到gradient很小,卡在critical
point,把那组参数出来分析,看看它比较像是saddle point,还是比较像是local
minima</p>
<ul>
<li>纵轴代表training的时候的loss,就是我们今天卡住了,那个loss没办法再下降了,那个loss是多少,那很多时候,你的loss在还很高的时候,训练就不动了
就卡在critical point,那很多时候loss可以降得很低,才卡在critical
point,这是纵轴的部分</li>
<li>横轴的部分是minimum ratio,minimum ratio是<strong>eigen
value的数目分之正的eigen value的数目</strong>,又<strong>如果所有的eigen
value都是正的,代表我们今天的critical point,是local
minima,如果有正有负代表saddle
point</strong>,那在实作上你会发现说,你几乎找不到完全所有eigen
value都是正的critical point,你看这边这个例子里面,这个minimum
ratio代表eigen value的数目分之正的eigen
value的数目,最大也不过0.5到0.6间而已,代表说只有一半的eigen
value是正的,还有一半的eigen value是负的,</li>
</ul>
<p>所以今天虽然在这个图上,越往右代表我们的critical point越像local
minima,<strong>但是它们都没有真的,变成local
minima</strong>,就算是在最极端的状况,我们仍然有一半的case,我们的eigen
value是负的,这一半的case eigen
value是正的,代表说在所有的维度里面有一半的路,这一半的路
如果要让loss上升,还有一半的路可以让loss下降。</p>
<p><strong><font color="red"> 所以从经验上看起来,其实local
minima并没有那么常见,多数的时候,你觉得你train到一个地方,你gradient真的很小,然后所以你的参数不再update了,往往是因为你卡在了一个saddle
point。</font></strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202305011446758.png" alt="image-20220616164405732" style="zoom: 33%;"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/TYS9XF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/TYS9XF/" class="post-title-link" itemprop="url">特征工程（8）【draft】时间序列处理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-06 20:36:41" itemprop="dateCreated datePublished" datetime="2022-06-06T20:36:41+08:00">2022-06-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:14:16" itemprop="dateModified" datetime="2023-04-22T19:14:16+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>261</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="时间序列数据的预处理">时间序列数据的预处理</span></h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/466086665"><em>时间序列</em>数据的预<em>处理</em></a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/vyDZfDdaH2Y7k75NNsMNJA">如何在实际场景中使用异常检测？阿里云Prometheus智能检测算子来了</a></li>
</ul>
<blockquote>
<p>在本文中，我们将主要讨论以下几点：</p>
<ul>
<li>时间序列数据的定义及其重要性。</li>
<li>时间序列数据的预处理步骤。</li>
<li>构建时间序列数据，查找缺失值，对特征进行去噪，并查找数据集中存在的异常值。</li>
</ul>
</blockquote>
<h3><span id="时间序列的定义">时间序列的定义</span></h3>
<p><strong>时间序列是在特定时间间隔内记录的一系列均匀分布的观测值</strong>。时间序列的一个例子是黄金价格。在这种情况下，我们的观察是在固定时间间隔后一段时间内收集的黄金价格。时间单位可以是分钟、小时、天、年等。但是任何两个连续样本之间的时间差是相同的。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2MJ41K7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2MJ41K7/" class="post-title-link" itemprop="url">python-环境变量</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-06-06 15:22:38 / 修改时间：15:22:48" itemprop="dateCreated datePublished" datetime="2022-06-06T15:22:38+08:00">2022-06-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">【draft】工程</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%B7%A5%E7%A8%8B/%E6%B5%81%E7%A8%8B%E7%9A%84Python/" itemprop="url" rel="index"><span itemprop="name">流程的Python</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3JZF773/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3JZF773/" class="post-title-link" itemprop="url">恶意软件检测（7）【draft】CADE: Detecting and Explaining Concept Drift Samples for Security Applications</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-05 20:05:05" itemprop="dateCreated datePublished" datetime="2022-06-05T20:05:05+08:00">2022-06-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-19 15:50:46" itemprop="dateModified" datetime="2023-04-19T15:50:46+08:00">2023-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/" itemprop="url" rel="index"><span itemprop="name">学术前沿</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="cadedetecting-and-explaining-concept-drift-samples-for-securityapplications">CADE:
Detecting and Explaining Concept Drift Samples for Security
Applications</span></h2>
<p>原文作者：Limin Yang, <em>University of Illinois at
Urbana-Champaign</em></p>
<p>原文链接：https://www.usenix.org/conference/usenixsecurity21/presentation/yang-limin</p>
<p>发表会议：USENIXSec 2021</p>
<p><strong>代码地址</strong>：https://github.com/whyisyoung/CADE</p>
<h3><span id="摘要">摘要</span></h3>
<p>概念漂移对部署机器学习模型来解决实际的安全问题提出了严峻的挑战。<strong>由于攻击者（和/或良性对手）的动态行为变化，随着时间的推移，测试数据分布往往会从原始的训练数据转移，从而导致部署的模型出现重大故障</strong>。</p>
<p>为了对抗概念漂移，我们提出了一种新的系统CADE，旨在（1）<strong>检测偏离现有类别的漂移样本</strong>；（2）<strong>解释检测到漂移的原因</strong>。与传统方法不同（需要大量新标签来统计确定概念漂移），我们的目标是在单个漂移样本到达时识别它们。认识到高维离群空间带来的挑战，我们建议将数据样本映射到低维空间，并自动学习距离函数来度量样本之间的相异性。通过对比学习，我们可以充分利用训练数据集中现有的标签来学习如何对样本进行比较和对比。<strong>为了解释检测到的漂移的意义，我们开发了一种基于距离的解释方法</strong>。我们表明，在这个问题背景下，解释“距离”比传统方法更有效，传统方法侧重于解释“决策边界”。我们通过两个案例来评估CADE：Android恶意软件分类和网络入侵检测。我们进一步与一家安全公司合作，在其恶意软件数据库上测试CADE。我们的结果表明，CADE可以有效地检测漂移样本，并提供语义上有意义的解释。</p>
<h3><span id="一-说明">一、说明</span></h3>
<p>由于概念漂移，部署基于机器学习的安全应用程序可能非常具有挑战性。无论是恶意软件分类、入侵检测还是在线滥用检测[6、12、17、42、48]，基于学习的模型都是在“封闭世界”假设下工作的，期望测试数据分布与训练数据大致匹配。然而，部署模型的环境通常会随着时间的推移而动态变化。这种变化可能既包括良性玩家的有机行为变化，也包括攻击者的恶意突变和适应。因此，测试数据分布从原始训练数据转移，这可能会导致模型出现严重故障[23]。</p>
<blockquote>
<p>[23] A survey on concept drift adaptation. ACM computing surveys
(CSUR), 2014.</p>
</blockquote>
<p>为了解决概念漂移问题，大多数基于学习的模型需要<strong>定期重新培训</strong>[36、39、52]。然而，再培训通常需要标记大量新样本（昂贵）。更重要的是，还很难确定何时应该对模型进行再培训。延迟的再培训会使过时的模型容易受到新的攻击。</p>
<p><font color="red"><strong>我们设想，对抗概念漂移需要建立一个监控系统来检查传入数据流和训练数据（和/或当前分类器）之间的关系</strong></font>。图1说明了高级思想。当原始分类器在生产空间中工作时，另一个系统应定期检查分类器对传入数据样本做出决策的能力。<strong>A检测模块(1)
可以过滤正在远离训练空间的漂移样本</strong>。更重要的是，为了<strong>解释漂移的原因（例如，攻击者突变、有机行为变化、以前未知的系统错误）</strong>，我们需要一种解释方法(2)
将检测决策与语义上有意义的特征联系起来。这两项功能对于为开放世界环境准备基于学习的安全应用程序至关重要。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550835.png" alt="image-20220605202728095" style="zoom: 67%;"></p>
<p>之前的工作已经探索了通过直接检查原始分类器（0）的预测置信度来检测漂移样本的方法
<strong>[32]</strong>。置信度较低可能表明传入样本是漂移样本。然而，该置信度得分是基于所有类别已知（封闭世界）的假设计算的概率（总和为1.0）。不属于任何现有类别的漂移样本可能会被分配到错误的类别，并具有很高的置信度（已通过现有工作验证[25、32、37]）。最近的一项工作提出了计算传入样本和每个现有类之间的不一致性度量的想法，以确定适合度[38]。<strong>该不合格度量基于距离函数计算，以量化样本之间的不相似性</strong>。<strong>然而，我们发现这种距离函数很容易失效，尤其是当数据稀疏且维数较高时。</strong></p>
<blockquote>
<p><strong>[32] A baseline for detecting misclassified and
out-of-distribution examples in neural networks.</strong></p>
</blockquote>
<p><strong>我们的方法</strong>。在本文中，我们提出了一种检测漂移样本的新方法，并结合一种解释检测决策的新方法。我们共同构建了一个称为CADE的系统，它是“用于漂移检测和解释的对比自动编码器
(“<strong>Contrastive Autoencoder for Drifting detection and
Explanation</strong>)”的缩写关键的挑战是<strong>推导一个有效的距离函数来衡量样本的相异性</strong>。我们没有随意选取距离函数，而是利用对比学习的思想[29]，根据现有的标签，从现有的训练数据中学习距离函数。给定原始分类器的训练数据（多个类别），我们将训练样本映射到低维潜在空间。映射函数通过对比样本来学习，以扩大不同类样本之间的距离，同时减少同一类样本之间的距离。<strong>我们证明了在潜在空间中得到的距离函数可以有效地检测和排序漂移样本。</strong></p>
<p>评价我们使用两个数据集评估我们的方法，包括<strong>Android恶意软件数据集[7]和2018年发布的入侵检测数据集[57]</strong>。我们的评估表明，我们的漂移检测方法具有很高的准确性，F1平均得分为0.96或更高，优于各种基线和现有方法。我们的分析还表明，使用对比学习可以减少检测决策的模糊性。对于解释模型，我们进行了定量和定性评估。案例研究还表明，所选特征与漂移样本的语义行为相匹配。</p>
<p>此外，我们还与一家安全公司的合作伙伴合作，在其内部恶意软件数据库上测试CADE。作为初步测试，我们从395个家庭中获得了2019年8月至2020年2月出现的20613个Windows
PE恶意软件样本。这使我们能够在不同的环境中测试更多恶意软件系列的系统性能。结果很有希望。<font color="red"><strong>例如，CADE在10个家庭中进行训练并在160个以前未见过的家庭中进行测试时，F1成绩达到0.95分。这使得人们有兴趣在生产系统中进一步测试和部署CADE。</strong>
</font></p>
<h4><span id="贡献">贡献：</span></h4>
<p>本文有三个主要贡献。</p>
<ul>
<li>我们提出CADE来补充现有的基于监督学习的安全应用程序，以对抗概念漂移。提出了<strong>一种基于对比表征学习的漂移样本检测方法</strong>。</li>
<li>我们说明了监督解释方法在解释异常样本方面的局限性，并<strong>介绍了一种基于距离的解释方法</strong>。</li>
<li>我们通过两个应用对所提出的方法进行了广泛的评估。我们与一家安保公司的初步测试表明，CADE是有效的。我们在此处发布了CADE代码1，以支持未来的研究。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2MX2YPX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2MX2YPX/" class="post-title-link" itemprop="url">安全场景（6）PowerShell</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-06-03 15:27:51 / 修改时间：15:28:05" itemprop="dateCreated datePublished" datetime="2022-06-03T15:27:51+08:00">2022-06-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/" itemprop="url" rel="index"><span itemprop="name">应用场景</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/30K2RMS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/30K2RMS/" class="post-title-link" itemprop="url">安全场景（5）恶意DNS检测</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-03 15:27:21" itemprop="dateCreated datePublished" datetime="2022-06-03T15:27:21+08:00">2022-06-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-25 22:19:34" itemprop="dateModified" datetime="2022-06-25T22:19:34+08:00">2022-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/" itemprop="url" rel="index"><span itemprop="name">应用场景</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>122</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="datacon-2019-dns-analysis">DataCon 2019: DNS Analysis</span></h1>
<p>https://ixyzero.com/blog/archives/4473.html</p>
<ul>
<li>Datacon 2019：https://github.com/shyoshyo/DataCon-9102-DNS</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/CCPH1H/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/CCPH1H/" class="post-title-link" itemprop="url">模型训练（7）【draft】参数初始化</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-29 18:37:10" itemprop="dateCreated datePublished" datetime="2022-05-29T18:37:10+08:00">2022-05-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-05-01 18:46:33" itemprop="dateModified" datetime="2023-05-01T18:46:33+08:00">2023-05-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>349</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="参数初始化">参数初始化</span></h2>
<blockquote>
<ul>
<li>https://paddlepedia.readthedocs.io/en/latest/tutorials/deep_learning/model_tuning/weight_initializer.html</li>
</ul>
</blockquote>
<p>在我们开始训练神经网络之前，首先要做的是给网络中的每一个权重和偏置赋值，这个赋值的过程就是参数初始化。<strong>合理的初始化可以缩短神经网络的训练时间，而不合理的初始化可能使网络难以收敛</strong>。那么，我们要如何对参数进行初始化呢？或许你有想到过将全部参数都设置为0，这看起来是一个简单又方便的办法，但遗憾的是神经网络中不能对权重进行全零初始化。在讨论如何对参数进行初始化前，我们先来看看为什么不能进行全零初始化。</p>
<h4><span id="一-为什么不能全零初始化">一、为什么不能全零初始化？</span></h4>
<p>以一个三层网络为例，假设其具体的网络示意图如图1所示。</p>
<p><img src="https://paddlepedia.readthedocs.io/en/latest/_images/net_for_params_init.png" alt="net_for_params_init" style="zoom:50%;"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2APFWS8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2APFWS8/" class="post-title-link" itemprop="url">工业落地-Sophos《Learning from Context: Exploiting and Interpreting File Path Information for Better Malware Detection》</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-28 15:50:06" itemprop="dateCreated datePublished" datetime="2022-05-28T15:50:06+08:00">2022-05-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 15:10:01" itemprop="dateModified" datetime="2023-04-18T15:10:01+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/" itemprop="url" rel="index"><span itemprop="name">工业落地</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/%E6%81%B6%E6%84%8F%E8%BD%AF%E4%BB%B6%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">恶意软件检测</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="learningfrom-context-exploiting-and-interpreting-file-path-information-forbetter-malware-detection">Learning
from Context: Exploiting and Interpreting File Path Information for
Better Malware Detection</span></h2>
<ul>
<li>https://ai.sophos.com/presentations/learning-from-context-a-multi-view-deep-learning-architecture-for-malware-detection/</li>
</ul>
<blockquote>
<p>用于静态可移植可执行（PE）恶意软件检测的机器学习（ML）通常使用每个文件的数字特征向量表示作为训练期间一个或多个目标标签的输入。然而，可以从查看文件的上下文中收集到许多正交信息。在本文中，我们<strong>建议使用静态上下文信息源（PE文件的路径）作为分类器的辅助输入</strong>。虽然文件路径本身不是恶意或良性的，但它们确实为恶意/良性判断提供了有价值的上下文。与动态上下文信息不同，文件路径的可用开销很小，并且可以无缝集成到多视图静态ML检测器中，在非常高的吞吐量下产生更高的检测率，同时基础结构的更改也很小。在这里，我们提出了一种多视图神经网络，它从PE文件内容以及相应的文件路径中提取特征向量作为输入并输出检测分数。为了确保真实的评估，我们使用了大约1000万个样本的数据集—来自实际安全供应商网络的用户端点的文件和文件路径。<strong>然后，我们通过LIME建模进行可解释性分析，以确保我们的分类器已学习到合理的表示，并查看文件路径的哪些部分对分类器得分的变化贡献最大</strong>。我们发现，我们的模型学习了文件路径的有用方面以进行分类，同时还学习了测试供应商产品的客户的工件，例如，通过下载恶意软件样本目录，每个样本都命名为其哈希。我们从我们的测试数据集中删减了这些工件，并证明在10−3假阳性率（FPR），10时为33.1%−4
FPR，基于类似拓扑的单输入PE文件内容模型。</p>
</blockquote>
<h3><span id="摘要">摘要</span></h3>
<p>用于恶意软件检测的机器学习（ML）分类器通常在进行恶意/良性判断时使用每个文件内容的数字表示。<strong>然而，也可以从文件所在的上下文中收集相关信息，这些信息通常被忽略。<font color="red">
上下文信息的一个来源是文件在磁盘上的位置。</font></strong>例如，如果检测器可以清楚地利用有关其所在路径的信息，则伪装为已知良性文件（例如Windows系统DLL）的恶意文件更有可能显得可疑。了解文件路径信息还可以更容易地检测那些试图通过将自己放置在特定位置来逃避磁盘扫描的文件**。文件路径也可以使用，开销很小，并且可以无缝集成到多视图静态ML检测器中，在非常高的吞吐量和最小的基础结构更改下，可能产生更高的检测率。</p>
<p>在这项工作中，我们提出了一种 <strong>multi-view</strong>
深度神经网络结构，该结构将PE文件内容中的特征向量以及相应的文件路径作为输入并输出检测分数。我们对大约1000万个样本的商业规模数据集进行了评估，这些样本是来自实际安全供应商提供服务的用户端点的文件和文件路径。<strong>然后，我们通过LIME建模进行可解释性分析，以确保我们的分类器已学习到合理的表示，并检查文件路径如何在不同情况下改变分类器的分数</strong>。我们发现，与只对PE文件内容进行操作的模型相比，<strong>我们的模型学习了文件路径的有用方面，在0.001假阳性率（FPR）下，真阳性率提高了26.6%，在0.0001
FPR下，提高了64.6%</strong>。</p>
<p><strong>keyword</strong> :
静态PE检测、文件路径、深度学习、多视图学习、模型解释</p>
<h3><span id="一-说明">一、说明</span></h3>
<p>商用便携式可执行（PE）恶意软件检测器由静态和动态分析引擎组成。<strong>静态检测通常首先用于标记可疑样本，它可以快速有效地检测大部分恶意软件</strong>。它涉及分析磁盘上的原始PE映像，可以非常快速地执行，但易受代码混淆技术的影响，例如压缩和多态/变形转换<strong>[1]</strong>。相比之下，动态检测需要在模拟器中运行PE，并在运行时分析行为[2]。当动态分析工作时，它不太容易受到代码混淆的影响，但与静态方法相比，它需要更大的计算容量和执行时间。此外，有些文件很难在仿真环境中执行，但仍然可以进行静态分析。<strong>因此，静态检测方法通常是端点恶意软件预防（在执行恶意软件之前阻止恶意软件）管道中最关键的部分</strong>。最近，由于采用了机器学习，静态检测方法的性能有所提高[3]，其中高度表达的分类器（例如深层神经网络）适合于数百万个文件的标记数据集。训练这些分类器时，它们使用静态文件内容作为输入，但不使用辅助数据。<strong>然而，我们注意到，由于辅助数据（例如网络流量、系统调用等），动态分析工作得很好。</strong>在这项工作中，我们试图使用文件路径作为正交输入信息来增强静态ML检测器。<strong>文件路径是静态可用的，无需操作系统的任何附加工具</strong>。通过将文件路径作为辅助输入，我们希望能够将有关文件的信息与在特定位置看到此类文件的可能性的信息结合起来，<strong>并识别与已知恶意软件和良性文件相关的常见目录层次结构和命名模式。</strong></p>
<blockquote>
<p><strong>静态检测</strong>：通用模块，快速有效地标记可疑样本；易受代码混淆技术（压缩和多态/变形转换）的影响。</p>
<p>[1] A. Moser, C. Kruegel, and E. Kirda, “Limits of static analysis
for malware detection,” in Twenty-Third Annual Computer Security
Applications Conference (ACSAC 2007). IEEE, 2007, pp. 421–430.</p>
<p><strong>动态检测</strong>：分析模块，需要更大的计算容量和执行时间；有些文件很难在仿真环境中执行。</p>
</blockquote>
<p>我们将分析重点放在三个模型上：</p>
<ul>
<li>仅基线文件内容（PE）模型，仅将PE功能作为输入并输出恶意软件置信度得分。</li>
<li>另一个基准文件路径仅内容（FP）模型，仅将文件的文件路径作为输入，并输出恶意软件置信度得分。</li>
<li>我们提出的多视图PE文件内容+上下文文件路径（PE+FP）模型，该模型同时考虑PE文件内容特征和文件路径，并输出恶意软件置信度得分。</li>
</ul>
<p>三个模型的示意图如图1所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181509919.png" alt="image-20220528162748125" style="zoom:50%;"></p>
<p>我们对从一家大型反恶意软件供应商的遥测数据中收集的时间分割数据集进行分析，发现我们在文件内容和上下文文件路径上训练的分类器在ROC曲线上，尤其是在低误报率（FPR）区域，产生了统计上显著更好的结果。</p>
<p><strong>本文的贡献如下：</strong></p>
<ul>
<li>我们从安全供应商的客户端点（而不是恶意软件/供应商标签聚合服务）获得一组真实、精心策划的文件和文件路径数据集。</li>
<li>我们证明，我们的多视图PE+FP恶意软件分类器在我们的数据集上的性能明显优于单独使用文件内容的模型。</li>
<li>我们将本地可解释模型不可知解释（LIME）<strong>[4]</strong>扩展到PE+FP模型，并使用它分析文件路径如何影响模型的恶意/良性决策。</li>
</ul>
<blockquote>
<p>[4] M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should i trust
you?: Explaining the predictions of any classifier,” in Proceedings of
the 22nd ACM SIGKDD international conference on knowledge discovery and
data mining. ACM, 2016, pp. 1135–1144.</p>
</blockquote>
<p>本手稿的其余部分结构如下：第二节涵盖重要的背景概念和相关工作。第三节讨论数据集收集和模型制定。第四节将我们的新多视图方法与拓扑结构相似的纯内容基线模型进行了比较，并对我们的模型进行了可解释性分析。第五节结束。</p>
<h3><span id="二-背景和相关工作">二、背景和相关工作</span></h3>
<p>在本节中，我们将描述机器学习如何普遍应用于静态PE检测，以及我们的方法如何通过提供上下文信息作为辅助输入而在高层意义上有所不同。然后，我们介绍了其他机器学习领域的相关工作。</p>
<h4><span id="21-静态ml恶意软件检测">2.1 静态ML恶意软件检测</span></h4>
<p>机器学习已经应用于计算机安全领域多年了[5]，但在商业规模上使用ML的静态PE模型的破坏性性能突破是一个较新的现象。商业模型通常依赖深度神经网络[6]或增强的决策树集合[7]，并已扩展到其他静态文件类型，包括web内容[8]、[9]、office文档[10]和档案[10]。</p>
<p>大多数用于信息安全的静态ML（ML-Sec）分类器操作的是文件部分（例如，标题）上的学习嵌入[11]，整个文件上的学习嵌入[12]，或者最常见的是，操作的是设计用于总结每个文件内容的预设计数字特征向量[6]、[13]–[19]。预先构建的特征向量表示往往更具可伸缩性，可以快速从每个文件中提取内容，同时保留有用的信息。有很多方法可以构建特征向量，包括在滑动窗口上跟踪每字节统计信息【6】、【18】、字节直方图【7】、【18】、ngram直方图【13】、将字节视为图像中的像素值（文件内容的可视化）【13】、【18】、操作码和函数调用图统计信息【18】、符号统计信息【18】、哈希/数字元数据值【6】、【7】、【18】–例如。，入口点作为文件的一部分，或散列导入和导出，以及分隔标记的散列[10]、[19]。在实际应用中，从文件内容中提取的几种不同类型的特征向量通常连接在一起，以获得优异的性能。</p>
<h4><span id="22-learning-from-multiplesources">2.2 Learning from Multiple
Sources</span></h4>
<p>使用深度神经网络进行静态ML恶意软件检测的相关研究已经检验了从多个信息源学习的方法，但这些方法与我们的方法有根本不同：Huang等人【20】和Rudd等人【21】对多个辅助损失函数使用多目标学习【22】，【23】，他们发现这些函数在主要恶意软件检测任务中的性能有所提高。这两项工作都在培训期间使用元数据作为辅助目标标签，为模型提供额外的信息，并在部署时使用单个输入来做出分类决策。我们的方法利用了多种输入类型/模式——一种是以类似于[6]的PE特征向量的形式描述恶意样本的内容，另一种是将原始字符串提供给一个字符嵌入层（类似于[8]），该层提供了有关该样本出现位置的信息。这种技术是一种多视图学习方法[24]。顾名思义，多视图学习的大多数应用都是在计算机视觉中进行的，在计算机视觉中，多个视图实际上是由来自不同输入摄像头/传感器的视图或来自同一摄像头/传感器在不同时间的不同视图组成的。<strong>在ML-Sec空间中，我们只能找到两种专门将自己称为多视图的方法：即[25]，Narayanan等人将多内核学习依赖图应用于Android恶意软件分类，以及[26]，</strong>Bai等人将多视图集合用于PE恶意软件检测。虽然这些方法在某些方面与我们的方法相似，但据我们所知，我们是第一个在商业规模上使用外部上下文反馈到深层神经网络并结合文件内容特征来执行恶意软件检测多视图建模的方法。</p>
<blockquote>
<p>[25] A. Narayanan, M. Chandramohan, L. Chen, and Y. Liu, “A
multi-view context-aware approach to android malware detection and
malicious code localization,” Empirical Software Engineering, pp. 1–53,
2018.</p>
<p>[26] J. Bai and J. Wang, “Improving malware detection using
multi-view ensemble learning,” Security and Communication Networks, vol.
9, no. 17, pp. 4227–4241, 2016.</p>
</blockquote>
<h3><span id="三-实施细节">三、实施细节</span></h3>
<p>在本节中，我们将介绍我们的方法的实现细节，包括从客户端点获取PE文件和文件路径的数据收集过程、我们的特征化策略以及我们的多视图深度神经网络和比较基线的体系结构。</p>
<h4><span id="31-数据集">3.1 数据集</span></h4>
<p>在我们的实验中，我们从一家著名反恶意软件供应商的遥测数据中收集了三个不同的数据集：一个训练集、一个验证集和一个测试集。培训集由9148143个样本组成，这些样本首次出现在2018年6月1日至11月15日之间，其中693272个样本被标记为恶意样本。验证集包括在2018年11月16日至12月1日期间观察到的2225094个不同样本，其中85041个被标记为恶意样本。最后，测试集在2019年1月1日至1月30日期间共有249783个样本，其中38767个被标记为恶意。这些文件的恶意/良性标签是使用类似于[6]、[8]的标准计算的，但结合其他专有信息可以生成更准确的标签。</p>
<h4><span id="32-特征工程">3.2 特征工程</span></h4>
<p><strong>为了使用文件路径作为神经网络模型的输入，我们首先将可变长度字符串转换为固定长度的数字向量</strong>。我们使用类似于<strong>[8]</strong>的矢量化方案来实现这一点，方法是在每个字符上创建一个键控的查找表，该表用一个表示每个字符的数值（介于0和字符集大小之间）来表示。实际上，我们将此表实现为Python字典。在遥测和早期实验数据的指导下，我们将文件路径缩减到最后100个字符。短于100个字符的文件路径的功能用零填充。对于字符集，我们考虑整个Unicode字符集，但将词汇限制为150个最常见的字符。见附录？？供进一步讨论。作为PE文件内容的特征，我们使用了由四种不同特征类型组成的浮点1024维特征向量，类似于[6]。总的来说，我们将每个样本表示为两个特征向量：<strong>1024维的PE内容特征向量和100维的上下文文件路径特征向量。</strong></p>
<blockquote>
<p>[8] J. Saxe and K. Berlin, “expose: <strong>A character-level
convolutional neural network with embeddings for detecting malicious
urls, file paths and registry keys</strong>,” arXiv preprint
arXiv:1702.08568, 2017.</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181509845.png" alt="image-20220530152922273" style="zoom: 25%;"></p>
</blockquote>
<h4><span id="33-网络体系结构">3.3 网络体系结构</span></h4>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181509415.png" alt="image-20220528204724740" style="zoom:50%;"></p>
<p>我们的多视图体系结构如图2所示。该模型有两个输入，<strong>1024个元素的PE内容特征向量xPE和100个元素的文件路径整数向量xFP</strong>，如第III-B节所述。每个不同的输入分别通过一系列具有各自参数θPE和θFP的层，用于PE特征和FP用于文件路径特征，并在训练期间联合优化。然后将这些层的输出连接（串联）并通过一系列最终隐藏层，即参数θO的联合输出路径。网络的最终输出由密集层和sigmoid激活组成。</p>
<ul>
<li><strong>PE
特征</strong>：PE输入臂θPE使xPE通过一系列块，每个块由四层组成：一个完全连接的层，一个使用[27]中所述技术实现的层规范化层，一个丢失概率为0.05的丢失层，以及一个校正线性单元（ReLU）激活。其中五个块依次连接，密集层大小分别为1024、768、512、512和512个节点。</li>
<li><strong>文件路径</strong>：
<ul>
<li>文件路径输入arm θFP 将
xFP（<strong>长度为100的向量</strong>）传递到嵌入层 ？？？</li>
<li><strong>该嵌入层将文件路径的每个字符转换为32维向量，从而为整个文件路径生成100x32的嵌入张量</strong>。</li>
<li><strong>然后将该嵌入馈入4个单独的卷积块</strong>，其中包含一个具有128个滤波器的1D卷积层、一个层归一化层和一个1D求和层，以将输出平坦化为向量。4个卷积块包含卷积层，卷积层的大小分别为2、3、4和5，用于处理2、3、4和5克的输入文件路径。</li>
<li>然后将这些卷积块的平坦输出<strong>串联</strong>起来，作为大小为1024和512个神经元的两个密集块的输入（与PE输入臂中的形式相同）。</li>
</ul></li>
<li><strong>输出层</strong>：PE
arm和文件路径arm的完全连接块的输出随后被连接并传递到由θO参数化的联合输出路径。该路径由层大小为512、256和128的密集连接块（与PE输入arm中的形式相同）组成。然后将这些块的128D输出馈送至致密层，该致密层将输出投射至1D，然后进行sigmoid激活，以提供模型的最终输出。</li>
</ul>
<p>仅PE模型只是PE+FP模型，但没有FP臂，输入xPE并拟合θPE和θO参数。类似地，FP模型是PE+FP模型，但没有3个授权的许可PE
arm，采用输入xFP拟合θFP和θO参数。适当调整输出子网络的第一层，以匹配前一层的输出。我们使用二进制交叉熵损失函数拟合所有模型。给定标签为y的输入x的深度学习模型f（x；θ）的输出∈
{0，1}，模型参数θ损失为：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220528171648185.png" alt="image-20220528171648185" style="zoom:50%;"></p>
<p>通过优化器，我们求解ˆθ的最佳参数集，以最小化数据集上的组合损失：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220528171710886.png" alt="image-20220528171710886" style="zoom:50%;"></p>
<p>其中M是数据集中的样本数，y（i）和x（i）分别是第i个训练样本的标签和特征向量。我们使用Keras框架构建和训练模型，使用Adam优化器和Keras的默认参数和1024个小批量。每个模型都经过15个阶段的训练，我们确定这些阶段足以使结果收敛。</p>
<h3><span id="4-实验分析">4、实验分析</span></h3>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181509685.png" alt="image-20220528172252766" style="zoom:50%;"></p>
<h3><span id="5-总结">5、总结</span></h3>
<p>我们已经证明，深度神经网络恶意软件检测器可以从合并来自文件路径的上下文信息中获益，即使这些信息本身不是恶意或良性的。<strong>将文件路径添加到我们的检测模型中不需要任何额外的端点检测，并且在整个相关FPR区域的总体ROC曲线中提供了统计上显著的改善</strong>。我们直接在客户端点分布上测量模型的性能这一事实表明，我们的多视图模型实际上可以部署到端点以检测恶意软件。我们在第IV-B节中进行的LIME分析表明，多视图模型能够提取暗示实际恶意/良性概念的上下文信息；不只是数据集的统计伪影，尽管正如我们所观察到的，它还可以学习这些伪影。除了端点部署之外，这项研究可以应用的另一个潜在领域是端点检测和响应（EDR）环境，在该环境中，我们的模型的输出可以用于根据事件的可疑程度对磁盘上的事件进行排序。有趣的是，石灰等技术在这方面也有应用。使用从LIME或类似方法得出的解释，可以创建分析工具，允许非恶意软件/取证专家的用户执行某种程度的威胁搜寻。如图4所示，重要性突出显示不仅对用户有用，而且是最近邻/相似性可视化方法的替代方法，该方法不会显示其他用户的潜在可识别信息（PII）。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/7X1P6X/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/7X1P6X/" class="post-title-link" itemprop="url">恶意软件检测（4）【draft】You are What you Do</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-27 21:30:20" itemprop="dateCreated datePublished" datetime="2022-05-27T21:30:20+08:00">2022-05-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 12:20:42" itemprop="dateModified" datetime="2023-04-18T12:20:42+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/" itemprop="url" rel="index"><span itemprop="name">学术前沿</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>120</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="youare-what-you-do-hunting-stealthy-malware-via-data-provenanceanalysis">You
Are What You Do: Hunting Stealthy Malware via Data Provenance
Analysis</span></h2>
<p>https://blog.csdn.net/ll14856lk/article/details/122151992</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/9/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/11/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
