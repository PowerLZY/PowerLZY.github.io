<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/10/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/10/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/10/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">117</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3PJZDND/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3PJZDND/" class="post-title-link" itemprop="url">特征工程（1）特征预处理*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-09 14:36:44" itemprop="dateModified" datetime="2023-04-09T14:36:44+08:00">2023-04-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>24 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="特征工程-特征处理">特征工程-特征处理</span></h2><blockquote>
<p>  <strong>机器学习中的特征工程（四）—— ==特征离散化处理方法==：</strong><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/918649ce379a">https://www.jianshu.com/p/918649ce379a</a></p>
<p>  <strong>机器学习中的特征工程（三）—— ==序数和类别特征处理方法==</strong>：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/3d828de72cd4">https://www.jianshu.com/p/3d828de72cd4</a></p>
<p>  <strong>机器学习中的特征工程（二）—— ==数值类型数据处理==</strong>：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/b0cc0710ef55">https://www.jianshu.com/p/b0cc0710ef55</a></p>
<p>  机器学习中的特征工程（一）—— 概览：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/172677f4ea4c">https://www.jianshu.com/p/172677f4ea4c</a></p>
<p>  特征工程完全手册 - 从预处理、构造、选择、降维、不平衡处理，到放弃：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94994902">https://zhuanlan.zhihu.com/p/94994902</a></p>
<p>  <strong>这9个特征工程使用技巧，解决90%机器学习问题！</strong> - Python与数据挖掘的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/462744763">https://zhuanlan.zhihu.com/p/462744763</a></p>
</blockquote>
<h3><span id="一-数值类型处理">一、 数值类型处理</span></h3><blockquote>
<p>  <strong>pandas 显示所有列：</strong></p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#显示所有列</span>
pd<span class="token punctuation">.</span>set_option<span class="token punctuation">(</span><span class="token string">'display.max_columns'</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment">#显示所有行</span>
pd<span class="token punctuation">.</span>set_option<span class="token punctuation">(</span><span class="token string">'display.max_rows'</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment">#设置value的显示长度为100，默认为50</span>
pd<span class="token punctuation">.</span>set_option<span class="token punctuation">(</span><span class="token string">'max_colwidth'</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>  <strong>pandas 查看缺失特征:</strong></p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python">train<span class="token punctuation">.</span>isnull<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sort_values<span class="token punctuation">(</span>ascending <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token operator">/</span> train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>  <strong>pandas 查看某一列的分布:</strong></p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python">df<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>col_name<span class="token punctuation">]</span><span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</blockquote>
<p><strong>特征提取方式是可以深挖隐藏在数据背后更深层次的信息的</strong>。其次，数值类型数据也并不是直观看上去那么简单易用，因为不同的数值类型的计量单位不一样，比如个数、公里、千克、DB、百分比之类，同样数值的大小也可能横跨好几个量级，比如小到头发丝直径约为0.00004米， 大到热门视频播放次数成千上万次。</p>
<h4><span id="11-数据归一化">1.1 数据归一化</span></h4><blockquote>
<p>  <strong>为什么要数据归一化？</strong></p>
<p>   <a href="../../AI深度学习/深度学习（3）Normalization*.md">深度学习（3）Normalization*.md</a> </p>
<ul>
<li><strong>可解释性</strong>：<strong>回归模型【无正则化】</strong>中自变量X的量纲不一致导致了<strong>==回归系数无法直接解读==</strong>或者错误解读；需要将X都处理到统一量纲下，这样才可比【可解释性】；<strong>取决于我们的逻辑回归是不是用了正则化</strong>。如果你不用正则，标准化并不是必须的，如果用正则，那么标准化是必须的。</li>
<li><strong>距离计算</strong>：机器学习任务和统计学任务中有很多地方要用到<strong>==“距离”的计算==</strong>，比如<strong>PCA、KNN，kmeans和SVM</strong>等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；</li>
<li><p><strong>加速收敛</strong>：参数估计时使用<strong>==梯度下降==</strong>，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即<strong>==提升模型的收敛速度==</strong>。</p>
<p><strong>需要归一化的模型：</strong>利用梯度下降法求解的模型一般需要归一化，<strong>线性回归、LR、SVM、KNN、神经网络</strong></p>
</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\tilde{x}=\frac{x-\min (x)}{\max (x)-\min (x)}</script><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> MinMaxScaler

<span class="token comment"># define data </span>
data <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">0.001</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                   <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                   <span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">0.005</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                   <span class="token punctuation">[</span><span class="token number">88</span><span class="token punctuation">,</span> <span class="token number">0.07</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                   <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># define min max scaler</span>
scaler <span class="token operator">=</span> MinMaxScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># transform data</span>
scaled <span class="token operator">=</span> scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>data<span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="12-数据标准化">1.2 数据标准化</span></h4><p>数据标准化是指通过改变数据的分布得到均值为0，标准差为1的服从标准正态分布的数据。主要目的是为了让不同特征之间具有相同的尺度（Scale），这样更有理化模型训练收敛。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler
<span class="token comment"># define standard scaler</span>
scaler <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># transform data</span>
scaled <span class="token operator">=</span> scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>data<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span>scaled<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="13-对数转换">==1.3 对数转换==</span></h4><p>log函数的定义为<img src="https://math.jianshu.com/math?formula=log_a(%5Calpha%5Ex" alt="log_a(\alpha^x) = x">%20%3D%20x)，其中a是log函数的底数，<img src="https://math.jianshu.com/math?formula=%5Calpha" alt="\alpha">是一个正常数，<img src="https://math.jianshu.com/math?formula=x" alt="x">可以是任何正数。由于<img src="https://math.jianshu.com/math?formula=%5Calpha%5E0%20%3D%201" alt="\alpha^0 = 1">，我们有<img src="https://math.jianshu.com/math?formula=log_a(1" alt="log_a(1) = 0">%20%3D%200)。这意味着log函数可以将一些介于0~1之间小范围的数字映射到<img src="https://math.jianshu.com/math?formula=(-%5Cinfty%2C%200" alt="(-\infty, 0)">)范围内。比如当a=10时，函数<img src="https://math.jianshu.com/math?formula=log_%7B10%7D(x" alt="log_{10}(x)">)可以将[1,10]映射到[0,1]，将[1,100]映射到[1,2]。<strong>换句话说，log函数压缩了大数的范围，扩大了小数的范围</strong>。x越大，log(x)增量越慢。log(x)函数的图像如下：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426154119370.png" alt="image-20220426154119370" style="zoom: 25%;"></p>
<p><strong>Log函数可以极大压缩数值的范围，相对而言就扩展了小数字的范围。该转换方法适用于长尾分布且值域范围很大的特征，变换后的特征趋向于正态分布。</strong>对数值类型使用对数转换一般有以下几种好处：</p>
<ul>
<li>缩小数据的绝对数值</li>
<li>取对数后，可以将乘法计算转换成加法计算</li>
<li>在数据的整个值域中不同区间的差异带来的影响不同</li>
<li>取对数后不会改变数据的性质和相关关系，但压缩了变量的尺度。</li>
<li>得到的数据易消除异方差问题</li>
</ul>
<h3><span id="二-序数和类别特征处理">二、序数和类别特征处理</span></h3><p>本文主要说明特征工程中关于<strong>序数特征</strong>和<strong>类别特征</strong>的常用处理方法。主要包含<strong>LabelEncoder</strong>、<strong>One-Hot编码</strong>、<strong>DummyCoding</strong>、<strong>FeatureHasher</strong>以及要重点介绍的<strong>WOE编码</strong>。</p>
<h4><span id="21-序数特征处理">2.1 序数特征处理</span></h4><p><strong>序数特征指的是有序但无尺度的特征</strong>。比如表示‘学历’的特征，’高中’、’本科’、’硕士’，这些特征彼此之间是有顺序关系的，但是特征本身无尺度，并且也可能不是数值类型。在实际应用中，一般是字符类型居多，为了将其转换成模型能处理的形式，通常需要先进行编码，比如LabelEncoding。如果序数特征本身就是数值类型变量，则可不进行该步骤。下面依次介绍序数特征相关的处理方式。</p>
<ul>
<li><h4><span id="label-encoding">Label Encoding</span></h4></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> LabelEncoder

x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">]</span>
encoder <span class="token operator">=</span> LabelEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
x1 <span class="token operator">=</span> encoder<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

x2 <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'category'</span><span class="token punctuation">)</span>
x2<span class="token punctuation">.</span>cat<span class="token punctuation">.</span>codes<span class="token punctuation">.</span>values
<span class="token comment"># pandas 因子化</span>
x2<span class="token punctuation">,</span> uniques <span class="token operator">=</span> pd<span class="token punctuation">.</span>factorize<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token comment"># pandas 二值化</span>
x2 <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
x2 <span class="token operator">=</span> <span class="token punctuation">(</span>x2 <span class="token operator">>=</span> <span class="token string">'b'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token comment">#令大于等于'b'的都为1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="22-类别特征处理">2.2 类别特征处理</span></h4><p><strong>类别特征由于没有顺序也没有尺度</strong>，因此处理较为麻烦，但是在CTR等领域却是非常常见的特征。比如<strong>商品的类型，颜色，用户的职业，兴趣</strong>等等。类别变量编码方法中最常使用的就是<strong>One-Hot编码</strong>，接下来结合具体实例来介绍。</p>
<ul>
<li><h4><span id="one-hot编码">One-Hot编码</span></h4></li>
</ul>
<p>One-Hot编码，又称为’独热编码’，其变换后的单列特征值只有一位是1。如下例所示，一个特征中包含3个不同的特征值(a,b,c)，编码转换后变成3个子特征，其中每个特征值中只有一位是有效位1。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> LabelEncoder<span class="token punctuation">,</span> OneHotEncoder

one_feature <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">]</span>
label_encoder <span class="token operator">=</span> LabelEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
feature <span class="token operator">=</span> label_encoder<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>one_feature<span class="token punctuation">)</span>
onehot_encoder <span class="token operator">=</span> OneHotEncoder<span class="token punctuation">(</span>sparse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
onehot_encoder<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>feature<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><h4><span id="labelbinarizer">LabelBinarizer</span></h4></li>
</ul>
<p>sklearn中的LabelBinarizer也具有同样的作用，代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> LabelBinarizer
feature <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
LabelBinarizer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>feature<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<ul>
<li><h4><span id="虚拟编码dummy-coding">虚拟编码Dummy Coding</span></h4></li>
</ul>
<p>同样，<strong>pandas中也内置了对应的处理方式,使用起来比Sklearn更加方便</strong>，产生n-1个特征。实例如下：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">one_feature <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'b'</span>, <span class="token string">'a'</span>, <span class="token string">'c'</span><span class="token punctuation">]</span>
pd.get_dummies<span class="token punctuation">(</span>one_feature, <span class="token assign-left variable">prefix</span><span class="token operator">=</span><span class="token string">'test'</span><span class="token punctuation">)</span> <span class="token comment"># 设置前缀test</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<ul>
<li><h4><span id="特征哈希feature-hashing"><strong><font color="red"> 特征哈希（feature hashing）</font></strong></span></h4></li>
</ul>
<p>按照上述编码方式，如果某个特征具有100个类别值，那么经过编码后将产生100个或99个新特征，这极大地增加了特征维度和特征的稀疏度，同时还可能会出现内存不足的情况。<strong>sklearn中的FeatureHasher接口采用了hash的方法，将不同的值映射到用户指定长度的数组中，使得输出特征的维度是固定的，该方法占用内存少，效率高，可以在多类别变量值中使用，但是由于采用了Hash函数的方式，所以具有冲突的可能，即不同的类别值可能映射到同一个特征变量值中。</strong></p>
<blockquote>
<p>  Feature hashing(特征哈希): <a target="_blank" rel="noopener" href="https://blog.csdn.net/laolu1573/article/details/79410187">https://blog.csdn.net/laolu1573/article/details/79410187</a></p>
<p>  <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing">https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing</a></p>
<p>  <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/264165760/answer/277634591">如何用通俗的语言解释CTR和推荐系统中常用的<em>Feature</em> <em>Hashing</em>技术以及其对应的优缺点？</a></p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction <span class="token keyword">import</span> FeatureHasher

h <span class="token operator">=</span> FeatureHasher<span class="token punctuation">(</span>n_features<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> input_type<span class="token operator">=</span><span class="token string">'string'</span><span class="token punctuation">)</span>
test_cat <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">,</span><span class="token string">'b'</span><span class="token punctuation">,</span><span class="token string">'c'</span><span class="token punctuation">,</span><span class="token string">'d'</span><span class="token punctuation">,</span><span class="token string">'e'</span><span class="token punctuation">,</span><span class="token string">'f'</span><span class="token punctuation">,</span><span class="token string">'g'</span><span class="token punctuation">,</span><span class="token string">'h'</span><span class="token punctuation">,</span><span class="token string">'i'</span><span class="token punctuation">,</span><span class="token string">'j'</span><span class="token punctuation">,</span><span class="token string">'a'</span><span class="token punctuation">,</span><span class="token string">'b'</span><span class="token punctuation">]</span>
f <span class="token operator">=</span> h<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>test_cat<span class="token punctuation">)</span>
f<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>如果hash的目标空间足够大，并且hash函数本身足够散列，不会损失什么特征信息。</strong></p>
<p>feature hashing简单来说和<strong>kernal的思想</strong>是类似的，就是把输入的特征映射到一个具有一些我们期望的较好性质的空间上去。在feature hasing这个情况下我们希望目标的空间具有如下的性质：</p>
<ol>
<li><strong>样本无关的维度大小，因为当在线学习，或者数据量非常大，提前对数据观察开销非常大的时候，这可以使得我们能够提前给算法分配存储和切分pattern。大大提高算法的工程友好性</strong>。</li>
<li>这个空间一般来说比输入的特征空间维度小很多。</li>
<li>另外我们假设在原始的特征空间里，样本的分布是非常稀疏的，只有很少一部分子空间是被取值的。</li>
<li><strong>保持内积的无偏</strong>（不变肯定是不可能的，因为空间变小了），否则很多机器学习方法就没法用了。</li>
</ol>
<blockquote>
<p>  <strong>原理</strong>：假设输入特征是一个N维的0/1取值的向量x。一个N-&gt;M的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=哈希函数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">哈希函数</a>h。那么 <img src="https://www.zhihu.com/equation?tex=%5Cphi_j%3D%5Csum_%7Bh%28i%29%3Dj%7D%7Bx_i%7D" alt="[公式]"></p>
<p>  好处：</p>
<ul>
<li>从某种程度上来讲，使得训练样本的特征在对应空间里的<strong>分布更均匀</strong>了。这个好处对于实际训练过程是非常大的，某种程度上起到了<strong>shuffle的作用</strong>。</li>
<li>特征的空间变小了，而且是一个可以预测的大小。比如说加入输入特征里有个东西叫做user_id，那么显然你也不知道到底有多少<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=userid&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">userid</a>的话，你需要先扫描一遍并且分配足够的空间给到它不然学着学着oom了。你也不能很好地提前优化<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=分片&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">分片</a>。</li>
<li><p>对在线学习非常友好。</p>
<p>坏处：</p>
</li>
<li><p>会给debug增加困难，为了debug你要保存记录h计算的过程数据，否则如果某个特征有毛病，你怎么知道到底是哪个原始特征呢？</p>
</li>
<li>没选好哈希函数的话，<strong>可能会造成碰撞</strong>，如果原始特征很稠密并且碰撞很严重，那可能会带来坏的训练效果。</li>
</ul>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">hashing_vectorizer</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>
  	x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> N
    <span class="token keyword">for</span> f <span class="token keyword">in</span> features<span class="token punctuation">:</span>
      	h <span class="token operator">=</span> <span class="token builtin">hash</span><span class="token punctuation">(</span>f<span class="token punctuation">)</span>
        idx <span class="token operator">=</span> h <span class="token operator">%</span> N
        <span class="token keyword">if</span> xt<span class="token punctuation">(</span>f<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token comment"># xt 2值hash函数减少hash冲突</span>
          	x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
         		x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">-=</span> <span class="token number">1</span>
    <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><h4><span id="多类别值处理方式-基于统计的编码方法">多类别值处理方式 -==- 基于统计的编码方法==</span></h4></li>
</ul>
<p>当类别值过多时，<strong>One-Hot 编码或者Dummy Coding都可能导致编码出来的特征过于稀疏</strong>，其次也会占用过多内存。<strong>如果使用FeatureHasher，n_features的设置不好把握，可能会造成过多冲突，造成信息损失</strong>。这里提供一种基于统计的编码方法，包括<strong>基于特征值的统计</strong>或者<strong>基于标签值的统计</strong>——基于标签的编码。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">import</span> seaborn as sns

<span class="token builtin class-name">test</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'a'</span>,<span class="token string">'b'</span>,<span class="token string">'c'</span>,<span class="token string">'d'</span>,<span class="token string">'e'</span>,<span class="token string">'a'</span>,<span class="token string">'a'</span>,<span class="token string">'c'</span><span class="token punctuation">]</span>
<span class="token function">df</span> <span class="token operator">=</span> pd.DataFrame<span class="token punctuation">(</span>test, <span class="token assign-left variable">columns</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'alpha'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
sns.countplot<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'alpha'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426130800412.png" alt="image-20220426130800412" style="zoom:50%;"></p>
<p>首先我们将每个类别值出现的频数计算出来，比如我们设置阈值为1，那么所有小于阈值1的类别值都会被编码为同一类，大于1的类别值会分别编码，如果出现频数一样的类别值，既可以都统一分为一个类，也可以按照某种顺序进行编码，这个可以根据业务需要自行决定。那么根据上图，可以得到其编码值为：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426130824066.png" alt="image-20220426130824066" style="zoom:50%;"></p>
<p> <strong>即（a,c）分别编码为一个不同的类别，（e,b,d）编码为同一个类别。</strong></p>
<h3><span id="23-二阶">2.3 二阶</span></h3><blockquote>
<p>  w * h = s</p>
</blockquote>
<h3><span id="三-特征离散化处理方法">三、 特征离散化处理方法</span></h3><p><strong>特征离散化指的是将连续特征划分离散的过程</strong>：将原始定量特征的一个区间一一映射到单一的值。离散化过程也被表述成分箱（Binning）的过程。特征离散化常应用于<strong>逻辑回归</strong>和金融领域的评分卡中，同时在规则提取，特征分类中也有对应的应用价值。本文主要介绍几种常见的分箱方法，包括<strong>等宽分箱、等频分箱、信息熵分箱</strong>、<strong>基于决策树分箱、卡方分箱</strong>等。</p>
<p><strong>可以看到在分箱之后，数据被规约和简化，有利于理解和解</strong>释。总来说特征离散化，即 分箱之后会带来如下优势：</p>
<ul>
<li>有助于模型部署和应用，加快模型迭代</li>
<li>增强模型鲁棒性</li>
<li>增加非线性表达能力：连续特征不同区间对模型贡献或者重要程度不一样时，分箱后不同的权重能直接体现这种差异，离散化后的特征再进行特征 交叉衍生能力会进一步加强。</li>
<li>提升模型的泛化能力</li>
<li><strong>扩展数据在不同各类型算法中的应用范围</strong></li>
</ul>
<p>当然特征离散化也有其缺点，总结如下：</p>
<ul>
<li>分箱操作必定会导致一定程度的信息损失</li>
<li>增加流程：建模过程中加入了额外的的离散化步骤</li>
<li>影响模型稳定性： 当一个特征值处于分箱点的边缘时，此时微小的偏差会造成该特征值的归属从一箱跃迁到另外一箱，影响模型的稳定性。</li>
</ul>
<h4><span id="31-等宽分箱equal-width-binning">3.1 等宽分箱（Equal-Width Binning)</span></h4><p><strong>等宽分箱指的是每个分隔点或者划分点的距离一样，即等宽</strong>。实践中一般指定分隔的箱数，等分计算后得到每个分隔点。例如将数据序列分为n份，则 分隔点的宽度计算公式为：</p>
<p><img src="https://math.jianshu.com/math?formula=w%20%3D%20%5Cfrac%20%7Bmax%20-%20min%7D%20%7Bn%7D" alt="w = \frac {max - min} {n}"></p>
<p>这样就将原始数据划分成了n个等宽的子区间，一般情况下，分箱后每个箱内的样本数量是不一致的。使用pandas中的cut函数来实现等宽分箱，代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">value<span class="token punctuation">,</span> cutoff <span class="token operator">=</span> pd<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'mean radius'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bins<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> retbins<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> precision<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>等宽分箱计算简单，但是当数值方差较大时，即数据离散程度很大，那么很可能出现没有任何数据的分箱</strong>，这个问题可以通过自适应数据分布的分箱方法—等频分箱来避免</p>
<h4><span id="32-等频分箱equal-frequency-binning">3.2 等频分箱（Equal-Frequency Binning）</span></h4><p><strong>等频分箱理论上分隔后的每个箱内得到数据量大小一致</strong>，但是当某个值出现次数较多时，会出现等<strong>分边界是同一个值</strong>，导致同一数值分到不同的箱内，这是不正确的。具体的实现可以<strong>去除分界处的重复值</strong>，但这也导致每箱的数量不一致。如下代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">s1 <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
value<span class="token punctuation">,</span> cutoff <span class="token operator">=</span> pd<span class="token punctuation">.</span>qcut<span class="token punctuation">(</span>s1<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> retbins<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
sns<span class="token punctuation">.</span>countplot<span class="token punctuation">(</span>value<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p><strong>上述的等宽和等频分箱容易出现的问题是每箱中信息量变化不大</strong>。例如，等宽分箱不太适合分布不均匀的数据集、离群值；等频方法不太适合特定的值占比过多的数据集，如<strong>长尾分布</strong>。</p>
<h4><span id="33-信息熵分箱有监督">3.3 信息熵分箱【有监督】</span></h4><p><strong>如果分箱后箱内样本对y的区分度好，那么这是一个好的分箱</strong>。通过信息论理论，我们可知信息熵衡量了这种区分能力。当特征按照某个分隔点划分为上下两部分后能达到最大的信息增益，那么这就是一个好的分隔点。由上可知，信息熵分箱是有监督的分箱方法。<strong><font color="red"> 其实决策树的节点分裂原理也是基于信息熵。</font></strong></p>
<p>首先我们需要明确信息熵和信息增益的计算方式，分别如下：<br><img src="https://math.jianshu.com/math?formula=Entropy(y" alt="Entropy(y) = - \sum_{i=1}^m p_i log_2{p_i} \\ Gain(x) = Entropy(y) - Info_{split}(x)">%20%3D%20-%20%5Csum<em>%7Bi%3D1%7D%5Em%20p_i%20log_2%7Bp_i%7D%20%5C%5C%20Gain(x)%20%3D%20Entropy(y)%20-%20Info</em>%7Bsplit%7D(x))</p>
<p>在二分类问题中，<img src="https://math.jianshu.com/math?formula=m%3D2" alt="m=2">。 信息增益的物理含义表达为：x的分隔带来的信息对y的不确定性带来的增益。<br>对于二值化的单点分隔，如果我们找到一个分隔点将数据一分为二，分成<img src="https://math.jianshu.com/math?formula=P_1" alt="P_1">和<img src="https://math.jianshu.com/math?formula=P_2" alt="P_2">两部分，那么划分后的信息熵的计算方式为：</p>
<p><img src="https://math.jianshu.com/math?formula=Info_%7Bsplit%7D(x" alt="Info_{split}(x) = P1_{ratio}Entropy(x_{p1}) + P2_{ratio}Entropy(x_{p2})">%20%3D%20P1<em>%7Bratio%7DEntropy(x</em>%7Bp1%7D)%20%2B%20P2<em>%7Bratio%7DEntropy(x</em>%7Bp2%7D))</p>
<p>同时也可以看出，当分箱后，某个箱中的标签y的类别（0或者1）的比例相等时，其熵值最大，表明此特征划分几乎没有区分度。而当某个箱中的数据的标签y为单个类别时，那么该箱的熵值达到最小的0，即纯度最纯，最具区分度。从结果上来看，最大信息增益对应分箱后的总熵值最小。</p>
<h4><span id="34-决策树分箱有监督">3.4 决策树分箱【有监督】</span></h4><p><strong>由于决策树的结点选择和划分也是根据信息熵来计算的，因此我们其实可以利用决策树算法来进行特征分箱</strong>，具体做法如下：</p>
<p>还是以乳腺癌数据为例，首先取其中‘mean radius’字段，和标签字段‘target’来拟合一棵决策树，代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier
dt <span class="token operator">=</span> DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">'entropy'</span><span class="token punctuation">,</span> max_depth<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment"># 树最大深度为3</span>
dt<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'mean radius'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> df<span class="token punctuation">[</span><span class="token string">'target'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>接着我们取出这课决策树的所有叶节点的分割点的阈值，如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">qts <span class="token operator">=</span> dt<span class="token punctuation">.</span>tree_<span class="token punctuation">.</span>threshold<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>dt<span class="token punctuation">.</span>tree_<span class="token punctuation">.</span>children_left <span class="token operator">></span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
qts <span class="token operator">=</span> np<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>qts<span class="token punctuation">)</span>
res <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> qts<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h4><span id="35-卡方分箱-有监督">3.5 卡方分箱 【有监督】</span></h4><blockquote>
<p>  <strong>特征选择之卡方分箱、WOE/IV</strong> - 云水僧的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/101771771">https://zhuanlan.zhihu.com/p/101771771</a></p>
</blockquote>
<p><strong><font color="red"> 卡方检验可以用来评估两个分布的相似性，因此可以将这个特性用到数据分箱的过程中。卡方分箱认为：理想的分箱是在同一个区间内标签的分布是相同的</font>;</strong> <strong>卡方分布是概率统计常见的一种概率分布，是卡方检验的基础。卡方分布定义为</strong>：若n个独立的随机变量<img src="https://math.jianshu.com/math?formula=Z_1%2C%20Z_2%2C%20...%2CZ_k" alt="Z_1, Z_2, ...,Z_k">满足标准正态分布<img src="https://math.jianshu.com/math?formula=N(0%2C1" alt="N(0,1)">)，则n个随机变量的平方和<img src="https://math.jianshu.com/math?formula=X%3D%5Csum_%7Bi%3D0%7D%5Ek%20Z_i%5E2" alt="X=\sum_{i=0}^k Z_i^2">为服从自由度为k的卡方分布，记为<img src="https://math.jianshu.com/math?formula=X%20%5Csim%20%5Cchi%5E2" alt="X \sim \chi^2">。参数n称为自由度（样本中独立或能自由变化的自变量的个数)，不同的自由度是不同的分布。</p>
<p><strong>卡方检验</strong> ：卡方检验属于非参数假设检验的一种，其本质都是度量频数之间的差异。其假设为：<strong>观察频数与期望频数无差异或者两组变量相互独立不相关。</strong></p>
<p><img src="https://math.jianshu.com/math?formula=%5Cchi%5E2%20%3D%20%5Csum%20%5Cfrac%20%7B(O-E" alt="\chi^2 = \sum \frac {(O-E)^2}{E}">%5E2%7D%7BE%7D)</p>
<ul>
<li>卡方拟合优度检验：用于检验样本是否来自于某一个分布，比如检验某样本是否为正态分布</li>
<li>独立性卡方检验，查看两组类别变量分布是否有差异或者相关，以列联表的形式比较。以列联表形式的卡方检验中，卡方统计量由上式给出。</li>
</ul>
<h4><span id="步骤">步骤：</span></h4><p>卡方分箱是自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。基本思想: 对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p>
<p><strong>理想的分箱是在同一个区间内标签的分布是相同的</strong>。卡方分箱就是不断的计算相邻区间的卡方值（卡方值越小表示分布越相似），将分布相似的区间（卡方值最小的）进行合并，直到相邻区间的分布不同，达到一个理想的分箱结果。下面用一个例子来解释：</p>
<p><img src="/Users/apple/Library/Application Support/typora-user-images/image-20220708173638301.png" alt="image-20220708173638301" style="zoom: 33%;"></p>
<p> 由上图，第一轮中初始化是5个区间，分别计算相邻区间的卡方值。找到1.2是最小的，合并2、3区间，为了方便，将合并后的记为第2区间，因此得到4个区间。第二轮中，由于合并了区间，影响该区间与前面的和后面的区间的卡方值，因此重新计算1和2,2和4的卡方值，由于4和5区间没有影响，因此不需要重新计算，这样就得到了新的卡方值列表，找到最小的取值2.5，因此该轮会合并2、4区间，并重复这样的步骤，一直到满足终止条件。</p>
<h4><span id="36-woe编码-有监督"><strong><font color="red"> 3.6 WOE编码 【有监督】</font></strong></span></h4><blockquote>
<p>  <strong><font color="red"> 风控模型—WOE与IV指标的深入理解应用</font></strong>: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80134853">https://zhuanlan.zhihu.com/p/80134853</a></p>
</blockquote>
<p><strong>WOE（Weight of Evidence，证据权重）编码利用了标签信息，属于有监督的编码方式。该方式广泛用于金融领域信用风险模型中，是该领域的经验做法。</strong>下面先给出WOE的计算公式：<br> <img src="https://math.jianshu.com/math?formula=WOE_i%20%3D%20ln%5Clbrace%20%5Cfrac%20%7BP_%7By1%7D%7D%7BP_%7By0%7D%7D%20%5Crbrace%20%3D%20ln%5Clbrace%20%5Cfrac%20%7BB_i%20%2F%20B%7D%7BG_i%2FG%7D%20%5Crbrace" alt="WOE_i = ln\lbrace \frac {P_{y1}}{P_{y0}} \rbrace = ln\lbrace \frac {B_i / B}{G_i/G} \rbrace"><br> ==<strong><img src="https://math.jianshu.com/math?formula=WOE_i" alt="WOE_i">值可解释为第<img src="https://math.jianshu.com/math?formula=i" alt="i">类别中好坏样本分布比值的对数</strong>==。其中各个分量的解释如下：</p>
<ul>
<li><img src="https://math.jianshu.com/math?formula=P_%7By1%7D" alt="P_{y1}">表示该类别中坏样本的分布</li>
<li><img src="https://math.jianshu.com/math?formula=P_%7By0%7D" alt="P_{y0}">表示该类别中好样本的分布</li>
<li><img src="https://math.jianshu.com/math?formula=B_i%2FB" alt="B_i/B">表示该类别中坏样本的数量在总体坏样本中的占比</li>
<li><img src="https://math.jianshu.com/math?formula=G_i%2FG" alt="G_i/G">表示该类别中好样本的数量在总体好样本中的占比</li>
</ul>
<p>很明显，如果整个分数的值大于1，那么WOE值为正，否则为负，所以WOE值的取值范围为正负无穷。<br> <strong>WOE值直观上表示的实际上是“当前分组中坏客户占所有坏客户的比例”和“当前分组中好客户占所有坏客户的比例”的差异。</strong>转化公式以后，也可以理解为：当前这个组中坏客户和好客户的比值，和所有样本中这个比值的差异。这个差异为这两个比值的比值，再取对数来表示的。 WOE越大，这种差异越大，这个分组里的样本坏样本可能性就越大，WOE越小，差异越小，这个分组里的坏样本可能性就越小。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment"># 随机生成1000行数据</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>
    <span class="token string">'x'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'R'</span><span class="token punctuation">,</span><span class="token string">'G'</span><span class="token punctuation">,</span><span class="token string">'B'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">'y'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>
<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3><span id="四-缺失值处理解析">四、缺失值处理解析</span></h3><blockquote>
<p>  看不懂你打我，史上最全的缺失值解析: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/379707046">https://zhuanlan.zhihu.com/p/379707046</a></p>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/137175585">https://zhuanlan.zhihu.com/p/137175585</a></p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>机器学习模型</th>
<th>是否支持缺失值</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>XGBoost</strong></td>
<td>是</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>是</td>
</tr>
<tr>
<td>线性回归</td>
<td>否</td>
</tr>
<tr>
<td>逻辑回归（LR）</td>
<td>否</td>
</tr>
<tr>
<td>随机森林（RF）</td>
<td>否</td>
</tr>
<tr>
<td>SVM</td>
<td>否</td>
</tr>
<tr>
<td>因子分解机(FM)</td>
<td>否</td>
</tr>
<tr>
<td>朴实贝叶斯（NB）</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="41-缺失值的替换">4.1 <strong>缺失值的替换</strong></span></h4><p><strong>scikit-learn中填充缺失值的API是Imputer类，使用方法如下：</strong></p>
<p>参数strategy有三个值可选：mean(平均值)，median(中位数)，most_frequent(众数)</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">rom sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> Imputer
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment"># 缺失值填补的时候必须得是float类型</span>
<span class="token comment"># 缺失值要填充为np.nan，它是浮点型，strategy是填充的缺失值类型，这里填充平均数，axis代表轴，这里第0轴是列</span>
im <span class="token operator">=</span> Imputer<span class="token punctuation">(</span>missing_values<span class="token operator">=</span><span class="token string">'NaN'</span><span class="token punctuation">,</span>strategy<span class="token operator">=</span><span class="token string">'mean'</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
data <span class="token operator">=</span> im<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                         <span class="token punctuation">[</span>np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                         <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="42-缺失值的删除">4.2 缺失值的删除</span></h4><h3><span id="五-异常值处理">五、异常值处理</span></h3><h2><span id="数据预处理qampa">数据预处理Q&amp;A</span></h2><h3><span id="1-lr为什么要离散化"><strong><font color="red"> 1、LR为什么要离散化？</font></strong></span></h3><h4><span id="学习-连续特征的离散化在什么情况下将连续的特征离散化之后可以获得更好的效果"></span></h4><p><strong>问题描述：</strong>发现CTR预估一般都是用LR，而且特征都是离散的，为什么一定要用离散特征呢？这样做的好处在哪里？求大拿们解答。</p>
<h4><span id="答案一严林"><strong>答案一（<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=严林&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">严林</a>）：</strong></span></h4><p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ol>
<li>离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>【鲁棒性】离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则为0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>【模型假设】<strong>逻辑回归属于广义线性模型，表达能力受限</strong>；单变量离散化为N个后，每个变量有独立的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li>
<li>【特征交叉】离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险；</li>
</ol>
<p><strong><font color="red"> <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=李沐&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">李沐</a>曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型”同“少量连续特征+复杂模型”的权衡。</font></strong></p>
<blockquote>
<p>  这里我写下我关于上面某些点的理解，有问题的欢迎指出：</p>
<ol>
<li><p>假设目前有两个连续的特征：『年龄』和『收入』，预测用户的『魅力指数』；</p>
<p>第三点: <strong>LR是广义线性模型</strong>，因此如果特征『年龄』不做离散化直接输入，那么只能得到『年龄』和魅力指数的一个线性关系。但是这种线性关系是不准确的，并非年龄越大魅力指一定越大；如果将年龄划分为M段，则可以针对每段有一个对应的权重；这种分段的能力为模型带来类似『折线』的能力，也就是所谓的非线性<br><strong>连续变量的划分，naive的可以通过人为先验知识划分，也可以通过训练单特征的决策树桩，根据Information Gain/Gini系数等来有监督的划分。</strong><br>假如『年龄』离散化后，共有N段，『收入』离散化后有M段；此时这两个离散化后的特征类似于<strong>CategoryFeature</strong>，对他们进行<strong>OneHotEncode</strong>，即可以得到 M + N的 01向量；例如： 0 1 0 0， 1 0 0 0 0；<br>第四点: <strong>特征交叉</strong>，可以理解为上述两个向量的互相作用，作用的方式可以例如是 &amp;和|操作（这种交叉方式可以产生一个 M * N的01向量；）<br>上面特征交叉，可以类比于决策树的决策过程。例如进行&amp;操作后，得到一个1，则可以认为产生一个特征 （a &lt; age &lt; b &amp;&amp; c &lt; income &lt; d）;将特征空间进行的非线性划分，也就是所谓的引入非线性；</p>
</li>
</ol>
</blockquote>
<h4><span id="答案二周开拓"><strong>答案二（周开拓）：</strong></span></h4><p><strong><font color="red"> 机器学习里当然并没有free lunch，一个方法能work，必定是有假设的。如果这个假设和真实的问题及数据比较吻合，就能work。</font></strong></p>
<p>对于LR这类的模型来说，假设基本如下：</p>
<ul>
<li><strong>局部<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=平坦性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">平坦性</a>，或者说连续性</strong>。对于连续特征x来说，在任何一个取值x0的邻域附近，这个特征对预估目标y的影响也在一个足够小的邻域内变化。比如，人年龄对点击率的影响，x0=30岁假设会产生一定的影响，那么x=31或者29岁，这个影响和x0=30岁的影响差距不会太大；</li>
<li><strong>x对y的影响，这个函数虽然局部比较平坦，但是不太规律，如果你知道这个影响是个严格的直线</strong>（或者你有先验知识知道这个影响一定可以近似于一个参数不太多的函数），<strong>显然也没必要去做离散化</strong>。当然这条基本对于绝大多数问题都是成立的，因为基本没有这种好事情。</li>
</ul>
<p>假设一个最简单的问题，binary classification，y=0/1，x是个连续值。你希望学到一个logloss足够小的y=f(x)。</p>
<p>那么有一种做法就是，在数据轴上切若干段，每一段观察训练样本里y为1的比例，以这个比例作为该段上y=f(x)的值。这个当然不是LR训练的过程，但是就是离散化的思想。你可以发现：</p>
<ul>
<li><strong>如果每一段里面都有足够多的样本，那么在这一段里的y=f(x)值的点估计就比较可信</strong>；</li>
<li><font color="red">如果x在数轴上分布不太均匀，比如是<strong>指数分布或者周期分布</strong>的，这么做可能会有问题，因而你要先<strong>对x取个log，或者去掉周期性</strong></font>；</li>
</ul>
<p>这就告诉了你应该怎么做离散化：<strong><font color="red"> 尽可能保证每个分段里面有足够多的样本，尽量让样本的分布在数轴上均匀一些。</font></strong></p>
<p>结语：<strong>本质上连续特征离散化，可以理解为连续信号怎么转化为数字信号，好比我们计算机画一条曲线，也是变成了画一系列线段的问题。</strong>用分段函数来表达一个连续的函数在大多数情况下，都是work的。想取得好的效果需要：</p>
<ul>
<li>你的分段足够小，以使得在每个分段内x对y的影响基本在一个不大的邻域内，或者你可以忍受这个变化的幅度；</li>
<li>你的分段足够大，以使得在每个分段内有足够的样本，以获得可信的f(x)也就是权重；</li>
<li>你的分段策略使得在每个x的分段中，样本的分布尽量均匀（当然这很难），一般会根据先验知识先对x做一些变化以使得变得均匀一些；</li>
<li>如果你有非常强的x对y的先验知识，比如严格线性之类的，也未必做离散化，但是这种先验在计算广告或者推荐系统里一般是不存在的，也许其他领域比如CV之类的里面是可能存在的；</li>
</ul>
<p>最后还有个特别大的<strong>LR用离散特征的好处就是LR的特征是并行的，每个特征是并行同权的</strong>，如果有异常值的情况下，如果这个异常值没见过，那么LR里因为没有这个值的权重，最后对<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=score&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">score</a>的贡献为0，最多效果不够好，但是不会错的太离谱。另外，如果你debug，很容易查出来是哪个段上的权重有问题，比较好定位和解决。</p>
<h3><span id="2-树模型为什么离散化"><strong><font color="red">2、树模型为什么离散化？</font></strong></span></h3><h4><span id="cart树的离散化">Cart树的离散化：</span></h4><p><strong>分类：</strong></p>
<ul>
<li><p><strong><font color="red">如果特征值是连续值：CART的处理思想与C4.5是相同的，即将连续特征值离散化。唯一不同的地方是度量的标准不一样，</font></strong> <strong>CART采用基尼指数，而C4.5采用信息增益比</strong>。</p>
</li>
<li><p>如果当前节点为连续属性，<strong>CART树中该属性（剩余的属性值）后面还可以参与子节点的产生选择过程</strong>。</p>
</li>
</ul>
<p><strong>回归：</strong></p>
<p>对于连续值的处理，<strong>CART 分类树采用基尼系数的大小来度量特征的各个划分点</strong>。<strong>在回归模型中，我们使用常见的和方差度量方式</strong>，对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> ，求出使 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 各自<strong>集合的均方差最小</strong>，同时 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=c_1" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 数据集的样本输出均值， <img src="https://www.zhihu.com/equation?tex=c_2" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 数据集的样本输出均值。</p>
<h4><span id="lgb直方图算法优点">LGB直方图算法优点：</span></h4><p><strong>内存小、复杂度降低、直方图加速【分裂、并行通信、缓存优化】</strong></p>
<ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于 直方图算法，则只需要(1x样本数x维 度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的 bin 值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p>
</li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为$k$的树的时间复杂度：对特征所有取值的排序为$O(NlogN)$，$N$为样本点数目，若有$D$维特征，则$O(kDNlogN)$，而直方图算法需要$O(kD \times bin)$ (bin是histogram 的横轴的数量，一般远小于样本数量$N$)。</p>
</li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>==两个维度==</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的$k$个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
</li>
<li><p><strong>数据并行优化</strong>，用 histgoram 可以大幅降低通信代价。用 pre-sorted 算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst 在并行的时候也使用 histogram 进行通信。</p>
</li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM 所使用直方图算法对 Cache 天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</p>
</li>
</ul>
<h3><span id="3-归一化">3、归一化？</span></h3><h3><span id="4-赵兄-特征处理">4、赵兄-特征处理</span></h3><p><img src="../../../../../Library/Containers/com.tencent.xinWeChat/Data/Library/Application Support/com.tencent.xinWeChat/2.0b4.0.9/076b7987d1501ed1ebeee6aecab0dccc/Message/MessageTemp/f381afde205729b015fb0a76a283f729/Image/891647863651_.pic.jpg" alt="891647863651_.pic"></p>
<ul>
<li>滑动窗口、随机采样、加权差分特征</li>
<li>均值插补插补，稀疏矩阵技术问题</li>
<li>分层统计、差分特征</li>
<li><strong>二阶交叉统计</strong></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/T8EY5H/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/T8EY5H/" class="post-title-link" itemprop="url">特征工程（2）特征选择</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-01 22:51:54" itemprop="dateModified" datetime="2022-07-01T22:51:54+08:00">2022-07-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="二-特征选择">二、特征选择</span></h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/74198735">【机器学习】特征选择(Feature Selection)方法汇总</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/306057603"><em>特征选择方法</em>全面总结</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/479948993"><em>特征选择</em>的基本<em>方法</em>总结</a></p>
</blockquote>
<p> <img src="https://pic4.zhimg.com/v2-05756baf02bd7a023f7b27842594bc2b_b.jpg" alt="img"></p>
<p>训练数据包含许多冗余或无用的特征，移除这些特征并不会导致丢失信息。其中冗余是指一个本身很有用的特征与另外一个有用的特征强相关，或它包含的信息能从其它特征推演出来; 特征很多但样本相对较少</p>
<ul>
<li><p>产生过程：产生特征或特征子集候选集合；</p>
</li>
<li><p>评价函数：衡量特征或特征子集的重要性或者好坏程度，即量化特征变量和目标变量之间的联系以及特征之间的相互联系。为了避免过拟合，可用交叉验证的方式来评估特征的好坏；</p>
</li>
<li><p>停止准则：为了减少计算复杂度，需设定一个阈值，当评价函数值达到阈值后搜索停止</p>
</li>
<li><p>验证过程：在验证数据集上验证选出来的特征子集的有效性</p>
</li>
</ul>
<h3><span id="21-特征选择的目的"><strong>2.1 特征选择的目的</strong></span></h3><p>1.<strong>简化模型</strong>，使模型更易于理解：去除不相关的特征会降低学习任务的难度。并且可解释性能对模型效果的稳定性有更多的把握</p>
<p>2.<strong>改善性能</strong>：节省存储和计算开销</p>
<p>3.<strong>改善通用性、降低过拟合风险</strong>：减轻维数灾难，特征的增多会大大增加模型的搜索空间，大多数模型所需要的训练样本随着特征数量的增加而显著增加。特征的增加虽然能更好地拟合训练数据，但也可能增加方差</p>
<h3><span id="22-特征选择常见方法">2.2 特征选择常见方法</span></h3><ul>
<li><p>==<strong>Filter(过滤法)</strong>==</p>
<ul>
<li><strong>覆盖率</strong></li>
<li><strong>方差选择</strong></li>
<li><strong>Pearson(皮尔森)相关系数</strong></li>
<li><strong>卡方检验</strong></li>
<li><strong>互信息法(KL散度、相对熵)和最大信息系数</strong> </li>
<li>Fisher得分</li>
<li>相关特征选择</li>
<li>最小冗余最大相关性</li>
</ul>
</li>
<li><p><strong>Wrapper(包装法)</strong></p>
<ul>
<li>完全搜索</li>
<li>启发搜索</li>
<li>随机搜索</li>
</ul>
</li>
<li>==<strong>Embedded(嵌入法)</strong>==<ul>
<li>L1 正则项</li>
<li>树模型选择</li>
<li>不重要性特征选择</li>
</ul>
</li>
</ul>
<h3><span id="221-filter过滤法-特征集">2.2.1 <strong>Filter(过滤法)</strong> 【特征集】</span></h3><p><img src="https://pic4.zhimg.com/v2-d91b1bdb2bf6034b9e26b3620bf8a233_b.jpg" alt="img"></p>
<h4><span id="定义"><strong>定义</strong></span></h4><ul>
<li><strong>过滤法的思想就是不依赖模型，仅从特征的角度来做特征的筛选</strong>，具体又可以分为两种方法，一种是根据特征里面包含的信息量，如方差选择法，如果一列特征的方差很小，每个样本的取值都一样的话，说明这个特征的作用不大，可以直接剔除。另一种是对每一个特征，都计算关于目标特征的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=相关度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;479948993&quot;}">相关度</a>，然后根据这个相关度来筛选特征，只保留高于某个阈值的特征，这里根据相关度的计算方式不同就可以衍生出一下很多种方法。</li>
</ul>
<h4><span id="分类"><strong>分类</strong></span></h4><ul>
<li><strong>单变量过滤方法</strong>：不需要考虑特征之间的相互关系，按照特征变量和目标变量之间的相关性或互信息对特征进行排序，过滤掉最不相关的特征变量。优点是计算效率高、不易过拟合。</li>
<li><strong>多变量过滤方法</strong>：考虑特征之间的相互关系，常用方法有基于相关性和一致性的特征选择</li>
</ul>
<h4><span id="覆盖率">==覆盖率==</span></h4><ul>
<li>即特征在训练集中出现的比例。若覆盖率很小，如有10000个样本，但某个特征只出现了5次，则次覆盖率对模型的预测作用不大，可删除</li>
</ul>
<h4><span id="方差选择法">==<strong>方差选择法</strong>==</span></h4><ul>
<li>先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> VarianceThreshold
<span class="token comment"># 方差选择法，返回值为特征选择后的数据</span>
<span class="token comment"># 参数threshold为方差的阈值</span>
VarianceThreshold<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="pearson皮尔森相关系数用于度量两个变量x和y之间的线性相关性">==Pearson皮尔森相关系数==:用于度量两个变量X和Y之间的线性相关性</span></h4><ul>
<li><strong>用于度量两个变量X和Y之间的线性相关性</strong>，结果的取值区间为[-1, 1]， -1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关性</li>
<li>计算方法为两个变量之间的<strong>协方差</strong>和<strong>标准差</strong>的商</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectKBest
<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> pearsonr
  <span class="token comment"># 选择K个最好的特征，返回选择特征后的数据</span>
  <span class="token comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，</span>
  <span class="token comment"># 输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。</span>
  <span class="token comment"># 在此为计算相关系数</span>
  <span class="token comment"># 其中参数k为选择的特征个数</span>
SelectKBest<span class="token punctuation">(</span><span class="token keyword">lambda</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">:</span> array<span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>pearsonr<span class="token punctuation">(</span>x<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span> 
              k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="卡方检验自变量对因变量的相关性">==<strong>卡方检验</strong>==:自变量对因变量的相关性</span></h4><ul>
<li><strong>检验定性自变量对定性因变量的相关性</strong>。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量:</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cchi%5E%7B2%7D%3D%5Csum+%5Cfrac%7B%28A-E%29%5E%7B2%7D%7D%7BE%7D+%5C%5C" alt="[公式]"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectKBest
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> chi2
<span class="token comment">#选择K个最好的特征，返回选择特征后的数据</span>
SelectKBest<span class="token punctuation">(</span>chi2<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="psi互信息法kl散度-相对熵和最大信息系数-mutual-information-and-maximal-information-coefficient-mic">==PSI互信息法==(KL散度、相对熵)和==最大信息系数== Mutual information and maximal information coefficient (MIC)</span></h4><blockquote>
<p>  <strong><font color="red"> 风控模型—群体稳定性指标(PSI)深入理解应用</font></strong>:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79682292">https://zhuanlan.zhihu.com/p/79682292</a></p>
</blockquote>
<ul>
<li>评价定性自变量对定性因变量的相关性，评价类别型变量对类别型变量的相关性，互信息越大表明两个变量相关性越高，互信息为0时，两个变量相互独立。互信息的计算公式为</li>
<li><img src="https://www.zhihu.com/equation?tex=I%28X+%3B+Y%29%3D%5Csum%5Climits_%7Bx+%5Cin+X%7D+%5Csum%5Climits_%7By+%5Cin+Y%7D+p%28x%2C+y%29+%5Clog+%5Cfrac%7Bp%28x%2C+y%29%7D%7Bp%28x%29+p%28y%29%7D%3DD_%7BK+L%7D%28p%28x%2C+y%29+%5C%7C+p%28x%29+p%28y%29%29+%5C%5C" alt="[公式]"></li>
</ul>
<h4><span id="fisher得分">==<strong>Fisher得分</strong>==</span></h4><p>对于分类问题，<strong>好的特征应该是在同一个类别中的取值比较相似</strong>，<strong>而在不同类别之间的取值差异比较大</strong>。因此特征i的重要性可用Fiser得分<img src="https://www.zhihu.com/equation?tex=S_i" alt="[公式]">来表示</p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7Bi%7D%3D%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E%7BK%7D+n_%7Bj%7D%5Cleft%28%5Cmu_%7Bi+j%7D-%5Cmu_%7Bi%7D%5Cright%29%5E%7B2%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BK%7D+n_%7Bj%7D+%5Crho_%7Bi+j%7D%5E%7B2%7D%7D+%5C%5C" alt="[公式]"> </p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=u_%7Bij%7D" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=%5Crho_%7Bij%7D" alt="[公式]">分别是特征i在类别j中均值和方差，<img src="https://www.zhihu.com/equation?tex=%5Cmu_i" alt="[公式]">为特征i的均值，<img src="https://www.zhihu.com/equation?tex=n_j" alt="[公式]">为类别j中的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=样本数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;306057603&quot;}">样本数</a>。Fisher得分越高，<strong>特征在不同类别之间的差异性越大、在同一类别中的差异性越小，则特征越重要</strong>;</p>
<h4><span id="相关特征选择">==<strong>相关特征选择</strong>==</span></h4><p>该方法基于的假设是，好的特征集合包含跟目标变量非常相关的特征，但这些特征之间彼此不相关</p>
<h4><span id="最小冗余最大相关性-mrmr">==<strong>最小冗余最大相关性( mRMR)</strong>==</span></h4><p>由于单变量过滤法只考虑了单特征变量和目标变量之间的相关性，因此选择的特征子集可能过于冗余。mRMR在进行特征时考虑到了特征之间的冗余性，具体做法是对跟已选择特征相关性较高的冗余特征进行惩罚;</p>
<h3><span id="222-wrapper包装法-特征集模型"><strong>2.2.2 Wrapper(包装法)</strong> 【特征集+模型】</span></h3><p><img src="https://pic4.zhimg.com/v2-bd321ff1e16c011d1a2bce86a5939a17_b.jpg" alt="img"></p>
<ul>
<li><p>使用<strong>机器学习算法评估特征子集</strong>的效果，可以检测两个或多个特征之间的交互关系，而且选择的特征子集让模型的效果达到最优。</p>
</li>
<li><p>这是<strong>特征子集搜索</strong>和<strong>评估指标相结合</strong>的方法。前者提供候选的新特征子集，后者基于新特征子集训练一个模型，并用验证集进行评估，为每一组特征子集进行打分。</p>
</li>
<li><p>最简单的方法是在<strong>每一个特征子集上训练并评估模型</strong>，从而找出最优的特征子集</p>
</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>需要对每一组特征子集训练一个模型，<strong>计算量很大</strong></li>
<li>样本不够充分的情况下<strong>容易过拟合</strong></li>
<li>特征变量较多时计算复杂度太高</li>
</ul>
<h4><span id="完全搜索">==完全搜索==</span></h4><ul>
<li>即穷举法，<strong>遍历所有可能的组合达到全局最优</strong>，时间复杂度<img src="https://www.zhihu.com/equation?tex=2%5En" alt="[公式]"></li>
</ul>
<h4><span id="启发式搜索">==<strong>启发式搜索</strong>==</span></h4><ul>
<li>序列向前选择：特征子集从空集开始，每次只加入一个特征，时间复杂度为<img src="https://www.zhihu.com/equation?tex=O%28n%2B%28n-1%29%2B%28n-2%29%2B%5Cldots%2B1%29%3DO%5Cleft%28n%5E%7B2%7D%5Cright%29" alt="[公式]"></li>
<li>序列向后选择：特征子集从全集开始，每次删除一个特征，时间复杂度为<img src="https://www.zhihu.com/equation?tex=O%28n%5E%7B2%7D%29" alt="[公式]"></li>
</ul>
<h4><span id="随机搜索">==<strong>随机搜索</strong>==</span></h4><ul>
<li>执行序列向前或向后选择时，随机选择特征子集</li>
</ul>
<h4><span id="递归特征消除法">==<strong>递归特征消除法</strong>==</span></h4><ul>
<li>使用一个基模型进行多轮训练，每轮训练后通过学习器返回的<strong>coef</strong><em>或者<strong>feature_importances</strong></em>消除若干权重较低的特征，再基于新的特征集进行下一轮训练</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> RFE
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token comment">#递归特征消除法，返回特征选择后的数据</span>
<span class="token comment">#参数estimator为基模型</span>
<span class="token comment">#参数n_features_to_select为选择的特征个数</span>
RFE<span class="token punctuation">(</span>estimator<span class="token operator">=</span>LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    n_features_to_select<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> 
                                          iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3><span id="223-embedded嵌入法">2.2.3 Embedded(嵌入法)</span></h3><p><img src="https://pic4.zhimg.com/v2-9420bbe9f86094fd683ff1fb8919631f_b.jpg" alt="img"></p>
<p>将特征选择嵌入到模型的构建过程中，具有<strong>包装法与机器学习算法相结合的优点</strong>，也具有<strong>过滤法计算效率高的优点</strong></p>
<h4><span id="lasso方法-l1正则项">==<strong>LASSO方法</strong>== L1正则项</span></h4><p>通过对回归系数添加<img src="https://www.zhihu.com/equation?tex=L_1" alt="[公式]">惩罚项来防止过拟合，可以让特定的回归系数变为0，从而可以选择一个不包含那些系数的更简单的模型；实际上，L1惩罚项降维的原理是，在多个对实际上，L1惩罚项降维的原理是，在多个对<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=目标值&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;306057603&quot;}">目标值</a>具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectFromModel
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token comment">#带L1惩罚项的逻辑回归作为基模型的特征选择</span>
SelectFromModel<span class="token punctuation">(</span>LogisticRegression<span class="token punctuation">(</span>
          penalty<span class="token operator">=</span><span class="token string">"l1"</span><span class="token punctuation">,</span> C<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>
               iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="基于树模型的特征选择方法">==<strong>基于树模型的特征选择方法</strong>==</span></h4><ul>
<li>在<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=决策树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;306057603&quot;}">决策树</a>中，深度较浅的节点一般对应的特征分类能力更强(可以将更多的样本区分开)</li>
<li>对于基于决策树的算法，如<strong>随机森林</strong>，重要的特征更有可能出现在深度较浅的节点，而且出现的次数可能越多</li>
<li>即可基于树模型中<strong>特征出现次数</strong>等指标对特征进行重要性排序</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectFromModel
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> GradientBoostingClassifier
<span class="token comment">#GBDT作为基模型的特征选择</span>
SelectFromModel<span class="token punctuation">(</span>
    GradientBoostingClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>
      iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4><span id="使用特征重要性来筛选特征的缺陷">使用特征重要性来筛选特征的缺陷？</span></h4><p>1、特征重要性只能说明哪些特征在训练时起到作用了，<strong>并不能说明特征和目标变量之间一定存在依赖关系</strong>。举例来说，随机生成一大堆没用的特征，然后用这些特征来训练模型，一样可以得到特征重要性，但是这个特征重要性并不会全是0，这是完全没有意义的。</p>
<p>2、<strong>特征重要性容易高估数值特征和基数高的类别特征的重要性</strong>。这个道理很简单，特征重要度是根据决策树分裂前后节点的不纯度的减少量（基尼系数或者MSE）来算的，那么对于数值特征或者基础高的类别特征，不纯度较少相对来说会比较多。</p>
<p>3、<strong>特征重要度在选择特征时需要决定阈值</strong>，要保留多少特征、删去多少特征，这些需要人为决定，并且删掉这些特征后模型的效果也不一定会提升。</p>
<h4><span id="non-importance-选择">==Non importance 选择==</span></h4>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/XZ18TA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/XZ18TA/" class="post-title-link" itemprop="url">特征工程（3）不平衡数据集*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-02 14:49:24" itemprop="dateModified" datetime="2022-07-02T14:49:24+08:00">2022-07-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="不平衡数据问题">不平衡数据问题</span></h2><blockquote>
<p>  <strong>实际上，很多时候，数据不平衡并没有啥负面影响，并不是数据不平衡了，就一定要处理。如果你只是为了做而做，我有99%的自信告诉你，你做了也是白做，啥收益都没有。</strong></p>
<ul>
<li>机器学习中不平衡数据的预处理：<a target="_blank" rel="noopener" href="https://capallen.gitee.io/2019/Deal-with-imbalanced-data-in-ML.html">https://capallen.gitee.io/2019/Deal-with-imbalanced-data-in-ML.html</a></li>
<li>如何处理数据中的「类别不平衡」？：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32940093">https://zhuanlan.zhihu.com/p/32940093</a></li>
<li><strong>==不平衡数据集处理方法==</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/asialee_bird/article/details/83714612">https://blog.csdn.net/asialee_bird/article/details/83714612</a></li>
<li><strong>==不平衡数据究竟出了什么问题？==</strong>：<a target="_blank" rel="noopener" href="https://www.zhihu.com/column/jiqizhixin">https://www.zhihu.com/column/jiqizhixin</a></li>
<li>数据挖掘时，当正负样本不均，代码如何实现改变正负样本权重? - 十三的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/356640889/answer/2299286791">https://www.zhihu.com/question/356640889/answer/2299286791</a></li>
<li><strong>样本权重对逻辑回归评分卡的影响探讨</strong> - 求是汪在路上的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/110982479">https://zhuanlan.zhihu.com/p/110982479</a></li>
</ul>
</blockquote>
<h4><span id="为什么很多模型在训练数据不均衡时会出问题">为什么很多模型在训练数据不均衡时会出问题？</span></h4><p><strong>本质原因是</strong>：<strong>模型在训练时优化的目标函数和在测试时使用的评价标准不一致。</strong>这种”不一致“可能是训练数据的样本分布与测试数据分布不一致；</p>
<h2><span id="一-不平衡数据集的主要处理方式"><strong>一、不平衡数据集的主要处理方式？</strong></span></h2><h3><span id="11-数据的角度">1.1 <strong>数据的角度</strong></span></h3><p>主要方法为采样，分为<strong>欠采样</strong>和<strong>过采样</strong>以及对应的一些改进方法。[<strong>python imblearn库</strong>]<strong><font color="red">尊重真实样本分布，人为主观引入样本权重，反而可能得出错误的结论。</font></strong></p>
<h4><span id="业务角度"><strong><font color="red"> 业务角度</font></strong>：</span></h4><ul>
<li><strong>时间因素</strong>，对近期样本提高权重，较远样本降低权重。这是考虑近期样本与未来样本之间的“相似度”更高，希望模型学到更多近期样本的模式。</li>
<li><strong>贷款类型</strong>，不同额度、利率、期限的样本赋予不同权重，这需要结合业务未来的发展方向。例如，未来业务模式希望是小额、短期、低利率，那就提高这批样本的权重。</li>
<li><strong>样本分群</strong>，不同群体赋予不同权重。例如，按流量获客渠道，如果未来流量渠道主要来自平台A，那么就提高这批样本权重。</li>
</ul>
<h4><span id="技术角度"><strong><font color="red"> 技术角度：</font></strong></span></h4><ul>
<li><p><strong>欠采样</strong>：</p>
<ul>
<li><p><strong>EasyEnsemble</strong>：从多数类$S_{max}$上随机抽取一个子集，与其他类训练一个分类器；重复若干次，多个分类器融合。</p>
</li>
<li><p><font color="red"><strong>BalanceCascade</strong></font>：从多数类$S_{max}$上随机抽取一个子集，与其他类训练一个分类器；<strong>剔除能被分类正确的分类器</strong>，重复若干次，多个分类器融合。</p>
</li>
<li><p>==<strong>NearMIss</strong>：利用K邻近信息挑选具有代表性的样本==。</p>
</li>
<li><p>==<strong>one-side Selection</strong>：采用数据清洗技术==。</p>
</li>
</ul>
</li>
<li><p><strong>==过采样==</strong>：</p>
<ul>
<li><p><strong>随机采样</strong></p>
</li>
<li><p><strong>SMOTE算法</strong>：对少数类$S_{min}$中每个样本x的K近邻随机选取一个样本y，在x，y的连线上随机选取一个点作为新的样本点。</p>
</li>
<li><p>==<strong>Borderline-SMOTE、ADASYN改进算法等</strong>==</p>
</li>
</ul>
</li>
<li><h5><span id="分层抽样技术批量训练分类器的分层抽样技术-当面对不平衡类问题时这种技术通过消除批次内的比例差异可使训练过程更加稳定">==分层抽样技术==：批量训练分类器的「分层抽样」技术。当面对不平衡类问题时，这种技术（通过消除批次内的比例差异）可使训练过程更加稳定。</span></h5></li>
</ul>
<h3><span id="1-2-算法的角度"><strong>1. 2 算法的角度</strong></span></h3><p>考虑<strong>不同误分类情况代价的差异性</strong>对算法进行优化，主要是基于<strong>代价敏感学习算法</strong>(Cost-Sensitive Learning)，代表的算法有<strong>==adacost==</strong>。<a href="实现基于代价敏感的AdaCost算法">实现基于代价敏感的AdaCost算法</a></p>
<ul>
<li><p><strong>代价函数</strong>：可以增加小类样本的权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。刚开始，可以设置每个类别的权值与样本个数比例的倒数，然后可以使用过采样进行调优。</p>
<blockquote>
<p>  这种方法的难点在于设置合理的权重，实际应用中一般让各个分类间的加权损失值近似相等。当然这并不是通用法则，还是需要具体问题具体分析。</p>
</blockquote>
</li>
<li><h5><span id="xgb自定义损失函数-jhwjhw0123imbalance-xgboost-focal-loss">XGB自定义损失函数 /<strong><a target="_blank" rel="noopener" href="https://github.com/jhwjhw0123/Imbalance-XGBoost">Imbalance-XGBoost </a></strong>【Focal Loss】：</span></h5><p> <img src="https://camo.githubusercontent.com/d79dd87cc30238a9124e1d26f787aed887a587d6061dcba4cdea9ebaec84836a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c6470697b3135307d2673706163653b4c5f7b777d2673706163653b3d2673706163653b2d5c73756d5f7b693d317d5e7b6d7d5c6861747b797d5f7b697d28312d795f7b697d295e7b5c67616d6d617d5c746578747b6c6f677d28795f7b697d292673706163653b2b2673706163653b28312d5c6861747b797d5f7b697d29795f7b697d5e7b5c67616d6d617d5c746578747b6c6f677d28312d795f7b697d29" alt="img" style="zoom: 67%;"></p>
</li>
</ul>
<h3><span id="13-分类方式">1.3 <strong>分类方式</strong></span></h3><p>可以把小类样本作为<strong>异常点</strong>(outliers)，把问题转化为<strong>异常点检测问题(anomaly detection)</strong>。此时分类器需要学习到大类的决策分界面，即分类器是一个<strong>单个类分类器（One Class Classifier）</strong>。代表的算法有<font color="red"> <strong>One-class SVM</strong></font>。</p>
<blockquote>
<h4><span id="一类分类算法">一类分类算法:</span></h4><p>  不平衡数据集的一类分类算法:<a target="_blank" rel="noopener" href="https://machinelearningmastery.com/one-class-classification-algorithms/">https://machinelearningmastery.com/one-class-classification-algorithms/</a></p>
<p>  一类分类是机器学习的一个领域，它提供了异常值和异常检测的技术,如何使一类分类算法适应具有严重偏斜类分布的不平衡分类,如何拟合和评估 SVM、隔离森林、椭圆包络、局部异常因子等一类分类算法。</p>
<h4><span id="不平数据集的划分方法">不平数据集的划分方法？</span></h4><ul>
<li>K折交叉验证？</li>
<li><p>自助法？</p>
<h4><span id="不平数据集的评价方法">不平数据集的评价方法？</span></h4><p>G-Mean和ROC曲线和AUC。Topk@P</p>
</li>
<li><p><strong>AP衡量的是学出来的模型在每个类别上的好坏，==mAP衡量==的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。</strong></p>
</li>
</ul>
</blockquote>
<h3><span id="二-类别不平衡如何得到一个不错的分类器">二、「类别不平衡」如何得到一个不错的分类器？</span></h3><blockquote>
<p>  <strong>微调</strong>：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32940093">如何处理数据中的「类别不平衡」？</a></p>
</blockquote>
<p>机器学习中常常会遇到数据的<strong>类别不平衡（class imbalance）</strong>，也叫数据偏斜（class skew）。以常见的二分类问题为例，我们希望预测病人是否得了某种罕见疾病。但在历史数据中，阳性的比例可能很低（如百分之0.1）。在这种情况下，学习出好的分类器是很难的，而且在这种情况下得到结论往往也是很具迷惑性的。</p>
<p>以上面提到的场景来说，如果我们的分类器<strong>总是</strong>预测一个人未患病，即预测为反例，那么我们依然有高达99.9%的预测准确率。然而这种结果是没有意义的，这提出了今天的第一个问题，如何有效在类别不平衡的情况下评估分类器？</p>
<p><strong>当然，本文最终希望解决的问题是：在数据偏斜的情况下，如何得到一个不错的分类器？如果可能，是否可以找到一个较为简单的解决方法，而规避复杂的模型、数据处理，降低我们的工作量。</strong></p>
<h4><span id="21-类别不平衡下的评估问题"><strong>2.1 类别不平衡下的评估问题</strong>?</span></h4><p>而<strong>当类别不平衡时，准确率就非常具有迷惑性</strong>，而且意义不大。给出几种主流的评估方法：</p>
<ul>
<li><strong>ROC</strong>是一种常见的替代方法，全名receiver operating curve，计算ROC曲线下的面积是一种主流方法</li>
<li><strong>Precision-recall curve</strong>和ROC有相似的地方，但定义不同，计算此曲线下的面积也是一种方法</li>
<li><strong>Precision@n</strong>是另一种方法，特制将分类阈值设定得到恰好n个正例时分类器的precision</li>
<li>Average precision也叫做平均精度，主要描述了precision的一般表现，在异常检测中有时候会用</li>
<li>直接使用Precision也是一种想法，但此时的假设是分类器的阈值是0.5，因此意义不大</li>
</ul>
<blockquote>
<p>  本文的目的不是介绍一般的分类评估标准，简单的科普可以参看：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/19645541">如何解释召回率与准确率？</a></p>
</blockquote>
<h4><span id="22-解决类别不平衡中的奇淫巧技有什么"><strong>2.2 解决类别不平衡中的“奇淫巧技”有什么？</strong></span></h4><p>对于类别不平衡的研究已经有很多年了，在资料[1]中就介绍了很多比较复杂的技巧。结合我的了解举几个简单的例子：</p>
<blockquote>
<p>  [1] He, H. and Garcia, E.A., 2009. Learning from imbalanced data. <em>IEEE Transactions on knowledge and data engineering</em>, <em>21</em>(9), pp.1263-1284.</p>
</blockquote>
<ul>
<li>对数据进行采用的过程中通过相似性同时生成并插样“少数类别数据”，叫做SMOTE算法</li>
<li>对数据先进行聚类，再将大的簇进行随机欠采样或者小的簇进行数据生成</li>
<li>把监督学习变为无监督学习，舍弃掉标签把问题转化为一个无监督问题，如异常检测</li>
<li><strong>先对多数类别进行随机的欠采样，并结合boosting算法进行集成学习</strong></li>
</ul>
<h4><span id="23-简单通用的算法有哪些">==<strong>2.3 简单通用的算法有哪些？</strong>==</span></h4><ul>
<li>对较多的那个类别进行欠采样(under-sampling)，舍弃一部分数据，使其与较少类别的数据相当</li>
<li>对较少的类别进行过采样(over-sampling)，重复使用一部分数据，使其与较多类别的数据相当</li>
<li><strong>阈值调整（threshold moving）</strong>，将原本默认为0.5的阈值调整到 较少类别/（较少类别+较多类别）即可</li>
</ul>
<p>当然很明显我们可以看出，<strong>第一种和第二种方法都会明显的改变数据分布，我们的训练数据假设不再是真实数据的无偏表述</strong>。在第一种方法中，我们<strong>浪费了很多数据</strong>。而第二类方法中有无中生有或者重复使用了数据，会导致过拟合的发生。</p>
<p><strong>因此欠采样的逻辑中往往会结合集成学习来有效的使用数据，假设正例数据n，而反例数据m个。我们可以通过欠采样，随机无重复的生成（k=n/m）个反例子集，并将每个子集都与相同正例数据合并生成k个新的训练样本。我们在k个训练样本上分别训练一个分类器，最终将k个分类器的结果结合起来，比如求平均值。这就是一个简单的思路，也就是==Easy Ensemble== [5]。</strong></p>
<p>但不难看出，其实这样的过程是需要花时间处理数据和编程的，对于很多知识和能力有限的人来说难度比较大。特此推荐两个简单易行且效果中上的做法：</p>
<ul>
<li>简单的调整阈值，不对数据进行任何处理。此处特指将分类阈值从0.5调整到正例比例</li>
<li>使用现有的集成学习分类器，如随机森林或者xgboost，<strong>==并调整分类阈值==</strong></li>
</ul>
<p><strong>提出这样建议的原因有很多。首先，==简单的阈值调整从经验上看往往比过采样和欠采样有效==</strong> [6]。其次，如果你对统计学知识掌握有限，而且编程能力一般，在集成过程中更容易出错，还不如使用现有的集成学习并调整分类阈值。</p>
<h4><span id="24-一个简单但有效的方案"><strong>2.4 一个简单但有效的方案</strong></span></h4><p>经过了上文的分析，我认为一个比较靠谱的解决方案是：</p>
<ul>
<li>不对数据进行过采样和欠采样，但使用现有的集成学习模型，如随机森林</li>
<li>输出随机森林的预测概率，<strong>调整阈值得到最终结果</strong></li>
<li><strong>选择合适的评估标准，如precision@n</strong></li>
</ul>
<h3><span id="三-脉脉数据集不平衡应该思考什么">三、脉脉：数据集不平衡应该思考什么</span></h3><p>首先, 猜测一下, 你研究的数据存在着较 大的不平衡, 你还是比较关注正类(少数类) 样本的, 比如【想要识别出 有信用风险 的 人】那么就要谈一下你所说的【模型指标还行】这个问题。<strong>auc这种复合指标先不提, precision代表的是, 你预测的信用风险人群, 其中有多少是真的信用风险人群。recall 代表的是, “真的信用风险人群”有多少被你识别出来了</strong>；</p>
<ul>
<li>所以, 倘若你比较关注的是【我想要找出 所有”可能有违约风险的人”】宁可错杀也不 放过。那么你应该重点关注的就是召回率 recall。在此基础上, 尽量提高precision。</li>
<li>你把训练集的正负样本控制在64左右, 那 么你是怎么控制的呢, 是单纯用了数据清理技术, 还是单纯生成了一些新的样本, 还是怎么做的？</li>
<li><font color="red">**如果条件允许, 可以查看一下你被错分的 样本, 看看被错分的原因可能是什么, 是因为类重叠, 还是有少数类的分离还是单纯的因为不平衡比太夸张所以使分类器产生偏倚?** </font></li>
<li><font color="red">不知道你用的什么模型, 但是现在有一些把重采样和分类器结合在一起的集成学习方法, 可以试试看。</font></li>
<li>维度太高的时候, <strong>特征的提取很重要</strong>呀！</li>
<li>当做异常检测问题可能会好一些?</li>
</ul>
<h3><span id="四-样本准备与权重指标">四、样本准备与权重指标</span></h3><blockquote>
<p>  样本权重对逻辑回归评分卡的影响探讨: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/110982479">https://zhuanlan.zhihu.com/p/110982479</a></p>
</blockquote>
<h4><span id="风控业务背景"><strong>风控业务背景</strong></span></h4><p><strong>在统计学习建模分析中，样本非常重要，它是我们洞察世界的窗口</strong>。在建立逻辑回归评分卡时，我们也会考虑对样本赋予不同的权重weight，希望模型学习更有侧重点。</p>
<p>诚然，我们可以通过实际数据测试来检验赋权前后的差异，但我们更希望从理论上分析其合理性。毕竟理论可以指导实践。本文尝试探讨样本权重对逻辑回归评分卡的影响，以及从业务和技术角度分析样本权重调整的操作方法。</p>
<h4><span id="part-1-样本加权对woe的影响"><strong>Part 1. 样本加权对WOE的影响</strong></span></h4><p>在<strong>《</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80134853">WOE与IV指标的深入理解应用</a><strong>》</strong>一文中，我们介绍了WOE的概念和计算方法。在逻辑回归评分卡中，其具有重要意义。其公式定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=++WOE_i+%3D+ln%28+%5Cfrac%7BGood_i%7D%7BGood_T%7D%2F+%5Cfrac%7BBad_i%7D%7BBad_T%7D+%29+%3D+ln%28%5Cfrac%7BGood_i%7D%7BBad_i%7D%29+-+ln%28%5Cfrac%7BGood_T%7D%7BBad_T%7D%29+%5Ctag%7B1%7D" alt="[公式]"></p>
<p><strong>现在，我们思考在计算WOE时，是否要考虑样本权重呢？</strong></p>
<p>如图1所示的样本，我们希望对某些样本加强学习，因此对年龄在46岁以下的样本赋予权重1.5，而对46岁以上的样本赋予权重1.0，也就是加上权重列weight。此时再计算WOE值，我们发现数值发生变化。这是因为权重的改变，既影响了局部bucket中的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> ，也影响了整体的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> 。</p>
<p><img src="https://pic1.zhimg.com/80/v2-538659592a96c40bd4098f29d30d144c_1440w.jpg" alt="img">我们有2种对样本赋权后的模型训练方案，如图2所示。</p>
<ul>
<li>方案1：WOE变换利用原训练集，LR模型训练时利用加权后的样本。</li>
<li>方案2: WOE变换和LR模型训练时，均使用加权后的样本。</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-af2b686a4cdddff60556e90545378f91_1440w.jpg" alt="img"></p>
<p>个人更倾向于第一种方案，原因在于：<strong>WOE变换侧重变量的可解释性，引入样本权重会引起不可解释的困扰。</strong></p>
<h4><span id="part-2-采样对lr系数的影响"><strong>Part 2. 采样对LR系数的影响</strong></span></h4><p>我们定义特征向量 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D+%3D+x_1%2C+x_2%2C...%2C+x_n" alt="[公式]"> 。记 <img src="https://www.zhihu.com/equation?tex=G+%3D+Good%2C+B+%3D+Bad" alt="[公式]"> ，那么逻辑回归的公式组成便是：</p>
<p><img src="https://www.zhihu.com/equation?tex=Ln%28odds%28G%7C%5Ctextbf%7Bx%7D%29%29+%3D+Ln%28%5Cfrac%7Bp_Gf%28%5Ctextbf%7Bx%7D%7CG%29%7D%7Bp_Bf%28%5Ctextbf%7Bx%7D%7CB%29%7D%29+%3D+Ln%28%5Cfrac%7Bp_G%7D%7Bp_B%7D%29+%2B+Ln%28%5Cfrac%7Bf%28%5Ctextbf%7Bx%7D%7CG%29%7D%7Bf%28%5Ctextbf%7Bx%7D%7CB%29%7D%29+%5C%5C+%3D+Ln%28%5Cfrac%7Bp_G%7D%7Bp_B%7D%29+%2B+Ln%28%5Cfrac%7Bf%28x_1%2Cx_2%2C...%2Cx_n%7CG%29%7D%7Bf%28x_1%2Cx_2%2C...%2Cx_n%7CB%29%7D%29+%5C%5C+%3D+Ln%28%5Cfrac%7Bp_G%7D%7Bp_B%7D%29+%2B+Ln%28%5Cfrac%7Bf%28x_1%7CG%29%7D%7Bf%28x_1%7CB%29%7D%29+%2B+Ln%28%5Cfrac%7Bf%28x_2%7CG%29%7D%7Bf%28x_2%7CB%29%7D+%2B+%5Cspace+...+%5Cspace+%2B+Ln%28%5Cfrac%7Bf%28x_n%7CG%29%7D%7Bf%28x_n%7CB%29%7D%5C%5C+%3D+Ln%28odds_%7Bpop%7D%29+%2B+Ln%28odds_%7Binfo%7D+%28%5Ctextbf%7Bx%7D%29%29+%5Ctag%7B2%7D" alt="[公式]"></p>
<p>其中，第2行到第3行的变换是基于朴素贝叶斯假设，即<strong>自变量 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 之间相互独立</strong>。</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=odds_%7Bpop%7D" alt="[公式]"> 是指总体（训练集）的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]">，指<strong>先验信息</strong><img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]">。</li>
<li><img src="https://www.zhihu.com/equation?tex=odds_%7Binfo%7D+%28%5Ctextbf%7Bx%7D%29" alt="[公式]"> 是指自变量引起的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> 变化，我们称为<strong>后验信息</strong><img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> 。</li>
</ul>
<p><strong><font color="red"> 因此，随着观察信息的不断加入，对群体的好坏 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> 判断将越来越趋于客观。</font></strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-0021358b05ebb5fea25073cceea1da2d_1440w.jpg" alt="img"></p>
<h5><span id="样本权重调整直接影响先验项也就是截距-那对系数的影响呢">样本权重调整直接影响先验项，也就是截距。那对系数的影响呢？</span></h5><p>接下来，我们以过采样（Oversampling）和欠采样（Undersampling）为例，分析采样对LR系数的影响。如图4所示，对于不平衡数据集，过采样是指对正样本简单复制很多份；欠采样是指对负样本随机抽样。最终，正负样本的比例将达到1:1平衡状态。<br><img src="https://pic4.zhimg.com/80/v2-c49197fdd37023e75daabd7e74feb11b_1440w.jpg" alt="img">图 4 - 欠采样（左）和过采样（右）</p>
<p>我们同样从贝叶斯角度进行解释：</p>
<h2><span id><img src="https://www.zhihu.com/equation?tex=P%28B%7C%5Ctextbf%7Bx%7D%29+%3D+%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CB%29P%28B%29%7D%7BP%28%5Ctextbf%7Bx%7D%29%7D++%3D+%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CB%29P%28B%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29P%28B%29+%2B+P%28%5Ctextbf%7Bx%7D%7CG%29P%28G%29%7D+%5C%5C+%5CLeftrightarrow+%5Cfrac%7B1%7D%7BP%28B%7C%5Ctextbf%7Bx%7D%29%7D+%3D++1+%2B+%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CG%29P%28G%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29P%28B%29%7D+%5C%5C+%5CLeftrightarrow++Ln%28%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CG%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29%7D%29+%3D+Ln%28%5Cfrac%7B1%7D%7BP%28B%7C%5Ctextbf%7Bx%7D%29%7D+-+1%29++-+Ln%28%5Cfrac%7BP%28G%29%7D%7BP%28B%29%7D%29+%5C%5C+%5CLeftrightarrow++Ln%28%5Cfrac%7BP%28G%7C%5Ctextbf%7Bx%7D%29%7D%7BP%28B%7C%5Ctextbf%7Bx%7D%29%7D%29++%3D+Ln%28%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CG%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29%7D%29+%2B+Ln%28%5Cfrac%7BP%28G%29%7D%7BP%28B%29%7D%29+%5Ctag%7B3%7D" alt="[公式]"></span></h2><p>假设采样处理后的训练集为 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D%27" alt="[公式]"> 。记 <img src="https://www.zhihu.com/equation?tex=%5C%23+B" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5C%23+G" alt="[公式]"> 分别表示正负样本数，那么显然：</p>
<p><img src="https://www.zhihu.com/equation?tex=odds+%3D+%5Cfrac%7B%5C%23+G%7D%7B%5C%23+B%7D+%5Cne+%5Cfrac%7B%5C%23+G%27%7D%7B%5C%23+B%27%7D+%3D+odds%27+%5Ctag%7B4%7D" alt="[公式]"></p>
<p>由于 <img src="https://www.zhihu.com/equation?tex=Ln%28%5Cfrac%7BP%28G%29%7D%7BP%28B%29%7D%29++%3D+Ln%28%5Cfrac%7B%5C%23+G%7D%7B%5C%23+B%7D%29+" alt="[公式]"> ，因此对应<strong>截距将发生变化</strong>。</p>
<p>无论是过采样，还是欠采样，处理后的新样本都和原样本服从同样的分布，即满足：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28%5Ctextbf%7Bx%7D%7CG%29+%3D+P%28%5Ctextbf%7Bx%7D%27%7CG%27%29+%5C%5C+P%28%5Ctextbf%7Bx%7D%7CB%29+%3D+P%28%5Ctextbf%7Bx%7D%27%7CB%27%29+%5Ctag%7B5%7D" alt="[公式]"></p>
<p>因此， <img src="https://www.zhihu.com/equation?tex=Ln%28%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CG%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29%7D%29++%3D+Ln%28%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%27%7CG%27%29%7D%7BP%28%5Ctextbf%7Bx%7D%27%7CB%27%29%7D%29+" alt="[公式]"> ，即<strong>系数不发生变化</strong>。</p>
<p><strong>实践证明，按照做评分卡的方式，做WOE变换，然后跑LR，单变量下确实只有截距影响。而对于多变量，理想情况下，当各自变量相互独立时，LR的系数是不变的，但实际自变量之间多少存在一定的相关性，所以还是会有一定的变化。</strong></p>
<h4><span id="part-3-样本准备与权重指标"><strong><font color="red"> Part 3. 样本准备与权重指标</font></strong></span></h4><p>风控建模的基本假设是<strong>未来样本和历史样本的分布是一致</strong>的。模型从历史样本中拟合 <img src="https://www.zhihu.com/equation?tex=X_%7Bold%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 之间的关系，并根据未来样本的 <img src="https://www.zhihu.com/equation?tex=X_%7Bnew%7D" alt="[公式]"> 进行预测。因此，我们总是在思考，<strong>如何选择能代表未来样本的训练样本。</strong></p>
<p>如图5所示，不同时间段、不同批次的样本总是存在差异，即<strong>服从不同的总体分布</strong>。因此，我们需要<strong>从多个维度来衡量两个样本集之间的相似性。</strong></p>
<p>从迁移学习的角度看，这是一个从源域（source domain）中学习模式，并应用到目标域（target domain）的过程。在这里，源域是训练集，目标域指测试集，或者未来样本。</p>
<p>这就会涉及到一些难点：</p>
<ul>
<li>假设测试集OOT与未来总体分布样本基本一致，但未来样本是不可知且总是在发生变化。</li>
<li>面向测试集效果作为评估指标，会出现在测试集上过拟合现象。<br><img src="https://pic4.zhimg.com/80/v2-b40bd56da51ab37d01fbffbdb8bc91f7_1440w.jpg" alt="img"></li>
</ul>
<p><strong>那么，建模中是否可以考虑建立一个权重指标体系，即综合多方面因素进行样本赋权？我们采取2种思路来分析如何开展。</strong></p>
<p><strong><font color="red"> 业务角度</font></strong>：</p>
<ol>
<li><strong>时间因素</strong>，对近期样本提高权重，较远样本降低权重。这是考虑近期样本与未来样本之间的“相似度”更高，希望模型学到更多近期样本的模式。</li>
<li><strong>贷款类型</strong>，不同额度、利率、期限的样本赋予不同权重，这需要结合业务未来的发展方向。例如，未来业务模式希望是小额、短期、低利率，那就提高这批样本的权重。</li>
<li><strong>样本分群</strong>，不同群体赋予不同权重。例如，按流量获客渠道，如果未来流量渠道主要来自平台A，那么就提高这批样本权重。</li>
</ol>
<p>结合以上各维度，可得到总体采样权重的一种融合方式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=w+%3D+w_1+%2A+w_2+%2A+w_3+%5Ctag%7B6%7D" alt="[公式]"></p>
<p><strong>这种业务角度的方案虽然解释性强，但实际拍定多大的权重显得非常主观，实践中往往需要不断尝试，缺少一些理论指导。</strong></p>
<p><strong><font color="red"> 技术角度：</font></strong></p>
<ol>
<li><strong>过采样、欠采样等，从样本组成上调整正负不平衡</strong>。</li>
<li>代价敏感学习，在损失函数对好坏样本加上不同的代价。比如，坏样本少，分错代价更高。</li>
<li>借鉴Adaboost的做法，对误判样本在下一轮训练时提高权重。</li>
</ol>
<p>在机器学习中，有一个常见的现象——<strong>Covariate Shift</strong>，是指当训练集的样本分布和测试集的样本分布不一致的时候，训练得到的模型无法具有很好的泛化 (Generalization) 能力。</p>
<p>其中一种做法，既然是希望让训练集尽可能像测试集，那就让模型帮助我们做这件事。如图6所示，将测试集标记为1，训练集标记为0，训练一个LR模型，在训练集上预测，概率越高，说明这个样例属于测试集的可能性越大。以此达到样本权重调整的目的。</p>
<p><img src="https://pic1.zhimg.com/80/v2-6a218af9de452a4c58e36a9e8e6d4bb0_1440w.jpg" alt="img"></p>
<h4><span id="part-4-常见工具包的样本赋权"><strong>Part 4. 常见工具包的样本赋权</strong></span></h4><p>现有Logistic Regression模块主要来自sklearn和scipy两个包。很不幸，scipy包并不支持直接赋予权重列。这是为什么呢？有统计学家认为，<strong><font color="red"> 尊重真实样本分布，人为主观引入样本权重，反而可能得出错误的结论。</font></strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-4942078e60d225438f77362c675bec20_1440w.jpg" alt="img"></p>
<p>因此，我们只能选择用scikit-learn。样本权重是如何体现在模型训练过程呢？查看源码后，发现目前主要是体现在<strong>损失函数</strong>中，即<strong>代价敏感学习。</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Logistic loss is the negative of the log of the logistic function.</span>
<span class="token comment"># 添加L2正则项的逻辑回归对数损失函数</span>
out <span class="token operator">=</span> <span class="token operator">-</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>sample_weight <span class="token operator">*</span> log_logistic<span class="token punctuation">(</span>yz<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">.5</span> <span class="token operator">*</span> alpha <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p><strong><font color="red"> 样本权重对决策分割面的影响:</font></strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-699ad7ac670c27a5b0963b8b104ad7fc_1440w.jpg" alt="img"></p>
<p>以下是scikit-learn包中的逻辑回归参数列表说明，可以发现调节样本权重的方法有两种：</p>
<ul>
<li>在class_weight参数中使用balanced</li>
<li>在调用fit函数时，通过sample_weight调节每个样本权重。</li>
</ul>
<p>如果同时设置上述2个参数，<strong>那么样本的真正权重是class_weight * sample_weight.</strong></p>
<p><strong>那么，在评估模型的指标时，是否需要考虑抽样权重，即还原真实场景下的模型评价指标？</strong>笔者认为，最终评估还是需要还原到真实场景下。例如，训练集正负比例被调节为1:1，但这并不是真实的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> ，在预测时将会偏高。因此，仍需要进行模型校准。</p>
<h4><span id="part-5-总结"><strong>Part 5. 总结</strong></span></h4><p>本文系统整理了样本权重的一些观点，但目前仍然没有统一的答案。据笔者所知，目前在实践中还是采取以下几种方案：</p>
<ol>
<li>尊重原样本分布，不予处理，LR模型训练后即为真实概率估计。</li>
<li>结合权重指标综合确定权重，训练完毕模型后再进行校准，还原至真实概率估计。</li>
</ol>
<p>值得指出的是，大环境总是在发生变化，造成样本分布总在偏移。因此，尽可能增强模型的鲁棒性，以及策略使用时根据实际情况灵活调整，两者相辅相成，可能是最佳的使用方法。欢迎大家一起讨论业界的一些做法。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3BJB9WW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3BJB9WW/" class="post-title-link" itemprop="url">工程设计-DaaS-Client</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:53:47" itemprop="dateCreated datePublished" datetime="2022-03-28T10:53:47+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-04-28 21:59:16" itemprop="dateModified" datetime="2022-04-28T21:59:16+08:00">2022-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" itemprop="url" rel="index"><span itemprop="name">模型部署</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="how-自动部署开源ai模型到生产环境">HOW 自动部署开源AI模型到生产环境？</span></h2><blockquote>
<p>  DaaS-Client、Sklearn、XGBoost、LightGBM、和PySpark<em>关注 AI/ML 模型上线、模型部署-程序员宅基地</em></p>
<p>  DaaS-Client：<a target="_blank" rel="noopener" href="https://github.com/autodeployai/daas-client">https://github.com/autodeployai/daas-client</a></p>
<p>3万字长文 PySpark入门级学习教程，框架思维:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/395431025">https://zhuanlan.zhihu.com/p/395431025</a></p>
<p><strong>DMatrix 格式</strong> 在xgboost当中运行速度更快，性能更好。</p>
</blockquote>
<h3><span id="一-背景介绍">一、背景介绍</span></h3><p>AI的广泛应用是由AI在开源技术的进步推动的，利用功能强大的开源模型库，数据科学家们可以很容易的训练一个性能不错的模型。但是因为模型生产环境和开发环境的不同，涉及到不同角色人员：模型训练是数据科学家和数据分析师的工作，但是模型部署是开发和运维工程师的事情，导致模型上线部署却不是那么容易。</p>
<p><strong>DaaS（Deployment-as-a-Service）是AutoDeployAI公司推出的基于Kubernetes的AI模型自动部署系统，提供一键式自动部署开源AI模型生成REST API，以方便在生产环境中调用</strong>。下面，我们主要演示在DaaS中如何部署经典机器学习模型，包括<strong>Scikit-learn、XGBoost、LightGBM、和PySpark ML Pipelines</strong>。关于深度学习模型的部署，会在下一章中介绍。</p>
<h3><span id="二-部署准备">二、部署准备</span></h3><p>我们使用DaaS提供的Python客户端（DaaS-Client）来部署模型，对于XGBoost和LightGBM，我们同样使用它们的Python API来作模型训练。在训练和部署模型之前，我们需要完成以下操作。</p>
<ul>
<li><strong>安装Python DaaS-Client</strong></li>
</ul>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> git+https://github.com/autodeployai/daas-client.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<ul>
<li><strong>初始化DaasClient</strong>。使用DaaS系统的URL、账户、密码登陆系统，文本使用的DaaS演示系统安装在本地的Minikube上。完整Jupyter Notebook，请参考：<a target="_blank" rel="noopener" href="https://github.com/aipredict/ai-deployment/blob/master/deploy-ai-models-in-daas/deploy-sklearn-xgboost-lightgbm-pyspark.ipynb">deploy-sklearn-xgboost-lightgbm-pyspark.ipynb</a></li>
</ul>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">from daas_client <span class="token function">import</span> DaasClient

client <span class="token operator">=</span> DaasClient<span class="token punctuation">(</span><span class="token string">'https://192.168.64.3:30931'</span>, <span class="token string">'username'</span>, <span class="token string">'password'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<ul>
<li><strong>创建项目</strong>。DaaS使用项目管理用户不同的分析任务，一个项目中可以包含用户的各种分析资产：模型、部署、程序脚本、数据、数据源等。项目创建成功后，设置为当前活动项目，发布的模型和创建的部署都会存储在该项目下。<code>create_project</code>函数接受三个参数：<ul>
<li><strong>项目名称</strong>：可以是任意有效的Linux文件目录名。</li>
<li><strong>项目路由</strong>：使用在部署的REST URL中来唯一表示当前项目，只能是小写英文字符(a-z)，数字(0-9)和中横线<code>-</code>，并且<code>-</code>不能在开头和结尾处。</li>
<li><strong>项目说明</strong>（可选）：可以是任意字符。</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">project <span class="token operator">=</span> <span class="token string">'部署测试'</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> client<span class="token punctuation">.</span>project_exists<span class="token punctuation">(</span>project<span class="token punctuation">)</span><span class="token punctuation">:</span>
    client<span class="token punctuation">.</span>create_project<span class="token punctuation">(</span>project<span class="token punctuation">,</span> <span class="token string">'deployment-test'</span><span class="token punctuation">,</span> <span class="token string">'部署测试项目'</span><span class="token punctuation">)</span>
client<span class="token punctuation">.</span>set_project<span class="token punctuation">(</span>project<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><strong>初始化数据</strong>。我们使用流行的分类数据集<code>iris</code>来训练不同的模型，并且把数据分割为训练数据集和测试数据集以方便后续使用。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

seed <span class="token operator">=</span> <span class="token number">123456</span>

iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
iris_target_name <span class="token operator">=</span> <span class="token string">'Species'</span>
iris_feature_names <span class="token operator">=</span> iris<span class="token punctuation">.</span>feature_names
iris_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> columns<span class="token operator">=</span>iris_feature_names<span class="token punctuation">)</span>
iris_df<span class="token punctuation">[</span>iris_target_name<span class="token punctuation">]</span> <span class="token operator">=</span> iris<span class="token punctuation">.</span>target

X<span class="token punctuation">,</span> y <span class="token operator">=</span> iris_df<span class="token punctuation">[</span>iris_feature_names<span class="token punctuation">]</span><span class="token punctuation">,</span> iris_df<span class="token punctuation">[</span>iris_target_name<span class="token punctuation">]</span>
X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span>seed<span class="token punctuation">)</span>    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><strong>模型部署流程。主要包含以下几步</strong>：<ul>
<li><strong>训练模型</strong>。使用模型库提供的API，在<code>iris</code>数据集上训练模型。</li>
<li><strong>发布模型</strong>。调用<code>publish</code>函数发布模型到DaaS系统。</li>
<li><strong>测试模型</strong>（可选）。调用<code>test</code>函数获取测试API信息，可以使用任意的REST客户端程序测试模型在DaaS中是否工作正常，使用的是DaaS系统模型测试API。第一次执行<code>test</code>会比较慢，因为DaaS系统需要启动测试运行时环境。</li>
<li><strong>部署模型</strong>。发布成功后，调用<code>deploy</code>函数部署部署模型。可以使用任意的REST客户端程序测试模型部署，使用的是DaaS系统正式部署API。</li>
</ul>
</li>
</ul>
<h3><span id="三-部署scikit-learn模型">三、部署Scikit-learn模型</span></h3><ul>
<li><strong>训练一个Scikit-learn分类模型</strong>：SVC</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>svm <span class="token keyword">import</span> SVC

model <span class="token operator">=</span> SVC<span class="token punctuation">(</span>probability<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span>seed<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><strong>发布Scikit-learn模型</strong></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">publish_resp <span class="token operator">=</span> client<span class="token punctuation">.</span>publish<span class="token punctuation">(</span>model<span class="token punctuation">,</span>
                            name<span class="token operator">=</span><span class="token string">'iris'</span><span class="token punctuation">,</span>
                            mining_function<span class="token operator">=</span><span class="token string">'classification'</span><span class="token punctuation">,</span>
                            X_test<span class="token operator">=</span>X_test<span class="token punctuation">,</span>
                            y_test<span class="token operator">=</span>y_test<span class="token punctuation">,</span>
                            description<span class="token operator">=</span><span class="token string">'A SVC model'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>test</strong>函数必须要指定前两个参数，第一个<strong>model</strong>是训练的模型对象，第二个是模型名称，其余是可选参数：</p>
<ul>
<li><strong>mining_function</strong>：指定挖掘功能，可以指定为<code>regression</code>（回归）、<code>classification</code>（分类）、和<code>clustering</code>（聚类）。</li>
<li><strong>X_test和y_test</strong>：指定测试训练集，发布时计算模型评估指标，比如针对分类模型，计算正确率（Accuracy），对于回归模型，计算可释方差（explained Variance）。</li>
<li><strong>data_test</strong>： 同样是指定测试训练集，但是该参数用在Spark模型上，非Spark模型通过<code>X_test</code>和<code>y_test</code>指定。</li>
<li><strong>description</strong>：模型描述。</li>
<li><strong>params</strong>：记录模型参数设置。</li>
</ul>
<p><strong>publish_resp</strong>是一个字典类型的结果，记录了模型名称，和发布的模型版本。该模型是<code>iris</code>模型的第一个版本。</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>'model_name'<span class="token operator">:</span> 'iris'<span class="token punctuation">,</span> 'model_version'<span class="token operator">:</span> '<span class="token number">1</span>'<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<ul>
<li><strong>测试Scikit-learn模型</strong></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">test_resp <span class="token operator">=</span> client<span class="token punctuation">.</span>test<span class="token punctuation">(</span>publish_resp<span class="token punctuation">[</span><span class="token string">'model_name'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                        model_version<span class="token operator">=</span>publish_resp<span class="token punctuation">[</span><span class="token string">'model_version'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p><code>test_resp</code>是一个字典类型的结果，记录了测试REST API信息。如下，其中<code>access_token</code>是访问令牌，一个长字符串，这里没有显示出来。<code>endpoint_url</code>指定测试REST API地址，<code>payload</code>提供了测试当前模型需要输入的请求正文格式。</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>
      'access_token'<span class="token operator">:</span> 'A-LONG-STRING-OF-BEARER-TOKEN-USED-IN-HTTP-HEADER-AUTHORIZATION'<span class="token punctuation">,</span>
			'endpoint_url'<span class="token operator">:</span> 'https<span class="token operator">:</span><span class="token comment">//192.168.64.3:30931/api/v1/test/deployment-test/daas-python37-faas/test',</span>
			'payload'<span class="token operator">:</span> <span class="token punctuation">&#123;</span>
      				'args'<span class="token operator">:</span> <span class="token punctuation">&#123;</span>
              			'X'<span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">&#123;</span> 'petal length (cm)'<span class="token operator">:</span> <span class="token number">1.5</span><span class="token punctuation">,</span>
                            'petal width (cm)'<span class="token operator">:</span> <span class="token number">0.4</span><span class="token punctuation">,</span>
                            'sepal length (cm)'<span class="token operator">:</span> <span class="token number">5.7</span><span class="token punctuation">,</span>
                            'sepal width (cm)'<span class="token operator">:</span> <span class="token number">4.4</span>
                          <span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
               'model_name'<span class="token operator">:</span> 'iris'<span class="token punctuation">,</span>
               'model_version'<span class="token operator">:</span> '<span class="token number">1</span>'<span class="token punctuation">&#125;</span>
      <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用requests调用测试API，这里我们直接使用<strong>test_resp</strong>返回的测试payload，您也可以使用自定义的数据<code>X</code>，但是参数<code>model_name</code>和<code>model_version</code>必须使用上面输出的值。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>test_resp<span class="token punctuation">[</span><span class="token string">'endpoint_url'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                        headers<span class="token operator">=</span><span class="token punctuation">&#123;</span>
      <span class="token string">'Authorization'</span><span class="token punctuation">:</span> <span class="token string">'Bearer &#123;token&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>token<span class="token operator">=</span>test_resp<span class="token punctuation">[</span><span class="token string">'access_token'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
                        json<span class="token operator">=</span>test_resp<span class="token punctuation">[</span><span class="token string">'payload'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                        verify<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>返回结果，不同于正式部署API，除了预测结果，测试API会同时返回标准控制台输出和标准错误输出内容，以方便用户碰到错误时，查看相关信息。</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json"># response.json()
<span class="token punctuation">&#123;</span>
      'result'<span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">&#123;</span>
      'PredictedValue'<span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
            'Probabilities'<span class="token operator">:</span> <span class="token punctuation">[</span><span class="token number">0.8977133931668801</span><span class="token punctuation">,</span>
                            <span class="token number">0.05476023239878367</span><span class="token punctuation">,</span>
                            <span class="token number">0.047526374434336216</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
			'stderr'<span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
			'stdout'<span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3><span id="四-部署pyspark模型">四、部署PySpark模型</span></h3><ul>
<li><strong>训练一个PySpark分类模型</strong>：RandomForestClassifier。PySpark模型必须是一个<code>PipelineModel</code>，也就是说必须使用Pipeline来建立模型，哪怕只有一个Pipeline节点。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>ml<span class="token punctuation">.</span>classification <span class="token keyword">import</span> RandomForestClassifier
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>ml<span class="token punctuation">.</span>feature <span class="token keyword">import</span> VectorAssembler
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>ml <span class="token keyword">import</span> Pipeline

spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
df <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>iris_df<span class="token punctuation">)</span>

df_train<span class="token punctuation">,</span> df_test <span class="token operator">=</span> df<span class="token punctuation">.</span>randomSplit<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.7</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> seed<span class="token operator">=</span>seed<span class="token punctuation">)</span>
assembler <span class="token operator">=</span> VectorAssembler<span class="token punctuation">(</span>inputCols<span class="token operator">=</span>iris_feature_names<span class="token punctuation">,</span>
                            outputCol<span class="token operator">=</span><span class="token string">'features'</span><span class="token punctuation">)</span>

rf <span class="token operator">=</span> RandomForestClassifier<span class="token punctuation">(</span>seed<span class="token operator">=</span>seed<span class="token punctuation">)</span><span class="token punctuation">.</span>setLabelCol<span class="token punctuation">(</span>iris_target_name<span class="token punctuation">)</span>
pipe <span class="token operator">=</span> Pipeline<span class="token punctuation">(</span>stages<span class="token operator">=</span><span class="token punctuation">[</span>assembler<span class="token punctuation">,</span> rf<span class="token punctuation">]</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> pipe<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>df_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><strong>发布PySpark模型</strong></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">publish_resp <span class="token operator">=</span> client<span class="token punctuation">.</span>publish<span class="token punctuation">(</span>model<span class="token punctuation">,</span>
                            name<span class="token operator">=</span><span class="token string">'iris'</span><span class="token punctuation">,</span>
                            mining_function<span class="token operator">=</span><span class="token string">'classification'</span><span class="token punctuation">,</span>
                            data_test<span class="token operator">=</span>df_test<span class="token punctuation">,</span>
                            description<span class="token operator">=</span><span class="token string">'A RandomForestClassifier of Spark model'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3><span id="五-模型部署管理">五、模型部署管理</span></h3><p>打开浏览器，登陆DaaS管理系统。进入项目<code>部署测试</code>，切换到<code>模型</code>标签页，有一个<code>iris</code>模型，最新版本是<code>v4</code>，类型是<code>Spark</code>即我们最后发布的模型。<br><img src="https://img-blog.csdnimg.cn/20190916185003145.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTYyNjkwMQ==,size_16,color_FFFFFF,t_70" alt="Daas-models" style="zoom: 33%;"></p>
<p>点击模型，进入模型主页（概述）。当前<code>v4</code>是一个Spark Pipeline模型，正确率是94.23%，并且显示了<code>iris</code>不同版本正确率历史图。下面罗列了模型的输入和输出变量，以及评估结果，当前为空，因为还没有在DaaS中执行任何的模型评估任务。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2FpcHJlZGljdC9haS1kZXBsb3ltZW50L21hc3Rlci9kZXBsb3ktYWktbW9kZWxzLWluLWRhYXMvZGFhcy1tb2RlbC1vdmVydmlldy12NC5qcGc?x-oss-process=image/format,png" alt="Daas-model-overview-v4" style="zoom: 50%;"></p>
<p>点击<code>v4</code>，可以自由切换到其他版本。比如，切换到<code>v1</code>。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2FpcHJlZGljdC9haS1kZXBsb3ltZW50L21hc3Rlci9kZXBsb3ktYWktbW9kZWxzLWluLWRhYXMvZGFhcy1tb2RlbC12ZXJzaW9ucy5qcGc?x-oss-process=image/format,png" alt="DaaS-model-versions" style="zoom:50%;"></p>
<p><code>v1</code>版本是一个Scikit-learn SVM分类模型，正确率是98.00%。其他信息与<code>v4</code>类似。<br><img src="https://img-blog.csdnimg.cn/20190916185146237.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTYyNjkwMQ==,size_16,color_FFFFFF,t_70" alt="DaaS-model-overview-v1" style="zoom:50%;"></p>
<p>切换到模型<code>部署</code>标签页，有一个我们刚才创建的部署<code>iris-svc</code>，鼠标移动到操作菜单，选择<code>修改设置</code>。可以看到，当前部署服务关联的是模型<code>v1</code>，就是我们刚才通过<code>deploy</code>函数部署的<code>iris</code>第一个版本Scikit-learn模型。选择最新的<code>v4</code>，点击命令<code>保存并且重新部署</code>，该部署就会切换到<code>v4</code>版本。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/WH7C02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/WH7C02/" class="post-title-link" itemprop="url">异常检测（1）概述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:12:56" itemprop="dateCreated datePublished" datetime="2022-03-24T14:12:56+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 20:27:42" itemprop="dateModified" datetime="2023-04-18T20:27:42+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">异常检测</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="异常检测-anomaly-detection">异常检测 (anomaly detection)</span></h1><blockquote>
<ul>
<li>「异常检测」开源工具库推荐 - 微调的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37132428">https://zhuanlan.zhihu.com/p/37132428</a></li>
<li><p><strong>数据挖掘中常见的「异常检测」算法有哪些？</strong> - 微调的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/280696035/answer/417091151">https://www.zhihu.com/question/280696035/answer/417091151</a></p>
</li>
<li><p>不得不推荐这门课：<a href="http://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs259d/">CS259D: Data Mining for Cyber Security</a> 虽然是网络安全方面的应用，但是方法都是通用的，看来也很有启发。<a target="_blank" rel="noopener" href="https://leotsui.gitbooks.io/cs259d-notes-cn/content/">https://leotsui.gitbooks.io/cs259d-notes-cn/content/</a></p>
</li>
<li><p>中科院在读美女博士带你全面了解“异常检测”领域 - 王晋东不在家的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/260651151">https://zhuanlan.zhihu.com/p/260651151</a></p>
</li>
<li>Python 时间序列异常检测 ADTK：<a target="_blank" rel="noopener" href="https://blog.csdn.net/BF02jgtRS00XKtCx/article/details/115343456">https://blog.csdn.net/BF02jgtRS00XKtCx/article/details/115343456</a></li>
<li>awesome-TS-anomaly-detection：<a target="_blank" rel="noopener" href="https://github.com/rob-med/awesome-TS-anomaly-detection">https://github.com/rob-med/awesome-TS-anomaly-detection</a></li>
</ul>
</blockquote>
<h5><span id="异常检测工具">异常检测工具：</span></h5><ul>
<li><p><strong>PyOD:超过30种算法，从经典模型到深度学习模型一应俱全，和sklearn的用法一致</strong></p>
</li>
<li><p><strong>Scikit-Learn:包含了4种常见的算法，简单易用</strong></p>
</li>
<li><p><strong>TODS:与PyOD类似，包含多种时间序列上的异常检测算法</strong></p>
</li>
</ul>
<p><strong>异常检测算法</strong>：</p>
<ul>
<li><strong>线性模型</strong>：<strong>PCA</strong></li>
<li><strong>基于相似度度量的算法</strong>：<strong>KNN</strong>、LOF、HBOS</li>
<li>基于概率的算法：COPOD</li>
<li><strong>集成检测</strong>：<strong>孤立森林</strong>，<strong>XGBOD</strong></li>
<li><strong>神经网络算法</strong>：<strong>自编码器</strong></li>
</ul>
<h5><span id="评估方法">评估方法：</span></h5><ul>
<li>ROC-AUC 曲线</li>
<li>Precision Topk：top K的准确率</li>
<li>AVE Precision：平均准确率</li>
</ul>
<h2><span id="一-概述">一、概述</span></h2><h4><span id="什么是异常检测">什么是异常检测？</span></h4><p>不同于常规模式下的问题和任务，<strong>异常检测针对的是少数、不可预测或不确定、罕见的事件</strong>，它具有独特的复杂性，使得一般的机器学习和深度学习技术无效。</p>
<h4><span id="异常检测面临的挑战"><strong>异常检测面临的挑战</strong></span></h4><ul>
<li><strong>未知性</strong>：异常与许多未知因素有关，例如，具有未知的突发行为、数据结构和分布的实例。它们直到真正发生时才为人所知，比如恐怖袭击、诈骗和网络入侵等应用；</li>
<li><strong>异常类的异构性</strong>： 异常是不规则的，一类异常可能表现出与另一类异常完全不同的异常特征。例如，在视频监控中，抢劫、交通事故和盗窃等异常事件在视觉上有很大差异；</li>
<li><strong>类别不均衡</strong>：异常通常是罕见的数据实例，而正常实例通常占数据的绝大部分。<strong>因此，收集大量标了标签的异常实例是困难的，甚至是不可能的。这导致在大多数应用程序中无法获得大规模的标记数据。</strong></li>
</ul>
<h4><span id="异常的种类"><strong>异常的种类：</strong></span></h4><ul>
<li><strong>点异常</strong>（point anomalies）指的是少数个体实例是异常的，大多数个体实例是正常的，例如正常人与病人的健康指标；</li>
<li><strong>条件异常</strong>（conditional anomalies），又称上下文异常，指的是在特定情境下个体实例是异常的，在其他情境下都是正常的，例如在特定时间下的温度突然上升或下降，在特定场景中的快速信用卡交易；</li>
<li><strong>群体异常</strong>（group anomalies）指的是在群体集合中的个体实例出现异常的情况，而该个体实例自身可能不是异常，例如社交网络中虚假账号形成的集合作为群体异常子集，但子集中的个体节点可能与真实账号一样正常。</li>
</ul>
<h4><span id="异常检测数据集"><strong>异常检测数据集：</strong></span></h4><ul>
<li>统计型数据static data（文本、网络流）</li>
<li><strong>序列型数据sequential data（sensor data ）</strong></li>
<li>空间型数据spatial data（图像、视频）</li>
</ul>
<h4><span id="异常检测的应用领域-1"><strong>异常检测的应用领域 [1] ：</strong></span></h4><ul>
<li><strong>入侵检测</strong>（Intrusion detection）：通过从计算机网络或计算机系统中的若干关键点收集信息并对其执行分析，从中发觉网络或系统中能不能有违反安全策略的行为和遭到袭击的迹象，并对此做出适当反应的流程。最普遍的两种入侵检测系统包括<strong>基于主机的入侵检测系统（HIDS）</strong>、<strong>网络入侵检测系统（NIDS）</strong>。</li>
<li><strong>故障检测</strong>（Fraud detection）：主要是监控系统，在故障发生时可以识别，并且准确指出故障的种类以及出现位置。主要应用领域包括银行欺诈、移动蜂窝网络故障、保险欺诈、医疗欺诈。</li>
<li><strong>恶意软件检测</strong>（Malware Detection）</li>
<li><strong>医疗异常检测</strong>（Medical Anomaly Detection）：通过X光片、核磁共振、CT等医学图像检测疾病或量化异常，也可以通过EEG、ECG等时序信号进行疾病检测或异常预警。</li>
<li><strong>深度学习用于社交网络中的异常检测</strong>（Deep learning for Anomaly detection in Social Networks）</li>
<li><strong>日志异常检测</strong>（Log Anomaly Detection）</li>
<li><strong>物联网大数据异常检测</strong>（Internet of things (IoT) Big Data Anomaly Detection）：通过监控数据流信息检测异常设备和系统行为。</li>
<li><strong>工业异常检测</strong>（Industrial Anomalies Detection）</li>
<li><strong>时间序列中的异常检测</strong>（Anomaly Detection in TimeSeries）</li>
<li><strong>视频监控</strong>（Video Surveillance）：检测视频中的异常场景。</li>
</ul>
<h4><span id="基于标签的可获得性划分异常检测"><strong>基于标签的可获得性划分异常检测：</strong></span></h4><ul>
<li><strong>有监督异常检测</strong>：在训练集中的正常实例和异常实例都有标签，这类方法的缺点在于数据标签难以获得或数据不均衡（正常样本数量远大于异常样本数量）。</li>
<li><strong>半监督异常检测</strong>：<strong>在训练集中只有单一类别（正常实例）的实例，没有异常实例参与训练，目前很多异常检测研究都集中在半监督方法上</strong>，有很多声称是无监督异常检测方法的研究其实也是半监督的，对其解释的是该异常检测是无监督异常检测，学习特征的方式是无监督的，但是评价方式使用了半监督的方法，因此对于无监督与半监督的界定感觉没有那么规范。</li>
<li><strong>无监督异常检测</strong>：在训练集中既有正常实例也可能存在异常实例，但假设数据的比例是正常实例远大于异常实例，模型训练过程中没有标签进行校正。</li>
<li><strong>弱监督异常检测</strong>：该类我研究的少，不是特别了解，主要是针对异常实例不完全、粗粒度标签、部分实例标签错误等情况进行算法设计。</li>
</ul>
<h4><span id="基于传统方法的异常检测模型">基于传统方法的异常检测模型</span></h4><ul>
<li><strong>基于重构的方法</strong>：<strong><font color="red"> 假设异常点是不可被压缩的或不能从低维映射空间有效地被重构的。</font></strong>常见的方法有<strong>PCA</strong>、<strong>Robust PCA</strong>、random projection等降维方法 [4,5] 。</li>
<li><strong>聚类分析方法</strong>：通过聚类可以创建数据的模型，而异常点的存在可以扭曲、破坏该模型。常见的方法有Gaussian Mixture Models、 k-means、 multivariate Gaussian Models [6,7,8]。</li>
<li><strong>一类分类方法</strong>：对正常数据建立区分性边界，异常点被划分到边界外。常见的方法有<strong>OC-SVM</strong> [9,10]。</li>
</ul>
<h2><span id="二-常见异常检测算法">二、常见异常检测算法</span></h2><p><strong>一般情况下, 可以把异常检测看成是数据不平衡下的分类问题</strong>。因此, 如果数据条件允许, 优先使 用有监督的异常检测[6]。实验结果 [4]发现直接用XGBOOST进行有监督异常检测往往也能得到不错 的结果，没有思路时不妨一试。</p>
<p><strong>而在仅有少量标签的情况下, 也可采用半监督异常检测模型</strong>。比如把无监督学习作为一种特征抽取方式来辅助监督学习 $[4,8]$, 和stacking比较类似。这种方法也可以理解成通过无监督的特征工程 对数据进行预处理后, 喂给有监督的分类模型。</p>
<p>但在现实情况中, <strong>异常检测问题往往是没有标签的, 训练数据中并末标出哪些是异常点, 因此必须用无监督学习。</strong>从实用角度出发, 我们把文章的重点放在无监督学习上。</p>
<p>本文结构如下: 1. 介绍常见的无监督异常算法及实现; 2. 对比多种算法的检测能力；3. 对比多种算法的运算开销；4. 总结并归纳如何处理异常检测问题。5. 代码重现步骤。</p>
<h3><span id="21-无监督异常检测">2.1 无监督异常检测</span></h3><p>如果归类的话, 无监督异常检测模型可以大致分为:</p>
<ul>
<li><strong>统计与概率模型</strong> (statistical and probabilistic and models) ：<strong>主要是对数据的分布做出假设, 并找出假设下所定义的“异常”, 因此往往会使用极值分析Q或者假设检验</strong>。比如对最简单的一维 数据假设高斯分布 $Q$, 然后将距离均值特定范围以外的数据当做异常点。而推广到高维后, 可以 假设<strong>每个维度各自独立</strong>, <strong>并将各个维度上的异常度相加</strong>。如果考虑特征间的相关性, 也可以用马 氏距离 (mahalanobis distance) 来衡量数据的异常度[12]。不难看出, 这类方法最大的好处就 是速度一般比较快, 但因为存在比较强的”假设”, 效果不一定很好。<strong>稍微引申一点的话, 其实给每个维度做个直方图做密度估计, 再加起来就是HBOS。</strong></li>
<li><p><strong>线性模型（linear models）</strong>：假设数据在低维空间上有嵌入, 那么无法、或者在低维空间投射后 表现不好的数据可以认为是离群点 $Q$ 。</p>
<ul>
<li><strong>举个简单的例子, PCA可以用于做异常检测</strong> [10], 一种方法 就是找到 $k$ 个特征向量 (eigenvectora), 并计算每个样本再经过这 $k$ 个特征向量投射后的<strong>重建误差 (reconstruction error)</strong>, 而正常点的重建误差应该小于异常点。</li>
<li>同理, 也可以计算每个样本 到这 $k$ 个选特征向量所构成的超空间的加权欧氏距离（特征值越小权重越大）。在相似的思路下, 我们也可以直接对协方差矩阵 $Q$ 进行分析, 并把样本的马氏距离 (在考虑特征间关系时样本到分 布中心的距离) 作为样本的异常度, 而这种方法也可以被理解为一种软性 (Soft PCA) [6]。</li>
<li>同时, 另一种经典算法One-class SVM[3]也一般被归类为线性模型。</li>
</ul>
</li>
<li><p><strong>基于相似度衡量</strong>的模型 (proximity based models) : <strong>异常点因为和正常点的分布不同, 因此相似度较低, 由此衍生了一系列算法通过相似度来识别异常点</strong>。</p>
<ul>
<li>比如最简单的<strong>K近邻</strong>就可以做异常 检测,一个样本和它第k个近邻的距离就可以被当做是异常值, 显然异常点的k近邻距离更大。</li>
<li>同理, <strong>基于密度分析如LOF</strong> [1]、<strong>LOCI</strong>和<strong>LoOP主要是通过局部的数据密度来检测异常</strong>。显然, 异常点所在空间的数据点少, 密度低。</li>
<li>相似的是, <strong><font color="red"> Isolation Forest[2]通过划分超平面Q来计算”孤立” 一个样本所需的超平面数量</font></strong>（可以想象成在想吃蛋糕上的樱桃所需的最少刀数）。在密度低的空间里 (异常点所在空间中), 孤例一个样本所需要的划分次数更少。</li>
<li>另一种相似的算法<strong>ABOD</strong>[7] <strong>是计算每个样本与所有其他样本对所形成的夹角的方差</strong>, 异常点因为远离正常点, 因此方差变化 小。换句话说, 大部分异常检测算法都可以被认为是一种估计相似度, 无论是通过密度、距离、 夹角或是划分超平面。通过聚类也可以被理解为一种相似度度量, 比较常见不再赘述。</li>
</ul>
</li>
<li><strong>集成异常检测与模型融合</strong>: 在无监督学习时, 提高模型的鲁棒性很重要, 因此集成学习就大有用 武之地。比如上面提到的lsolation Forest, 就是基于构建多棵决策树实现的。最早的集成检测框 架feature bagging[9]与分类问题中的随机森林 (random forest) 很像, 先将训练数据随机划分 (每次选取所有样本的 $d / 2-d$ 个特征, $d$ 代表特征数) , 得到多个子训练集, 再在每个训练集上训 练一个独立的模型（默认为LOF）并最终合并所有的模型结果（如通过平均）。值得注意的是, 因为没有标签, 异常检测往往是通过bagging和feature bagging比较多, 而boosting比较少见。 boosting情况下的异常检测, 一般需要生成伪标签Q, 可参靠 $[13,14]$ 。集成异常检测是一个新兴但很有趣的领域, 综述文章可以参考 $[16,17,18]$ 。</li>
<li><strong>特定领域上的异常检测</strong>：比如图像异常检测 [21], 顺序及<strong>流数据异常检测（时间序列异常检测）</strong> [22], 以及高维空间上的异常检测 [23], 比如前文提到的Isolation Forest就很适合高维数据上的 异常检测。</li>
</ul>
<p><strong>不难看出, 上文提到的划分标准其实是互相交织的</strong>。比如k-近邻可以看做是概率模型非参数化后的 一种变形, 而通过马氏距离计算异常度虽然是线性模型但也对分布有假设（高斯分布）。Isolation Forest虽然是集成学习, 但其实和分析数据的密度有关, 并且适合高维数据上的异常检测。在这种 基础上, 多种算法其实是你中有我, 我中有你, <strong><font color="red"> 相似的理念都可以被推广和应用, 比如计算重建误 差不仅可以用PCA, 也可以用神经网络中的auto-encoder。</font></strong>另一种划分异常检测模型的标准可以理 解为局部算法 (local) 和全局算法 (global), 这种划分方法是考虑到异常点的特性。想要了解更多异常检测还是推荐看经典教科书Outlier Analysis [6], 或者综述文章[15]。</p>
<p><strong>虽然一直有新的算法被提出, 但因为需要采用无监督学习, 且我们对数据分布的有限了解, 模型选 择往往还是采用试错法,</strong> 因此快速迭代地尝试大量的算法就是一个必经之路。在这个回答下, 我们 会对比多种算法的预测能力、运算开销及模型特点。如无特别说明，本文中的图片、代码均来自于 开源Python异常检测工具库Pyod。文中实验所使用的17个数据集均来自于 (ODDS - Outlier Detection DataSets) 。</p>
<h3><span id="一-isolation-forest">一、Isolation Forest</span></h3><blockquote>
<p>  孤立森林(isolation Forest)-一个通过瞎几把乱分进行异常检测的算法 - 小伍哥聊风控的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/484495545">https://zhuanlan.zhihu.com/p/484495545</a></p>
<h5><span id="isolation-forest算法梳理isolation-forest算法梳理">Isolation Forest算法梳理🌳</span></h5></blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026280.png" alt="Isolation Forest算法梳理🌳"></p>
<p><strong>异常检测 (anomaly detection)</strong>，或者又被称为“<strong>离群点检测</strong>” (outlier detection)，是机器学习研究领域中跟现实紧密联系、有广泛应用需求的一类问题。但是，什么是异常，并没有标准答案，通常因具体应用场景而异。如果要给一个比较通用的定义，很多文献通常会引用 Hawkins 在文章开头那段话。很多后来者的说法，跟这个定义大同小异。这些定义虽然笼统，但其实暗含了认定“异常”的两个标准或者说假设：</p>
<h3><span id="二-hbos">二、HBOS</span></h3><p><strong>HBOS全名为：Histogram-based Outlier Score</strong>。它是一种单变量方法的组合，不能对特征之间的依赖关系进行建模，但是计算速度较快，对大数据集友好，其基本假设是数据集的每个维度相互独立，然后对<strong>每个维度进行区间(bin)划分，区间的密度越高，异常评分越低。理解了这句话，基本就理解了这个算法。</strong></p>
<h4><span id="21-hbos算法流程"><strong>2.1 HBOS算法流程</strong></span></h4><h5><span id="1-静态宽度直方图"><strong>1、静态宽度直方图</strong></span></h5><p>标准的直方图构建方法，在值范围内使用k个等宽箱，样本落入每个箱的频率（相对数量）作为密度（箱子高度）的估计，时间复杂度：O(n)</p>
<p><strong>注意：</strong>等宽分箱，每个箱中的数据宽度相同，不是指数据个数相同。例如序列[5,10,11,13,15,35,50,55,72,92,204,215]，数据集中最大值是215，最小值是5，分成3个箱，故每个箱的宽度应该为（215-5）/3=70，所以箱的宽度是70，这就要求箱中数据之差不能超过70，并且要把不超过70的数据全放在一起，最后的分箱结果如下：</p>
<p><strong>箱一：5,10，11,13,15,35,50,55,72；箱二：92；箱三：204,215</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026218.png" alt="图片" style="zoom:48%;"></p>
<h5><span id="2-动态宽度直方图"><strong>2、动态宽度直方图</strong></span></h5><p>首先对所有值进行排序，然后固定数量的N/k 个连续值装进一个箱里，其 中N是总实例数，k是箱个数，<strong>直方图中的箱面积表示实例数</strong>，因为箱的宽度是由箱中第一个值和最后一个值决定的，所有箱的面积都一样，因此每一个箱的高度都是可计算的。这意味着跨度大的箱的高度低，即密度小，只有一种情况例外，超过k个数相等，此时允许在同一个箱里超过N/k值，时间复杂度：O(n×log(n))</p>
<p>还是用序列[<strong>5,10,11,13,15</strong>,<strong>35,50,55,72</strong>,92,204,215]举例，也是假如分3箱，那么每箱都是4个，宽度为边缘之差，第一个差为15-5=10，第二差为72-35=37，第三个箱宽为215-92=123，为了保持面积相等，所以导致后面的很矮，前面的比较高，如下图所示（非严格按照规则）：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026012.png" alt="图片" style="zoom:50%;"></p>
<h5><span id="3-算法推导过程"><strong>3、算法推导过程</strong></span></h5><p><strong>对每个维度都计算了一个独立的直方图，其中每个箱子的高度表示密度的估计，然后为了使得最大高度为1（确保了每个特征与异常值得分的权重相等），对直方图进行归一化处理。</strong>最后，每一个实例的HBOS值由以下公式计算：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026886.png" alt="图片" style="zoom: 67%;"></p>
<p><strong>推导过程：</strong></p>
<p>假设样本p第 i 个特征的概率密度为pi ( p ) ，则p的概率密度可以计算为，d为总的特征的个数：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026098.png" alt="图片" style="zoom: 67%;"></p>
<p>两边取对数：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026442.png" alt="图片" style="zoom: 67%;"></p>
<p>概率密度越大，异常评分越小，为了方便评分，两边乘以“-1”：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026675.png" alt="图片" style="zoom: 67%;"></p>
<p>最后可得：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026797.png" alt="图片" style="zoom: 67%;"></p>
<p>PyOD是一个可扩展的Python工具包，用于检测多变量数据中的异常值。它可以在一个详细记录API下访问大约20个离群值检测算法。</p>
<h3><span id="三-xgbod">三、 XGBOD</span></h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349519844">【异常检测】<em>XGBOD</em>：用无监督表示学习改进有监督异常检测</a></p>
</blockquote>
<h4><span id="四-copod用统计机器学习检测异常">四、COPOD：用「统计」+「机器学习」检测异常</span></h4><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338189299">COPOD：用「统计」+「机器学习」检测异常</a></p>
</blockquote>
<h3><span id="五-more-about-anomaly-detection">五、More about Anomaly Detection</span></h3><p>那这个异常检测啊,其实也是另外一门学问,那我们课堂上就没有时间讲了,异常检测不是只能用 Aauto-Encoder 这个技术,Aauto-Encoder 这个技术,只是众多可能方法里面的其中一个,我们拿它来当做 Aauto-Encoder 的作业,因为我相信,你未来有很多的机会用得上异常检测这个技术,那实际上有关异常检测更完整的介绍,我们把过去上课的录影放在这边,给大家参考,</p>
<p>Part 1: <a target="_blank" rel="noopener" href="https://youtu.be/gDp2LXGnVLQ">https://youtu.be/gDp2LXGnVLQ</a></p>
<ul>
<li><h5><span id="简介">简介</span></h5></li>
</ul>
<p>Part 2: <a target="_blank" rel="noopener" href="https://youtu.be/cYrNjLxkoXs">https://youtu.be/cYrNjLxkoXs</a></p>
<ul>
<li><strong>信心分数</strong></li>
</ul>
<p>Part 3: <a target="_blank" rel="noopener" href="https://youtu.be/ueDlm2FkCnw">https://youtu.be/ueDlm2FkCnw</a></p>
<ul>
<li><h5><span id="异常检测系统的评估">异常检测系统的评估？</span></h5><ul>
<li>no ACC</li>
<li>cost loss设计</li>
<li>RUC</li>
</ul>
</li>
</ul>
<p>Part 4: <a target="_blank" rel="noopener" href="https://youtu.be/XwkHOUPbc0Q">https://youtu.be/XwkHOUPbc0Q</a></p>
<p>Part 5: <a target="_blank" rel="noopener" href="https://youtu.be/Fh1xFBktRLQ">https://youtu.be/Fh1xFBktRLQ</a></p>
<ul>
<li>无监督</li>
</ul>
<p>Part 6: <a target="_blank" rel="noopener" href="https://youtu.be/LmFWzmn2rFY">https://youtu.be/LmFWzmn2rFY</a></p>
<p>Part 7: <a target="_blank" rel="noopener" href="https://youtu.be/6W8FqUGYyDo">https://youtu.be/6W8FqUGYyDo</a></p>
<p>那以上就是有关 Aauto-Encoder 的部分</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/8Q5WCT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/8Q5WCT/" class="post-title-link" itemprop="url">机器学习-损失函数</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:53" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:53+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-17 21:20:41" itemprop="dateModified" datetime="2023-04-17T21:20:41+08:00">2023-04-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>机器学习中的监督学习本质上是给定一系列训练样本 $\left(x_i, y_i\right)$, 尝试学习 $x \rightarrow y$ 的映射关系, 使得给定一个 $x$ , 即便这个 $x$ 不在训练样本中, 也能够得到尽量接近真实 $y$ 的输出 $\hat{y}$ 。<strong>损失函数 (Loss Function) 则是这个过程中关键的一个组成部分, 用来衡量模型的输出 $\hat{y}$ 与真实的 $y$ 之间的差距, 给模型的优化指明方向。</strong></p>
<p>本文将介绍机器学习、深度学习中分类与回归常用的几种损失函数, 包括<strong>均方差损失 Mean Squared Loss、平 均绝对误差损失 Mean Absolute Error Loss、Huber Loss、分位数损失 Quantile Loss、交叉樀损失函数 Cross Entropy Loss、Hinge 损失 Hinge Loss</strong>。主要介绍各种损失函数的基本形式、原理、特点等方面。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/8Q5WCT/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/7N6QMR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/7N6QMR/" class="post-title-link" itemprop="url">机器学习-模型评估</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-17 21:20:48" itemprop="dateModified" datetime="2023-04-17T21:20:48+08:00">2023-04-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="六、A-B-测试"><a href="#六、A-B-测试" class="headerlink" title="六、A/B 测试"></a>六、A/B 测试</h2><blockquote>
<p>  【AB测试最全干货】史上最全知识点及常见面试题（上篇） - 数据分析狗一枚的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/375902281">https://zhuanlan.zhihu.com/p/375902281</a></p>
</blockquote>
<h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>科学家门捷列夫说「没有测量，就没有科学」，在AI场景下我们同样需要定量的数值化指标来指导我们更好地应用模型对数据进行学习和建模。</p>
<p>事实上，在机器学习领域，对模型的测量和评估至关重要。选择与问题相匹配的评估方法，能帮助我们快速准确地发现在模型选择和训练过程中出现的问题，进而对模型进行优化和迭代。本文我们系统地讲解一下机器学习模型评估相关知识。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/7N6QMR/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/27302/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/27302/" class="post-title-link" itemprop="url">机器学习（1）评价指标</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-17 21:20:54" itemprop="dateModified" datetime="2023-04-17T21:20:54+08:00">2023-04-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>  一文看懂机器学习指标：准确率、精准率、召回率、F1、ROC曲线、AUC曲线:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93107394">https://zhuanlan.zhihu.com/p/93107394</a></p>
<p>  <strong>机器学习-最全面的评价指标体系: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/359997979">https://zhuanlan.zhihu.com/p/359997979</a></strong></p>
<p>  <a target="_blank" rel="noopener" href="https://github.com/HaoMood/homepage/blob/master/files/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-03-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.pdf">机器学习工程师面试宝典-03-模型评估</a></p>
<p>  <strong><a target="_blank" rel="noopener" href="http://www.china-nb.cn/gongsidongtai/17-85.html">分类模型评估指标——准确率、精准率、召回率、F1、ROC曲线、AUC曲线</a></strong></p>
</blockquote>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/27302/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2MXAK3G/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2MXAK3G/" class="post-title-link" itemprop="url">机器学习（18）TF-IDF</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:06:42" itemprop="dateCreated datePublished" datetime="2022-03-16T21:06:42+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-19 22:50:55" itemprop="dateModified" datetime="2022-07-19T22:50:55+08:00">2022-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="tf-idf">TF-IDF</span></h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/u010417185/article/details/87905899">https://blog.csdn.net/u010417185/article/details/87905899</a></p>
</blockquote>
<p><strong>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)</strong>是一种用于资讯检索与资讯探勘的常用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加权技术&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;97273457&quot;}">加权技术</a>。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p>
<p>上述引用总结就是, <strong>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。</strong>这也就是TF-IDF的含义。</p>
<h4><span id="11-tf"><strong>1.1 TF</strong></span></h4><p><strong>TF(Term Frequency, ==词频==)</strong>表示词条在文本中出现的频率，这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。TF用公式表示如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=TF_%7Bi%2Cj%7D%3D%5Cfrac%7Bn_%7Bi%2Cj%7D%7D%7B%5Csum_%7Bk%7D%7Bn_%7Bk%2Cj%7D%7D%7D%5Ctag%7B1%7D+%5C%5C" alt="[公式]"></p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=n_%7Bi%2Cj%7D" alt="[公式]"> 表示词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 在文档 <img src="https://www.zhihu.com/equation?tex=d_j" alt="[公式]"> 中出现的次数，<img src="https://www.zhihu.com/equation?tex=TF_%7Bi%2Cj%7D" alt="[公式]"> 就是表示词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 在文档 <img src="https://www.zhihu.com/equation?tex=d_j" alt="[公式]"> 中出现的频率。</p>
<p>但是，需要注意， 一些<strong>通用的词语对于主题并没有太大的作用</strong>， <strong>反倒是一些出现频率较少的词才能够表达文章的主题</strong>， 所以单纯使用是TF不合适的。权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章的主题的作用很大，这些词的权重应该设计的较大。IDF就是在完成这样的工作。</p>
<h4><span id="12-idf"><strong>1.2 IDF</strong></span></h4><p><strong>IDF(Inverse Document Frequency, ==逆文件频率==)</strong>表示关键词的普遍程度。如果包含词条 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 的文档越少， <strong>IDF</strong>越大，则说明该词条具有很好的类别区分能力。某一特定词语的<strong>IDF</strong>，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到:</p>
<p><img src="https://www.zhihu.com/equation?tex=IDF_i%3D%5Clog%5Cfrac%7B%5Cleft%7CD+%5Cright%7C%7D%7B1%2B%5Cleft%7Cj%3A+t_i+%5Cin+d_j%5Cright%7C%7D%5Ctag%7B2%7D+%5C%5C" alt="[公式]"></p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=%5Cleft%7CD+%5Cright%7C" alt="[公式]"> 表示所有<strong>文档的数量</strong>，<img src="https://www.zhihu.com/equation?tex=%5Cleft%7Cj%3A+t_i+%5Cin+d_j%5Cright%7C" alt="[公式]"> 表示包<strong>含词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 的文档数量</strong>，为什么这里要加 1 呢？主要是<strong>防止包含词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 的数量为 0 从而导致运算出错的现象发生</strong>。</p>
<p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于<strong>过滤掉常见的词语，保留重要的词语</strong>，表达为</p>
<p><img src="https://www.zhihu.com/equation?tex=TF+%5Ctext%7B-%7DIDF%3D+TF+%5Ccdot+IDF%5Ctag%7B3%7D+%5C%5C" alt="[公式]"></p>
<p>==<strong>最后</strong>在计算完文档中每个字符的tfidf之后，对其进行归一化，将值保留在0-1之间，并保存成稀疏矩阵。==</p>
<h2><span id="tf-idf-qampa">TF-IDF Q&amp;A</span></h2><h3><span id="1-究竟应该是对整个语料库进行tf-idf呢还是先对训练集进行tf-idf然后再对xtest进行tf-idf呢两者有什么区别"><strong>1、究竟应该是对整个语料库进行tf-idf呢？还是先对训练集进行tf-idf，然后再对xtest进行tf-idf呢？两者有什么区别？</strong></span></h3><blockquote>
<h4><span id="fit">fit</span></h4><p>  学习输入的数据有多少个不同的单词，以及每个单词的idf</p>
<h4><span id="transform-训练集">transform 训练集</span></h4><p>  返回我们一个document-term matrix.</p>
<h4><span id="transform-测试集">transform 测试集</span></h4></blockquote>
<p>transform的过程也很让人好奇。要知道，他是将测试集的数据中的文档数量纳入进来，重新计算每个词的idf呢，还是<strong>直接用训练集学习到的idf去计算测试集里面每一个tf-idf</strong>呢？</p>
<p><strong>如果纳入了测试集新词，就等于预先知道测试集中有什么词，影响了idf的权重。这样预知未来的行为，会导致算法丧失了泛化性。</strong></p>
<h3><span id="2-tf-idf-模型加载太慢">2、TF-IDF 模型加载太慢</span></h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/">https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/</a></p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scipy<span class="token punctuation">.</span>sparse <span class="token keyword">as</span> sp
<span class="token keyword">from</span> idfs <span class="token keyword">import</span> idfs <span class="token comment"># numpy array with our pre-computed idfs</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfVectorizer

<span class="token comment"># subclass TfidfVectorizer</span>
<span class="token keyword">class</span> <span class="token class-name">MyVectorizer</span><span class="token punctuation">(</span>TfidfVectorizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># plug our pre-computed IDFs</span>
    TfidfVectorizer<span class="token punctuation">.</span>idf_ <span class="token operator">=</span> idfs

<span class="token comment"># instantiate vectorizer</span>
vectorizer <span class="token operator">=</span> MyVectorizer<span class="token punctuation">(</span>lowercase <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
                          min_df <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>
                          norm <span class="token operator">=</span> <span class="token string">'l2'</span><span class="token punctuation">,</span>
                          smooth_idf <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># plug _tfidf._idf_diag</span>
vectorizer<span class="token punctuation">.</span>_tfidf<span class="token punctuation">.</span>_idf_diag <span class="token operator">=</span> sp<span class="token punctuation">.</span>spdiags<span class="token punctuation">(</span>idfs<span class="token punctuation">,</span>
                                         diags <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
                                         m <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>idfs<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                         n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>idfs<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1D9QYCW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1D9QYCW/" class="post-title-link" itemprop="url">机器学习（13）EM算法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 17:16:27" itemprop="dateCreated datePublished" datetime="2022-03-16T17:16:27+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-07 19:17:59" itemprop="dateModified" datetime="2022-07-07T19:17:59+08:00">2022-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" itemprop="url" rel="index"><span itemprop="name">贝叶斯分类器</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="em期望最大-概率模型">EM——期望最大 [概率模型]</span></h1><blockquote>
<p>  <strong>EM 算法通过引入隐含变量，使用 MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM 算法首先会固定其中的第一个参数，然后使用 MLE 计算第二个变量值；接着通过固定第二个变量，再使用 MLE 估测第一个变量值，依次迭代，直至收敛到局部最优解。</strong></p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/27976634">怎么通俗易懂地解释 EM 算法并且举个例子?</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/zouxy09/article/details/8537620">从最大似然到 EM 算法浅解</a></li>
<li><h5><span id="em算法"></span></h5></li>
</ol>
</blockquote>
<p><strong><font color="red"> EM 算法，全称 Expectation Maximization Algorithm。期望最大算法是一种迭代算法，用于含有隐变量（Hidden Variable）的概率参数模型的最大似然估计或极大后验概率估计。</font></strong></p>
<p>本文思路大致如下：先简要介绍其思想，然后举两个例子帮助大家理解，有了感性的认识后再进行严格的数学公式推导。</p>
<h2><span id="1-思想">1. 思想</span></h2><p>EM 算法的核心思想非常简单，分为两步：<strong>Expection-Step</strong> 和 <strong>Maximization-Step</strong>。<strong>E-Step 主要通过观察数据和现有模型来估计参数</strong>，然后用这个估计的参数值来计算似然函数的期望值；而 M-Step 是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后<strong>似然函数都会增加</strong>，所以函数最终会收敛。</p>
<p><img src="https://www.zhihu.com/equation?tex=EM" alt="[公式]"> <strong>算法一句话总结就是</strong>： <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步固定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 优化 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步固定 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 优化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 。</p>
<h3><span id="2-例子">2 例子</span></h3><h4><span id="21-例子-a">2.1 例子 A</span></h4><p>假设有两枚硬币 A 和 B，他们的随机抛掷的结果如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-4e19d89b47e21cf284644b0576e9af0f_1440w.jpg" alt="img"></p>
<p>我们很容易估计出两枚硬币抛出正面的概率：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_A+%3D+24%2F30+%3D0.8+%5C%5C%5Ctheta_B+%3D+9%2F20+%3D0.45++%5C%5C" alt="[公式]"></p>
<p>现在我们加入<strong>隐变量</strong>，抹去每轮投掷的硬币标记：</p>
<p><img src="https://pic1.zhimg.com/80/v2-caa896173185a8f527c037c122122258_1440w.jpg" alt="img"></p>
<p>碰到这种情况，我们该如何估计 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值？</p>
<p>我们多了一个隐变量 <img src="https://www.zhihu.com/equation?tex=Z%3D%28z_1%2C+z_2%2C+z_3%2C+z_4%2C+z_5%29" alt="[公式]"> ，代表每一轮所使用的硬币，我们需要知道每一轮抛掷所使用的硬币这样才能估计 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值，但是估计隐变量 Z 我们又需要知道 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值，才能用极大似然估计法去估计出 Z。这就陷入了一个鸡生蛋和蛋生鸡的问题。</p>
<p>其解决方法就是先<strong>随机初始化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"></strong> ，然后用去估计 Z， 然后基于 Z 按照最大似然概率去估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> ，循环至收敛。</p>
<h4><span id="212-计算"><strong>2.1.2 计算</strong></span></h4><p>随机初始化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A%3D0.6" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B%3D0.5" alt="[公式]"></p>
<p>对于第一轮来说，如果是硬币 A，得出的 5 正 5 反的概率为： <img src="https://www.zhihu.com/equation?tex=0.6%5E5%2A0.4%5E5" alt="[公式]"> ；如果是硬币 B，得出的 5 正 5 反的概率为： <img src="https://www.zhihu.com/equation?tex=0.5%5E5%2A0.5%5E5" alt="[公式]"> 。我们可以算出使用是硬币 A 和硬币 B 的概率分别为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P_A%3D%5Cfrac%7B0.6%5E5+%2A+0.4%5E5%7D%7B%280.6%5E5+%2A+0.4%5E5%29+%2B+%280.5%5E5+%2A+0.5%5E5%29%7D+%3D+0.45%5C%5C+P_B%3D%5Cfrac%7B0.5%5E5+%2A+0.5%5E5%7D%7B%280.6%5E5+%2A+0.4%5E5%29+%2B+%280.5%5E5+%2A+0.5%5E5%29%7D+%3D+0.55+%5C%5C" alt="[公式]"></p>
<p><img src="https://pic4.zhimg.com/80/v2-b325de65a5bcac196fc0939f346410d7_1440w.jpg" alt="img"></p>
<p>从期望的角度来看，对于第一轮抛掷，使用硬币 A 的概率是 0.45，使用硬币 B 的概率是 0.55。同理其他轮。这一步我们实际上是<strong>估计出了 Z 的概率分布</strong>，这部就是 <strong>E-Step</strong>。</p>
<p>结合硬币 A 的概率和上一张投掷结果，我们利用期望可以求出硬币 A 和硬币 B 的贡献。以第二轮硬币 A 为例子，计算方式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=H%3A+0.80%2A9+%3D7.2+%5C%5C+T%3A+0.80%2A1%3D0.8+%5C%5C" alt="[公式]"></p>
<p>于是我们可以得到：</p>
<p><img src="https://pic1.zhimg.com/80/v2-9b6e8c50c0761c6ac19909c26e0a71d4_1440w.jpg" alt="img"></p>
<p>然后用极大似然估计来估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_A+%3D+%5Cfrac%7B21.3%7D%7B21.3%2B8.6%7D+%3D+0.71+%5C%5C+%5Ctheta_B+%3D+%5Cfrac%7B11.7%7D%7B11.7+%2B+8.4%7D+%3D+0.58+%5C%5C" alt="[公式]"></p>
<p>这步就对应了 M-Step，重新估计出了参数值。如此反复迭代，我们就可以算出最终的参数值。</p>
<p>上述讲解对应下图：</p>
<p><img src="https://pic3.zhimg.com/v2-6cac968d6500cbca58fc90347c288466_r.jpg" alt="preview" style="zoom:50%;"></p>
<h4><span id="22-例子-b">2.2 例子 B</span></h4><p>如果说例子 A 需要计算你可能没那么直观，那就举更一个简单的例子：</p>
<p>现在一个班里有 50 个男生和 50 个女生，且男女生分开。我们假定男生的身高服从正态分布： <img src="https://www.zhihu.com/equation?tex=N%28%5Cmu_1%2C+%5Csigma%5E2_1+%29" alt="[公式]"> ，女生的身高则服从另一个正态分布： <img src="https://www.zhihu.com/equation?tex=N%28%5Cmu_2%2C+%5Csigma%5E2_2+%29" alt="[公式]"> 。这时候我们可以用极大似然法（MLE），分别通过这 50 个男生和 50 个女生的样本来估计这两个正态分布的参数。</p>
<p>但现在我们让情况复杂一点，就是这 50 个男生和 50 个女生混在一起了。我们拥有 100 个人的身高数据，却不知道这 100 个人每一个是男生还是女生。</p>
<p>这时候情况就有点尴尬，因为通常来说，我们只有知道了精确的男女身高的正态分布参数我们才能知道每一个人更有可能是男生还是女生。但从另一方面去考量，我们只有知道了每个人是男生还是女生才能尽可能准确地估计男女各自身高的正态分布的参数。</p>
<p>这个时候有人就想到我们必须从某一点开始，并用迭代的办法去解决这个问题：<strong>==我们先设定男生身高和女生身高分布的几个参数（初始值），然后根据这些参数去判断每一个样本（人）是男生还是女生，之后根据标注后的样本再反过来重新估计参数。之后再多次重复这个过程，直至稳定。这个算法也就是 EM 算法。==</strong></p>
<h3><span id="3-推导">3. 推导</span></h3><p>给定数据集，假设样本间相互独立，我们想要拟合模型 <img src="https://www.zhihu.com/equation?tex=p%28x%3B%5Ctheta%29" alt="[公式]"> 到数据的参数。根据分布我们可以得到如下<strong>似然函数</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+L%28%5Ctheta%29+%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dlog+p%28x_i%3B%5Ctheta%29++%5C%5C+%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dlog+%5Csum_%7Bz%7Dp%28x_i%2C+z%3B%5Ctheta%29+%5Cend%7Baligned%7D+%5C%5C" alt="[公式]"></p>
<p>第一步是<strong>对极大似然函数取对数</strong>，第二步是对每个样本的每个可能的类别 z 求<strong>联合概率分布之和</strong>。如果这个 z 是已知的数，那么使用极大似然法会很容易。但如果 z 是隐变量，我们就需要用 EM 算法来求。<strong>事实上，隐变量估计问题也可以通过梯度下降等优化算法，但事实由于求和项将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而 EM 算法则可看作一种非梯度优化方法。</strong></p>
<h4><span id="31-求解含有隐变量的概率模型">3.1 求解含有隐变量的概率模型</span></h4><p><strong>为了求解含有隐变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 的概率模型</strong> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D%5Chat%7B%5Ctheta%7D%3D%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%5Cend%7Baligned%7D" alt="[公式]"> <strong>需要一些特殊的技巧</strong>，通过引入隐变量 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%28i%29%7D" alt="[公式]"> 的概率分布为 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> ，<strong>==因为 <img src="https://www.zhihu.com/equation?tex=%5Clog+%28x%29" alt="[公式]"> 是凹函数故结合凹函数形式下的詹森不等式进行放缩处理==</strong><br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7D+Q_i%28z%5E%7B%28i%29%7D%29%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C+%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Cmathbb%7BE%7D%28%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5C%5C+%26%5Cge%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Cmathbb%7BE%7D%5B%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5D%5C%5C+%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>其中由概率分布的充要条件 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%3D1%E3%80%81Q_i%28z%5E%7B%28i%29%7D%29%5Cge0" alt="[公式]"> 可看成下述关于 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 函数分布列的形式：</p>
<p><img src="https://pic2.zhimg.com/v2-cb7ddb5cdc34761ec70d63c97189b102_720w.jpg?source=d16d100b" alt="img" style="zoom:50%;"></p>
<p><strong>这个过程可以看作是对 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 求了下界</strong>，假设 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 已经给定那么 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的值就取决于 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%29" alt="[公式]"> 了，因此可以通过调整这两个概率使下界不断上升，以逼近 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的真实值，当不等式变成等式时说明调整后的概率能够等价于 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> ，所以必须找到使得等式成立的条件，即寻找<br> <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5B%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5D%3D%5Clog+%5Cmathbb%7BE%7D%5B%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5D%5C%5C" alt="[公式]"><br>由期望得性质可知当<br> <img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%3DC%2C%5C+%5C+%5C+%5C+%5C+C%5Cin%5Cmathbb%7BR%7D+%5C+%5C+%5C+%5C+%5C+%28%2A%29%5C%5C" alt="[公式]"><br>等式成立，对上述等式进行变形处理可得<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DCQ_i%28z%5E%7B%28i%29%7D%29%5C%5C+%26%5CLeftrightarrow+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DC%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%3DC%5C%5C+%26%5CLeftrightarrow+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DC+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28%2A%2A%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>把 <img src="https://www.zhihu.com/equation?tex=%28%2A%2A%29" alt="[公式]"> 式带入 <img src="https://www.zhihu.com/equation?tex=%28%2A%29" alt="[公式]"> 化简可知<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+Q_i%28z%5E%7B%28i%29%7D%29%26%3D%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7B%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%5C%5C+%26%3D%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7Bp%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%5C%5C+%26%3Dp%28z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>至此，可以推出<strong>在固定参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 后</strong>， <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 的<strong>计算公式就是后验概率</strong>，解决了 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 如何选择得问题。这一步称为 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步，建立 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 得下界；接下来得 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步，就是在给定 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 后，调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 去极大化 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的下界即<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+p%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Cleft%5B%5Clog+p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29-%5Clog+Q_i%28z%5E%7B%28i%29%7D%29%5Cright%5D%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog+p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>因此EM算法的迭代形式为：</p>
<p><img src="https://pic2.zhimg.com/80/v2-8a4f41596e78bfeb1b4044212b259524_1440w.jpg?source=d16d100b" alt="img" style="zoom:50%;"></p>
<p><img src="https://pic3.zhimg.com/80/v2-2f7fc5ca144d2f85f14d46e88055dd86_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>这张图的意思就是：<strong>首先我们固定</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>使下界</strong> <img src="https://www.zhihu.com/equation?tex=J%28z%2CQ%29" alt="[公式]"> <strong>上升至与</strong> <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="[公式]"> <strong>在此点</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>处相等（绿色曲线到蓝色曲线），然后固定</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>使下界</strong> <img src="https://www.zhihu.com/equation?tex=J%28z%2CQ%29" alt="[公式]"> <strong>达到最大值（</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta_t" alt="[公式]"> <strong>到</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D" alt="[公式]"> <strong>），然后再固定</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>，一直到收敛到似然函数</strong> <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="[公式]"> <strong>的最大值处的</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 。</p>
<p><strong><font color="red"> EM 算法通过引入隐含变量，使用 MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM 算法首先会固定其中的第一个参数，然后使用 MLE 计算第二个变量值；接着通过固定第二个变量，再使用 MLE 估测第一个变量值，依次迭代，直至收敛到局部最优解。</font></strong></p>
<h4><span id="32-em算法的收敛性">3.2 EM算法的收敛性</span></h4><p>不妨假设 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 是EM算法第 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 次迭代和第 <img src="https://www.zhihu.com/equation?tex=k%2B1" alt="[公式]"> 次迭代的结果，要确保 <img src="https://www.zhihu.com/equation?tex=EM" alt="[公式]"> 算法收敛那么等价于证明 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%5Cle%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29" alt="[公式]"> 也就是说极大似然估计单调增加，那么算法最终会迭代到极大似然估计的最大值。在选定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 后可以得到 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步 <img src="https://www.zhihu.com/equation?tex=Q_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%3Dp%28z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> ，这一步保证了在给定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 时，詹森不等式中的等式成立即<br> <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C" alt="[公式]"><br>然后再进行 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步，固定 <img src="https://www.zhihu.com/equation?tex=Q_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 并将 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 视作变量，对上式 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> 求导后得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 因此有如下式子成立<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28a%29%5C%5C+%26%5Cle+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28b%29%5C%5C+%26%5Cle%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28c%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>首先 <img src="https://www.zhihu.com/equation?tex=%28a%29" alt="[公式]"> 式是前面 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步所保证詹森不等式中的等式成立的条件， <img src="https://www.zhihu.com/equation?tex=%28a%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步的定义，<img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28c%29" alt="[公式]">对任意参数都成立，而其等式的条件是固定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 并调整好 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 时成立，<img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28c%29" alt="[公式]">只是固定 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> ，在得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 时，只是最大化 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> ，也就是 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29" alt="[公式]"> 的一个下界而没有使等式成立。</p>
<h3><span id="4-另一种理解">4. 另一种理解</span></h3><p>坐标上升法（Coordinate ascent）：</p>
<p><img src="https://pic4.zhimg.com/80/v2-b28bfe68513ff86d9643fec10786b827_1440w.jpg" alt="img"></p>
<p>途中直线为迭代优化路径，因为每次只优化一个变量，所以可以看到它没走一步都是平行与坐标轴的。</p>
<p>EM 算法类似于坐标上升法，E 步：固定参数，优化 Q；M 步：固定 Q，优化参数。交替将极值推向最大。</p>
<h4><span id="5-应用">5. 应用</span></h4><p>EM 的应用有很多，比如、混合高斯模型、聚类、HMM 等等。其中 <strong>EM 在 K-means 中的用处</strong>，我将在介绍 K-means 中的给出。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/9/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/11/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
