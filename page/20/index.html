<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/20/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/20/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/20/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">251</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/XGJYS5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/XGJYS5/" class="post-title-link" itemprop="url">特征工程（2）特征预处理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:22:40" itemprop="dateModified" datetime="2023-04-26T14:22:40+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="特征工程-特征处理">特征工程-特征处理</span></h3>
<h3><span id="一-数值类型处理">一、 数值类型处理</span></h3>
<blockquote>
<p><strong>pandas 显示所有列：</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示所有列</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#显示所有行</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_rows&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#设置value的显示长度为100，默认为50</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;max_colwidth&#x27;</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p><strong>pandas 查看缺失特征:</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.isnull().<span class="built_in">sum</span>().sort_values(ascending = <span class="literal">False</span>) / train.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<p><strong>pandas 查看某一列的分布:</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[:,col_name].value_counts()</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p><strong>特征提取方式是可以深挖隐藏在数据背后更深层次的信息的</strong>。其次，数值类型数据也并不是直观看上去那么简单易用，因为不同的数值类型的计量单位不一样，比如个数、公里、千克、DB、百分比之类，同样数值的大小也可能横跨好几个量级，比如小到头发丝直径约为0.00004米，
大到热门视频播放次数成千上万次。</p>
<h4><span id="11-数据归一化">1.1 数据归一化</span></h4>
<blockquote>
<p><strong>为什么要数据归一化？</strong></p>
<p><a href="../../AI深度学习/深度学习（3）Normalization*.md">深度学习（3）Normalization*.md</a></p>
<ul>
<li><strong>可解释性</strong>：<strong>回归模型【无正则化】</strong>中自变量X的量纲不一致导致了<strong>回归系数无法直接解读</strong>或者错误解读；需要将X都处理到统一量纲下，这样才可比【可解释性】；<strong>取决于我们的逻辑回归是不是用了正则化</strong>。如果你不用正则，标准化并不是必须的，如果用正则，那么标准化是必须的。</li>
<li><strong>距离计算</strong>：机器学习任务和统计学任务中有很多地方要用到<strong>“距离”的计算</strong>，比如<strong>PCA、KNN，kmeans和SVM</strong>等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；</li>
<li><strong>加速收敛</strong>：参数估计时使用<strong>梯度下降</strong>，在使用梯度下降的方法求解最优化问题时，
归一化/标准化后可以加快梯度下降的求解速度，即<strong>提升模型的收敛速度</strong>。</li>
</ul>
<p><strong>需要归一化的模型：</strong>利用梯度下降法求解的模型一般需要归一化，<strong>线性回归、LR、SVM、KNN、神经网络</strong></p>
</blockquote>
<p><span class="math display">\[
\tilde{x}=\frac{x-\min (x)}{\max (x)-\min (x)}
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># define data </span></span><br><span class="line">data = np.asarray([[<span class="number">100</span>, <span class="number">0.001</span>], </span><br><span class="line">                   [<span class="number">8</span>, <span class="number">0.05</span>], </span><br><span class="line">                   [<span class="number">50</span>, <span class="number">0.005</span>], </span><br><span class="line">                   [<span class="number">88</span>, <span class="number">0.07</span>], </span><br><span class="line">                   [<span class="number">4</span>, <span class="number">0.1</span>]])</span><br><span class="line"><span class="comment"># define min max scaler</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line"><span class="comment"># transform data</span></span><br><span class="line">scaled = scaler.fit_transform(data) </span><br></pre></td></tr></table></figure>
<h4><span id="12-数据标准化">1.2 数据标准化</span></h4>
<p>数据标准化是指通过改变数据的分布得到均值为0，标准差为1的服从标准正态分布的数据。主要目的是为了让不同特征之间具有相同的尺度（Scale），这样更有理化模型训练收敛。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment"># define standard scaler</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"><span class="comment"># transform data</span></span><br><span class="line">scaled = scaler.fit_transform(data) </span><br><span class="line"><span class="built_in">print</span>(scaled)</span><br></pre></td></tr></table></figure>
<h4><span id="13-对数转换">1.3 对数转换</span></h4>
<p><span class="math inline">\(\log\)</span> 函数的定义为 <span class="math inline">\(\log _a\left(\alpha^x\right)=x\)</span>, 其中
<span class="math inline">\(\mathrm{a}\)</span> 是 <span class="math inline">\(\log\)</span> 函数的底数, <span class="math inline">\(\alpha\)</span> 是一个正常数, <span class="math inline">\(x\)</span> 可以是任何正数。由于 <span class="math inline">\(\alpha^0=1\)</span> <span class="math inline">\(a=10\)</span> 时, 函数 <span class="math inline">\(\log _{10}(x)\)</span> 可以将 <span class="math inline">\([1,10]\)</span> 映射到[ <span class="math inline">\([0,1]\)</span>, 将 <span class="math inline">\([1,100]\)</span> 映射到 <span class="math inline">\([1,2]\)</span> 。换句话说,
<strong>log函数压缩了大数的范 围, 扩大了小数的范围</strong>。 <span class="math inline">\(\mathrm{x}\)</span> 越大, <span class="math inline">\(\log (\mathrm{x})\)</span> 增量越慢。 <span class="math inline">\(\log (\mathrm{x})\)</span> 函数的图像如下:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261226003.png" alt="image-20220426154119370" style="zoom: 25%;"></p>
<p><strong>Log函数可以极大压缩数值的范围，相对而言就扩展了小数字的范围。该转换方法适用于长尾分布且值域范围很大的特征，变换后的特征趋向于正态分布。</strong>对数值类型使用对数转换一般有以下几种好处：</p>
<ul>
<li>缩小数据的绝对数值</li>
<li>取对数后，可以将乘法计算转换成加法计算</li>
<li>在数据的整个值域中不同区间的差异带来的影响不同</li>
<li>取对数后不会改变数据的性质和相关关系，但压缩了变量的尺度。</li>
<li>得到的数据易消除异方差问题</li>
</ul>
<h3><span id="二-序数和类别特征处理">二、序数和类别特征处理</span></h3>
<p>本文主要说明特征工程中关于<strong>序数特征</strong>和<strong>类别特征</strong>的常用处理方法。主要包含<strong>LabelEncoder</strong>、<strong>One-Hot编码</strong>、<strong>DummyCoding</strong>、<strong>FeatureHasher</strong>以及要重点介绍的<strong>WOE编码</strong>。</p>
<h4><span id="21-序数特征处理">2.1 序数特征处理</span></h4>
<p><strong>序数特征指的是有序但无尺度的特征</strong>。比如表示‘学历’的特征，'高中'、'本科'、'硕士'，这些特征彼此之间是有顺序关系的，但是特征本身无尺度，并且也可能不是数值类型。在实际应用中，一般是字符类型居多，为了将其转换成模型能处理的形式，通常需要先进行编码，比如LabelEncoding。如果序数特征本身就是数值类型变量，则可不进行该步骤。下面依次介绍序数特征相关的处理方式。</p>
<ul>
<li><h4><span id="label-encoding">Label Encoding</span></h4></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">x = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">encoder = LabelEncoder()</span><br><span class="line">x1 = encoder.fit_transform(x)</span><br><span class="line"></span><br><span class="line">x2 = pd.Series(x).astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">x2.cat.codes.values</span><br><span class="line"><span class="comment"># pandas 因子化</span></span><br><span class="line">x2, uniques = pd.factorize(x)</span><br><span class="line"><span class="comment"># pandas 二值化</span></span><br><span class="line">x2 = pd.Series(x)</span><br><span class="line">x2 = (x2 &gt;= <span class="string">&#x27;b&#x27;</span>).astype(<span class="built_in">int</span>) <span class="comment">#令大于等于&#x27;b&#x27;的都为1</span></span><br></pre></td></tr></table></figure>
<h4><span id="22-类别特征处理">2.2 类别特征处理</span></h4>
<p><strong>类别特征由于没有顺序也没有尺度</strong>，因此处理较为麻烦，但是在CTR等领域却是非常常见的特征。比如<strong>商品的类型，颜色，用户的职业，兴趣</strong>等等。类别变量编码方法中最常使用的就是<strong>One-Hot编码</strong>，接下来结合具体实例来介绍。</p>
<ul>
<li><h4><span id="one-hot编码">One-Hot编码</span></h4></li>
</ul>
<p>One-Hot编码，又称为'独热编码'，其变换后的单列特征值只有一位是1。如下例所示，一个特征中包含3个不同的特征值(a,b,c)，编码转换后变成3个子特征，其中每个特征值中只有一位是有效位1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line"></span><br><span class="line">one_feature = [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">feature = label_encoder.fit_transform(one_feature)</span><br><span class="line">onehot_encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line">onehot_encoder.fit_transform(feature.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="labelbinarizer">LabelBinarizer</span></h4></li>
</ul>
<p>sklearn中的LabelBinarizer也具有同样的作用，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line">feature = np.array([<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">LabelBinarizer().fit_transform(feature)</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="虚拟编码dummy-coding">虚拟编码Dummy Coding</span></h4></li>
</ul>
<p>同样，<strong>pandas中也内置了对应的处理方式,使用起来比Sklearn更加方便</strong>，产生n-1个特征。实例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one_feature = [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">pd.get_dummies(one_feature, prefix=<span class="string">&#x27;test&#x27;</span>) <span class="comment"># 设置前缀test</span></span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="特征哈希feature-hashing"><strong><font color="red">
特征哈希（feature hashing）</font></strong></span></h4></li>
</ul>
<p>按照上述编码方式，如果某个特征具有100个类别值，那么经过编码后将产生100个或99个新特征，这极大地增加了特征维度和特征的稀疏度，同时还可能会出现内存不足的情况。<strong>sklearn中的FeatureHasher接口采用了hash的方法，将不同的值映射到用户指定长度的数组中，使得输出特征的维度是固定的，该方法占用内存少，效率高，可以在多类别变量值中使用，但是由于采用了Hash函数的方式，所以具有冲突的可能，即不同的类别值可能映射到同一个特征变量值中。</strong></p>
<blockquote>
<p>Feature hashing(特征哈希):
https://blog.csdn.net/laolu1573/article/details/79410187</p>
<p>https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/264165760/answer/277634591">如何用通俗的语言解释CTR和推荐系统中常用的<em>Feature</em>
<em>Hashing</em>技术以及其对应的优缺点？</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">h = FeatureHasher(n_features=<span class="number">5</span>, input_type=<span class="string">&#x27;string&#x27;</span>)</span><br><span class="line">test_cat = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;f&#x27;</span>,<span class="string">&#x27;g&#x27;</span>,<span class="string">&#x27;h&#x27;</span>,<span class="string">&#x27;i&#x27;</span>,<span class="string">&#x27;j&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">f = h.transform(test_cat)</span><br><span class="line">f.toarray()</span><br></pre></td></tr></table></figure>
<p><strong>如果hash的目标空间足够大，并且hash函数本身足够散列，不会损失什么特征信息。</strong></p>
<p>feature
hashing简单来说和<strong>kernal的思想</strong>是类似的，就是把输入的特征映射到一个具有一些我们期望的较好性质的空间上去。在feature
hasing这个情况下我们希望目标的空间具有如下的性质：</p>
<ol type="1">
<li><strong>样本无关的维度大小，因为当在线学习，或者数据量非常大，提前对数据观察开销非常大的时候，这可以使得我们能够提前给算法分配存储和切分pattern。大大提高算法的工程友好性</strong>。</li>
<li>这个空间一般来说比输入的特征空间维度小很多。</li>
<li>另外我们假设在原始的特征空间里，样本的分布是非常稀疏的，只有很少一部分子空间是被取值的。</li>
<li><strong>保持内积的无偏</strong>（不变肯定是不可能的，因为空间变小了），否则很多机器学习方法就没法用了。</li>
</ol>
<p><strong>原理</strong>：假设输入特征是一个 <span class="math inline">\(\mathrm{N}\)</span> 维的0/1取值的向量 <span class="math inline">\(\mathrm{x}_{\circ} 一 个
\mathrm{~N}-&gt;\mathrm{M}\)</span> 的哈希函数 <span class="math inline">\(\mathrm{h}\)</span> 。那么 <span class="math inline">\(\phi_j=\sum_{h(i)=j} x_i\)</span></p>
<p><strong>好处：</strong></p>
<ul>
<li>从某种程度上来讲，使得训练样本的特征在对应空间里的<strong>分布更均匀</strong>了。这个好处对于实际训练过程是非常大的，某种程度上起到了<strong>shuffle的作用</strong>。</li>
<li>特征的空间变小了，而且是一个可以预测的大小。比如说加入输入特征里有个东西叫做user_id，那么显然你也不知道到底有多少<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=userid&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%22277634591%22%7D">userid</a>的话，你需要先扫描一遍并且分配足够的空间给到它不然学着学着oom了。你也不能很好地提前优化<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=分片&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%22277634591%22%7D">分片</a>。</li>
<li>对在线学习非常友好。</li>
</ul>
<p><strong>坏处：</strong></p>
<ul>
<li>会给debug增加困难，为了debug你要保存记录h计算的过程数据，否则如果某个特征有毛病，你怎么知道到底是哪个原始特征呢？</li>
<li>没选好哈希函数的话，<strong>可能会造成碰撞</strong>，如果原始特征很稠密并且碰撞很严重，那可能会带来坏的训练效果。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hashing_vectorizer</span>(<span class="params">features, N</span>):</span><br><span class="line">  	x = [<span class="number">0</span>] * N</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">      	h = <span class="built_in">hash</span>(f)</span><br><span class="line">        idx = h % N</span><br><span class="line">        <span class="keyword">if</span> xt(f)  <span class="number">1</span>: <span class="comment"># xt 2值hash函数减少hash冲突</span></span><br><span class="line">          	x[idx] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">         		x[idx] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="多类别值处理方式-基于统计的编码方法">多类别值处理方式 --
基于统计的编码方法</span></h4></li>
</ul>
<p>当类别值过多时，<strong>One-Hot 编码或者Dummy
Coding都可能导致编码出来的特征过于稀疏</strong>，其次也会占用过多内存。<strong>如果使用FeatureHasher，n_features的设置不好把握，可能会造成过多冲突，造成信息损失</strong>。这里提供一种基于统计的编码方法，包括<strong>基于特征值的统计</strong>或者<strong>基于标签值的统计</strong>——基于标签的编码。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line"><span class="built_in">test</span> = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="built_in">df</span> = pd.DataFrame(<span class="built_in">test</span>, columns=[<span class="string">&#x27;alpha&#x27;</span>])</span><br><span class="line">sns.countplot(<span class="built_in">df</span>[<span class="string">&#x27;alpha&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261226350.png" alt="image-20220426130800412" style="zoom:50%;"></p>
<p>首先我们将每个类别值出现的频数计算出来，比如我们设置阈值为1，那么所有小于阈值1的类别值都会被编码为同一类，大于1的类别值会分别编码，如果出现频数一样的类别值，既可以都统一分为一个类，也可以按照某种顺序进行编码，这个可以根据业务需要自行决定。那么根据上图，可以得到其编码值为：</p>
<p><span class="math display">\[
\left\{a^{\prime}: 0, &#39; c^{\prime}: 1, &#39; e^{\prime}: 2, &#39;
b^{\prime}: 2, &#39; d &#39;: 2\right\}
\]</span>
<strong>即（a,c）分别编码为一个不同的类别，（e,b,d）编码为同一个类别。</strong></p>
<h4><span id="23-二阶">2.3 二阶</span></h4>
<blockquote>
<p>w * h = s</p>
</blockquote>
<h3><span id="三-特征离散化处理方法">三、 特征离散化处理方法</span></h3>
<p><strong>特征离散化指的是将连续特征划分离散的过程</strong>：将原始定量特征的一个区间一一映射到单一的值。离散化过程也被表述成分箱（Binning）的过程。特征离散化常应用于<strong>逻辑回归</strong>和金融领域的评分卡中，同时在规则提取，特征分类中也有对应的应用价值。本文主要介绍几种常见的分箱方法，包括<strong>等宽分箱、等频分箱、信息熵分箱</strong>、<strong>基于决策树分箱、卡方分箱</strong>等。</p>
<p><strong>可以看到在分箱之后，数据被规约和简化，有利于理解和解</strong>释。总来说特征离散化，即
分箱之后会带来如下优势：</p>
<ul>
<li>有助于模型部署和应用，加快模型迭代</li>
<li>增强模型鲁棒性</li>
<li>增加非线性表达能力：连续特征不同区间对模型贡献或者重要程度不一样时，分箱后不同的权重能直接体现这种差异，离散化后的特征再进行特征
交叉衍生能力会进一步加强。</li>
<li>提升模型的泛化能力</li>
<li><strong>扩展数据在不同各类型算法中的应用范围</strong></li>
</ul>
<p>当然特征离散化也有其缺点，总结如下：</p>
<ul>
<li>分箱操作必定会导致一定程度的信息损失</li>
<li>增加流程：建模过程中加入了额外的的离散化步骤</li>
<li>影响模型稳定性：
当一个特征值处于分箱点的边缘时，此时微小的偏差会造成该特征值的归属从一箱跃迁到另外一箱，影响模型的稳定性。</li>
</ul>
<h4><span id="31-等宽分箱equal-widthbinning">3.1 等宽分箱（Equal-Width
Binning)</span></h4>
<p><strong>等宽分箱指的是每个分隔点或者划分点的距离一样，即等宽</strong>。实践中一般指定分隔的箱数，等分计算后得到每个分隔点。例如将数据序列分为n份，则
分隔点的宽度计算公式为： <span class="math display">\[
w=\frac{\max -\min }{n}
\]</span>
这样就将原始数据划分成了n个等宽的子区间，一般情况下，分箱后每个箱内的样本数量是不一致的。使用pandas中的cut函数来实现等宽分箱，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value, cutoff = pd.cut(df[<span class="string">&#x27;mean radius&#x27;</span>], bins=<span class="number">4</span>, retbins=<span class="literal">True</span>, precision=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>等宽分箱计算简单，但是当数值方差较大时，即数据离散程度很大，那么很可能出现没有任何数据的分箱</strong>，这个问题可以通过自适应数据分布的分箱方法--等频分箱来避免</p>
<h4><span id="32-等频分箱equal-frequencybinning">3.2 等频分箱（Equal-Frequency
Binning）</span></h4>
<p><strong>等频分箱理论上分隔后的每个箱内得到数据量大小一致</strong>，但是当某个值出现次数较多时，会出现等<strong>分边界是同一个值</strong>，导致同一数值分到不同的箱内，这是不正确的。具体的实现可以<strong>去除分界处的重复值</strong>，但这也导致每箱的数量不一致。如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1 = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">value, cutoff = pd.qcut(s1, <span class="number">3</span>, retbins=<span class="literal">True</span>)</span><br><span class="line">sns.countplot(value)</span><br></pre></td></tr></table></figure>
<p><strong>上述的等宽和等频分箱容易出现的问题是每箱中信息量变化不大</strong>。例如，等宽分箱不太适合分布不均匀的数据集、离群值；等频方法不太适合特定的值占比过多的数据集，如<strong>长尾分布</strong>。</p>
<h4><span id="33-信息熵分箱有监督">3.3 信息熵分箱【有监督】</span></h4>
<p><strong>如果分箱后箱内样本对y的区分度好，那么这是一个好的分箱</strong>。通过信息论理论，我们可知信息熵衡量了这种区分能力。当特征按照某个分隔点划分为上下两部分后能达到最大的信息增益，那么这就是一个好的分隔点。由上可知，信息熵分箱是有监督的分箱方法。<strong><font color="red">
其实决策树的节点分裂原理也是基于信息熵。</font></strong></p>
<p>首先我们需要明确信息熵和信息增益的计算方式, 分别如下: <span class="math display">\[
\begin{gathered}
\operatorname{Entropy}(y)=-\sum_{i=1}^m p_i \log _2 p_i \\
\operatorname{Gain}(x)=\operatorname{Entropy}(y)-\operatorname{Infos}_{\text
{split }}(x)
\end{gathered}
\]</span> 在二分类问题中, <span class="math inline">\(m=2\)</span>
。信息增益的物理含义表达为： <span class="math inline">\(x\)</span>
的分隔带来的信息对 <span class="math inline">\(y\)</span>
的不确定性带来的增益。 对于二值化的单点分隔,
如果我们找到一个分隔点将数据一分为二, 分成 <span class="math inline">\(P_1\)</span> 和 <span class="math inline">\(P_2\)</span> 两部分, 那么划分后的信息熵
的计算方式为: <span class="math display">\[
\operatorname{Info}_{\text {split }}(x)=P 1_{\text {ratio }}
\operatorname{Entropy}\left(x_{p 1}\right)+P 2_{\text {ratio }}
\operatorname{Entropy}\left(x_{p 2}\right)
\]</span>
同时也可以看出，当分箱后，某个箱中的标签y的类别（0或者1）的比例相等时，其熵值最大，表明此特征划分几
乎没有区分度。而当某个箱中的数据的标签 <span class="math inline">\(y\)</span> 为单个类别时, 那么该箱的熵值达到最小的
0 , 即纯度最纯, 最具区 分度。从结果上来看,
最大信息增益对应分箱后的总熵值最小。</p>
<h4><span id="34-决策树分箱有监督">3.4 决策树分箱【有监督】</span></h4>
<p><strong>由于决策树的结点选择和划分也是根据信息熵来计算的，因此我们其实可以利用决策树算法来进行特征分箱</strong>，具体做法如下：</p>
<p>还是以乳腺癌数据为例，首先取其中‘mean
radius’字段，和标签字段‘target’来拟合一棵决策树，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>, max_depth=<span class="number">3</span>) <span class="comment"># 树最大深度为3</span></span><br><span class="line">dt.fit(df[<span class="string">&#x27;mean radius&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>), df[<span class="string">&#x27;target&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>接着我们取出这课决策树的所有叶节点的分割点的阈值，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qts = dt.tree_.threshold[np.where(dt.tree_.children_left &gt; -<span class="number">1</span>)]</span><br><span class="line">qts = np.sort(qts)</span><br><span class="line">res = [np.<span class="built_in">round</span>(x, <span class="number">3</span>) <span class="keyword">for</span> x <span class="keyword">in</span> qts.tolist()]</span><br></pre></td></tr></table></figure>
<h4><span id="35-卡方分箱-有监督">3.5 卡方分箱 【有监督】</span></h4>
<blockquote>
<p><strong>特征选择之卡方分箱、WOE/IV</strong> - 云水僧的文章 - 知乎
https://zhuanlan.zhihu.com/p/101771771</p>
</blockquote>
<p><strong><font color="red">
卡方检验可以用来评估两个分布的相似性，因此可以将这个特性用到数据分箱的过程中。卡方分箱认为：理想的分箱是在同一个区间内标签的分布是相同的</font>;</strong>
<strong>卡方分布是概率统计常见的一种概率分布，是卡方检验的基础。</strong></p>
<p>布定义为: 若n个独立的随机变量 <span class="math inline">\(Z_1, Z_2,
\ldots, Z_k\)</span> 满足标准正态分布 <span class="math inline">\(N(0,1)\)</span>, 则 <span class="math inline">\(\mathrm{n}\)</span> 个随机变量的平方和 <span class="math inline">\(X=\sum_{i=0}^k Z_i^2\)</span> 为服从自由度为 <span class="math inline">\(\mathrm{k}\)</span> 的卡方分布, 记为 <span class="math inline">\(X \sim \chi^2\)</span> 。参数 <span class="math inline">\(\mathrm{n}\)</span>
称为自由度（样本中独立或能自由变化的自变 量的个数),
不同的自由度是不同的分布。</p>
<p>卡方检验：卡方检验属于非参数假设检验的一种，其本质都是度量频数之间的差异。其假设为：观察频数与期望
频数无差异或者两组变量相互独立不相关。 <span class="math display">\[
\chi^2=\sum \frac{(O-E)^2}{E}
\]</span></p>
<ul>
<li>卡方拟合优度检验：用于检验样本是否来自于某一个分布，比如检验某样本是否为正态分布</li>
<li>独立性卡方检验，查看两组类别变量分布是否有差异或者相关，以列联表的形式比较。以列联表形式的卡方检验中，卡方统计量由上式给出。</li>
</ul>
<h4><span id="步骤">步骤：</span></h4>
<p>卡方分箱是自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。基本思想:
对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p>
<p><strong>理想的分箱是在同一个区间内标签的分布是相同的</strong>。卡方分箱就是不断的计算相邻区间的卡方值（卡方值越小表示分布越相似），将分布相似的区间（卡方值最小的）进行合并，直到相邻区间的分布不同，达到一个理想的分箱结果。下面用一个例子来解释：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261231284.png" alt="image-20220708173638301" style="zoom: 33%;"></p>
<p>由上图，第一轮中初始化是5个区间，分别计算相邻区间的卡方值。找到1.2是最小的，合并2、3区间，为了方便，将合并后的记为第2区间，因此得到4个区间。第二轮中，由于合并了区间，影响该区间与前面的和后面的区间的卡方值，因此重新计算1和2,2和4的卡方值，由于4和5区间没有影响，因此不需要重新计算，这样就得到了新的卡方值列表，找到最小的取值2.5，因此该轮会合并2、4区间，并重复这样的步骤，一直到满足终止条件。</p>
<h4><span id="36-woe编码有监督"><strong><font color="red"> 3.6 WOE编码
【有监督】</font></strong></span></h4>
<blockquote>
<p><strong><font color="red">
风控模型—WOE与IV指标的深入理解应用</font></strong>:
https://zhuanlan.zhihu.com/p/80134853</p>
</blockquote>
<p><strong>WOE (Weight of Evidence, 证据权重）编码利用了标签信息,
属于有监督的编码方式</strong>。该方式广泛用于金融领 域信用风险模型中,
是该领域的经验做法。下面先给出WOE的计算公式: <span class="math display">\[
W O E_i=\ln \left\{\frac{P_{y 1}}{P_{y 0}}\right\}=\ln \left\{\frac{B_i
/ B}{G_i / G}\right\}
\]</span> <span class="math inline">\(W O E_i\)</span> 值可解释为第
<span class="math inline">\(i\)</span>
类别中好坏样本分布比值的对数。其中各个分量的解释如下: - <span class="math inline">\(P_{y 1}\)</span> 表示该类别中坏样本的分布 - <span class="math inline">\(P_{y 0}\)</span> 表示该类别中好样本的分布 - <span class="math inline">\(B_i / B\)</span>
表示该类别中坏样本的数量在总体坏样本中的占比 - <span class="math inline">\(G_i / G\)</span>
表示该类别中好样本的数量在总体好样本中的占比</p>
<p>很明显，如果整个分数的值大于1，那么WOE值为正，否则为负，所以WOE值的取值范围为正负无穷。
<strong>WOE值直观上表示的实际上是“当前分组中坏客户占所有坏客户的比例”和“当前分组中好客户占所有坏客户的比例”的差异。</strong>转化公式以后，也可以理解为：当前这个组中坏客户和好客户的比值，和所有样本中这个比值的差异。这个差异为这两个比值的比值，再取对数来表示的。
WOE越大，这种差异越大，这个分组里的样本坏样本可能性就越大，WOE越小，差异越小，这个分组里的坏样本可能性就越小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 随机生成1000行数据</span></span><br><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;x&#x27;</span>: np.random.choice([<span class="string">&#x27;R&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="string">&#x27;B&#x27;</span>], <span class="number">1000</span>),</span><br><span class="line">    <span class="string">&#x27;y&#x27;</span>: np.random.randint(<span class="number">2</span>, size=<span class="number">1000</span>)</span><br><span class="line">&#125;)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<h3><span id="四-缺失值处理解析">四、缺失值处理解析</span></h3>
<blockquote>
<p>看不懂你打我，史上最全的缺失值解析:
https://zhuanlan.zhihu.com/p/379707046</p>
<p>https://zhuanlan.zhihu.com/p/137175585</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>机器学习模型</th>
<th>是否支持缺失值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>XGBoost</strong></td>
<td>是</td>
</tr>
<tr class="even">
<td><strong>LightGBM</strong></td>
<td>是</td>
</tr>
<tr class="odd">
<td>线性回归</td>
<td>否</td>
</tr>
<tr class="even">
<td>逻辑回归（LR）</td>
<td>否</td>
</tr>
<tr class="odd">
<td>随机森林（RF）</td>
<td>否</td>
</tr>
<tr class="even">
<td>SVM</td>
<td>否</td>
</tr>
<tr class="odd">
<td>因子分解机(FM)</td>
<td>否</td>
</tr>
<tr class="even">
<td>朴实贝叶斯（NB）</td>
<td>否</td>
</tr>
</tbody>
</table>
<h4><span id="41-缺失值的替换">4.1 <strong>缺失值的替换</strong></span></h4>
<p><strong>scikit-learn中填充缺失值的API是Imputer类，使用方法如下：</strong></p>
<p>参数strategy有三个值可选：mean(平均值)，median(中位数)，most_frequent(众数)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rom sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 缺失值填补的时候必须得是float类型</span></span><br><span class="line"><span class="comment"># 缺失值要填充为np.nan，它是浮点型，strategy是填充的缺失值类型，这里填充平均数，axis代表轴，这里第0轴是列</span></span><br><span class="line">im = Imputer(missing_values=<span class="string">&#x27;NaN&#x27;</span>,strategy=<span class="string">&#x27;mean&#x27;</span>,axis=<span class="number">0</span>)</span><br><span class="line">data = im.fit_transform([[<span class="number">1</span>, <span class="number">2</span>], </span><br><span class="line">                         [np.nan, <span class="number">3</span>], </span><br><span class="line">                         [<span class="number">7</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<h4><span id="42-缺失值的删除">4.2 缺失值的删除</span></h4>
<h3><span id="五-异常值处理">五、异常值处理</span></h3>
<h3><span id="数据预处理qampa">数据预处理Q&amp;A</span></h3>
<h4><span id="1-lr为什么要离散化"><strong><font color="red"> 1、LR为什么要离散化？</font></strong></span></h4>
<p>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/122387176">学习]
连续<em>特征的离散化</em>：在什么情况<em>下</em>将连续的<em>特征离散化</em>之后可以获得更好的效果？</a></p>
<p><strong>问题描述：</strong>发现CTR预估一般都是用LR，而且特征都是离散的，为什么一定要用离散特征呢？这样做的好处在哪里？求大拿们解答。</p>
<h5><span id="答案一严林"><strong>答案一（<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=严林&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22122387176%22%7D">严林</a>）：</strong></span></h5>
<p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ol type="1">
<li>离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>【鲁棒性】离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则为0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>【模型假设】<strong>逻辑回归属于广义线性模型，表达能力受限</strong>；单变量离散化为N个后，每个变量有独立的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li>
<li>【特征交叉】离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险；</li>
</ol>
<h5><span id="李沐曾经说过模型是使用离散特征还是连续特征其实是一个海量离散特征简单模型同少量连续特征复杂模型的权衡"><strong><font color="red">
<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=李沐&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22122387176%22%7D">李沐</a>曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型”同“少量连续特征+复杂模型”的权衡。</font></strong></span></h5>
<blockquote>
<p>这里我写下我关于上面某些点的理解，有问题的欢迎指出：</p>
<ol start="0" type="1">
<li>假设目前有两个连续的特征：『年龄』和『收入』，预测用户的『魅力指数』；</li>
</ol>
<p>第三点:
<strong>LR是广义线性模型</strong>，因此如果特征『年龄』不做离散化直接输入，那么只能得到『年龄』和魅力指数的一个线性关系。但是这种线性关系是不准确的，并非年龄越大魅力指一定越大；如果将年龄划分为M段，则可以针对每段有一个对应的权重；这种分段的能力为模型带来类似『折线』的能力，也就是所谓的非线性
<strong>连续变量的划分，naive的可以通过人为先验知识划分，也可以通过训练单特征的决策树桩，根据Information
Gain/Gini系数等来有监督的划分。</strong>
假如『年龄』离散化后，共有N段，『收入』离散化后有M段；此时这两个离散化后的特征类似于<strong>CategoryFeature</strong>，对他们进行<strong>OneHotEncode</strong>，即可以得到
M + N的 01向量；例如： 0 1 0 0， 1 0 0 0 0； 第四点:
<strong>特征交叉</strong>，可以理解为上述两个向量的互相作用，作用的方式可以例如是
&amp;和|操作（这种交叉方式可以产生一个 M * N的01向量；）
上面特征交叉，可以类比于决策树的决策过程。例如进行&amp;操作后，得到一个1，则可以认为产生一个特征
（a &lt; age &lt; b &amp;&amp; c &lt; income &lt;
d）;将特征空间进行的非线性划分，也就是所谓的引入非线性；</p>
</blockquote>
<h5><span id="答案二周开拓"><strong>答案二（周开拓）：</strong></span></h5>
<p><strong><font color="red"> 机器学习里当然并没有free
lunch，一个方法能work，必定是有假设的。如果这个假设和真实的问题及数据比较吻合，就能work。</font></strong></p>
<p>对于LR这类的模型来说，假设基本如下：</p>
<ul>
<li><strong>局部<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=平坦性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22122387176%22%7D">平坦性</a>，或者说连续性</strong>。对于连续特征x来说，在任何一个取值x0的邻域附近，这个特征对预估目标y的影响也在一个足够小的邻域内变化。比如，人年龄对点击率的影响，x0=30岁假设会产生一定的影响，那么x=31或者29岁，这个影响和x0=30岁的影响差距不会太大；</li>
<li><strong>x对y的影响，这个函数虽然局部比较平坦，但是不太规律，如果你知道这个影响是个严格的直线</strong>（或者你有先验知识知道这个影响一定可以近似于一个参数不太多的函数），<strong>显然也没必要去做离散化</strong>。当然这条基本对于绝大多数问题都是成立的，因为基本没有这种好事情。</li>
</ul>
<p>假设一个最简单的问题，binary
classification，y=0/1，x是个连续值。你希望学到一个logloss足够小的y=f(x)。</p>
<p>那么有一种做法就是，在数据轴上切若干段，每一段观察训练样本里y为1的比例，以这个比例作为该段上y=f(x)的值。这个当然不是LR训练的过程，但是就是离散化的思想。你可以发现：</p>
<ul>
<li><strong>如果每一段里面都有足够多的样本，那么在这一段里的y=f(x)值的点估计就比较可信</strong>；</li>
<li><font color="red">如果x在数轴上分布不太均匀，比如是<strong>指数分布或者周期分布</strong>的，这么做可能会有问题，因而你要先<strong>对x取个log，或者去掉周期性</strong></font>；</li>
</ul>
<p>这就告诉了你应该怎么做离散化：<strong><font color="red">
尽可能保证每个分段里面有足够多的样本，尽量让样本的分布在数轴上均匀一些。</font></strong></p>
<p>结语：<strong>本质上连续特征离散化，可以理解为连续信号怎么转化为数字信号，好比我们计算机画一条曲线，也是变成了画一系列线段的问题。</strong>用分段函数来表达一个连续的函数在大多数情况下，都是work的。想取得好的效果需要：</p>
<ul>
<li>你的分段足够小，以使得在每个分段内x对y的影响基本在一个不大的邻域内，或者你可以忍受这个变化的幅度；</li>
<li>你的分段足够大，以使得在每个分段内有足够的样本，以获得可信的f(x)也就是权重；</li>
<li>你的分段策略使得在每个x的分段中，样本的分布尽量均匀（当然这很难），一般会根据先验知识先对x做一些变化以使得变得均匀一些；</li>
<li>如果你有非常强的x对y的先验知识，比如严格线性之类的，也未必做离散化，但是这种先验在计算广告或者推荐系统里一般是不存在的，也许其他领域比如CV之类的里面是可能存在的；</li>
</ul>
<p>最后还有个特别大的<strong>LR用离散特征的好处就是LR的特征是并行的，每个特征是并行同权的</strong>，如果有异常值的情况下，如果这个异常值没见过，那么LR里因为没有这个值的权重，最后对<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=score&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22122387176%22%7D">score</a>的贡献为0，最多效果不够好，但是不会错的太离谱。另外，如果你debug，很容易查出来是哪个段上的权重有问题，比较好定位和解决。</p>
<h4><span id="2-树模型为什么离散化"><strong><font color="red">2、树模型为什么离散化？</font></strong></span></h4>
<h5><span id="cart树的离散化">Cart树的离散化：</span></h5>
<p><strong>分类：</strong></p>
<ul>
<li><p><strong><font color="red">如果特征值是连续值：CART的处理思想与C4.5是相同的，即将连续特征值离散化。唯一不同的地方是度量的标准不一样，</font></strong>
<strong>CART采用基尼指数，而C4.5采用信息增益比</strong>。</p></li>
<li><p>如果当前节点为连续属性，<strong>CART树中该属性（剩余的属性值）后面还可以参与子节点的产生选择过程</strong>。</p></li>
</ul>
<p><strong>回归：</strong></p>
<p><strong>对于连续值的处理, CART
分类树采用基尼系数的大小来度量特征的各个划分点</strong>。在回归模型中,
我们使用常见的和方差度量方式, 对于任意划分特征 <span class="math inline">\(\mathrm{A}\)</span>, 对应的任意划分点 <span class="math inline">\(\mathrm{s}\)</span> 两边划分成的数据集 <span class="math inline">\(D_1\)</span> 和 <span class="math inline">\(D_2\)</span>, 求出使 <span class="math inline">\(D_1\)</span> 和 <span class="math inline">\(D_2\)</span> 各自集合的均方差最小, 同时 <span class="math inline">\(D_1\)</span> 和 <span class="math inline">\(D_2\)</span>
的均方差之和最小所对应的特征和特征值划分点。表达式为: <span class="math display">\[
\min _{a, s}\left[\min _{c_1} \sum_{x_i \in
D_1}\left(y_i-c_1\right)^2+\min _{c_2} \sum_{x_i \in
D_2}\left(y_i-c_2\right)^2\right]
\]</span> 其中, <span class="math inline">\(c_1\)</span> 为 <span class="math inline">\(D_1\)</span> 数据集的样本输出均值, <span class="math inline">\(c_2\)</span> 为 <span class="math inline">\(D_2\)</span> 数据集的样本输出均值。</p>
<h5><span id="lgb直方图算法优点">LGB直方图算法优点：</span></h5>
<p><strong>内存小、复杂度降低、直方图加速【分裂、并行通信、缓存优化】</strong></p>
<ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于
直方图算法，则只需要(1x样本数x维
度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的
bin
值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p></li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为<span class="math inline">\(k\)</span>的树的时间复杂度：对特征所有取值的排序为<span class="math inline">\(O(NlogN)\)</span>，<span class="math inline">\(N\)</span>为样本点数目，若有<span class="math inline">\(D\)</span>维特征，则<span class="math inline">\(O(kDNlogN)\)</span>，而直方图算法需要<span class="math inline">\(O(kD \times bin)\)</span> (bin是histogram
的横轴的数量，一般远小于样本数量<span class="math inline">\(N\)</span>)。</p></li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>两个维度</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的<span class="math inline">\(k\)</span>个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p></li>
<li><p><strong>数据并行优化</strong>，用 histgoram
可以大幅降低通信代价。用 pre-sorted
算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst
在并行的时候也使用 histogram 进行通信。</p></li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost
的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost
提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM
所使用直方图算法对 Cache
天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在
Cache Miss的问题。</p></li>
</ul>
<h4><span id="3-归一化">3、归一化？</span></h4>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li><p><strong>机器学习中的特征工程（四）----
特征离散化处理方法：</strong>https://www.jianshu.com/p/918649ce379a</p></li>
<li><p><strong>机器学习中的特征工程（三）----
序数和类别特征处理方法</strong>：https://www.jianshu.com/p/3d828de72cd4</p></li>
<li><p><strong>机器学习中的特征工程（二）----
数值类型数据处理</strong>：https://www.jianshu.com/p/b0cc0710ef55</p></li>
<li><p>机器学习中的特征工程（一）----
概览：https://www.jianshu.com/p/172677f4ea4c</p></li>
<li><p>特征工程完全手册 -
从预处理、构造、选择、降维、不平衡处理，到放弃：https://zhuanlan.zhihu.com/p/94994902</p></li>
<li><p><strong>这9个特征工程使用技巧，解决90%机器学习问题！</strong> -
Python与数据挖掘的文章 - 知乎
https://zhuanlan.zhihu.com/p/462744763</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2DEZT90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2DEZT90/" class="post-title-link" itemprop="url">特征工程（3）特征选择</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:49:50" itemprop="dateModified" datetime="2023-04-26T14:49:50+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="特征选择">特征选择</span></h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/74198735">【机器学习】特征选择(Feature
Selection)方法汇总</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/306057603"><em>特征选择方法</em>全面总结</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/479948993"><em>特征选择</em>的基本<em>方法</em>总结</a></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261231076.jpg" style="zoom: 67%;"></p>
<p>训练数据包含许多冗余或无用的特征，移除这些特征并不会导致丢失信息。其中冗余是指一个本身很有用的特征与另外一个有用的特征强相关，或它包含的信息能从其它特征推演出来;
特征很多但样本相对较少。</p>
<ul>
<li><p><strong>产生过程</strong>：产生特征或特征子集候选集合；</p></li>
<li><p><strong>评价函数</strong>：衡量特征或特征子集的重要性或者好坏程度，即量化特征变量和目标变量之间的联系以及特征之间的相互联系。为了避免过拟合，可用交叉验证的方式来评估特征的好坏；</p></li>
<li><p><strong>停止准则</strong>：为了减少计算复杂度，需设定一个阈值，当评价函数值达到阈值后搜索停止</p></li>
<li><p><strong>验证过程</strong>：在验证数据集上验证选出来的特征子集的有效性</p></li>
</ul>
<h3><span id="一-特征选择的目的"><strong>一、特征选择的目的</strong></span></h3>
<p>1.<strong>简化模型</strong>，使模型更易于理解：去除不相关的特征会降低学习任务的难度。并且可解释性能对模型效果的稳定性有更多的把握</p>
<p>2.<strong>改善性能</strong>：节省存储和计算开销</p>
<p>3.<strong>改善通用性、降低过拟合风险</strong>：减轻维数灾难，特征的增多会大大增加模型的搜索空间，大多数模型所需要的训练样本随着特征数量的增加而显著增加。特征的增加虽然能更好地拟合训练数据，但也可能增加方差</p>
<h3><span id="二-特征选择常见方法">二、特征选择常见方法</span></h3>
<ul>
<li><strong>Filter(过滤法)</strong>
<ul>
<li><strong>覆盖率</strong></li>
<li><strong>方差选择</strong></li>
<li><strong>Pearson(皮尔森)相关系数</strong></li>
<li><strong>卡方检验</strong></li>
<li><strong>互信息法(KL散度、相对熵)和最大信息系数</strong></li>
<li>Fisher得分</li>
<li>相关特征选择</li>
<li>最小冗余最大相关性</li>
</ul></li>
<li><strong>Wrapper(包装法)</strong>
<ul>
<li>完全搜索</li>
<li>启发搜索</li>
<li>随机搜索</li>
</ul></li>
<li><strong>Embedded(嵌入法)</strong>
<ul>
<li>L1 正则项</li>
<li>树模型选择</li>
<li>不重要性特征选择</li>
</ul></li>
</ul>
<h3><span id="三-filter过滤法特征集">三、<strong>Filter(过滤法)</strong>
【特征集】</span></h3>
<h3><span id></span></h3>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261231951.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h5><span id="定义"><strong>定义</strong></span></h5>
<ul>
<li><strong>过滤法的思想就是不依赖模型，仅从特征的角度来做特征的筛选</strong>，具体又可以分为两种方法，一种是根据特征里面包含的信息量，如方差选择法，如果一列特征的方差很小，每个样本的取值都一样的话，说明这个特征的作用不大，可以直接剔除。另一种是对每一个特征，都计算关于目标特征的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=相关度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22479948993%22%7D">相关度</a>，然后根据这个相关度来筛选特征，只保留高于某个阈值的特征，这里根据相关度的计算方式不同就可以衍生出一下很多种方法。</li>
</ul>
<h5><span id="分类"><strong>分类</strong></span></h5>
<ul>
<li><strong>单变量过滤方法</strong>：不需要考虑特征之间的相互关系，按照特征变量和目标变量之间的相关性或互信息对特征进行排序，过滤掉最不相关的特征变量。优点是计算效率高、不易过拟合。</li>
<li><strong>多变量过滤方法</strong>：考虑特征之间的相互关系，常用方法有基于相关性和一致性的特征选择</li>
</ul>
<h5><span id="覆盖率">覆盖率</span></h5>
<ul>
<li>即特征在训练集中出现的比例。若覆盖率很小，如有10000个样本，但某个特征只出现了5次，则次覆盖率对模型的预测作用不大，可删除</li>
</ul>
<h5><span id="1方差选择法">（1）<strong>方差选择法</strong></span></h5>
<ul>
<li>先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="comment"># 方差选择法，返回值为特征选择后的数据</span></span><br><span class="line"><span class="comment"># 参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>
<h5><span id="2pearson皮尔森相关系数">（2）Pearson皮尔森相关系数</span></h5>
<p><font color="red"><strong>用于度量两个变量X和Y之间的线性相关性</strong></font></p>
<ul>
<li><strong>用于度量两个变量X和Y之间的线性相关性</strong>，结果的取值区间为[-1,
1]，
-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关性</li>
<li>计算方法为两个变量之间的<strong>协方差</strong>和<strong>标准差</strong>的商</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">  <span class="comment"># 选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">  <span class="comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，</span></span><br><span class="line">  <span class="comment"># 输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。</span></span><br><span class="line">  <span class="comment"># 在此为计算相关系数</span></span><br><span class="line">  <span class="comment"># 其中参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, </span><br><span class="line">              k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h5><span id="3卡方检验">（3）<strong>卡方检验</strong></span></h5>
<p><font color="red">自变量对因变量的相关性</font></p>
<p><strong>检验定性自变量对定性因变量的相关性</strong>。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量:
<span class="math display">\[
\chi^2=\sum \frac{(A-E)^2}{E}
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="comment">#选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h5><span id="4psi互信息法kl散度-相对熵和最大信息系数mutual-information-and-maximal-information-coefficient-mic">（4）PSI互信息法(KL散度、相对熵)和最大信息系数
Mutual information and maximal information coefficient (MIC)</span></h5>
<blockquote>
<p><strong><font color="red">
风控模型—群体稳定性指标(PSI)深入理解应用</font></strong>:https://zhuanlan.zhihu.com/p/79682292</p>
</blockquote>
<p>评价定性自变量对定性因变量的相关性，评价类别型变量对类别型变量的相关性，互信息越大表明两个变量相关性越高，互信息为0时，两个变量相互独立。互信息的计算公式为
<span class="math display">\[
I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x)
p(y)}=D_{K L}(p(x, y) \| p(x) p(y))
\]</span></p>
<h5><span id="5fisher得分">（5）<strong>Fisher得分</strong></span></h5>
<p>对于分类问题, <strong>好的特征应该是在同一个类别中的取值比较相似,
而在不同类别之间的取值差异比较大</strong>。因此特征 <span class="math inline">\(\mathrm{i}\)</span> 的重要性可用Fiser得分 <span class="math inline">\(S_i\)</span> 来表示 <span class="math display">\[
S_i=\frac{\sum_{j=1}^K n_j\left(\mu_{i j}-\mu_i\right)^2}{\sum_{j=1}^K
n_j \rho_{i j}^2}
\]</span> 其中, <span class="math inline">\(u_{i j}\)</span> 和 <span class="math inline">\(\rho_{i j}\)</span> 分别是特征i在类别 <span class="math inline">\(j\)</span> 中均值和方差, <span class="math inline">\(\mu_i\)</span> 为特征i的均值, <span class="math inline">\(n_j\)</span>
为类别中中的样本数。<strong>Fisher得分越高,
特征在不同类别之间的差异性越大、在同一类别中的差异性越小，则特征越重要;</strong></p>
<h5><span id="6相关特征选择">（6）<strong>相关特征选择</strong></span></h5>
<p>该方法基于的假设是，好的特征集合包含跟目标变量非常相关的特征，但这些特征之间彼此不相关</p>
<h5><span id="7最小冗余最大相关性mrmr">（7）<strong>最小冗余最大相关性(
mRMR)</strong></span></h5>
<p>由于单变量过滤法只考虑了单特征变量和目标变量之间的相关性，因此选择的特征子集可能过于冗余。mRMR在进行特征时考虑到了特征之间的冗余性，具体做法是对跟已选择特征相关性较高的冗余特征进行惩罚;</p>
<h4><span id="四-wrapper包装法特征集模型">四、Wrapper(包装法)
【特征集+模型】</span></h4>
<p><img src="https://pic4.zhimg.com/v2-bd321ff1e16c011d1a2bce86a5939a17_b.jpg"></p>
<p><strong>定义</strong></p>
<ul>
<li><p>使用<strong>机器学习算法评估特征子集</strong>的效果，可以检测两个或多个特征之间的交互关系，而且选择的特征子集让模型的效果达到最优。</p></li>
<li><p>这是<strong>特征子集搜索</strong>和<strong>评估指标相结合</strong>的方法。前者提供候选的新特征子集，后者基于新特征子集训练一个模型，并用验证集进行评估，为每一组特征子集进行打分。</p></li>
<li><p>最简单的方法是在<strong>每一个特征子集上训练并评估模型</strong>，从而找出最优的特征子集</p></li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>需要对每一组特征子集训练一个模型，<strong>计算量很大</strong></li>
<li>样本不够充分的情况下<strong>容易过拟合</strong></li>
<li>特征变量较多时计算复杂度太高</li>
</ul>
<h5><span id="1完全搜索">（1）完全搜索</span></h5>
<p>即穷举法, 遍历所有可能的组合达到全局最优, 时间复杂度 <span class="math inline">\(2^n\)</span></p>
<h5><span id="2启发式搜索">（2）启发式搜索</span></h5>
<p>序列向前选择: 特征子集从空集开始, 每次只加入一个特征, 时间复杂度为
<span class="math inline">\(O(n+(n-1)+(n-2)+\ldots+1)=O\left(n^2\right)\)</span></p>
<p>序列向后选择: 特征子集从全集开始, 每次删除一个特征, 时间复杂度为
<span class="math inline">\(O\left(n^2\right)\)</span></p>
<h5><span id="3随机搜索">（3）<strong>随机搜索</strong></span></h5>
<p>执行序列向前或向后选择时，随机选择特征子集</p>
<h5><span id="4递归特征消除法">（4）<strong>递归特征消除法</strong></span></h5>
<p>使用一个基模型进行多轮训练，每轮训练后通过学习器返回的<strong>coef</strong>_或者<strong>feature_importances</strong>_消除若干权重较低的特征，再基于新的特征集进行下一轮训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#递归特征消除法，返回特征选择后的数据</span></span><br><span class="line"><span class="comment">#参数estimator为基模型</span></span><br><span class="line"><span class="comment">#参数n_features_to_select为选择的特征个数</span></span><br><span class="line">RFE(estimator=LogisticRegression(), </span><br><span class="line">    n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, </span><br><span class="line">                                          iris.target)</span><br></pre></td></tr></table></figure>
<h3><span id="五-embedded嵌入法">五、Embedded（嵌入法）</span></h3>
<p>将特征选择嵌入到模型的构建过程中，具有包装法与机器学习算法相结合的优点，也具有过滤法计算效率高的优点</p>
<h5><span id="1lasso方法-l1正则项">（1）LASSO方法 L1正则项</span></h5>
<p>通过对回归系数添加惩罚项来防止过拟合，可以让特定的回归系数变为0，从而可以选择一个不包含那些系数的更简单的模型；实际上，L1惩罚项降维的原理是，在多个对实际上，L1惩罚项降维的原理是，在多个对目标值具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#带L1惩罚项的逻辑回归作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(LogisticRegression(</span><br><span class="line">          penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(</span><br><span class="line">               iris.data,iris.target)</span><br></pre></td></tr></table></figure>
<h5><span id="2基于树模型的特征选择方法">（2）基于树模型的特征选择方法</span></h5>
<ul>
<li>在决策树中，深度较浅的节点一般对应的特征分类能力更强(可以将更多的样本区分开)</li>
<li>对于基于决策树的算法，如随机森林，重要的特征更有可能出现在深度较浅的节点，而且出现的次数可能越多</li>
<li>即可基于树模型中特征出现次数等指标对特征进行重要性排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="comment">#GBDT作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(</span><br><span class="line">    GradientBoostingClassifier()).fit_transform(</span><br><span class="line">      iris.data,iris.target)</span><br></pre></td></tr></table></figure>
<h5><span id="3使用特征重要性来筛选特征的缺陷">（3）使用特征重要性来筛选特征的缺陷？</span></h5>
<ul>
<li>特征重要性只能说明哪些特征在训练时起到作用了，并不能说明特征和目标变量之间一定存在依赖关系。举例来说，随机生成一大堆没用的特征，然后用这些特征来训练模型，一样可以得到特征重要性，但是这个特征重要性并不会全是0，这是完全没有意义的。</li>
<li>特征重要性容易高估数值特征和基数高的类别特征的重要性。这个道理很简单，特征重要度是根据决策树分裂前后节点的不纯度的减少量（基尼系数或者MSE）来算的，那么对于数值特征或者基础高的类别特征，不纯度较少相对来说会比较多。</li>
<li>特征重要度在选择特征时需要决定阈值，要保留多少特征、删去多少特征，这些需要人为决定，并且删掉这些特征后模型的效果也不一定会提升。</li>
</ul>
<h5><span id="4non-importance-选择">（4）Non importance 选择</span></h5>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/16MJEZ4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/16MJEZ4/" class="post-title-link" itemprop="url">特征工程（4）不平衡数据集*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:51:47" itemprop="dateModified" datetime="2023-04-26T14:51:47+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="不平衡数据问题">不平衡数据问题</span></h2>
<p><strong>实际上，很多时候，数据不平衡并没有啥负面影响，并不是数据不平衡了，就一定要处理。如果你只是为了做而做，我有99%的自信告诉你，你做了也是白做，啥收益都没有。</strong></p>
<h4><span id="为什么很多模型在训练数据不均衡时会出问题">为什么很多模型在训练数据不均衡时会出问题？</span></h4>
<p><strong>本质原因是</strong>：<strong>模型在训练时优化的目标函数和在测试时使用的评价标准不一致。</strong>这种”不一致“可能是训练数据的样本分布与测试数据分布不一致；</p>
<h3><span id="一-不平衡数据集的主要处理方式"><strong>一、不平衡数据集的主要处理方式？</strong></span></h3>
<h4><span id="11-数据的角度">1.1 <strong>数据的角度</strong></span></h4>
<p>主要方法为采样，分为<strong>欠采样</strong>和<strong>过采样</strong>以及对应的一些改进方法。[<strong>python
imblearn库</strong>]<strong><font color="red">尊重真实样本分布，人为主观引入样本权重，反而可能得出错误的结论。</font></strong></p>
<h5><span id="业务角度"><strong><font color="red">
业务角度</font></strong>：</span></h5>
<ul>
<li><strong>时间因素</strong>，对近期样本提高权重，较远样本降低权重。这是考虑近期样本与未来样本之间的“相似度”更高，希望模型学到更多近期样本的模式。</li>
<li><strong>贷款类型</strong>，不同额度、利率、期限的样本赋予不同权重，这需要结合业务未来的发展方向。例如，未来业务模式希望是小额、短期、低利率，那就提高这批样本的权重。</li>
<li><strong>样本分群</strong>，不同群体赋予不同权重。例如，按流量获客渠道，如果未来流量渠道主要来自平台A，那么就提高这批样本权重。</li>
</ul>
<h5><span id="技术角度"><strong><font color="red">
技术角度：</font></strong></span></h5>
<ul>
<li><p><strong>欠采样</strong>：</p>
<ul>
<li><p><strong>EasyEnsemble</strong>：从多数类<span class="math inline">\(S_{max}\)</span>上随机抽取一个子集，与其他类训练一个分类器；重复若干次，多个分类器融合。</p></li>
<li><p><font color="red"><strong>BalanceCascade</strong></font>：从多数类<span class="math inline">\(S_{max}\)</span>上随机抽取一个子集，与其他类训练一个分类器；<strong>剔除能被分类正确的分类器</strong>，重复若干次，多个分类器融合。</p></li>
<li><p><strong>NearMIss</strong>：利用K邻近信息挑选具有代表性的样本。</p></li>
<li><p><strong>one-side Selection</strong>：采用数据清洗技术。</p></li>
</ul></li>
<li><p><strong>过采样</strong>：</p>
<ul>
<li><p><strong>随机采样</strong></p></li>
<li><p><strong>SMOTE算法</strong>：对少数类<span class="math inline">\(S_{min}\)</span>中每个样本x的K近邻随机选取一个样本y，在x，y的连线上随机选取一个点作为新的样本点。</p></li>
<li><p><strong>Borderline-SMOTE、ADASYN改进算法等</strong></p></li>
</ul></li>
<li><h5><span id="分层抽样技术批量训练分类器的分层抽样技术-当面对不平衡类问题时这种技术通过消除批次内的比例差异可使训练过程更加稳定">分层抽样技术：批量训练分类器的「分层抽样」技术。当面对不平衡类问题时，这种技术（通过消除批次内的比例差异）可使训练过程更加稳定。</span></h5></li>
</ul>
<h4><span id="12-算法的角度"><strong>1.2 算法的角度</strong></span></h4>
<p>考虑<strong>不同误分类情况代价的差异性</strong>对算法进行优化，主要是基于<strong>代价敏感学习算法</strong>(Cost-Sensitive
Learning)，代表的算法有<strong>adacost</strong>。<a href="实现基于代价敏感的AdaCost算法">实现基于代价敏感的AdaCost算法</a></p>
<ul>
<li><p><strong>代价函数</strong>：可以增加小类样本的权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。刚开始，可以设置每个类别的权值与样本个数比例的倒数，然后可以使用过采样进行调优。</p>
<blockquote>
<p>这种方法的难点在于设置合理的权重，实际应用中一般让各个分类间的加权损失值近似相等。当然这并不是通用法则，还是需要具体问题具体分析。</p>
</blockquote></li>
<li><h5><span id="xgb自定义损失函数jhwjhw0123imbalance-xgboostfocalloss">XGB自定义损失函数
/<strong><a target="_blank" rel="noopener" href="https://github.com/jhwjhw0123/Imbalance-XGBoost">Imbalance-XGBoost</a></strong>【Focal
Loss】：</span></h5></li>
</ul>
<p><span class="math display">\[
L_w=-\sum_{i=1}^m \hat{y}_i\left(1-y_i\right)^\gamma \log
\left(y_i\right)+\left(1-\hat{y}_i\right) y_i^\gamma \log
\left(1-y_i\right)
\]</span></p>
<h4><span id="13-分类方式">1.3 <strong>分类方式</strong></span></h4>
<p>可以把小类样本作为<strong>异常点</strong>(outliers)，把问题转化为<strong>异常点检测问题(anomaly
detection)</strong>。此时分类器需要学习到大类的决策分界面，即分类器是一个<strong>单个类分类器（One
Class Classifier）</strong>。代表的算法有<font color="red">
<strong>One-class SVM</strong></font>。</p>
<h5><span id="一类分类算法">一类分类算法：</span></h5>
<p>不平衡数据集的一类分类算法:https://machinelearningmastery.com/one-class-classification-algorithms/</p>
<p>一类分类是机器学习的一个领域，它提供了异常值和异常检测的技术,如何使一类分类算法适应具有严重偏斜类分布的不平衡分类,如何拟合和评估
SVM、隔离森林、椭圆包络、局部异常因子等一类分类算法。</p>
<h5><span id="不平数据集的划分方法">不平数据集的划分方法？</span></h5>
<ul>
<li><p>K折交叉验证？</p></li>
<li><p>自助法？</p></li>
</ul>
<h5><span id="不平数据集的评价方法">不平数据集的评价方法？</span></h5>
<p>G-Mean和ROC曲线和AUC。Topk@P</p>
<ul>
<li><strong>AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。</strong></li>
</ul>
<h3><span id="二-类别不平衡如何得到一个不错的分类器">二、「类别不平衡」如何得到一个不错的分类器？</span></h3>
<blockquote>
<p><strong>微调</strong>：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32940093">如何处理数据中的「类别不平衡」？</a></p>
</blockquote>
<p>机器学习中常常会遇到数据的<strong>类别不平衡（class
imbalance）</strong>，也叫数据偏斜（class
skew）。以常见的二分类问题为例，我们希望预测病人是否得了某种罕见疾病。但在历史数据中，阳性的比例可能很低（如百分之0.1）。在这种情况下，学习出好的分类器是很难的，而且在这种情况下得到结论往往也是很具迷惑性的。</p>
<p>以上面提到的场景来说，如果我们的分类器<strong>总是</strong>预测一个人未患病，即预测为反例，那么我们依然有高达99.9%的预测准确率。然而这种结果是没有意义的，这提出了今天的第一个问题，如何有效在类别不平衡的情况下评估分类器？</p>
<p><strong>当然，本文最终希望解决的问题是：在数据偏斜的情况下，如何得到一个不错的分类器？如果可能，是否可以找到一个较为简单的解决方法，而规避复杂的模型、数据处理，降低我们的工作量。</strong></p>
<h4><span id="21类别不平衡下的评估问题"><strong>2.1
类别不平衡下的评估问题</strong>?</span></h4>
<p>而<strong>当类别不平衡时，准确率就非常具有迷惑性</strong>，而且意义不大。给出几种主流的评估方法：</p>
<ul>
<li><strong>ROC</strong>是一种常见的替代方法，全名receiver operating
curve，计算ROC曲线下的面积是一种主流方法</li>
<li><strong>Precision-recall
curve</strong>和ROC有相似的地方，但定义不同，计算此曲线下的面积也是一种方法</li>
<li><strong>Precision@n</strong>是另一种方法，特制将分类阈值设定得到恰好n个正例时分类器的precision</li>
<li>Average
precision也叫做平均精度，主要描述了precision的一般表现，在异常检测中有时候会用</li>
<li>直接使用Precision也是一种想法，但此时的假设是分类器的阈值是0.5，因此意义不大</li>
</ul>
<blockquote>
<p>本文的目的不是介绍一般的分类评估标准，简单的科普可以参看：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/19645541">如何解释召回率与准确率？</a></p>
</blockquote>
<h4><span id="22解决类别不平衡中的奇淫巧技有什么"><strong>2.2
解决类别不平衡中的“奇淫巧技”有什么？</strong></span></h4>
<p>对于类别不平衡的研究已经有很多年了，在资料[1]中就介绍了很多比较复杂的技巧。结合我的了解举几个简单的例子：</p>
<blockquote>
<p>[1] He, H. and Garcia, E.A., 2009. Learning from imbalanced data.
<em>IEEE Transactions on knowledge and data engineering</em>,
<em>21</em>(9), pp.1263-1284.</p>
</blockquote>
<ul>
<li>对数据进行采用的过程中通过相似性同时生成并插样“少数类别数据”，叫做SMOTE算法</li>
<li>对数据先进行聚类，再将大的簇进行随机欠采样或者小的簇进行数据生成</li>
<li>把监督学习变为无监督学习，舍弃掉标签把问题转化为一个无监督问题，如异常检测</li>
<li><strong>先对多数类别进行随机的欠采样，并结合boosting算法进行集成学习</strong></li>
</ul>
<h4><span id="23简单通用的算法有哪些"><strong>2.3
简单通用的算法有哪些？</strong></span></h4>
<ul>
<li>对较多的那个类别进行欠采样(under-sampling)，舍弃一部分数据，使其与较少类别的数据相当</li>
<li>对较少的类别进行过采样(over-sampling)，重复使用一部分数据，使其与较多类别的数据相当</li>
<li><strong>阈值调整（threshold
moving）</strong>，将原本默认为0.5的阈值调整到
较少类别/（较少类别+较多类别）即可</li>
</ul>
<p>当然很明显我们可以看出，<strong>第一种和第二种方法都会明显的改变数据分布，我们的训练数据假设不再是真实数据的无偏表述</strong>。在第一种方法中，我们<strong>浪费了很多数据</strong>。而第二类方法中有无中生有或者重复使用了数据，会导致过拟合的发生。</p>
<p><strong>因此欠采样的逻辑中往往会结合集成学习来有效的使用数据，假设正例数据n，而反例数据m个。我们可以通过欠采样，随机无重复的生成（k=n/m）个反例子集，并将每个子集都与相同正例数据合并生成k个新的训练样本。我们在k个训练样本上分别训练一个分类器，最终将k个分类器的结果结合起来，比如求平均值。这就是一个简单的思路，也就是Easy
Ensemble [5]。</strong></p>
<p>但不难看出，其实这样的过程是需要花时间处理数据和编程的，对于很多知识和能力有限的人来说难度比较大。特此推荐两个简单易行且效果中上的做法：</p>
<ul>
<li>简单的调整阈值，不对数据进行任何处理。此处特指将分类阈值从0.5调整到正例比例</li>
<li>使用现有的集成学习分类器，如随机森林或者xgboost，<strong>并调整分类阈值</strong></li>
</ul>
<p><strong>提出这样建议的原因有很多。首先，简单的阈值调整从经验上看往往比过采样和欠采样有效</strong>
[6]。其次，如果你对统计学知识掌握有限，而且编程能力一般，在集成过程中更容易出错，还不如使用现有的集成学习并调整分类阈值。</p>
<h4><span id="24一个简单但有效的方案"><strong>2.4
一个简单但有效的方案</strong></span></h4>
<p>经过了上文的分析，我认为一个比较靠谱的解决方案是：</p>
<ul>
<li>不对数据进行过采样和欠采样，但使用现有的集成学习模型，如随机森林</li>
<li>输出随机森林的预测概率，<strong>调整阈值得到最终结果</strong></li>
<li><strong>选择合适的评估标准，如precision@n</strong></li>
</ul>
<h3><span id="三-脉脉数据集不平衡应该思考什么">三、脉脉：数据集不平衡应该思考什么</span></h3>
<p>首先, 猜测一下, 你研究的数据存在着较 大的不平衡,
你还是比较关注正类(少数类) 样本的, 比如【想要识别出 有信用风险 的
人】那么就要谈一下你所说的【模型指标还行】这个问题。<strong>auc这种复合指标先不提,
precision代表的是, 你预测的信用风险人群,
其中有多少是真的信用风险人群。recall 代表的是,
"真的信用风险人群"有多少被你识别出来了</strong>；</p>
<ul>
<li>所以, 倘若你比较关注的是【我想要找出
所有"可能有违约风险的人"】宁可错杀也不
放过。那么你应该重点关注的就是召回率 recall。在此基础上,
尽量提高precision。</li>
<li>你把训练集的正负样本控制在64左右, 那 么你是怎么控制的呢,
是单纯用了数据清理技术, 还是单纯生成了一些新的样本, 还是怎么做的？</li>
<li><font color="red"><strong>如果条件允许, 可以查看一下你被错分的 样本,
看看被错分的原因可能是什么, 是因为类重叠,
还是有少数类的分离还是单纯的因为不平衡比太夸张所以使分类器产生偏倚?</strong>
</font></li>
<li><font color="red">不知道你用的什么模型,
但是现在有一些把重采样和分类器结合在一起的集成学习方法,
可以试试看。</font></li>
<li>维度太高的时候, <strong>特征的提取很重要</strong>呀！</li>
<li>当做异常检测问题可能会好一些?</li>
</ul>
<h3><span id="四-样本准备与权重指标">四、样本准备与权重指标</span></h3>
<blockquote>
<p>样本权重对逻辑回归评分卡的影响探讨:
https://zhuanlan.zhihu.com/p/110982479</p>
</blockquote>
<h4><span id="风控业务背景"><strong>风控业务背景</strong></span></h4>
<p><strong>在统计学习建模分析中，样本非常重要，它是我们洞察世界的窗口</strong>。在建立逻辑回归评分卡时，我们也会考虑对样本赋予不同的权重weight，希望模型学习更有侧重点。</p>
<p>诚然，我们可以通过实际数据测试来检验赋权前后的差异，但我们更希望从理论上分析其合理性。毕竟理论可以指导实践。本文尝试探讨样本权重对逻辑回归评分卡的影响，以及从业务和技术角度分析样本权重调整的操作方法。</p>
<h4><span id="part-1样本加权对woe的影响"><strong>Part 1.
样本加权对WOE的影响</strong></span></h4>
<p>在<strong>《</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80134853">WOE与IV指标的深入理解应用</a><strong>》</strong>一文中,
我们介绍了WOE的概念和计算方法。在逻辑回归评分卡中, 其具有
重要意义。其公式定义如下:</p>
<p><span class="math display">\[
W_O=\ln \left(\frac{\text { Good }_i}{\text { Good }_T} / \frac{\text {
Bad }_i}{\operatorname{Bad}_T}\right)=\ln \left(\frac{\text { Good
}_i}{\text { Bad }_i}\right)-\ln \left(\frac{\text { Good
}_T}{\operatorname{Bad}_T}\right)
\]</span>
<strong>现在，我们思考在计算WOE时，是否要考虑样本权重呢?</strong></p>
<p>如图1所示的样本, 我们希望对某些样本加强学习,
因此对年龄在46岁以下的样本赋予权重1.5, 而对46岁以上的样本赋予权重1.0,
也就是加上权重列weight。此时再计算WOE值,
我们发现数值发生变化。这是因为权重的改 变, 既影响了局部bucket中的
odds，也影响了整体的 odds 。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261249252.jpg" alt="img">我们有2种对样本赋权后的模型训练方案，如图2所示。</p>
<ul>
<li>方案1: WOE变换利用原训练集，LR模型训练时利用加权后的样本。</li>
<li>方案2: WOE变换和LR模型训练时，均使用加权后的样本。</li>
</ul>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261249494.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>个人更倾向于第一种方案，原因在于：<strong>WOE变换侧重变量的可解释性，引入样本权重会引起不可解释的困扰。</strong></p>
<h4><span id="part-2采样对lr系数的影响"><strong>Part 2.
采样对LR系数的影响</strong></span></h4>
<p>我们定义特征向量 <span class="math inline">\(\mathbf{x}=x_1, x_2,
\ldots, x_n\)</span> 。记 <span class="math inline">\(G=\operatorname{Good}, B=B a d\)</span>,
那么逻辑回归的公式组成便是: <span class="math display">\[
\begin{aligned}
&amp; \operatorname{Ln}(\text { odds }(G \mid
\mathbf{x}))=\operatorname{Ln}\left(\frac{p_G f(\mathbf{x} \mid G)}{p_B
f(\mathbf{x} \mid
B)}\right)=\operatorname{Ln}\left(\frac{p_G}{p_B}\right)+\operatorname{Ln}\left(\frac{f(\mathbf{x}
\mid G)}{f(\mathbf{x} \mid B)}\right) \\
&amp;
=\operatorname{Ln}\left(\frac{p_G}{p_B}\right)+\operatorname{Ln}\left(\frac{f\left(x_1,
x_2, \ldots, x_n \mid G\right)}{f\left(x_1, x_2, \ldots, x_n \mid
B\right)}\right) \\
&amp;
=\operatorname{Ln}\left(\frac{p_G}{p_B}\right)+\operatorname{Ln}\left(\frac{f\left(x_1
\mid G\right)}{f\left(x_1 \mid
B\right)}\right)+\operatorname{Ln}\left(\frac{f\left(x_2 \mid
G\right)}{f\left(x_2 \mid
B\right)}\right)+\ldots+\operatorname{Ln}\left(\frac{f\left(x_n \mid
G\right)}{f\left(x_n \mid B\right)}\right) \\
&amp; =L n\left(\text { odd } s_{\text {pop
}}\right)+\operatorname{Ln}\left(\text { odd } s_{i n f
o}(\mathbf{x})\right) \\
&amp;
\end{aligned}
\]</span> 其中, 第2行到第3行的变换是基于朴素贝叶斯假设, 即自变量 <span class="math inline">\(x_i\)</span> 之间相互独立。</p>
<ul>
<li><span class="math inline">\(o d d s_{p o p}\)</span> 是指总体
(训练集) 的 <span class="math inline">\(o d d s\)</span>, 指先验信息
<span class="math inline">\(o d d s\)</span> 。</li>
<li><span class="math inline">\(o d d s_{i n f o}(\mathbf{x})\)</span>
是指自变量引起的 <span class="math inline">\(o d d s\)</span> 变化,
我们称为后验信息 <span class="math inline">\(o d d s 。\)</span></li>
</ul>
<p><font color="red">因此，随着观察信息的不断加入，对群体的好坏 <span class="math inline">\(o d d s\)</span> 判断将越来越趋于客观。</font></p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-0021358b05ebb5fea25073cceea1da2d_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h5><span id="样本权重调整直接影响先验项也就是截距-那对系数的影响呢">样本权重调整直接影响先验项，也就是截距。那对系数的影响呢？</span></h5>
<p>接下来，我们以过采样（Oversampling）和欠采样（Undersampling）为例，分析采样对LR系数的影响。如图4所示，对于不平衡数据集，过采样是指对正样本简单复制很多份；欠采样是指对负样本随机抽样。最终，正负样本的比例将达到1:1平衡状态。</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-c49197fdd37023e75daabd7e74feb11b_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>我们同样从贝叶斯角度进行解释:</strong> <span class="math display">\[
\begin{gathered}
P(B \mid \mathbf{x})=\frac{P(\mathbf{x} \mid B)
P(B)}{P(\mathbf{x})}=\frac{P(\mathbf{x} \mid B) P(B)}{P(\mathbf{x} \mid
B) P(B)+P(\mathbf{x} \mid G) P(G)} \\
\Leftrightarrow \frac{1}{P(B \mid \mathbf{x})}=1+\frac{P(\mathbf{x} \mid
G) P(G)}{P(\mathbf{x} \mid B) P(B)} \\
\Leftrightarrow \operatorname{Ln}\left(\frac{P(\mathbf{x} \mid
G)}{P(\mathbf{x} \mid B)}\right)=\operatorname{Ln}\left(\frac{1}{P(B
\mid
\mathbf{x})}-1\right)-\operatorname{Ln}\left(\frac{P(G)}{P(B)}\right) \\
\Leftrightarrow \operatorname{Ln}\left(\frac{P(G \mid \mathbf{x})}{P(B
\mid \mathbf{x})}\right)=\operatorname{Ln}\left(\frac{P(\mathbf{x} \mid
G)}{P(\mathbf{x} \mid
B)}\right)+\operatorname{Ln}\left(\frac{P(G)}{P(B)}\right)
\end{gathered}
\]</span> 假设采样处理后的训练集为 <span class="math inline">\(\mathbf{x}^{\prime}\)</span> 。记 <span class="math inline">\(\# B\)</span> 和 <span class="math inline">\(\#
G\)</span> 分别表示正负样本数, 那么显然: <span class="math display">\[
o d d s=\frac{\# G}{\# B} \neq \frac{\# G^{\prime}}{\# B^{\prime}}=o d d
s^{\prime}
\]</span> 由于 <span class="math inline">\(\operatorname{Ln}\left(\frac{P(G)}{P(B)}\right)=\operatorname{Ln}\left(\frac{\#
G}{\# B}\right)\)</span> ，因此对应截距将发生变化。</p>
<p><font color="red">无论是过采样, 还是欠采样,
处理后的新样本都和原样本服从同样的分布, 即满足</font>: <span class="math display">\[
\begin{aligned}
&amp; P(\mathbf{x} \mid G)=P\left(\mathbf{x}^{\prime} \mid
G^{\prime}\right) \\
&amp; P(\mathbf{x} \mid B)=P\left(\mathbf{x}^{\prime} \mid
B^{\prime}\right)
\end{aligned}
\]</span> 因此, <span class="math inline">\(\operatorname{Ln}\left(\frac{P(\mathbf{x} \mid
G)}{P(\mathbf{x} \mid
B)}\right)=\operatorname{Ln}\left(\frac{P\left(\mathbf{x}^{\prime} \mid
G^{\prime}\right)}{P\left(\mathbf{x}^{\prime} \mid
B^{\prime}\right)}\right)\)</span>, 即系数不发生变化。</p>
<p><strong>实践证明，按照做评分卡的方式，做WOE变换，然后跑LR，单变量下确实只有截距影响。而对于多变量，理想情况下，当各自变量相互独立时，LR的系数是不变的，但实际自变量之间多少存在一定的相关性，所以还是会有一定的变化。</strong></p>
<h4><span id="part-3样本准备与权重指标"><strong><font color="red"> Part 3.
样本准备与权重指标</font></strong></span></h4>
<p><strong>风控建模的基本假设是末来样本和历史样本的分布是一致的</strong>。模型从历史样本中拟合
<span class="math inline">\(X_{\text {old }}\)</span> 和 <span class="math inline">\(y\)</span> 之间的关系，并 根据末来样本的 <span class="math inline">\(X_{\text {new }}\)</span>
进行预测。<font color="red">因此, 我们总是在思考,
如何选择能代表末来样本的训练样本。</font></p>
<p>如图5所示，不同时间段、不同批次的样本总是存在差异，即<strong>服从不同的总体分布</strong>。因此，我们需要<strong>从多个维度来衡量两个样本集之间的相似性。</strong></p>
<p>从迁移学习的角度看，这是一个从源域（source
domain）中学习模式，并应用到目标域（target
domain）的过程。在这里，源域是训练集，目标域指测试集，或者未来样本。</p>
<p>这就会涉及到一些难点：</p>
<ul>
<li>假设测试集OOT与未来总体分布样本基本一致，但未来样本是不可知且总是在发生变化。</li>
<li>面向测试集效果作为评估指标，会出现在测试集上过拟合现象。 <img src="https://pic4.zhimg.com/80/v2-b40bd56da51ab37d01fbffbdb8bc91f7_1440w.jpg" alt="img"></li>
</ul>
<p><strong>那么，建模中是否可以考虑建立一个权重指标体系，即综合多方面因素进行样本赋权？我们采取2种思路来分析如何开展。</strong></p>
<p><strong><font color="red"> 业务角度</font></strong>：</p>
<ol type="1">
<li><strong>时间因素</strong>，对近期样本提高权重，较远样本降低权重。这是考虑近期样本与未来样本之间的“相似度”更高，希望模型学到更多近期样本的模式。</li>
<li><strong>贷款类型</strong>，不同额度、利率、期限的样本赋予不同权重，这需要结合业务未来的发展方向。例如，未来业务模式希望是小额、短期、低利率，那就提高这批样本的权重。</li>
<li><strong>样本分群</strong>，不同群体赋予不同权重。例如，按流量获客渠道，如果未来流量渠道主要来自平台A，那么就提高这批样本权重。</li>
</ol>
<p>结合以上各维度，可得到总体采样权重的一种融合方式为： <span class="math display">\[
w=w_1 * w_2 * w_3
\]</span>
<strong>这种业务角度的方案虽然解释性强，但实际拍定多大的权重显得非常主观，实践中往往需要不断尝试，缺少一些理论指导。</strong></p>
<p><strong><font color="red"> 技术角度：</font></strong></p>
<ol type="1">
<li><strong>过采样、欠采样等，从样本组成上调整正负不平衡</strong>。</li>
<li><strong>代价敏感学习，在损失函数对好坏样本加上不同的代价。比如，坏样本少，分错代价更高。</strong></li>
<li><strong>借鉴Adaboost的做法，对误判样本在下一轮训练时提高权重。</strong></li>
</ol>
<p>在机器学习中，有一个常见的现象——<strong>Covariate
Shift</strong>，是指当训练集的样本分布和测试集的样本分布不一致的时候，训练得到的模型无法具有很好的泛化
(Generalization) 能力。</p>
<p>其中一种做法，既然是希望让训练集尽可能像测试集，那就让模型帮助我们做这件事。如图6所示，将测试集标记为1，训练集标记为0，训练一个LR模型，在训练集上预测，概率越高，说明这个样例属于测试集的可能性越大。以此达到样本权重调整的目的。</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-6a218af9de452a4c58e36a9e8e6d4bb0_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4><span id="part-4常见工具包的样本赋权"><strong>Part 4.
常见工具包的样本赋权</strong></span></h4>
<p>现有Logistic
Regression模块主要来自sklearn和scipy两个包。很不幸，scipy包并不支持直接赋予权重列。这是为什么呢？有统计学家认为，<strong><font color="red">
尊重真实样本分布，人为主观引入样本权重，反而可能得出错误的结论。</font></strong></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-4942078e60d225438f77362c675bec20_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>因此，我们只能选择用scikit-learn。样本权重是如何体现在模型训练过程呢？查看源码后，发现目前主要是体现在<strong>损失函数</strong>中，即<strong>代价敏感学习。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Logistic loss is the negative of the log of the logistic function.</span></span><br><span class="line"><span class="comment"># 添加L2正则项的逻辑回归对数损失函数</span></span><br><span class="line">out = -np.<span class="built_in">sum</span>(sample_weight * log_logistic(yz)) + <span class="number">.5</span> * alpha * np.dot(w, w)</span><br></pre></td></tr></table></figure>
<p><strong><font color="red">
样本权重对决策分割面的影响:</font></strong></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-699ad7ac670c27a5b0963b8b104ad7fc_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以下是scikit-learn包中的逻辑回归参数列表说明，可以发现调节样本权重的方法有两种：</p>
<ul>
<li>在class_weight参数中使用balanced</li>
<li>在调用fit函数时，通过sample_weight调节每个样本权重。</li>
</ul>
<p>如果同时设置上述2个参数，<strong>那么样本的真正权重是class_weight *
sample_weight.</strong></p>
<p><strong>那么，在评估模型的指标时，是否需要考虑抽样权重，即还原真实场景下的模型评价指标？</strong>笔者认为，最终评估还是需要还原到真实场景下。例如，训练集正负比例被调节为1:1，但这并不是真实的<span class="math inline">\(odds\)</span>，在预测时将会偏高。因此，仍需要进行模型校准。</p>
<h4><span id="part-5-总结"><strong>Part 5. 总结</strong></span></h4>
<p>本文系统整理了样本权重的一些观点，但目前仍然没有统一的答案。据笔者所知，目前在实践中还是采取以下几种方案：</p>
<ol type="1">
<li>尊重原样本分布，不予处理，LR模型训练后即为真实概率估计。</li>
<li>结合权重指标综合确定权重，训练完毕模型后再进行校准，还原至真实概率估计。</li>
</ol>
<p>值得指出的是，大环境总是在发生变化，造成样本分布总在偏移。因此，尽可能增强模型的鲁棒性，以及策略使用时根据实际情况灵活调整，两者相辅相成，可能是最佳的使用方法。欢迎大家一起讨论业界的一些做法。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li>机器学习中不平衡数据的预处理：https://capallen.gitee.io/2019/Deal-with-imbalanced-data-in-ML.html</li>
<li>如何处理数据中的「类别不平衡」？：https://zhuanlan.zhihu.com/p/32940093</li>
<li><strong><font color="blue">不平衡数据集处理方法</font></strong>：https://blog.csdn.net/asialee_bird/article/details/83714612</li>
<li><strong><font color="blue">不平衡数据究竟出了什么问题？</font></strong>：https://www.zhihu.com/column/jiqizhixin</li>
<li>数据挖掘时，当正负样本不均，代码如何实现改变正负样本权重? -
十三的回答 - 知乎
https://www.zhihu.com/question/356640889/answer/2299286791</li>
<li><strong>样本权重对逻辑回归评分卡的影响探讨</strong> -
求是汪在路上的文章 - 知乎 https://zhuanlan.zhihu.com/p/110982479</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3FG16R6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3FG16R6/" class="post-title-link" itemprop="url">模型部署（4）DaaS-Client</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:53:47" itemprop="dateCreated datePublished" datetime="2022-03-28T10:53:47+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 15:57:46" itemprop="dateModified" datetime="2023-04-26T15:57:46+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" itemprop="url" rel="index"><span itemprop="name">模型部署</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="how自动部署开源ai模型到生产环境">HOW
自动部署开源AI模型到生产环境？</span></h2>
<h3><span id="一-背景介绍">一、背景介绍</span></h3>
<p>AI的广泛应用是由AI在开源技术的进步推动的，利用功能强大的开源模型库，数据科学家们可以很容易的训练一个性能不错的模型。但是因为模型生产环境和开发环境的不同，涉及到不同角色人员：模型训练是数据科学家和数据分析师的工作，但是模型部署是开发和运维工程师的事情，导致模型上线部署却不是那么容易。</p>
<p><strong>DaaS（Deployment-as-a-Service）是AutoDeployAI公司推出的基于Kubernetes的AI模型自动部署系统，提供一键式自动部署开源AI模型生成REST
API，以方便在生产环境中调用</strong>。下面，我们主要演示在DaaS中如何部署经典机器学习模型，包括<strong>Scikit-learn、XGBoost、LightGBM、和PySpark
ML Pipelines</strong>。关于深度学习模型的部署，会在下一章中介绍。</p>
<p><strong>DMatrix 格式</strong>
在xgboost当中运行速度更快，性能更好。</p>
<h3><span id="二-部署准备">二、部署准备</span></h3>
<p>我们使用DaaS提供的Python客户端（DaaS-Client）来部署模型，对于XGBoost和LightGBM，我们同样使用它们的Python
API来作模型训练。在训练和部署模型之前，我们需要完成以下操作。</p>
<ul>
<li><strong>安装Python DaaS-Client</strong></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade git+https://github.com/autodeployai/daas-client.git</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>初始化DaasClient</strong>。使用DaaS系统的URL、账户、密码登陆系统，文本使用的DaaS演示系统安装在本地的Minikube上。完整Jupyter
Notebook，请参考：<a target="_blank" rel="noopener" href="https://github.com/aipredict/ai-deployment/blob/master/deploy-ai-models-in-daas/deploy-sklearn-xgboost-lightgbm-pyspark.ipynb">deploy-sklearn-xgboost-lightgbm-pyspark.ipynb</a></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from daas_client import DaasClient</span><br><span class="line"></span><br><span class="line">client = DaasClient(&#x27;https://192.168.64.3:30931&#x27;, &#x27;username&#x27;, &#x27;password&#x27;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>创建项目</strong>。DaaS使用项目管理用户不同的分析任务，一个项目中可以包含用户的各种分析资产：模型、部署、程序脚本、数据、数据源等。项目创建成功后，设置为当前活动项目，发布的模型和创建的部署都会存储在该项目下。<code>create_project</code>函数接受三个参数：
<ul>
<li><strong>项目名称</strong>：可以是任意有效的Linux文件目录名。</li>
<li><strong>项目路由</strong>：使用在部署的REST
URL中来唯一表示当前项目，只能是小写英文字符(a-z)，数字(0-9)和中横线<code>-</code>，并且<code>-</code>不能在开头和结尾处。</li>
<li><strong>项目说明</strong>（可选）：可以是任意字符。</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">project = <span class="string">&#x27;部署测试&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> client.project_exists(project):</span><br><span class="line">    client.create_project(project, <span class="string">&#x27;deployment-test&#x27;</span>, <span class="string">&#x27;部署测试项目&#x27;</span>)</span><br><span class="line">client.set_project(project)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>初始化数据</strong>。我们使用流行的分类数据集<code>iris</code>来训练不同的模型，并且把数据分割为训练数据集和测试数据集以方便后续使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">seed = <span class="number">123456</span></span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris_target_name = <span class="string">&#x27;Species&#x27;</span></span><br><span class="line">iris_feature_names = iris.feature_names</span><br><span class="line">iris_df = pd.DataFrame(iris.data, columns=iris_feature_names)</span><br><span class="line">iris_df[iris_target_name] = iris.target</span><br><span class="line"></span><br><span class="line">X, y = iris_df[iris_feature_names], iris_df[iris_target_name]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=seed)    </span><br></pre></td></tr></table></figure>
<ul>
<li><strong>模型部署流程。主要包含以下几步</strong>：
<ul>
<li><strong>训练模型</strong>。使用模型库提供的API，在<code>iris</code>数据集上训练模型。</li>
<li><strong>发布模型</strong>。调用<code>publish</code>函数发布模型到DaaS系统。</li>
<li><strong>测试模型</strong>（可选）。调用<code>test</code>函数获取测试API信息，可以使用任意的REST客户端程序测试模型在DaaS中是否工作正常，使用的是DaaS系统模型测试API。第一次执行<code>test</code>会比较慢，因为DaaS系统需要启动测试运行时环境。</li>
<li><strong>部署模型</strong>。发布成功后，调用<code>deploy</code>函数部署部署模型。可以使用任意的REST客户端程序测试模型部署，使用的是DaaS系统正式部署API。</li>
</ul></li>
</ul>
<h3><span id="三-部署scikit-learn模型">三、部署Scikit-learn模型</span></h3>
<ul>
<li><strong>训练一个Scikit-learn分类模型</strong>：SVC</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">model = SVC(probability=<span class="literal">True</span>, random_state=seed)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>发布Scikit-learn模型</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">publish_resp = client.publish(model,</span><br><span class="line">                            name=<span class="string">&#x27;iris&#x27;</span>,</span><br><span class="line">                            mining_function=<span class="string">&#x27;classification&#x27;</span>,</span><br><span class="line">                            X_test=X_test,</span><br><span class="line">                            y_test=y_test,</span><br><span class="line">                            description=<span class="string">&#x27;A SVC model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>test</strong>函数必须要指定前两个参数，第一个<strong>model</strong>是训练的模型对象，第二个是模型名称，其余是可选参数：</p>
<ul>
<li><strong>mining_function</strong>：指定挖掘功能，可以指定为<code>regression</code>（回归）、<code>classification</code>（分类）、和<code>clustering</code>（聚类）。</li>
<li><strong>X_test和y_test</strong>：指定测试训练集，发布时计算模型评估指标，比如针对分类模型，计算正确率（Accuracy），对于回归模型，计算可释方差（explained
Variance）。</li>
<li><strong>data_test</strong>：
同样是指定测试训练集，但是该参数用在Spark模型上，非Spark模型通过<code>X_test</code>和<code>y_test</code>指定。</li>
<li><strong>description</strong>：模型描述。</li>
<li><strong>params</strong>：记录模型参数设置。</li>
</ul>
<p><strong>publish_resp</strong>是一个字典类型的结果，记录了模型名称，和发布的模型版本。该模型是<code>iris</code>模型的第一个版本。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>&#x27;model_name&#x27;<span class="punctuation">:</span> &#x27;iris&#x27;<span class="punctuation">,</span> &#x27;model_version&#x27;<span class="punctuation">:</span> &#x27;<span class="number">1</span>&#x27;<span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>测试Scikit-learn模型</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_resp = client.test(publish_resp[<span class="string">&#x27;model_name&#x27;</span>],</span><br><span class="line">                        model_version=publish_resp[<span class="string">&#x27;model_version&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><code>test_resp</code>是一个字典类型的结果，记录了测试REST
API信息。如下，其中<code>access_token</code>是访问令牌，一个长字符串，这里没有显示出来。<code>endpoint_url</code>指定测试REST
API地址，<code>payload</code>提供了测试当前模型需要输入的请求正文格式。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;access_token&#x27;<span class="punctuation">:</span> &#x27;A-LONG-STRING-OF-BEARER-TOKEN-USED-IN-HTTP-HEADER-AUTHORIZATION&#x27;<span class="punctuation">,</span></span><br><span class="line">			&#x27;endpoint_url&#x27;<span class="punctuation">:</span> &#x27;https<span class="punctuation">:</span><span class="comment">//192.168.64.3:30931/api/v1/test/deployment-test/daas-python37-faas/test&#x27;,</span></span><br><span class="line">			&#x27;payload&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      				&#x27;args&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">              			&#x27;X&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span> &#x27;petal length (cm)&#x27;<span class="punctuation">:</span> <span class="number">1.5</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;petal width (cm)&#x27;<span class="punctuation">:</span> <span class="number">0.4</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;sepal length (cm)&#x27;<span class="punctuation">:</span> <span class="number">5.7</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;sepal width (cm)&#x27;<span class="punctuation">:</span> <span class="number">4.4</span></span><br><span class="line">                          <span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">               &#x27;model_name&#x27;<span class="punctuation">:</span> &#x27;iris&#x27;<span class="punctuation">,</span></span><br><span class="line">               &#x27;model_version&#x27;<span class="punctuation">:</span> &#x27;<span class="number">1</span>&#x27;<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>使用requests调用测试API，这里我们直接使用<strong>test_resp</strong>返回的测试payload，您也可以使用自定义的数据<code>X</code>，但是参数<code>model_name</code>和<code>model_version</code>必须使用上面输出的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">response = requests.post(test_resp[<span class="string">&#x27;endpoint_url&#x27;</span>],</span><br><span class="line">                        headers=&#123;</span><br><span class="line">      <span class="string">&#x27;Authorization&#x27;</span>: <span class="string">&#x27;Bearer &#123;token&#125;&#x27;</span>.<span class="built_in">format</span>(token=test_resp[<span class="string">&#x27;access_token&#x27;</span>])&#125;,</span><br><span class="line">                        json=test_resp[<span class="string">&#x27;payload&#x27;</span>],</span><br><span class="line">                        verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>返回结果，不同于正式部署API，除了预测结果，测试API会同时返回标准控制台输出和标准错误输出内容，以方便用户碰到错误时，查看相关信息。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># response.json()</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;result&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;PredictedValue&#x27;<span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">            &#x27;Probabilities&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">0.8977133931668801</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="number">0.05476023239878367</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="number">0.047526374434336216</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">			&#x27;stderr&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">			&#x27;stdout&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3><span id="四-部署pyspark模型">四、部署PySpark模型</span></h3>
<p><strong>训练一个PySpark分类模型</strong>：RandomForestClassifier。PySpark模型必须是一个<code>PipelineModel</code>，也就是说必须使用Pipeline来建立模型，哪怕只有一个Pipeline节点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.getOrCreate()</span><br><span class="line">df = spark.createDataFrame(iris_df)</span><br><span class="line"></span><br><span class="line">df_train, df_test = df.randomSplit([<span class="number">0.7</span>, <span class="number">0.3</span>], seed=seed)</span><br><span class="line">assembler = VectorAssembler(inputCols=iris_feature_names,</span><br><span class="line">                            outputCol=<span class="string">&#x27;features&#x27;</span>)</span><br><span class="line"></span><br><span class="line">rf = RandomForestClassifier(seed=seed).setLabelCol(iris_target_name)</span><br><span class="line">pipe = Pipeline(stages=[assembler, rf])</span><br><span class="line">model = pipe.fit(df_train)</span><br></pre></td></tr></table></figure>
<p><strong>发布PySpark模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">publish_resp = client.publish(model,</span><br><span class="line">                            name=<span class="string">&#x27;iris&#x27;</span>,</span><br><span class="line">                            mining_function=<span class="string">&#x27;classification&#x27;</span>,</span><br><span class="line">                            data_test=df_test,</span><br><span class="line">                            description=<span class="string">&#x27;A RandomForestClassifier of Spark model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="五-模型部署管理">五、模型部署管理</span></h3>
<p>打开浏览器，登陆DaaS管理系统。进入项目<code>部署测试</code>，切换到<code>模型</code>标签页，有一个<code>iris</code>模型，最新版本是<code>v4</code>，类型是<code>Spark</code>即我们最后发布的模型。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261549637.jpeg" alt="Daas-models" style="zoom: 33%;"></p>
<p>点击模型，进入模型主页（概述）。当前<code>v4</code>是一个Spark
Pipeline模型，正确率是94.23%，并且显示了<code>iris</code>不同版本正确率历史图。下面罗列了模型的输入和输出变量，以及评估结果，当前为空，因为还没有在DaaS中执行任何的模型评估任务。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261549145.jpeg" alt="Daas-model-overview-v4" style="zoom: 50%;"></p>
<p>点击<code>v4</code>，可以自由切换到其他版本。比如，切换到<code>v1</code>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261549781.png" alt="DaaS-model-versions" style="zoom:50%;"></p>
<p><code>v1</code>版本是一个Scikit-learn
SVM分类模型，正确率是98.00%。其他信息与<code>v4</code>类似。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261549645.jpeg" alt="DaaS-model-overview-v1" style="zoom:50%;"></p>
<p>切换到模型<code>部署</code>标签页，有一个我们刚才创建的部署<code>iris-svc</code>，鼠标移动到操作菜单，选择<code>修改设置</code>。可以看到，当前部署服务关联的是模型<code>v1</code>，就是我们刚才通过<code>deploy</code>函数部署的<code>iris</code>第一个版本Scikit-learn模型。选择最新的<code>v4</code>，点击命令<code>保存并且重新部署</code>，该部署就会切换到<code>v4</code>版本。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li>DaaS-Client、Sklearn、XGBoost、LightGBM、和PySpark_关注 AI/ML
模型上线、模型部署-程序员宅基地_</li>
<li>DaaS-Client：https://github.com/autodeployai/daas-client</li>
<li>3万字长文
PySpark入门级学习教程，框架思维:https://zhuanlan.zhihu.com/p/395431025</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/8AJ5QK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/8AJ5QK/" class="post-title-link" itemprop="url">深度学习Q&A</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:13:46" itemprop="dateCreated datePublished" datetime="2022-03-24T14:13:46+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-06 20:39:50" itemprop="dateModified" datetime="2022-07-06T20:39:50+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="残差神经网络为什么可以缓解梯度消失">残差神经网络为什么可以缓解梯度消失？</span></h2>
<blockquote>

</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/WH7C02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/WH7C02/" class="post-title-link" itemprop="url">异常检测（1）概述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:12:56" itemprop="dateCreated datePublished" datetime="2022-03-24T14:12:56+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 15:23:16" itemprop="dateModified" datetime="2023-04-26T15:23:16+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">异常检测</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="异常检测-anomaly-detection">异常检测 (anomaly detection)</span></h3>
<p><strong>异常检测工具</strong></p>
<ul>
<li><p>PyOD:超过30种算法，从经典模型到深度学习模型一应俱全，和sklearn的用法一致</p></li>
<li><p>Scikit-Learn:包含了4种常见的算法，简单易用</p></li>
<li><p>TODS:与PyOD类似，包含多种时间序列上的异常检测算法</p></li>
</ul>
<p><strong>异常检测算法</strong></p>
<ul>
<li>线性模型：PCA</li>
<li>基于相似度度量的算法：KNN、LOF、HBOS</li>
<li>基于概率的算法：COPOD</li>
<li>集成检测：孤立森林，XGBOD</li>
<li>神经网络算法：自编码器</li>
</ul>
<p><strong>评估方法</strong></p>
<ul>
<li>ROC-AUC 曲线</li>
<li>Precision Topk：top K的准确率</li>
<li>AVE Precision：平均准确率</li>
</ul>
<h3><span id="一-概述">一、概述</span></h3>
<h4><span id="11-什么是异常检测">1.1 什么是异常检测？</span></h4>
<p>不同于常规模式下的问题和任务，<strong>异常检测针对的是少数、不可预测或不确定、罕见的事件</strong>，它具有独特的复杂性，使得一般的机器学习和深度学习技术无效。</p>
<h4><span id="12-异常检测面临的挑战">1.2 <strong>异常检测面临的挑战</strong></span></h4>
<ul>
<li><strong>未知性</strong>：异常与许多未知因素有关，例如，具有未知的突发行为、数据结构和分布的实例。它们直到真正发生时才为人所知，比如恐怖袭击、诈骗和网络入侵等应用；</li>
<li><strong>异常类的异构性</strong>：
异常是不规则的，一类异常可能表现出与另一类异常完全不同的异常特征。例如，在视频监控中，抢劫、交通事故和盗窃等异常事件在视觉上有很大差异；</li>
<li><strong>类别不均衡</strong>：异常通常是罕见的数据实例，而正常实例通常占数据的绝大部分。<strong>因此，收集大量标了标签的异常实例是困难的，甚至是不可能的。这导致在大多数应用程序中无法获得大规模的标记数据。</strong></li>
</ul>
<h4><span id="13-异常的种类">1.3 <strong>异常的种类：</strong></span></h4>
<ul>
<li><strong>点异常</strong>（point
anomalies）指的是少数个体实例是异常的，大多数个体实例是正常的，例如正常人与病人的健康指标；</li>
<li><strong>条件异常</strong>（conditional
anomalies），又称上下文异常，指的是在特定情境下个体实例是异常的，在其他情境下都是正常的，例如在特定时间下的温度突然上升或下降，在特定场景中的快速信用卡交易；</li>
<li><strong>群体异常</strong>（group
anomalies）指的是在群体集合中的个体实例出现异常的情况，而该个体实例自身可能不是异常，例如社交网络中虚假账号形成的集合作为群体异常子集，但子集中的个体节点可能与真实账号一样正常。</li>
</ul>
<h4><span id="14-异常检测数据集">1.4 <strong>异常检测数据集：</strong></span></h4>
<ul>
<li>统计型数据static data（文本、网络流）</li>
<li><strong>序列型数据sequential data（sensor data ）</strong></li>
<li>空间型数据spatial data（图像、视频）</li>
</ul>
<h4><span id="15-异常检测的应用领域">1.5 <strong>异常检测的应用领域</strong></span></h4>
<ul>
<li><strong>入侵检测</strong>（Intrusion
detection）：通过从计算机网络或计算机系统中的若干关键点收集信息并对其执行分析，从中发觉网络或系统中能不能有违反安全策略的行为和遭到袭击的迹象，并对此做出适当反应的流程。最普遍的两种入侵检测系统包括<strong>基于主机的入侵检测系统（HIDS）</strong>、<strong>网络入侵检测系统（NIDS）</strong>。</li>
<li><strong>故障检测</strong>（Fraud
detection）：主要是监控系统，在故障发生时可以识别，并且准确指出故障的种类以及出现位置。主要应用领域包括银行欺诈、移动蜂窝网络故障、保险欺诈、医疗欺诈。</li>
<li><strong>恶意软件检测</strong>（Malware Detection）</li>
<li><strong>医疗异常检测</strong>（Medical Anomaly
Detection）：通过X光片、核磁共振、CT等医学图像检测疾病或量化异常，也可以通过EEG、ECG等时序信号进行疾病检测或异常预警。</li>
<li><strong>深度学习用于社交网络中的异常检测</strong>（Deep learning for
Anomaly detection in Social Networks）</li>
<li><strong>日志异常检测</strong>（Log Anomaly Detection）</li>
<li><strong>物联网大数据异常检测</strong>（Internet of things (IoT) Big
Data Anomaly
Detection）：通过监控数据流信息检测异常设备和系统行为。</li>
<li><strong>工业异常检测</strong>（Industrial Anomalies Detection）</li>
<li><strong>时间序列中的异常检测</strong>（Anomaly Detection in
TimeSeries）</li>
<li><strong>视频监控</strong>（Video
Surveillance）：检测视频中的异常场景。</li>
</ul>
<h4><span id="16基于标签的可获得性划分异常检测">1.6
<strong>基于标签的可获得性划分异常检测：</strong></span></h4>
<ul>
<li><strong>有监督异常检测</strong>：在训练集中的正常实例和异常实例都有标签，这类方法的缺点在于数据标签难以获得或数据不均衡（正常样本数量远大于异常样本数量）。</li>
<li><strong>半监督异常检测</strong>：<strong>在训练集中只有单一类别（正常实例）的实例，没有异常实例参与训练，目前很多异常检测研究都集中在半监督方法上</strong>，有很多声称是无监督异常检测方法的研究其实也是半监督的，对其解释的是该异常检测是无监督异常检测，学习特征的方式是无监督的，但是评价方式使用了半监督的方法，因此对于无监督与半监督的界定感觉没有那么规范。</li>
<li><strong>无监督异常检测</strong>：在训练集中既有正常实例也可能存在异常实例，但假设数据的比例是正常实例远大于异常实例，模型训练过程中没有标签进行校正。</li>
<li><strong>弱监督异常检测</strong>：该类我研究的少，不是特别了解，主要是针对异常实例不完全、粗粒度标签、部分实例标签错误等情况进行算法设计。</li>
</ul>
<h4><span id="17-基于传统方法的异常检测模型">1.7 基于传统方法的异常检测模型</span></h4>
<ul>
<li><strong>基于重构的方法</strong>：<strong><font color="red">
假设异常点是不可被压缩的或不能从低维映射空间有效地被重构的。</font></strong>常见的方法有<strong>PCA</strong>、<strong>Robust
PCA</strong>、random projection等降维方法 [4,5] 。</li>
<li><strong>聚类分析方法</strong>：通过聚类可以创建数据的模型，而异常点的存在可以扭曲、破坏该模型。常见的方法有Gaussian
Mixture Models、 k-means、 multivariate Gaussian Models [6,7,8]。</li>
<li><strong>一类分类方法</strong>：对正常数据建立区分性边界，异常点被划分到边界外。常见的方法有<strong>OC-SVM</strong>
[9,10]。</li>
</ul>
<h3><span id="二-常见异常检测算法">二、常见异常检测算法</span></h3>
<p><strong>一般情况下,
可以把异常检测看成是数据不平衡下的分类问题</strong>。因此,
如果数据条件允许, 优先使 用有监督的异常检测[6]。实验结果
[4]发现直接用XGBOOST进行有监督异常检测往往也能得到不错
的结果，没有思路时不妨一试。</p>
<p><strong>而在仅有少量标签的情况下,
也可采用半监督异常检测模型</strong>。比如把无监督学习作为一种特征抽取方式来辅助监督学习
<span class="math inline">\([4,8]\)</span>,
和stacking比较类似。这种方法也可以理解成通过无监督的特征工程
对数据进行预处理后, 喂给有监督的分类模型。</p>
<p>但在现实情况中, <strong>异常检测问题往往是没有标签的,
训练数据中并末标出哪些是异常点,
因此必须用无监督学习。</strong>从实用角度出发,
我们把文章的重点放在无监督学习上。</p>
<p>本文结构如下: 1. 介绍常见的无监督异常算法及实现; 2.
对比多种算法的检测能力；3. 对比多种算法的运算开销；4.
总结并归纳如何处理异常检测问题。5. 代码重现步骤。</p>
<h4><span id="21-无监督异常检测">2.1 无监督异常检测</span></h4>
<p>如果归类的话, 无监督异常检测模型可以大致分为:</p>
<ul>
<li><strong>统计与概率模型</strong> (statistical and probabilistic and
models) ：<strong>主要是对数据的分布做出假设,
并找出假设下所定义的“异常”,
因此往往会使用极值分析Q或者假设检验</strong>。比如对最简单的一维
数据假设高斯分布 <span class="math inline">\(Q\)</span>,
然后将距离均值特定范围以外的数据当做异常点。而推广到高维后, 可以
假设<strong>每个维度各自独立</strong>,
<strong>并将各个维度上的异常度相加</strong>。如果考虑特征间的相关性,
也可以用马 氏距离 (mahalanobis distance)
来衡量数据的异常度[12]。不难看出, 这类方法最大的好处就 是速度一般比较快,
但因为存在比较强的"假设", 效果不一定很好。<strong>稍微引申一点的话,
其实给每个维度做个直方图做密度估计, 再加起来就是HBOS。</strong></li>
<li><strong>线性模型（linear
models）</strong>：假设数据在低维空间上有嵌入,
那么无法、或者在低维空间投射后 表现不好的数据可以认为是离群点 <span class="math inline">\(Q\)</span> 。
<ul>
<li><strong>举个简单的例子, PCA可以用于做异常检测</strong> [10],
一种方法 就是找到 <span class="math inline">\(k\)</span> 个特征向量
(eigenvectora), 并计算每个样本再经过这 <span class="math inline">\(k\)</span> 个特征向量投射后的<strong>重建误差
(reconstruction error)</strong>, 而正常点的重建误差应该小于异常点。</li>
<li>同理, 也可以计算每个样本 到这 <span class="math inline">\(k\)</span>
个选特征向量所构成的超空间的加权欧氏距离（特征值越小权重越大）。在相似的思路下,
我们也可以直接对协方差矩阵 <span class="math inline">\(Q\)</span>
进行分析, 并把样本的马氏距离 (在考虑特征间关系时样本到分 布中心的距离)
作为样本的异常度, 而这种方法也可以被理解为一种软性 (Soft PCA) [6]。</li>
<li>同时, 另一种经典算法One-class SVM[3]也一般被归类为线性模型。</li>
</ul></li>
<li><strong>基于相似度衡量</strong>的模型 (proximity based models) :
<strong>异常点因为和正常点的分布不同, 因此相似度较低,
由此衍生了一系列算法通过相似度来识别异常点</strong>。
<ul>
<li>比如最简单的<strong>K近邻</strong>就可以做异常
检测,一个样本和它第k个近邻的距离就可以被当做是异常值,
显然异常点的k近邻距离更大。</li>
<li>同理, <strong>基于密度分析如LOF</strong>
[1]、<strong>LOCI</strong>和<strong>LoOP主要是通过局部的数据密度来检测异常</strong>。显然,
异常点所在空间的数据点少, 密度低。</li>
<li>相似的是, <strong><font color="red"> Isolation
Forest[2]通过划分超平面Q来计算"孤立"
一个样本所需的超平面数量</font></strong>（可以想象成在想吃蛋糕上的樱桃所需的最少刀数）。在密度低的空间里
(异常点所在空间中), 孤例一个样本所需要的划分次数更少。</li>
<li>另一种相似的算法<strong>ABOD</strong>[7]
<strong>是计算每个样本与所有其他样本对所形成的夹角的方差</strong>,
异常点因为远离正常点, 因此方差变化 小。换句话说,
大部分异常检测算法都可以被认为是一种估计相似度, 无论是通过密度、距离、
夹角或是划分超平面。通过聚类也可以被理解为一种相似度度量,
比较常见不再赘述。</li>
</ul></li>
<li><strong>集成异常检测与模型融合</strong>: 在无监督学习时,
提高模型的鲁棒性很重要, 因此集成学习就大有用
武之地。比如上面提到的lsolation Forest,
就是基于构建多棵决策树实现的。最早的集成检测框 架feature
bagging[9]与分类问题中的随机森林 (random forest) 很像,
先将训练数据随机划分 (每次选取所有样本的 <span class="math inline">\(d /
2-d\)</span> 个特征, <span class="math inline">\(d\)</span> 代表特征数)
, 得到多个子训练集, 再在每个训练集上训
练一个独立的模型（默认为LOF）并最终合并所有的模型结果（如通过平均）。值得注意的是,
因为没有标签, 异常检测往往是通过bagging和feature bagging比较多,
而boosting比较少见。 boosting情况下的异常检测, 一般需要生成伪标签Q,
可参靠 <span class="math inline">\([13,14]\)</span>
。集成异常检测是一个新兴但很有趣的领域, 综述文章可以参考 <span class="math inline">\([16,17,18]\)</span> 。</li>
<li><strong>特定领域上的异常检测</strong>：比如图像异常检测 [21],
顺序及<strong>流数据异常检测（时间序列异常检测）</strong> [22],
以及高维空间上的异常检测 [23], 比如前文提到的Isolation
Forest就很适合高维数据上的 异常检测。</li>
</ul>
<p><strong>不难看出,
上文提到的划分标准其实是互相交织的</strong>。比如k-近邻可以看做是概率模型非参数化后的
一种变形,
而通过马氏距离计算异常度虽然是线性模型但也对分布有假设（高斯分布）。Isolation
Forest虽然是集成学习, 但其实和分析数据的密度有关,
并且适合高维数据上的异常检测。在这种 基础上, 多种算法其实是你中有我,
我中有你, <strong><font color="red"> 相似的理念都可以被推广和应用,
比如计算重建误 差不仅可以用PCA,
也可以用神经网络中的auto-encoder。</font></strong>另一种划分异常检测模型的标准可以理
解为局部算法 (local) 和全局算法 (global),
这种划分方法是考虑到异常点的特性。想要了解更多异常检测还是推荐看经典教科书Outlier
Analysis [6], 或者综述文章[15]。</p>
<p><strong>虽然一直有新的算法被提出, 但因为需要采用无监督学习,
且我们对数据分布的有限了解, 模型选 择往往还是采用试错法,</strong>
因此快速迭代地尝试大量的算法就是一个必经之路。在这个回答下, 我们
会对比多种算法的预测能力、运算开销及模型特点。如无特别说明，本文中的图片、代码均来自于
开源Python异常检测工具库Pyod。文中实验所使用的17个数据集均来自于 (ODDS -
Outlier Detection DataSets) 。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li><p>「异常检测」开源工具库推荐 - 微调的文章 - 知乎
https://zhuanlan.zhihu.com/p/37132428</p></li>
<li><p><strong>数据挖掘中常见的「异常检测」算法有哪些？</strong> -
微调的回答 - 知乎
https://www.zhihu.com/question/280696035/answer/417091151</p></li>
<li><p>不得不推荐这门课：<a href="http://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs259d/">CS259D:
Data Mining for Cyber Security</a>
虽然是网络安全方面的应用，但是方法都是通用的，看来也很有启发。https://leotsui.gitbooks.io/cs259d-notes-cn/content/</p></li>
<li><p>中科院在读美女博士带你全面了解“异常检测”领域 - 王晋东不在家的文章
- 知乎 https://zhuanlan.zhihu.com/p/260651151</p></li>
<li><p>Python 时间序列异常检测
ADTK：https://blog.csdn.net/BF02jgtRS00XKtCx/article/details/115343456</p></li>
<li><p>awesome-TS-anomaly-detection：https://github.com/rob-med/awesome-TS-anomaly-detection</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3FW9EBT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3FW9EBT/" class="post-title-link" itemprop="url">异常检测（3）HBOS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:12:56" itemprop="dateCreated datePublished" datetime="2022-03-24T14:12:56+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 15:24:27" itemprop="dateModified" datetime="2023-04-26T15:24:27+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">异常检测</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-hbos">一、HBOS</span></h3>
<p><strong>HBOS全名为：Histogram-based Outlier
Score</strong>。它是一种单变量方法的组合，不能对特征之间的依赖关系进行建模，但是计算速度较快，对大数据集友好，其基本假设是数据集的每个维度相互独立，然后对<strong>每个维度进行区间(bin)划分，区间的密度越高，异常评分越低。理解了这句话，基本就理解了这个算法。</strong></p>
<h4><span id="11-hbos算法流程"><strong>1.1 HBOS算法流程</strong></span></h4>
<h5><span id="1-静态宽度直方图"><strong>1、静态宽度直方图</strong></span></h5>
<p>标准的直方图构建方法，在值范围内使用k个等宽箱，样本落入每个箱的频率（相对数量）作为密度（箱子高度）的估计，时间复杂度：O(n)</p>
<p><strong>注意：</strong>等宽分箱，每个箱中的数据宽度相同，不是指数据个数相同。例如序列[5,10,11,13,15,35,50,55,72,92,204,215]，数据集中最大值是215，最小值是5，分成3个箱，故每个箱的宽度应该为（215-5）/3=70，所以箱的宽度是70，这就要求箱中数据之差不能超过70，并且要把不超过70的数据全放在一起，最后的分箱结果如下：</p>
<p><strong>箱一：5,10，11,13,15,35,50,55,72；箱二：92；箱三：204,215</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026218.png" alt="图片" style="zoom:48%;"></p>
<h5><span id="2-动态宽度直方图"><strong>2、动态宽度直方图</strong></span></h5>
<p>首先对所有值进行排序，然后固定数量的N/k 个连续值装进一个箱里，其
中N是总实例数，k是箱个数，<strong>直方图中的箱面积表示实例数</strong>，因为箱的宽度是由箱中第一个值和最后一个值决定的，所有箱的面积都一样，因此每一个箱的高度都是可计算的。这意味着跨度大的箱的高度低，即密度小，只有一种情况例外，超过k个数相等，此时允许在同一个箱里超过N/k值，时间复杂度：O(n×log(n))</p>
<p>还是用序列[<strong>5,10,11,13,15</strong>,<strong>35,50,55,72</strong>,92,204,215]举例，也是假如分3箱，那么每箱都是4个，宽度为边缘之差，第一个差为15-5=10，第二差为72-35=37，第三个箱宽为215-92=123，为了保持面积相等，所以导致后面的很矮，前面的比较高，如下图所示（非严格按照规则）：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026012.png" alt="图片" style="zoom:50%;"></p>
<h5><span id="3-算法推导过程"><strong>3、算法推导过程</strong></span></h5>
<p><strong>对每个维度都计算了一个独立的直方图，其中每个箱子的高度表示密度的估计，然后为了使得最大高度为1（确保了每个特征与异常值得分的权重相等），对直方图进行归一化处理。</strong>最后，每一个实例的HBOS值由以下公式计算：
<span class="math display">\[
H B O S(p)=\sum_{i=0}^d \log
\left(\frac{1}{\operatorname{hist}_i(p)}\right)
\]</span> 推导过程: 假设样本 <span class="math inline">\(\mathrm{p}\)</span> 第 <span class="math inline">\(\mathrm{i}\)</span> 个特征的概率密度为 pi ( <span class="math inline">\(p\)</span> ) , 则p的概率密度可以计算为, <span class="math inline">\(d\)</span> 为总的特征的个数： <span class="math display">\[
P(p)=P_1(p) P_2(p) \cdots P_d(p)
\]</span> 两边取对数： <span class="math display">\[
\log (P(p))=\log \left(P_1(p) P_2(p) \cdots P_d(p)\right)=\sum_{i=1}^d
\log \left(P_i(p)\right)
\]</span> 概率密度越大，异常评分越小，为了方便评分，两边乘以“-1”: <span class="math display">\[
-\log (P(p))=-1 \sum_{i=1}^d \log \left(P_t(p)\right)=\sum_{i=1}^d
\frac{1}{\log \left(P_i(p)\right)}
\]</span> 最后可得： <span class="math display">\[
H B O S(p)=-\log (P(p))=\sum_{i=1}^d \frac{1}{\log \left(P_i(p)\right)}
\]</span>
PyOD是一个可扩展的Python工具包，用于检测多变量数据中的异常值。它可以在一个详细记录API下访问大约
20 个离群值检测算法。</p>
<h3><span id="三-xgbod">三、 XGBOD</span></h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349519844">【异常检测】<em>XGBOD</em>：用无监督表示学习改进有监督异常检测</a></p>
</blockquote>
<h3><span id="四-copod用统计机器学习检测异常">四、COPOD：用「统计」+「机器学习」检测异常</span></h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338189299">COPOD：用「统计」+「机器学习」检测异常</a></p>
</blockquote>
<h3><span id="五-more-about-anomalydetection">五、More about Anomaly
Detection</span></h3>
<p>那这个异常检测啊,其实也是另外一门学问,那我们课堂上就没有时间讲了,异常检测不是只能用
Aauto-Encoder 这个技术,Aauto-Encoder
这个技术,只是众多可能方法里面的其中一个,我们拿它来当做 Aauto-Encoder
的作业,因为我相信,你未来有很多的机会用得上异常检测这个技术,那实际上有关异常检测更完整的介绍,我们把过去上课的录影放在这边,给大家参考,</p>
<p>Part 1: https://youtu.be/gDp2LXGnVLQ</p>
<ul>
<li><strong>简介</strong></li>
</ul>
<p>Part 2: https://youtu.be/cYrNjLxkoXs</p>
<ul>
<li><strong>信心分数</strong></li>
</ul>
<p>Part 3: https://youtu.be/ueDlm2FkCnw</p>
<ul>
<li><p>异常检测系统的评估？</p>
<ul>
<li><p>no ACC</p></li>
<li><p>cost loss设计</p></li>
<li><p>RUC</p></li>
</ul></li>
</ul>
<p>Part 4: https://youtu.be/XwkHOUPbc0Q</p>
<p>Part 5: https://youtu.be/Fh1xFBktRLQ</p>
<ul>
<li>无监督</li>
</ul>
<p>Part 6: https://youtu.be/LmFWzmn2rFY</p>
<p>Part 7: https://youtu.be/6W8FqUGYyDo</p>
<p>那以上就是有关 Aauto-Encoder 的部分</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1QZMAVB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1QZMAVB/" class="post-title-link" itemprop="url">理论基础（2）损失函数</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:53" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:53+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-17 20:28:26" itemprop="dateModified" datetime="2023-04-17T20:28:26+08:00">2023-04-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="损失函数">损失函数</span></h2>
<p>机器学习中的监督学习本质上是给定一系列训练样本 <span class="math inline">\(\left(x_i, y_i\right)\)</span>, 尝试学习 <span class="math inline">\(x \rightarrow y\)</span> 的映射关系, 使得给定一个
<span class="math inline">\(x\)</span> , 即便这个 <span class="math inline">\(x\)</span> 不在训练样本中, 也能够得到尽量接近真实
<span class="math inline">\(y\)</span> 的输出 <span class="math inline">\(\hat{y}\)</span> 。而损失函数 (Loss Function)
则是这 个过程中关键的一个组成部分, 用来衡量模型的输出 <span class="math inline">\(\hat{y}\)</span> 与真实的 <span class="math inline">\(y\)</span> 之间的差距, 给模型的优化指明方向。</p>
<p>本文将介绍机器学习、深度学习中分类与回归常用的几种损失函数,
包括<strong>均方差损失 Mean Squared Loss、平 均绝对误差损失 Mean
Absolute Error Loss、Huber Loss、分位数损失 Quantile
Loss、交叉樀损失函数 Cross Entropy Loss、Hinge 损失 Hinge
Loss</strong>。主要介绍各种损失函数的基本形式、原理、特点等方面。</p>
<p><img src="https://s2.loli.net/2023/04/17/GA3Y4CvZ1PlLOup.png" alt="img" style="zoom: 67%;"></p>
<h3><span id="前言">前言</span></h3>
<p>在正文开始之前, 先说下关于 Loss Function、Cost Function 和 Objective
Function 的区别和联系。在机器学习 的语境下这三个术语经常被交叉使用。</p>
<ul>
<li><strong>损失函数</strong> Loss Function
通常是<strong>针对单个训练样本而言,</strong> 给定一个模型输出 <span class="math inline">\(\hat{y}\)</span> 和一个真实 <span class="math inline">\(y\)</span>, 损失函数输 出一个实值损失 <span class="math inline">\(L=f\left(y_i, \hat{y_i}\right)\)</span></li>
<li><strong>代价函数</strong> Cost Function
通常是<strong>针对整个训练集</strong>（或者在使用 mini-batch gradient
descent 时一个 minibatch）的总损失 <span class="math inline">\(J=\sum_{i=1}^N f\left(y_i,
\hat{y}_i\right)\)</span></li>
<li><strong>目标函数</strong> Objective Function 是一个更通用的术语,
表示任意希望被优化的函数, 用于机器学习领域和非机 器学习领域
(比如运筹优化)</li>
</ul>
<p>一句话总结三者的关系就是：<font color="red"> <strong>A loss function
is a part of a cost function which is a type of an objective
function.</strong></font></p>
<p>由于损失函数和代价函数只是在针对样本集上有区别，因此在本文中统一使用了损失函数这个术语，但下文的相关公式实际上采用的是代价函数
Cost Function 的形式，请读者自行留意。</p>
<h4><span id="结构风险函数">结构风险函数</span></h4>
<p>损失函数（loss function）是用来估量模型的预测值f(x)与真实值<span class="math inline">\(Y\)</span>不一致的程度，它是一个非负实数值函数，通常使用<span class="math inline">\(L(Y,f(x))\)</span>来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下的式子：</p>
<p><img src="https://s2.loli.net/2023/04/17/mEXD8ocy2UtFlbN.png" alt="image-20220821223950768" style="zoom:50%;"></p>
<p>前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty
term）,它可以是L1，也可以是L2等其他的正则函数。整个式子表示的意思是找到使目标函数最小时的θ值。下面列出集中常见的损失函数。</p>
<h3><span id="一-对数损失函数逻辑回归mle-交叉熵损失函数">一、
对数损失函数（逻辑回归）MLE 【交叉熵损失函数】</span></h3>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/52100927</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/tsyccnh/article/details/79163834">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35709485">损失函数｜交叉熵损失函数</a></p>
</blockquote>
<p>有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。<strong>平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到</strong>，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，<strong>它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数</strong>，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：</p>
<p><strong>最小化负的似然函数</strong>（即<span class="math inline">\(maxF(y,f(x))—&gt;min−F(y,f(x))\)</span>)。从损失函数的视角来看，它就成了<strong>log损失函数了。</strong></p>
<h4><span id="原理解释1条件概率下方便计算极大似然估计">原理解释1：<strong>==条件概率下方便计算极大似然估计==</strong></span></h4>
<p>Log损失函数的标准形式：</p>
<p><span class="math inline">\(L(Y,P(Y|X))=−logP(Y|X)\)</span></p>
<p>刚刚说到，<strong>取对数是为了方便计算极大似然估计</strong>，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数<span class="math inline">\(L(Y.P(Y|X))\)</span>表达的是样本在分类<span class="math inline">\(Y\)</span>的情况下，使概率<span class="math inline">\(P(Y|X)\)</span>达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以<span class="math inline">\(logP(Y|X)\)</span>也会达到最大值，因此在前面加上负号之后，最大化<span class="math inline">\(P(Y|X)\)</span>就等价于最小化<span class="math inline">\(L\)</span>了。</p>
<p><strong>logistic回归</strong>的<span class="math inline">\(P(y|x)\)</span>表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：</p>
<p><img src="https://s2.loli.net/2023/04/17/2aLAcdN97nRS8QV.png" alt="image-20220322202521355" style="zoom:50%;"></p>
<p>将上面的公式合并在一起，可得到第i个样本正确预测的概率：</p>
<p><img src="https://s2.loli.net/2023/04/17/ebZvW8hzNMuED3d.png" alt="image-20220322202548296" style="zoom:50%;"></p>
<p>上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布为：</p>
<p><img src="https://s2.loli.net/2023/04/17/2hZLDVKEaSQMPys.png" alt="image-20220322202618734" style="zoom:50%;"></p>
<p>将上式代入到对数损失函数中，得到最终的损失函数为：</p>
<p><img src="https://s2.loli.net/2023/04/17/ZLaI9mACg5pojqv.png" alt="image-20220322202653661" style="zoom:50%;"></p>
<h4><span id="原理解释2相对熵kl散度推理">原理解释2：相对熵（KL散度）推理</span></h4>
<blockquote>
<p>相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布
P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL)
divergence）来衡量这两个分布的差异.<span class="math inline">\(DKL\)</span>的值越小，表示q分布和p分布越接近.</p>
</blockquote>
<h5><span id="相对熵">相对熵:</span></h5>
<p><img src="https://s2.loli.net/2023/04/17/y8ulY5HqIKrJRZW.png" alt="image-20220330133351613" style="zoom:50%;"></p>
<h5><span id="相对熵-信息熵-交叉熵">相对熵 = 信息熵 + 交叉熵 ：</span></h5>
<p><img src="https://s2.loli.net/2023/04/17/ZiDJTfWudUFmwx7.png" alt="image-20220330134202064" style="zoom:50%;"></p>
<p><strong>【对数损失函数（Log loss
function）】和【交叉熵损失函数（Cross-entroy loss
funtion）】在很多文献内是一致的，因为他们的表示式的本质是一样的。</strong></p>
<h3><span id="二-平方损失函数线性回归gbdt最小二乘法ordinary-leastsquaresmse">二、
平方损失函数（线性回归，GBDT，最小二乘法，Ordinary Least
Squares）MSE</span></h3>
<p>最小二乘法是线性回归的一种，OLS
将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central
limit
theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。换言之，OLS是基于<strong>距离</strong>的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean
squared error， MSE），主要有以下几个原因：</p>
<ul>
<li>简单，计算方便；</li>
<li>欧氏距离是一种很好的相似性度量标准；</li>
<li>在不同的表示域变换后特征性质不变。</li>
</ul>
<p>平方损失（Square loss）的标准形式如下：<span class="math inline">\(L(Y,f(X))=(Y−f(x))^2\)</span>当样本个数为n时，此时的损失函数变为：</p>
<p><img src="https://s2.loli.net/2023/04/17/NfeyG4SLkFhJBwt.png" alt="image-20220322202912962" style="zoom:50%;"></p>
<p><span class="math inline">\(Y−f(X)\)</span>
表示的是<strong>残差</strong>，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual
sum of squares，RSS）。</p>
<p>而在实际应用中，通常会使用<strong>均方差</strong>（MSE）作为一项衡量指标，公式如下：</p>
<p><img src="https://s2.loli.net/2023/04/17/9CsrcO3pPXwHoKZ.png" alt="image-20220322202957484" style="zoom:50%;"></p>
<h3><span id="三-指数损失函数adaboost">三、 指数损失函数（Adaboost）</span></h3>
<blockquote>
<p>Adaboost训练误差以指数下降。所以说，指数损失本身并没有带来优化上的特殊，优点在于计算和表达简单。</p>
</blockquote>
<p>学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到<span class="math inline">\(fm(x)\)</span>:</p>
<p><img src="https://s2.loli.net/2023/04/17/2FKCHj4EDLwxob3.png" alt="image-20220322203050695" style="zoom:50%;"></p>
<p><strong>Adaboost</strong>每次迭代时的目的是为了找到最小化下列式子时的参数
<span class="math inline">\(a\)</span> 和 <span class="math inline">\(G\)</span>：</p>
<p><img src="https://s2.loli.net/2023/04/17/XIEqZ21U8ikoOt4.png" alt="image-20220322203141435" style="zoom:50%;"></p>
<p>而指数损失函数(exp-loss）的标准形式如下:</p>
<p><img src="https://s2.loli.net/2023/04/17/wDyvbnklec61FYp.png" alt="image-20220322203221432" style="zoom:50%;"></p>
<p>可以看出，Adaboost的目标式子就是指数损失，在给定N个样本的情况下，Adaboost的损失函数为：</p>
<p><img src="https://s2.loli.net/2023/04/17/6t2QfviewSNLbzV.png" alt="image-20220322203238853" style="zoom:50%;"></p>
<h3><span id="四-hinge合页损失函数svmadvgan">四、 ==Hinge
合页损失函数==（SVM，advGAN）</span></h3>
<p><img src="https://s2.loli.net/2023/04/17/EgYO37JubfLwke8.png" alt="image-20220401165315551" style="zoom:50%;"></p>
<p>线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数：</p>
<p><img src="https://s2.loli.net/2023/04/17/kMKy9zPi5hmITxL.png" alt="image-20220322205741804" style="zoom:50%;"></p>
<p>目标函数的第一项是经验损失或经验风险函数：</p>
<p><img src="https://s2.loli.net/2023/04/17/uJzUqdVDQMf7kCb.png" alt="image-20220322205801232" style="zoom:50%;"></p>
<p>称为<strong>合页损失函数</strong>（hinge loss
function）。下标”+”表示以下取正值的函数：</p>
<p><img src="https://s2.loli.net/2023/04/17/w6VqUnRyaLfETKh.png" alt="image-20220322205844003" style="zoom:50%;"></p>
<p>这就是说，当样本点<span class="math inline">\((xi,yi)\)</span>被正确分类且函数间隔（确信度）<span class="math inline">\(yi(w·xi+b)\)</span>大于1时，损失是0，否则损失是<span class="math inline">\(1−yi(w·xi+b)\)</span>。目标函数的第二项是系数为
<span class="math inline">\(λ\)</span> 的 <span class="math inline">\(w\)</span> 的 <span class="math inline">\(L2\)</span> 范数，是正则化项。</p>
<p>接下来证明线性支持向量机原始最优化问题：</p>
<p><img src="https://s2.loli.net/2023/04/17/jDeMzx3QAoZaPw2.png" alt="image-20220322210000477" style="zoom:50%;"></p>
<p><img src="https://s2.loli.net/2023/04/17/qKU6kzHTI21BmLV.png" alt="image-20220322210121285" style="zoom:50%;"></p>
<p>先令<span class="math inline">\([1−yi(w·xi+b)]+=ξi\)</span>，则<span class="math inline">\(ξi≥0\)</span>，第二个约束条件成立；由<span class="math inline">\([1−yi(w·xi+b)]+=ξi\)</span>，当<span class="math inline">\(1−yi(w·xi+b)&gt;0\)</span>时，有<span class="math inline">\(yi(w·xi+b)=1−ξi\)</span>;当<span class="math inline">\(1−yi(w·xi+b)≤0\)</span>时，<span class="math inline">\(ξi=0\)</span>，有<span class="math inline">\(yi(w·xi+b)≥1−ξi\)</span>，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作</p>
<p><img src="https://s2.loli.net/2023/04/17/gi1eSxGrA7MYTQh.png" alt="image-20220322210943775" style="zoom:50%;"></p>
<p>若取 <span class="math inline">\(λ=1/2C\)</span> 则:</p>
<p><img src="https://s2.loli.net/2023/04/17/8Q6BU1lkfF4S3Dd.png" alt="image-20220322211012150" style="zoom:50%;"></p>
<h3><span id="五-softmax函数和sigmoid函数的区别与联系">五、Softmax函数和Sigmoid函数的区别与联系</span></h3>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/356976844</p>
</blockquote>
<h4><span id="51-分类任务">5.1 分类任务</span></h4>
<h4><span id="sigmoid">sigmoid</span></h4>
<blockquote>
<p>Sigmoid
=<strong>==多标签分类问题==</strong>=多个正确答案=非独占输出（例如胸部X光检查、住院）。构建分类器，解决有多个正确答案的问题时，用Sigmoid函数分别处理各个原始输出值。</p>
</blockquote>
<blockquote>
<p>Softmax
=<strong>多类别分类问题</strong>=只有一个正确答案=互斥输出（例如手写数字，鸢尾花）。构建分类器，解决只有唯一正确答案的问题时，用Softmax函数处理各个原始输出值。Softmax函数的分母综合了原始输出值的所有因素，这意味着，Softmax函数得到的不同概率之间相互关联。</p>
</blockquote>
<p><strong>Sigmoid函数</strong>是一种logistic函数，它将任意的值转换到
<img src="https://www.zhihu.com/equation?tex=%5B0%2C+1%5D" alt="[公式]"> 之间，如图1所示，函数表达式为： <img src="https://www.zhihu.com/equation?tex=Sigmoid%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D" alt="[公式]"> 。</p>
<p>它的导函数为： <img src="https://www.zhihu.com/equation?tex=Sigmoid%5E%7B%27%7D%28x%29%3DSigmoid%28x%29%5Ccdot+%281-Sigmoid%28x%29%29" alt="[公式]"> 。</p>
<figure>
<img src="https://s2.loli.net/2023/04/17/i1SyjnQTHW7pYRA.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>优点</strong>：</p>
<ol type="1">
<li>Sigmoid函数的输出在(0,1)之间，输出范围有限，优化稳定，可以用作<strong>输出层</strong>。</li>
<li>连续函数，便于<strong>求导</strong>。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol type="1">
<li>最明显的就是<strong>饱和性</strong>，从上图也不难看出其两侧导数逐渐趋近于0，容易造成<strong>梯度消失</strong>。</li>
</ol>
<p>2.激活函数的偏移现象。Sigmoid函数的输出值均大于0，使得输出不是0的均值，这会导致后一层的神经元将得到上一层非0均值的信号作为输入，这会对梯度产生影响。</p>
<ol start="3" type="1">
<li>计算复杂度高，因为Sigmoid函数是指数形式。</li>
</ol>
<h4><span id="softmax">Softmax</span></h4>
<p><strong>Softmax函数</strong>，又称<strong>归一化指数函数</strong>，函数表达式为：
<img src="https://www.zhihu.com/equation?tex=Softmax%28x%29%3D%5Cfrac%7Be%5E%7Bx_%7Bi%7D%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%7Be%5E%7Bx_%7Bj%7D%7D%7D%7D" alt="[公式]"> 。</p>
<p><img src="https://s2.loli.net/2023/04/17/tuUkTdM47K6jVXx.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>Softmax函数是二分类函数Sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。</strong>如图2所示，Softmax直白来说就是将原来输出是3,1,-3通过Softmax函数一作用，就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标。</p>
<p>由于Softmax函数先拉大了输入向量元素之间的差异（通过指数函数），然后才归一化为一个概率分布，在应用到分类问题时，它使得各个类别的概率差异比较显著，最大值产生的概率更接近1，这样输出分布的形式更接近真实分布。</p>
<p><strong>Softmax可以由三个不同的角度来解释。从不同角度来看softmax函数，可以对其应用场景有更深刻的理解：</strong></p>
<ol type="1">
<li><strong>softmax可以当作argmax的一种平滑近似</strong>，与arg
max操作中暴力地选出一个最大值（产生一个one-hot向量）不同，softmax将这种输出作了一定的平滑，即将one-hot输出中最大值对应的1按输入元素值的大小分配给其他位置。</li>
<li><strong>softmax将输入向量归一化映射到一个类别概率分布</strong>，即
<img src="https://www.zhihu.com/equation?tex=n" alt="[公式]">
个类别上的概率分布（前文也有提到）。这也是为什么在深度学习中常常将softmax作为MLP的最后一层，并配合以交叉熵损失函数（对分布间差异的一种度量)。</li>
<li>从<strong>概率图模型</strong>的角度来看，softmax的这种形式可以理解为一个概率无向图上的联合概率。因此你会发现，条件最大熵模型与softmax回归模型实际上是一致的，诸如这样的例子还有很多。由于概率图模型很大程度上借用了一些热力学系统的理论，因此也可以从物理系统的角度赋予softmax一定的内涵。</li>
</ol>
<h4><span id="52-总结">5.2 总结</span></h4>
<ol type="1">
<li>如果模型输出为非互斥类别，且可以同时选择多个类别，则采用Sigmoid函数计算该网络的原始输出值。</li>
<li>如果模型输出为<strong>互斥类别</strong>，且只能选择一个类别，则采用Softmax函数计算该网络的原始输出值。</li>
<li><strong>Sigmoid函数</strong>可以用来解决<strong>多标签问题</strong>，<strong>Softmax</strong>函数用来解决<strong>单标签问题</strong>。</li>
<li>对于某个分类场景，当Softmax函数能用时，Sigmoid函数一定可以用。</li>
</ol>
<h3><span id="6-损失函数qampa">6 损失函数Q&amp;A</span></h3>
<h4><span id="平方误差损失函数和交叉熵损失函数分别适合什么场景">==平方误差损失函数和交叉熵损失函数分别适合什么场景？==</span></h4>
<p>一般还说，平方损失函数更适合输出为连续，并且最后一层不含sigmod或softmax激活函数的神经网络；交叉熵损失函数更适合二分类或多分类的场景。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2WWJ575/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2WWJ575/" class="post-title-link" itemprop="url">理论基础（4）其他评价指标</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 12:09:01" itemprop="dateModified" datetime="2023-04-26T12:09:01+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-相似性度量指标">一、相似性度量指标</span></h3>
<blockquote>
<p>机器学习中的相似性度量方法 - 天下客的文章 - 知乎
https://zhuanlan.zhihu.com/p/411876558</p>
</blockquote>
<p>描述样本之间相似度的方法有很多种，一般来说常用的有相关系数和欧式距离。本文对机器学习中常用的相似性度量方法进行了总结。<strong>在做分类时，常常需要估算不同样本之间的相似性度量（Similarity
Measurement），</strong>这时通常采用的方法就是计算样本间的“距离”（distance）。采用什么样的方法计算距离是很讲究的，甚至关系到分类的正确与否。</p>
<ul>
<li><strong>欧式距离</strong>：k-means</li>
<li><strong>曼哈顿距离</strong>：</li>
<li><strong>切比雪夫距离</strong>：</li>
<li>闵可夫斯基距离</li>
<li>标准化欧氏距离</li>
<li>马氏距离</li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=夹角余弦&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2255493039%22%7D">夹角余弦</a></li>
<li><strong>汉明距离</strong>：simhash</li>
<li><strong>杰卡德距离&amp;杰卡德相似系数</strong>:
<strong>杰卡德相似系数是衡量两个集合的相似度一种指标。</strong></li>
<li>相关系数&amp;相关距离</li>
<li>信息熵</li>
</ul>
<h3><span id="二-推荐算法评价指标">二、推荐算法评价指标</span></h3>
<ul>
<li>推荐算法评价指标 - 一干正事就犯困的文章 - 知乎
https://zhuanlan.zhihu.com/p/359528909</li>
</ul>
<h4><span id="21-ap">2.1 AP</span></h4>
<p><code>AP</code> 衡量的是训练好的模型在每个类别上的好坏；</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232155563.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>AP总结了一个精确召回曲线，作为在每个阈值处获得的精度的加权平均值,
并且与以前的阈值相比, 召回率的增 加用作权重</strong>: <span class="math display">\[
A P=\sum_n\left(R_n-R_{(n-1))} P_n)\right.
\]</span> 其中和分别是第 <span class="math inline">\(\mathrm{n}\)</span>
个阈值 1 时的精度和召回率。此实现末进行揷值,
并且与使用梯形规则计算精确调用曲线下的面
积有所不同，后者使用线性揷值并且可能过于乐观。</p>
<h4><span id="22-map">2.2 MAP</span></h4>
<p><strong>MAP (Mean Average Precision)
常用于排序任务，MAP的计算涉及另外两个指标：Precision和Recall</strong></p>
<ul>
<li><strong>Precision和Precision@k</strong>,
推荐算法中的精度precision计算如下：</li>
</ul>
<p><span class="math display">\[
\text { precision }=\frac{\text { 算法结果中相关的item数量 }}{\text {
推荐的item总数量 }}
\]</span></p>
<p>可以看出Precision的计算没有考虑结果列表中item的顺序，Precision@k则通过切片的方式将顺序隐含在结果
中。Precision@k表示列表前k项的Precision, 随着k的变化,
可以得到一系列precision值, 用 <span class="math inline">\(P(k)\)</span>
表示。</p>
<ul>
<li><strong>Recall和Recall@k</strong>,
推荐算法中的召回率recall计算如下：</li>
</ul>
<p><span class="math display">\[
\text { recall }=\frac{\text { 算法结果中相关的 } i t e m \text { 数量
}}{\text { 所有相关的item数量 }}
\]</span></p>
<p>与Precision@kk相似, recall@k表示结果列表前k项的recall, 随着k的变化,
可以得到一系列的recall值, 用 <span class="math inline">\(r(k)\)</span>
表示。</p>
<ul>
<li><strong>AP@N</strong>, AP (Average Precision)
平均精度的计算以Precision@k为基础, 可以体现出结果列表中item顺序的重要性,
其 计算过程如下:</li>
</ul>
<p><span class="math display">\[
A P @ N=\frac{1}{m} \sum_{k=1}^N(P(k) \quad \text { if kth item is
relevant })=\frac{1}{m} \sum_{k=1}^N P(k) \cdot r e l(k)
\]</span></p>
<p>其中, <span class="math inline">\(\mathrm{N}\)</span> 表示要求推荐的
<span class="math inline">\(\mathrm{N}\)</span> 个item, <span class="math inline">\(\mathrm{m}\)</span> 表示所有相关的item总数, <span class="math inline">\(r e l(k)\)</span> 表示第 <span class="math inline">\(k\)</span> 个item是否相关, 相关为 1 , 反 之为
0</p>
<p>AP@N的值越大，表示推荐列表中相关的item数量越多以及相关item的排名越靠前</p>
<ul>
<li><strong>MAP@N</strong></li>
</ul>
<p><strong>AP@N评价了算法对单个用户的性能，MAP@N则是算法对多个用户的平均值，是平均数的平均，其计算过程如下</strong>：
<span class="math display">\[
M A P @ N=\frac{1}{|U|} \sum_{u=1}^{|U|}(A P @ N) u=\frac{1}{|U|} \sum
u=1^{|U|}\left(\frac{1}{m} \sum_{k=1}^N P_u(k) \cdot r e l_u(k)\right)
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2Z6ZKRX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2Z6ZKRX/" class="post-title-link" itemprop="url">理论基础（4）聚类评价指标</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 12:06:37" itemprop="dateModified" datetime="2023-04-26T12:06:37+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-聚类算法评价指标">一、聚类算法评价指标</span></h3>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/343667804</p>
<p>十分钟掌握聚类算法的评估指标：https://juejin.cn/post/6997913127572471821</p>
</blockquote>
<h4><span id="前言-外部评估-内部指标">前言 【外部评估】+ 【内部指标】</span></h4>
<p>如同之前介绍的其它算法模型一样，对于聚类来讲我们同样会通过一些评价指标来衡量聚类算法的优与劣。在聚类任务中，常见的评价指标有：<strong>纯度（Purity）</strong>、<strong>兰德系数（Rand
Index,
RI）</strong>、<strong>F值（F-score）</strong>和<strong>调整兰德系数（Adjusted
Rand
Index,ARI）</strong>。同时，这四种评价指标也是聚类相关论文中出现得最多的评价方法。下面，我们就来对这些算法一一进行介绍。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232157697.jpg" alt="img" style="zoom: 67%;"></p>
<p>好的聚类算法，一般要求类簇具有：</p>
<ul>
<li><strong>簇内 (intra-cluster) 相似度高</strong></li>
<li><strong>簇间 (inter-cluster) 相似度底</strong></li>
</ul>
<p>一般来说，评估聚类质量有两个标准，内部评估评价指标和外部评估指标。内部评估指标主要基于数据集的集合结构信息从紧致性、分离性、连通性和重叠度等方面对聚类划分进行评价。即基于数据聚类自身进行评估的。</p>
<h4><span id="11聚类纯度-聚类的准确率"><strong>1.1聚类纯度</strong> -
聚类的准确率</span></h4>
<p><strong>在聚类结果的评估标准中,
一种最简单最直观的方法就是计算它的聚类纯度
(purity)</strong>，别看纯度听起来很陌生,
但实际上和<strong>分类问题中的准确率有着异曲同工之妙</strong>。<strong>因为聚类纯度的总体思想也用聚类正确的样本数除以总的样本
数,
因此它也经常被称为聚类的准确率</strong>。只是对于聚类后的结果我们并不知道每个簇所对应的真实类别,
因此需要 取每种情况下的最大值。具体的，纯度的计算公式定义如下： <span class="math display">\[
P=(\Omega, \mathbb{C})=\frac{1}{N} \sum_k \max _j\left|\omega_k \cap
c_j\right|
\]</span> 其中 <span class="math inline">\(N\)</span> 表示总的样本数;
<span class="math inline">\(\Omega=\left\{\omega_1, \omega_2, \ldots,
\omega_K\right\}\)</span> 表示一个个聚类后的簇, 而 <span class="math inline">\(\mathbb{C}=\left\{c_{1,2}, \ldots
c_J\right\}\)</span> 表示正确的类别; <span class="math inline">\(\omega_k\)</span> 表示聚类后第 <span class="math inline">\(k\)</span> 个簇中的所有样本, <span class="math inline">\(c_j\)</span> 表示第 <span class="math inline">\(j\)</span> 个类别中真实的样本。在这里 <span class="math inline">\(P\)</span> 的取值范围为 <span class="math inline">\([0,1]\)</span> ，越大表示 聚类效果越好。</p>
<h4><span id="12-兰德系数与f值同簇混淆矩阵"><strong>1.2 兰德系数与F值</strong>
[同簇混淆矩阵]</span></h4>
<p>在介绍完了纯度这一评价指标后，我们再来看看兰德系数（Rand
Index）和F值。虽然兰德系数听起来是一个陌生 的名词,
但它的计算过程却也与准确率的计算过程类似。同时,
虽然这里也有一个叫做值的指标, 并且它的计算 过程也和分类指标中的F值类似,
但是两者却有着本质的差别。说了这么多, 那这两个指标到底该怎么算呢? 同分
类问题中的沘淆矩阵类似，这里我们也要先定义四种情况进行计数，然后再进行指标的计算。</p>
<p><strong>为了说明兰德系数背后的思想，我们还是以图1中的聚类结果为例进行说明（为了方便观察，我们再放一张图在这
里):</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232158691.jpg" alt="img" style="zoom: 67%;"></p>
<ul>
<li><span class="math inline">\(T P\)</span> :
表示两个同类样本点在同一个簇（布袋）中的情况数量;</li>
<li><span class="math inline">\(F P\)</span> :
表示两个非同类样本点在同一个簇中的情况数量;</li>
<li><span class="math inline">\(T N\)</span> :
表示两个非同类样本点分别在两个簇中的情况数量;</li>
<li><span class="math inline">\(F N\)</span> :
表示两个同类样本点分别在两个簇中的情况数量;</li>
</ul>
<p>由此，我们便能得到如下所示的对<strong>混淆矩阵（Pair Confusion
Matrix）</strong>：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232158915.png" alt="img" style="zoom:75%;"></p>
<p>有了上面各种情况的统计值，我们就可以定义出兰德系数和F值的计算公式：
<span class="math display">\[
Precision =\frac{T P}{T P+F P}
\]</span></p>
<p><span class="math display">\[
Recall =\frac{T P}{T P+F N}
\]</span></p>
<p><span class="math display">\[
R I  =\frac{T P+T N}{T P+F P+F N+T N}
\]</span></p>
<p><span class="math display">\[
F_\beta  =\left(1+\beta^2\right) \frac{\text { Precision } \cdot \text {
Recall }}{\beta^2 \cdot \text { Precision }+ \text { Recall }}
\]</span></p>
<p>从上面的计算公式来看, (3)(4)
从形式上看都非常像分类问题中的准确率与F值, 但是有着本质的却别。同时, 在
这里 <span class="math inline">\(R I\)</span> 和 <span class="math inline">\(F_\beta\)</span> 的取值范围均为 <span class="math inline">\([0,1]\)</span> ，越大表示聚类效果越好。</p>
<h4><span id="13调整兰德系数adjusted-rand-index归一化">1.3
调整兰德系数（Adjusted Rand index）【归一化】</span></h4>
<p>对于随机结果，RI并不能保证分数接近零。<strong>为了实现“在聚类结果随机产生的情况下，指标应该接近零”</strong>，调整兰德系数（Adjusted
rand index）被提出，它具有更高的区分度。</p>
<p>其公式为： <span class="math display">\[
\mathrm{ARI}=\frac{\mathrm{RI}-E[\mathrm{RI}]}{\max
(\mathrm{RI})-E[\mathrm{RI}]}
\]</span> <span class="math inline">\(A R\)</span> 取值范围为 <span class="math inline">\([-1,1]\)</span>,
值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲,
ARI衡量的是两个数据分布 的吻合程度。</p>
<p>优点:</p>
<ul>
<li>对任意数量的聚类中心和样本数, 随机聚类的ARI都非常接近于 0 。</li>
<li>取值在 <span class="math inline">\([-1,1]\)</span> 之间,
负数代表结果不好, 越接近于1越好。</li>
<li>对簇的结构不需作出任何假设：可以用于比较聚类算法。</li>
</ul>
<p>缺点:</p>
<ul>
<li>ARI 需要 ground truth classes 的相关知识, ARI需要真实标签,
而在实践中几乎不可用, 或者需要人工 标注者
手动分配（如在监督学习环境中）。</li>
</ul>
<h4><span id="14-标准化互信息nmi-normalized-mutualinformation">1.4
<strong><font color="red"> 标准化互信息（NMI, Normalized Mutual
Information）</font></strong></span></h4>
<p>互信息是用来衡量两个数据分布的吻合程度。它也是一有用的信息度量，它是指两个事件集合之间的相关性。互信息越大，词条和类别的相关程度也越大。</p>
<h4><span id="15-轮廓系数silhouette-coefficient">1.5 <strong><font color="red">
轮廓系数（Silhouette Coefficient）</font></strong></span></h4>
<p><strong>轮廓系数适用于实际类别信息末知的情况。</strong>对于单个样本,
设 <span class="math inline">\(a\)</span>
是与它同类别中其他样本的平均距离, <span class="math inline">\(b\)</span>
是与它距离最近不同类别中样本的平均距离, 其轮廓 系数为: <span class="math display">\[
s=\frac{b-a}{\max (a, b)}
\]</span> 对于一个样本集合,
它的轮廓系数是所有样本轮廓系数的平均值。轮廓系数的取值范围是 <span class="math inline">\([-1,1]\)</span>, 同类别样本距离 越相近,
不同类别样本距离越远, 值越大。当值为负数时, 说明聚类效果很差。</p>
<h4><span id="16calinski-harabaz指数calinski-harabaz-index">1.6
Calinski-Harabaz指数（Calinski-Harabaz Index）</span></h4>
<p>在真实的分群label不知道的情况下，Calinski-Harabasz可以作为评估模型的一个指标。</p>
<p>Calinski-Harabasz指数通过<strong>计算类中各点与类中心的距离平方和来度量类内的紧密度</strong>，通过<strong>计算各类中心点与数据集中心点距离平方和来度量数据集的分离度</strong>，CH指标<strong>由分离度与紧密度的比值得到</strong>。从而，CH越大代表着类自身越紧密，类与类之间越分散，即更优的聚类结果。</p>
<p><strong>优点</strong></p>
<ul>
<li>当簇的密集且分离较好时，分数更高。</li>
<li>得分计算很快，与轮廓系数的对比，最大的优势：快！相差几百倍！毫秒级。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>凸的簇的CH指数通常高于其他类型的簇。例如，通过 DBSCAN
获得基于密度的簇；所以，不适合基于密度的聚类算法（DBSCAN）。</li>
</ul>
<h4><span id="17-戴维森堡丁指数dbidavies-bouldin-index">1.7 戴维森堡丁指数（DBI,
Davies-Bouldin Index）</span></h4>
<p><strong>DB指数是计算任意两类别的类内距离平均距离之和除以两聚类中心距离求最大值</strong>。DB越小，意味着类内距
离越小同时类间距离越大。<strong>零是可能的最低值,
接近零的值表示更好的分区</strong>。 <span class="math display">\[
\begin{gathered}
R_{i j}=\frac{s_{i}+s_{j}}{d_{i j}} \\
D B=\frac{1}{k} \sum_{i=1}^{k} \max _{i \neq j} R_{i j}
\end{gathered}
\]</span> 其中, <span class="math inline">\(s_{i}\)</span>
表示簇的每个点与该簇的质心之间的平均距离, 也称为簇直径。 <span class="math inline">\(d_{i j}\)</span> 表示聚类和的质心之间的距 离。
算法生成的聚类结果越是朝着簇内距离最小（类内相似性最大）和笶间距离最大（类间相似性最小）变化，
那么Davies-Bouldin指数就会越小。 <strong>缺点</strong>:</p>
<ul>
<li>因使用欧式距离, 所以对于环状分布聚类评测很差。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/19/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><span class="page-number current">20</span><a class="page-number" href="/page/21/">21</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/21/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
