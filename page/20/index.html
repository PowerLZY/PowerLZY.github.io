<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/20/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/20/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/20/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">255</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">52</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1D4WS02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1D4WS02/" class="post-title-link" itemprop="url">深度学习（2）【draft】神经网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-04-05 16:23:58" itemprop="dateCreated datePublished" datetime="2022-04-05T16:23:58+08:00">2022-04-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 21:34:05" itemprop="dateModified" datetime="2023-04-18T21:34:05+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>932</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/10.%20Neural%20Network</p>
</blockquote>
<h2><span id="1-深度学习有哪些应用">1. 深度学习有哪些应用</span></h2>
<ul>
<li>图像：图像识别、物体识别、图片美化、图片修复、目标检测。</li>
<li>自然语言处理：机器创作、个性化推荐、文本分类、翻译、自动纠错、情感分析。</li>
<li>数值预测、量化交易</li>
</ul>
<h2><span id="2-什么是神经网络">2. 什么是神经网络</span></h2>
<p>我们以房价预测的案例来说明一下，把房屋的面积作为神经网络的输入（我们称之为𝑥），通过一个节点（一个小圆圈），最终输出了价格（我们用𝑦表示）。其实这个小圆圈就是一个单独的神经元，就像人的大脑神经元一样。如果这是一个单神经元网络，不管规模大小，<strong>它正是通过把这些单个神经元叠加在一起来形成。如果你把这些神经元想象成单独的乐高积木，你就通过搭积木来完成一个更大的神经网络。</strong></p>
<p>神经网络与大脑关联不大。这是一个过度简化的对比，把一个神经网络的逻辑单元和右边的生物神经元对比。至今为止其实连神经科学家们都很难解释，究竟一个神经元能做什么。</p>
<h3><span id="21-什么是感知器">2.1 什么是感知器</span></h3>
<p>这要从逻辑回归讲起，我们都知道逻辑回归的目标函数如下所示：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0601d86aeca916b9cb31dda374efd5479b996583a2851b396438b9d61b6ed13d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f7a3d25354374686574615f302b25354374686574615f31585f312b25354374686574615f32585f32"><img src="https://camo.githubusercontent.com/0601d86aeca916b9cb31dda374efd5479b996583a2851b396438b9d61b6ed13d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f7a3d25354374686574615f302b25354374686574615f31585f312b25354374686574615f32585f32" alt="img"></a></p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/1c7f212210a1b511dedbe985ecb20d77de3350d75a31db9208dc9e99b5009eb9/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f303036333044656667793167356e6c796d3734766f6a333035763031726a72362e6a7067"><img src="https://camo.githubusercontent.com/1c7f212210a1b511dedbe985ecb20d77de3350d75a31db9208dc9e99b5009eb9/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f303036333044656667793167356e6c796d3734766f6a333035763031726a72362e6a7067" alt="img"></a></p>
<p>我们用网络来表示，这个网络就叫做感知器：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/25c5593cb68bf5448a7d7767e355741fc6e8798c8e631558799721639839cd5f/687474703a2f2f7778312e73696e61696d672e636e2f6d773639302f30303633304465666c79316735677a6f72367a7a766a3330617a30386a7461662e6a7067"><img src="https://camo.githubusercontent.com/25c5593cb68bf5448a7d7767e355741fc6e8798c8e631558799721639839cd5f/687474703a2f2f7778312e73696e61696d672e636e2f6d773639302f30303633304465666c79316735677a6f72367a7a766a3330617a30386a7461662e6a7067" alt="img"></a></p>
<p>如果在这个感知器的基础上加上隐藏层，就会得到下面我们要说的神经网络结构了。</p>
<h3><span id="22-神经网络的结构">2.2 神经网络的结构</span></h3>
<p>神经网络的一般结构是由<strong>输入层、隐藏层(神经元)、输出层</strong>构成的。隐藏层可以是1层或者多层叠加，层与层之间是相互连接的，如下图所示。<strong>神经网络具有非线性切分能力</strong>。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f4750f566fdf6d5dcc101af292f4bd1b2bebb0b993012598bdfc4e0bc95a2f91/687474703a2f2f7778342e73696e61696d672e636e2f6d773639302f3030363330446566677931673567717376646278666a3330703130636f6e34712e6a7067"><img src="https://camo.githubusercontent.com/f4750f566fdf6d5dcc101af292f4bd1b2bebb0b993012598bdfc4e0bc95a2f91/687474703a2f2f7778342e73696e61696d672e636e2f6d773639302f3030363330446566677931673567717376646278666a3330703130636f6e34712e6a7067" alt="img"></a></p>
<p><strong>一般说到神经网络的层数是这样计算的，输入层不算，从隐藏层开始一直到输出层，一共有几层就代表着这是一个几层的神经网络</strong>，例如上图就是一个三层结构的神经网络。</p>
<p><strong>解释隐藏层的含义：</strong>在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入𝑥也包含了目标输出𝑦，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值。</p>
<ul>
<li>多隐藏层的神经网络比单隐藏层的神经网络工程效果好很多。</li>
<li>提升隐层层数或者隐层神经元个数，神经网络“容量”会变大，空间表达力会变强。</li>
<li>过多的隐层和神经元节点，会带来过拟合问题。</li>
<li><strong>不要试图通过降低神经网络参数量来减缓过拟合，用正则化或者dropout</strong>。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2Q49YXX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2Q49YXX/" class="post-title-link" itemprop="url">推荐算法（0）【draft】概述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-04-03 16:35:09" itemprop="dateCreated datePublished" datetime="2022-04-03T16:35:09+08:00">2022-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 15:26:22" itemprop="dateModified" datetime="2023-04-26T15:26:22+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">推荐算法</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="推荐算法">推荐算法</span></h2>
<blockquote>
<p>百度飞浆，推荐系统：https://paddlepedia.readthedocs.io/en/latest/tutorials/recommendation_system/recommender_system.html#id2</p>
<p><strong>为什么LR可以用来做CTR预估？</strong> - AI牛的回答 - 知乎
https://www.zhihu.com/question/23652394/answer/2352984912</p>
<p><strong>推荐系统中常用的embedding方法</strong> - 十三的文章 - 知乎
https://zhuanlan.zhihu.com/p/476673607</p>
</blockquote>
<h3><span id="一-背景介绍">一、背景介绍</span></h3>
<h4><span id="11-推荐系统的产生">1.1 推荐系统的产生</span></h4>
<p>在网络技术不断发展和电子商务规模不断扩大的背景下，商品数量和种类快速增长，用户需要花费大量时间才能找到自己想买的商品，这就是信息超载问题。为了解决这个难题，个性化推荐系统（Recommender
System）应运而生。</p>
<p><strong>个性化推荐系统是信息过滤系统（Information Filtering
System）的子集</strong>，它可以用在很多领域，如电影、音乐、电商和 Feed
流推荐等。个性化推荐系统通过分析、挖掘用户行为，发现用户的个性化需求与兴趣特点，将用户可能感兴趣的信息或商品推荐给用户。与搜索引擎不同，个性化推荐系统不需要用户准确地描述出自己的需求，而是根据用户的历史行为进行建模，主动提供满足用户兴趣和需求的信息。</p>
<p>1994年明尼苏达大学推出的GroupLens系统一般被认为是个性化推荐系统成为一个相对独立的研究方向的标志。该系统首次提出了基于协同过滤来完成推荐任务的思想，此后，基于该模型的协同过滤推荐引领了个性化推荐系统十几年的发展方向。</p>
<h4><span id="12-推荐系统的方法">1.2 推荐系统的方法</span></h4>
<p>传统的个性化推荐系统方法主要有：</p>
<ul>
<li><strong>协同过滤推荐</strong>（Collaborative Filtering
Recommendation）：该方法是应用最广泛的技术之一，需要收集和分析用户的历史行为、活动和偏好。它通常可以分为两个子类：基于用户
（User-Based）的推荐和基于物品（Item-Based）的推荐。该方法的一个关键优势是它不依赖于机器去分析物品的内容特征，因此它无需理解物品本身也能够准确地推荐诸如电影之类的复杂物品；缺点是对于没有任何行为的新用户存在冷启动的问题，同时也存在用户与商品之间的交互数据不够多造成的稀疏问题。值得一提的是，社交网络或地理位置等上下文信息都可以结合到协同过滤中去。</li>
<li><strong>基于内容过滤推荐</strong>（Content-based Filtering
Recommendation）：该方法利用商品的内容描述，抽象出有意义的特征，通过计算用户的兴趣和商品描述之间的相似度，来给用户做推荐。优点是简单直接，不需要依据其他用户对商品的评价，而是通过商品属性进行商品相似度度量，从而推荐给用户所感兴趣商品的相似商品；缺点是对于没有任何行为的新用户同样存在冷启动的问题。</li>
<li><strong>组合推荐（</strong>Hybrid
Recommendation）：运用不同的输入和技术共同进行推荐，以弥补各自推荐技术的缺点。
近些年来，深度学习在很多领域都取得了巨大的成功。学术界和工业界都在尝试将深度学习应用于个性化推荐系统领域中。深度学习具有优秀的自动提取特征的能力，能够学习多层次的抽象特征表示，并对异质或跨域的内容信息进行学习，可以一定程度上处理个性化推荐系统冷启动问题</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/39Y47BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/39Y47BA/" class="post-title-link" itemprop="url">推荐算法（1）FM算法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-31 10:51:33" itemprop="dateCreated datePublished" datetime="2022-03-31T10:51:33+08:00">2022-03-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 15:30:01" itemprop="dateModified" datetime="2023-04-26T15:30:01+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">推荐算法</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-fm-算法">一、FM 算法</span></h3>
<blockquote>
<p>一文读懂FM模型：https://zhuanlan.zhihu.com/p/109980037</p>
<p><strong>FM算法简单梳理</strong>🍈:
https://zhuanlan.zhihu.com/p/73798236</p>
<p>准确得预估ctr，对于提高流量得价值，增加广告收入有重要作用。业界常用得方法：人工特征+LR，gbdt，LR,FM,FFM。这些模型中FM、FFM表现突出，今天我们就来看看学习FM，下一篇我们在学习FFM。</p>
</blockquote>
<p><strong>FM（factor
Machine，因子分解机）算法是一种基于矩阵分解的机器学习算法，是为了解决大规模稀疏矩阵中特征组合问题。</strong></p>
<p><strong>作用</strong>：</p>
<ul>
<li><p>特征组合是许多机器学习建模过程中遇到的问题，如果直接建模，可能忽略特征与特征之间的关联信息，因此，可通通过构建新的交叉特征
这一特征组合方式提高模型效果。其实就是<strong>增加特征交叉项</strong>。</p>
<blockquote>
<p>在一般的线性模型中，是各个特征独立思考的，没有考虑到特征之间的相互关系。但是实际上，大量特征之间是关联。
一般女性用户看化妆品服装之类的广告比较多，而男性更青睐各种球类装备。那很明显，女性这个特征与化妆品类服装类商品有很大的关联性，男性这个特征与球类装备的关联性更为密切。如果我们能将这些有关联的特征找出来，显然是很有意义的。</p>
</blockquote></li>
<li><p>高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，<strong>特征权重更新缓慢</strong>。</p>
<blockquote>
<p>而FM的优势，就是在于这两方面问题的处理。首先是特征组合，通过两两特征组合，引入交叉特征，提高模型得分。其次是高维灾难，通过引入隐向量，对特征参数进行估计。</p>
<p>总结FM的优点：可以在非常稀疏的数据中进行合理的参数估计；FM模型的时间复杂度是线性的；FM是一个通用模型，它可以用于任何特征为实值的情况；同时解决了特征组合问题。</p>
</blockquote></li>
</ul>
<p><strong>优势</strong>：</p>
<ul>
<li>可以在非常稀疏的数据中，进行合理的参数估计</li>
<li>FM模型的复杂度是线性的，优化效果好，不需要像svm一样依赖于支持向量</li>
<li>FM是一个通用的模型，他咳哟用于任何特征为实值得情况。而其他得因式分解模型只能用于一些输入数据比较固定得情况。</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-7f85bd0c6cc1210e14d18889b651cd81_1440w.jpg?source=172ae18b" alt="FM算法简单梳理🍈" style="zoom: 50%;"></p>
<h4><span id="11-fm-特征组合">1.1 FM 特征组合</span></h4>
<p><strong>实对称矩阵分解求解</strong>: <span class="math inline">\(F
\mathrm{FM}\)</span> 为每个特征 <span class="math inline">\(\mathrm{i}\)</span> 引入隐向量 <span class="math inline">\(v_i\)</span>,
用两个特征隐向量的内积表示这两个特征的权重, 即 组合特征 <span class="math inline">\(x_i . x_j\)</span> 的权重为 <span class="math inline">\(\left\langle v_i, v_j\right\rangle\)</span> 。</p>
<p>在传统的线性模型中,
各个特征之间都是独立考虑的，并没有涉及到特征与特征之间的交互关系，但实际上大量的
特征之间是相互关联的。如何寻找相互关联的特征, 基于上述思想 <span class="math inline">\(F M\)</span> 算法应运而生。传统的线性模型为：
<span class="math display">\[
y=w_0+\sum_{i=1}^n w_i x_i
\]</span> <strong>在传统的线性模型的基础上中引入特征交叉项</strong>可得
<span class="math display">\[
y=w_0+\sum_{i=1}^n w_i x_i+\sum_{i=1}^{n-1} \sum_{j=i+1}^n w_{i j} x_i
x_j
\]</span> 在数据非常稀疏的情况下很难满足 <span class="math inline">\(x_i
、 x_j\)</span> 都不为 0 , 这样将会导致 <span class="math inline">\(w_{i
j}\)</span> 不能够通过训练得到, 因此无法进行相
应的参数估计。<strong>可以发现参数矩阵 <span class="math inline">\(w\)</span> 是一个实对称矩阵, <span class="math inline">\(w_{i j}\)</span>
可以使用矩阵分解的方法求解，通过引入辅助向量 <span class="math inline">\(V\)</span></strong> 。 <span class="math display">\[
V=\left[\begin{array}{ccccc}
v_{11} &amp; v_{12} &amp; v_{13} &amp; \cdots &amp; v_{1 k} \\
v_{21} &amp; v_{22} &amp; v_{23} &amp; \cdots &amp; v_{2 k} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v_{n 1} &amp; v_{n 2} &amp; v_{n 3} &amp; \cdots &amp; v_{n k}
\end{array}\right]=\left[\begin{array}{c}
\mathbf{v}_1 \\
\mathbf{v}_2 \\
\vdots \\
\mathbf{v}_n
\end{array}\right]
\]</span> 然后用 <span class="math inline">\(w_{i j}=\mathbf{v}_i
\mathbf{v}_j^T\)</span> 对 <span class="math inline">\(w\)</span>
进行分解 <span class="math display">\[
w=V V^T=\left[\begin{array}{c}
\mathbf{v}_1 \\
\mathbf{v}_2 \\
\vdots \\
\mathbf{v}_n
\end{array}\right]\left[\begin{array}{llll}
\mathbf{v}_1^T &amp; \mathbf{v}_2^T &amp; \ldots &amp; \mathbf{v}_n^T
\end{array}\right]
\]</span> 综上可以发现原始模型的二项式参数为 <span class="math inline">\(\frac{n(n-1)}{2}\)</span> 个, 现在减少为 <span class="math inline">\(k n(k \ll n)\)</span> 个。引入辅助向量 <span class="math inline">\(V\)</span> 最为重要的 一点是使得 <span class="math inline">\(x_t x_i\)</span> 和 <span class="math inline">\(x_i x_j\)</span> 的参数不再相互独立,
这样就能够在样本数据稀疏的情况下合理的估计模型交叉项的参 数 <span class="math display">\[
\begin{aligned}
\left\langle\mathbf{v}_t, \mathbf{v}_i\right\rangle &amp; =\sum_{f=1}^k
\mathbf{v}_{t f} \cdot \mathbf{v}_{i f} \\
\left\langle\mathbf{v}_i, \mathbf{v}_j\right\rangle &amp; =\sum_{f=1}^k
\mathbf{v}_{i f} \cdot \mathbf{v}_{j f}
\end{aligned}
\]</span> <span class="math inline">\(x_t x_i\)</span> 和 <span class="math inline">\(x_i x_j\)</span> 的参数分别为 <span class="math inline">\(\left\langle\mathbf{v}_t,
\mathbf{v}_i\right\rangle\)</span> 和 <span class="math inline">\(\left\langle\mathbf{v}_i,
\mathbf{v}_j\right\rangle\)</span>, 它们之间拥有共同项 <span class="math inline">\(\mathbf{v}_i\)</span>, 即所有包含 <span class="math inline">\(\mathbf{v}_i\)</span> 的非零组合特征的
样本都可以用来学习隐向量 <span class="math inline">\(\mathbf{v}_i\)</span>, 而原始模型中 <span class="math inline">\(w_{t i}\)</span> 和 <span class="math inline">\(w_{i j}\)</span> 却是相互独立的,
这在很大程度上避免了数据稀疏造
成的参数估计不准确的影响。因此原始模型可以改写为最终的FM算法。 <span class="math display">\[
y=w_0+\sum_{i=1}^n w_i x_i+\sum_{i=1}^{n-1}
\sum_{j=i+1}^n\left\langle\mathbf{v}_i, \mathbf{v}_j\right\rangle x_i
x_j
\]</span></p>
<p>由于求解上述式子的时间复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2%29" alt="[公式]">
，可以看出主要是最后一项计算比较复杂，因此从数学上对该式最后一项进行一些改写可以把时间复杂度降为
<img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28kn%29" alt="[公式]"></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+%26+%5Csum_%7Bi%3D1%7D%5E%7Bn-1%7D+%5Csum_%7Bj%3Di%2B1%7D%5E%7Bn%7D%5Cleft%5Clangle%5Cmathbf%7Bv%7D_%7Bi%7D%2C+%5Cmathbf%7Bv%7D_%7Bj%7D%5Cright%5Crangle+x_%7Bi%7D+x_%7Bj%7D+%5C%5C%3D%26+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Cleft%5Clangle%5Cmathbf%7Bv%7D_%7Bi%7D%2C+%5Cmathbf%7Bv%7D_%7Bj%7D%5Cright%5Crangle+x_%7Bi%7D+x_%7Bj%7D-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cleft%5Clangle%5Cmathbf%7Bv%7D_%7Bi%7D%2C+%5Cmathbf%7Bv%7D_%7Bi%7D%5Cright%5Crangle+x_%7Bi%7D+x_%7Bi%7D+%5C%5C%3D%26+%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D+%5Csum_%7Bf%3D1%7D%5E%7Bk%7D+%5Cmathbf%7Bv%7D_%7Bif%7D+%5Cmathbf%7Bv%7D_%7Bjf%7D+x_%7Bi%7D+x_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Csum_%7Bf%3D1%7D%5E%7Bk%7D+%5Cmathbf%7Bv%7D_%7Bif%7D+%5Cmathbf%7Bv%7D_%7Bif%7D+x_%7Bi%7D+x_%7Bi%7D%5Cright%29+%5C%5C%3D%26+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bf%3D1%7D%5E%7Bk%7D%5Cleft%28%5Cleft%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cmathbf%7Bv%7D_%7Bif%7D+x_%7Bi%7D%5Cright%29%5Cleft%28%5Csum_%7Bj%3D1%7D%5E%7Bn%7D+%5Cmathbf%7Bv%7D_%7Bjf%7D+x_%7Bj%7D%5Cright%29-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cmathbf%7Bv%7D_%7Bif%7D%5E%7B2%7D+x_%7Bi%7D%5E%7B2%7D%5Cright%29+%5C%5C%3D%26+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bf%3D1%7D%5E%7Bk%7D%5Cleft%28%5Cleft%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cmathbf%7Bv%7D_%7Bif%7D+x_%7Bi%7D%5Cright%29%5E%7B2%7D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cmathbf%7Bv%7D_%7Bif%7D%5E%7B2%7D+x_%7Bi%7D%5E%7B2%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>第一步：<strong>对称矩阵中上三角的内积之和等于
整个向量的乘积和减去对角线上元素的和</strong>。</p>
<p>至此, FM算法的公式推导结束了, 为了更直观的了解怎么计算的,
举下面一个例子: 例如 <span class="math display">\[
V=\left[\begin{array}{l}
V 1 \\
V 2 \\
V 3
\end{array}\right]=\left[\begin{array}{lll}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
7 &amp; 8 &amp; 9
\end{array}\right] \Rightarrow W=V * V^T \Rightarrow
W=\left[\begin{array}{ccc}
14 &amp; 32 &amp; 50 \\
32 &amp; 77 &amp; 122 \\
50 &amp; 122 &amp; 194
\end{array}\right] \Rightarrow W=V * V^T
\]</span> 可推出下面公式: <span class="math display">\[
\begin{gathered}
\sum_i^3 \sum_j^3 w_{i j} * x_i x_j= \\
14 * x_1 x_1+32 * x_1 x_2+50 * x_1 * x_3 \\
32 * x_2 x_1+77 * x_2 x_2+122 * x_2 * x_3 \\
50 * x_3 x_1+122 * x_3 x_2+194 * x_3 * x_3
\end{gathered}
\]</span>
<strong>而FM公式的第二个因子就是以上展开式"上三角“的元素。</strong></p>
<h4><span id="12-参数更新">1.2 参数更新</span></h4>
<p>采用随机梯度下降法SGD求解参数： <span class="math display">\[
\begin{gathered}
\frac{\partial y}{\partial w_0}=1 \\
\frac{\partial y}{\partial w_i}=x_i \\
\frac{\partial y}{\partial v_i f}=x_i \sum_{j=1}^n v_{j, f} x_j-v_{i, f}
x_i^2
\end{gathered}
\]</span> 由上式可知: 参数 <span class="math inline">\(v_{i, f}\)</span>
只需要样本的 <span class="math inline">\(x_i\)</span> 特征非零即可,
因此FM算法适用于稀疏场景, 并且FM算法的训练和预测 都可以在线性时间内完成,
FM是一个非常高效的算法。</p>
<h4><span id="13-fm算法小结">1.3 FM算法小结</span></h4>
<ul>
<li>FM算法提升了参数学习效率和特征交叉后模型预估的能力。</li>
<li>FM算法降低了因数据稀疏，导致特征交叉项参数学习不充分的影响；</li>
<li>模型的组合特征的参数在nk级别，通过公式推导，模型复杂度为<span class="math inline">\(O(nk)\)</span>
，因此模型可以非常高效的进行训练和预测。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/15NJNGM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/15NJNGM/" class="post-title-link" itemprop="url">理论基础（1）模型定义</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-30 21:02:16" itemprop="dateCreated datePublished" datetime="2022-03-30T21:02:16+08:00">2022-03-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 11:43:11" itemprop="dateModified" datetime="2023-04-26T11:43:11+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="机器学习理论">机器学习理论</span></h1>
<h3><span id="一-机器学习中参数模型和非参数模型理解"><strong>一、 <a target="_blank" rel="noopener" href="https://blog.csdn.net/FrankieHello/article/details/94022594">机器学习中参数模型和非参数模型理解</a></strong></span></h3>
<p><strong>参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型</strong>；非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p>
<p><strong>参数模型</strong>：线性回归、逻辑回归、感知机、基本型的SVM</p>
<p><strong>非参数模型</strong>：决策树、对偶型的SVM、朴素贝叶斯、神经网络</p>
<h3><span id="二-判别模型-vs-生成模型">二、 判别模型 VS 生成模型</span></h3>
<blockquote>
<p>判别模型与生成模型，概率模型与非概率模型、参数模型与非参数模型总结 -
Eureka的文章 - 知乎 https://zhuanlan.zhihu.com/p/37821985</p>
<p><strong>机器学习中的判别式模型和生成式模型</strong> -
Microstrong的文章 - 知乎 https://zhuanlan.zhihu.com/p/74586507</p>
</blockquote>
<p><img src="https://s2.loli.net/2023/04/17/AYyZUiraIdN5Dn1.png" alt="image-20230417171758407" style="zoom:50%;"></p>
<p><img src="v2-9b345d3e93a81dc4e7a88fccff3720b3_b.png" alt="img" style="zoom: 67%;"></p>
<h5><span id="判别模型感知机-逻辑斯特回归-支持向量机-神经网络-k近邻都属于判别学习模型">判别模型：感知机、逻辑斯特回归、支持向量机、神经网络、k近邻都属于判别学习模型。</span></h5>
<p><strong>判别模型分为两种:</strong></p>
<ul>
<li>直接对输入空间到输出空间的映射进行建模, 也就是学习函数 <span class="math inline">\(h\)</span> :</li>
</ul>
<p><span class="math display">\[
h: X \rightarrow Y, s . t . y=h(x)
\]</span></p>
<ul>
<li>对条件概率 <span class="math inline">\(P(y \mid x)\)</span>
进行建模, 然后根据贝叶斯风险最小化的准则进行分类: 【</li>
</ul>
<p><span class="math display">\[
y=\arg \max _{y \in\{-1,1\}} P(y \mid x)
\]</span></p>
<h5><span id="生成模型">生成模型：</span></h5>
<p>生成模型是间接地, 先对 <span class="math inline">\(P(x, y)\)</span>
进行建模, 再根据贝叶斯公式: <span class="math display">\[
P(y \mid x)=\frac{P(x \mid y) P(y)}{P(x)}
\]</span> 算出 <span class="math inline">\(P(y \mid x)\)</span>,
最后根据 <span class="math inline">\(\arg \max _{y \in\{-1,1\}} P(y \mid
x)\)</span> 来做分类 (由此可知, 判别模型实际上不需要对 <span class="math inline">\(P(x, y)\)</span> 进行建模)。</p>
<h3><span id="三-非概率模型-vs-概率模型">三、 非概率模型 VS 概率模型</span></h3>
<p>两者的本质区别在于是否涉及到概率分布。</p>
<h4><span id="概率模型"><strong>概率模型</strong></span></h4>
<blockquote>
<p><strong>线性回归（高斯分布）、LR（伯努利分布）、高斯判别分析、朴素贝叶斯</strong></p>
</blockquote>
<p><strong>概率模型指出了学习的目的是学出 <span class="math inline">\(P(x, y)\)</span> 或 <span class="math inline">\(P(y \mid x)\)</span>, 但最后都是根据 <span class="math inline">\(\arg \max _{y \in\{-1,1\}} P(y \mid x)\)</span>
来做判别归类。</strong>对于 <span class="math inline">\(P(x, y)\)</span>
的估计, 一般是根据乘法公式 <span class="math inline">\(P(x, y)=P(x \mid
y) P(y)\)</span> 将其拆解成 <span class="math inline">\(P(x \mid y),
P(y)\)</span> 分别进行估计。无论是对 <span class="math inline">\(P(x
\mid y), P(y)\)</span> 还是 <span class="math inline">\(P(y \mid
x)\)</span> 的估计, 都是会先假设分布的形式, 例如逻辑斯特回归就假设了
<span class="math inline">\(Y \mid X\)</span> 服从伯努利分
布。分布形式固定以后,
剩下的就是分布参数的估计问题。<strong>常用的估计有极大似然估计(MLE)和极大后验概率估计
(MAP) 等</strong>。其中, 极大后验概率估计涉及到分布参数的先验概率,
这为我们注入先验知识提供了途径。逻辑斯特回归、高斯判别分析、朴素贝叶斯都属于概率模型。</p>
<p>在一定的条件下，非概率模型与概率模型有以下对应关系:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232129434.jpg" alt="img" style="zoom:50%;"></p>
<h4><span id="非概率模型">非概率模型</span></h4>
<blockquote>
<p><strong>感知机、支持向量机、神经网络、k近邻都属于非概率模型</strong>。线性支持向量机可以显式地写出损失函数——hinge损失。神经网络也可以显式地写出损失函数——平方损失。</p>
</blockquote>
<p><font color="red">非概率模型指的是直接学习输入空间到输出空间的映射
<span class="math inline">\(h\)</span>,
学习的过程中基本不涉及概率密度的估计, 概率密度 的积分等操作,
问题的关键在于最优化问题的求解。</font>通常, 为了学习假设 <span class="math inline">\(h(x)\)</span>, 我们会先根据一些先验知识 (prior
knowledge) 来选择一个特定的假设空间 <span class="math inline">\(H(x)\)</span> (函数空间),
例如一个由所有线性函数构成的空间, 然后在
这个空间中找出泛化误差最小的假设出来, <span class="math display">\[
h^*=\arg \min _{h \in H} \varepsilon(h)=\arg \min _{h \in H} \sum_{x, y}
l(h(x), y) P(x, y)
\]</span> 其中 <span class="math inline">\(l(h(x), y)\)</span>
是我们选取的损失函数, 选择不同的损失函数,
得到假设的泛化误差就会不一样。由于我们并不知 道 <span class="math inline">\(P(x, y)\)</span>, 所以即使我们选好了损失函数,
也无法计算出假设的泛化误差, 更别提找到那个给出最小泛化误差的 假设。于是,
我们转而去找那个使得经验误差最小的假设, <span class="math display">\[
g=\arg \min _{h \in H} \hat{\varepsilon}(h)=\arg \min _{h \in H}
\frac{1}{m} \sum_{i=1}^{m} l\left(h\left(x^{(i)}\right), y^{(i)}\right)
\]</span> <strong><font color="red">
这种学习的策略叫经验误差最小化(ERM)，理论依据是大数定律：当训练样例无穷多的时候，假设的经验误差会依概率收敛到假设的泛化误差。</font></strong>要想成功地学习一个问题，必须在学习的过程中注入先验知识。前面，我们根据先验知识来选择假设空间，其实，在选定了假设空间后，先验知识还可以继续发挥作用，这一点体现在为我们的优化问题加上正则化项上，例如常用的<span class="math inline">\(L1\)</span>正则化， <span class="math inline">\(L2\)</span>正则化等。 <span class="math display">\[
g=\arg \min _{h \in H} \hat{\varepsilon}(h)=\arg \min _{h \in H}
\frac{1}{m} \sum_{i=1}^{m} l\left(h\left(x^{(i)}\right),
y^{(i)}\right)+\lambda \Omega(h)
\]</span></p>
<h3><span id="四-过拟合和欠拟合">四、 过拟合和欠拟合</span></h3>
<blockquote>
<p>欠拟合、过拟合及如何防止过拟合 - G-kdom的文章 - 知乎
https://zhuanlan.zhihu.com/p/72038532</p>
</blockquote>
<h4><span id="41-欠拟合">4.1 欠拟合</span></h4>
<p><strong>欠拟合是指模型不能在训练集上获得足够低的误差</strong>。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。</p>
<h4><span id="42-欠拟合解决方法">4.2 欠拟合解决方法</span></h4>
<p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法。</p>
<h4><span id="43-过拟合">4.3 过拟合</span></h4>
<p>过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<strong>模型在训练集上表现很好，但在测试集上却表现很差</strong>。模型对训练集"死记硬背"（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，<strong>泛化能力差</strong>。</p>
<p>造成原因主要有以下几种：
1、<strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。
2、<strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。
3、<strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。</p>
<h4><span id="44-如何防止过拟合">4.4 如何防止过拟合</span></h4>
<p>要想解决过拟合问题，就要显著减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。</p>
<h5><span id="1-使用正则化regularization方法">1、<strong>使用正则化（Regularization）方法。</strong></span></h5>
<p>那什么是<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=正则化&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2272038532%22%7D">正则化</a>呢？<strong>正则化是指修改学习算法，使其降低泛化误差而非训练误差</strong>。</p>
<p>常用的正则化方法根据具体的使用策略不同可分为：（1）直接提供正则化约束的参数正则化方法，如L1/L2正则化；（2）通过工程上的技巧来实现更低泛化误差的方法，如提前终止(Early
stopping)和Dropout；（3）不直接提供约束的隐式正则化方法，如数据增强等。</p>
<p><strong>L2正则化起到使得权重参数<span class="math inline">\(w\)</span>变小的效果，为什么能防止过拟合呢？</strong>因为更小的权重参数<span class="math inline">\(w\)</span>意味着模型的复杂度更低，对训练数据的拟合刚刚好，不会过分拟合训练数据，从而提高模型的泛化能力。</p>
<h5><span id="2-获取和使用更多的数据数据集增强解决过拟合的根本性方法">2、<strong>获取和使用更多的数据（数据集增强）——解决过拟合的根本性方法</strong></span></h5>
<p>让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是<strong>创建“假数据”并添加到训练集中——数据集增强</strong>。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。</p>
<p>我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。另外补充一句，在物体分类问题里，<strong>CNN在图像识别的过程中有强大的“不变性”规则，即待辨识的物体在图像中的形状、姿势、位置、图像整体明暗度都不会影响分类结果</strong>。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。</p>
<h5><span id="3采用合适的模型控制模型的复杂度"><strong>3.
采用合适的模型（控制模型的复杂度）</strong></span></h5>
<p>过于复杂的模型会带来过拟合问题。对于模型的设计，目前公认的一个深度学习规律"deeper
is
better"。国内外各种大牛通过实验和竞赛发现，对于CNN来说，层数越多效果越好，但是也更容易产生过拟合，并且计算所耗费的时间也越长。</p>
<p>根据<strong>奥卡姆剃刀法则</strong>：在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。对于模型的设计而言，我们应该<strong>选择简单、合适的模型解决复杂的问题</strong>。</p>
<h5><span id="4-降低特征的数量"><strong>4. 降低特征的数量</strong></span></h5>
<p>对于一些特征工程而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题。</p>
<h5><span id="5-dropout"><strong>5. Dropout</strong></span></h5>
<p>Dropout是在训练网络时用的一种技巧（trike），相当于在隐藏单元增加了噪声。<strong>Dropout
指的是在训练过程中每次按一定的概率（比如50%）随机地“删除”一部分隐藏单元（神经元）。</strong>所谓的“删除”不是真正意义上的删除，其实就是将该部分神经元的激活函数设为0（激活函数的输出为0），让这些神经元不计算而已。</p>
<h5><span id="dropout为什么有助于防止过拟合呢"><strong>Dropout为什么有助于防止过拟合呢？</strong></span></h5>
<p>（a）在训练过程中会产生不同的训练模型，不同的训练模型也会产生不同的的计算结果。随着训练的不断进行，计算结果会在一个范围内波动，但是均值却不会有很大变化，因此可以把最终的训练结果看作是不同模型的平均输出。</p>
<p>（b）它消除或者减弱了神经元节点间的联合，降低了网络对单个神经元的依赖，从而增强了泛化能力。</p>
<h5><span id="6-earlystopping提前终止"><strong>6. Early
stopping（提前终止）</strong></span></h5>
<p>对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=梯度下降&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2272038532%22%7D">梯度下降</a>（Gradient
descent）。<strong>Early
stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合</strong>。</p>
<p>为了获得性能良好的神经网络，训练过程中可能会经过很多次<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=epoch&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2272038532%22%7D">epoch</a>（遍历整个数据集的次数，一次为一个epoch）。如果epoch数量太少，网络有可能发生欠拟合；如果epoch数量太多，则有可能发生过拟合。Early
<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=stopping&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2272038532%22%7D">stopping</a>旨在解决epoch数量需要手动设置的问题。具体做法：<strong>每个epoch（或每N个epoch）结束后，在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练，将停止之后的权重作为网络的最终参数。</strong></p>
<p><strong>为什么能防止过拟合？</strong>当还未在神经网络运行太多迭代过程的时候，w参数接近于0，因为随机初始化<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=w值&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2272038532%22%7D">w值</a>的时候，它的值是较小的随机值。当你开始迭代过程，w的值会变得越来越大。到后面时，w的值已经变得十分大了。所以early
stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。</p>
<p><strong>Early
Stopping缺点：没有采取不同的方式来解决优化损失函数和过拟合这两个问题</strong>，而是用一种方法同时解决两个问题
，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化损失函数，你可能会发现损失函数的值不够小，同时你又不希望过拟合。</p>
<h3><span id="五-损失函数loss与评价指标metric的区别">五、损失函数(loss)与评价指标(metric)的区别？</span></h3>
<p><strong>当建立一个学习算法时，我们希望最大化一个给定的评价指标matric（比如说准确度），但算法在学习过程中会尝试优化一个不同的损失函数loss（比如说MSE/Cross-entropy）。</strong></p>
<h4><span id="那为什么不把评价指标matric作为学习算法的损失函数loss呢">那为什么不把评价指标matric作为学习算法的损失函数loss呢？</span></h4>
<ul>
<li><p>一般来说，我认为你应该尝试优化一个与你最关心的评价指标相对应的损失函数。例如，在做分类时，我认为你需要给我一个很好的理由，让我不要优化交叉熵。也就是说，交叉熵并不是一个非常直观的指标，所以一旦你完成了训练，你可能还想知道你的分类准确率有多高，以了解你的模型是否真的能在现实世界中发挥作用，总之，在每个epoch训练完后，你都会有多个评估指标。这样作的主要原因是为了了解你的模型在做什么。这意味着你想要最大化指标A，以便得到一个接近最大化指标B的解决方案。</p></li>
<li><p>通常情况下，MSE/交叉熵比精度更容易优化，因为它们对模型参数是可微的，在某些情况下甚至是凸的，这使得它更容易。</p></li>
</ul>
<h3><span id="六-标准化和归一化">六、标准化和归一化</span></h3>
<blockquote>
<p>PCA、k-means、SVM、回归模型、<strong>神经网络</strong></p>
</blockquote>
<h4><span id="定义">定义</span></h4>
<p><strong>归一化和标准化</strong>都是对<strong>数据做变换</strong>的方式，将原始的一列数据转换到某个范围，或者某种形态，具体的：</p>
<blockquote>
<p><strong>归一化(Normalization)</strong>：将一列数据变化到某个固定区间(范围)中，通常，这个区间是[0,
1]，广义的讲，可以是各种区间，比如映射到[0，1]一样可以继续映射到其他范围，图像中可能会映射到[0,255]，其他情况可能映射到[-1,1]；</p>
<p><strong>标准化(Standardization)</strong>：将数据变换为均值为0，标准差为1的分布切记，<strong>并非一定是正态的；</strong></p>
<p><strong>中心化</strong>：另外，还有一种处理叫做中心化，也叫零均值处理，就是将每个原始数据减去这些数据的均值。</p>
</blockquote>
<h4><span id="差异">差异</span></h4>
<blockquote>
<p><strong>归一化：对处理后的数据范围有严格要求;</strong></p>
<p><strong>标准化: 数据不为稳定，存在极端的最大最小值;
涉及距离度量、协方差计算的时候;</strong></p>
</blockquote>
<ul>
<li><strong>归一化会严格的限定变换后数据的范围</strong>，比如按之前最大最小值处理的，它的范围严格在[
0 , 1
]之间；而<strong>标准化</strong>就没有严格的区间，变换后的数据没有范围，只是其均值是0，标准差为1。</li>
<li><strong>归一化的缩放比例仅仅与极值有关</strong>，容易受到异常值的影响。</li>
</ul>
<h4><span id="用处">用处</span></h4>
<ul>
<li>回归模型，自变量X的量纲不一致导致了<strong>回归系数无法直接解读</strong>或者错误解读；需要将X都处理到统一量纲下，这样才可比；</li>
<li>机器学习任务和统计学任务中有很多地方要用到<strong>“距离”的计算</strong>，比如PCA，比如KNN，比如kmeans等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；</li>
<li>参数估计时使用<strong>梯度下降</strong>，在使用梯度下降的方法求解最优化问题时，
归一化/标准化后可以加快梯度下降的求解速度，即<strong>提升模型的收敛速度</strong>。</li>
</ul>
<p>其他：log、sigmod、softmax 变换</p>
<h3><span id="七-回归-vs-分类">七、回归 vs 分类</span></h3>
<p>回归问题可以理解为是定量输出的问题，是一个连续变量预测；分类问题可以理解为是定性输出的问题，是一个离散变量预测。</p>
<p><font color="red">如何理解回归与分类？</font></p>
<h3><span id="八-常见损失函数求导">八、常见损失函数求导</span></h3>
<h4><span id="sigmod-交叉熵求导">sigmod 交叉熵求导</span></h4>
<p>交叉熵损失函数为：</p>
<p><span class="math display">\[
J(\theta)=-\frac{1}{m} \sum_{i=1}^m y^{(i)} \log
\left(h_\theta\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log
\left(1-h_\theta\left(x^{(i)}\right)\right)
\]</span> 其中: <span class="math display">\[
\begin{gathered}
\log h_\theta\left(x^{(i)}\right)=\log \frac{1}{1+e^{-\theta^T
x^{(i)}}}=-\log \left(1+e^{-\theta^T x^{(i)}}\right), \\
\log \left(1-h_\theta\left(x^{(i)}\right)\right)=\log
\left(1-\frac{1}{1+e^{-\theta^T x^{(i)}}}\right)=\log
\left(\frac{e^{-\theta^T x^{(i)}}}{1+e^{-\theta^T x^{(i)}}}\right) \\
=\log \left(e^{-\theta^T x^{(i)}}\right)-\log \left(1+e^{-\theta^T
x^{(i)}}\right)=-\theta^T x^{(i)}-\log \left(1+e^{-\theta^T
x^{(i)}}\right)
\end{gathered}
\]</span> 由此, 得到: <span class="math display">\[
\begin{gathered}
J(\theta)=-\frac{1}{m} \sum_{i=1}^m\left[-y^{(i)}\left(\log
\left(1+e^{-\theta^T
x^{(i)}}\right)\right)+\left(1-y^{(i)}\right)\left(-\theta^T
x^{(i)}-\log \left(1+e^{-\theta^T x^{(i)}}\right)\right)\right] \\
=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\theta^T
x^{(i)}-\log \left(1+e^{-\theta^T x^{(i)}}\right)\right] \\
=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\log
e^{\theta^T x^{(i)}}-\log \left(1+e^{-\theta^T x^{(i)}}\right)\right] \\
=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\left(\log
e^{\theta^T x^{(i)}}+\log \left(1+e^{-\theta^T
x^{(i)}}\right)\right)\right]\\
=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\log
\left(1+e^{\theta^T x^{(i)}}\right)\right]
\end{gathered}
\]</span> 由此, 得到: <span class="math display">\[
\begin{aligned}
&amp; J(\theta)=-\frac{1}{m} \sum_{i=1}^m {\left[-y^{(i)}\left(\log
\left(1+e^{-\theta^T
x^{(i)}}\right)\right)+\left(1-y^{(i)}\right)\left(-\theta^T
x^{(i)}-\log \left(1+e^{-\theta^T x^{(i)}}\right)\right)\right] } \\
&amp;=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\theta^T
x^{(i)}-\log \left(1+e^{-\theta^T x^{(i)}}\right)\right] \\
&amp;=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\log
e^{\theta^T x^{(i)}}-\log \left(1+e^{-\theta^T x^{(i)}}\right)\right] \\
&amp;=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\left(\log
e^{\theta^T x^{(i)}}+\log \left(1+e^{-\theta^T
x^{(i)}}\right)\right)\right] \\
&amp;=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \theta^T x^{(i)}-\log
\left(1+e^{\theta^T x^{(i)}}\right)\right]
\end{aligned}
\]</span> 求导: <span class="math display">\[
\begin{aligned}
&amp; \frac{\partial}{\partial \theta_j}
J(\theta)=\frac{\partial}{\partial \theta_j}\left(\frac{1}{m}
\sum_{i=1}^m\left[\log \left(1+e^{\theta^T x^{(i)}}\right)-y^{(i)}
\theta^T x^{(i)}\right]\right) \\
&amp; =\frac{1}{m} \sum_{i=1}^m\left[\frac{\partial}{\partial \theta_j}
\log \left(1+e^{\theta^T x^{(i)}}\right)-\frac{\partial}{\partial
\theta_j}\left(y^{(i)} \theta^T x^{(i)}\right)\right] \\
&amp; =\frac{1}{m} \sum_{i=1}^m\left(\frac{x_j^{(i)} e^{\theta^T
x^{(i)}}}{1+e^{\theta^T x^{(i)}}}-y^{(i)} x_j^{(i)}\right) \\
&amp; =\frac{1}{m}
\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right) x_j^{(i)}
\\
&amp;
\end{aligned}
\]</span> 这就是交叉熵对参数的导数： <span class="math display">\[
\frac{\partial}{\partial \theta_j} J(\theta)=\frac{1}{m}
\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right) x_j^{(i)}
\]</span></p>
<h4><span id="平方损失函数绝对值-hubor损失为例gbdt残差"><strong>平方损失函数</strong>【绝对值、hubor损失】为例（GBDT
残差）：</span></h4>
<blockquote>
<p><span class="math display">\[
  \begin{aligned}
  &amp;g_{i}=\frac{\partial\left(\hat{y}^{t-1}-y_{i}\right)^{2}}{\partial
\hat{y}^{t-1}}=2\left(\hat{y}^{t-1}-y_{i}\right) \\
  &amp;h_{i}=\frac{\partial^{2}\left(\hat{y}^{t-1}-y_{i}\right)^{2}}{\hat{y}^{t-1}}=2
  \end{aligned}
  \]</span></p>
</blockquote>
<h4><span id="softmax-函数求导">softmax 函数求导</span></h4>
<p>softmax 回归的参数矩阵 <span class="math inline">\(\theta\)</span>
可以记为 <span class="math display">\[
\theta=\left[\begin{array}{c}
\theta_{1}^{T} \\
\theta_{2}^{T} \\
\vdots \\
\theta_{k}^{T}
\end{array}\right]
\]</span> 定义 softmax 回归的代价函数 <span class="math display">\[
L(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} \sum_{j=1}^{k}
1\left\{y_{i}=j\right\} \log \frac{e^{\theta_{j}^{T}
x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}\right]
\]</span> 其中, 1{:}是示性函数, 即 <span class="math inline">\(1\{\)</span> 值为真的表达式 <span class="math inline">\(\}=1 ， 1\{\)</span> 值为假的表达式 <span class="math inline">\(\}=0\)</span> 。跟 logistic 函数一样,
利用梯度下降法最小化代价函数, 下面 求解 <span class="math inline">\(\theta\)</span> 的梯度。 <span class="math inline">\(L(\theta)\)</span> 关于 <span class="math inline">\(\theta_{j}\)</span> 的梯度求解为 <span class="math display">\[
\begin{aligned}
\frac{\partial L(\theta)}{\partial \theta_{j}} &amp;=-\frac{1}{m}
\frac{\partial}{\partial \theta_{j}}\left[\sum_{i=1}^{m} \sum_{j=1}^{k}
1\left\{y_{i}=j\right\} \log \frac{e^{\theta_{j}^{T}
x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}\right] \\
&amp;=-\frac{1}{m} \frac{\partial}{\partial
\theta_{j}}\left[\sum_{i=1}^{m} \sum_{j=1}^{k}
1\left\{y_{i}=j\right\}\left(\theta_{j}^{T} x_{i}-\log \sum_{l=1}^{k}
e^{\theta_{l}^{T} x_{i}}\right)\right] \\
&amp;=-\frac{1}{m}\left[\sum_{i=1}^{m}
1\left\{y_{i}=j\right\}\left(x_{i}-\sum_{j=1}^{k}
\frac{e^{\theta_{j}^{T} x_{i}} \cdot x_{i}}{\sum_{l=1}^{k}
e^{\theta_{l}^{T} x_{i}}}\right)\right] \\
&amp;=-\frac{1}{m}\left[\sum_{i=1}^{m} x_{i}
1\left\{y_{i}=j\right\}\left(1-\sum_{j=1}^{k} \frac{e^{\theta_{j}^{T}
x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}\right)\right] \\
&amp;=-\frac{1}{m}\left[\sum_{i=1}^{m}
x_{i}\left(1\left\{y_{i}=j\right\}-\sum_{j=1}^{k}
1\left\{y_{i}=j\right\} \frac{e^{\theta_{j}^{T} x_{i}}}{\sum_{l=1}^{k}
e^{\theta_{l}^{T} x_{i}}}\right)\right] \\
&amp;=-\frac{1}{m}\left[\sum_{i=1}^{m}
x_{i}\left(1\left\{y_{i}=j\right\}-\frac{e^{\theta_{j}^{T}
x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}\right)\right] \\
&amp;=-\frac{1}{m}\left[\sum_{i=1}^{m}
x_{i}\left(1\left\{y_{i}=j\right\}-p\left(y_{i}=j \mid x_{i} ;
\theta\right)\right)\right]
\end{aligned}
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/XGJYS5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/XGJYS5/" class="post-title-link" itemprop="url">特征工程（2）特征预处理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:22:40" itemprop="dateModified" datetime="2023-04-26T14:22:40+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="特征工程-特征处理">特征工程-特征处理</span></h3>
<h3><span id="一-数值类型处理">一、 数值类型处理</span></h3>
<blockquote>
<p><strong>pandas 显示所有列：</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示所有列</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#显示所有行</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_rows&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#设置value的显示长度为100，默认为50</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;max_colwidth&#x27;</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p><strong>pandas 查看缺失特征:</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.isnull().<span class="built_in">sum</span>().sort_values(ascending = <span class="literal">False</span>) / train.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<p><strong>pandas 查看某一列的分布:</strong></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[:,col_name].value_counts()</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p><strong>特征提取方式是可以深挖隐藏在数据背后更深层次的信息的</strong>。其次，数值类型数据也并不是直观看上去那么简单易用，因为不同的数值类型的计量单位不一样，比如个数、公里、千克、DB、百分比之类，同样数值的大小也可能横跨好几个量级，比如小到头发丝直径约为0.00004米，
大到热门视频播放次数成千上万次。</p>
<h4><span id="11-数据归一化">1.1 数据归一化</span></h4>
<blockquote>
<p><strong>为什么要数据归一化？</strong></p>
<p><a href="../../AI深度学习/深度学习（3）Normalization*.md">深度学习（3）Normalization*.md</a></p>
<ul>
<li><strong>可解释性</strong>：<strong>回归模型【无正则化】</strong>中自变量X的量纲不一致导致了<strong>回归系数无法直接解读</strong>或者错误解读；需要将X都处理到统一量纲下，这样才可比【可解释性】；<strong>取决于我们的逻辑回归是不是用了正则化</strong>。如果你不用正则，标准化并不是必须的，如果用正则，那么标准化是必须的。</li>
<li><strong>距离计算</strong>：机器学习任务和统计学任务中有很多地方要用到<strong>“距离”的计算</strong>，比如<strong>PCA、KNN，kmeans和SVM</strong>等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；</li>
<li><strong>加速收敛</strong>：参数估计时使用<strong>梯度下降</strong>，在使用梯度下降的方法求解最优化问题时，
归一化/标准化后可以加快梯度下降的求解速度，即<strong>提升模型的收敛速度</strong>。</li>
</ul>
<p><strong>需要归一化的模型：</strong>利用梯度下降法求解的模型一般需要归一化，<strong>线性回归、LR、SVM、KNN、神经网络</strong></p>
</blockquote>
<p><span class="math display">\[
\tilde{x}=\frac{x-\min (x)}{\max (x)-\min (x)}
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># define data </span></span><br><span class="line">data = np.asarray([[<span class="number">100</span>, <span class="number">0.001</span>], </span><br><span class="line">                   [<span class="number">8</span>, <span class="number">0.05</span>], </span><br><span class="line">                   [<span class="number">50</span>, <span class="number">0.005</span>], </span><br><span class="line">                   [<span class="number">88</span>, <span class="number">0.07</span>], </span><br><span class="line">                   [<span class="number">4</span>, <span class="number">0.1</span>]])</span><br><span class="line"><span class="comment"># define min max scaler</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line"><span class="comment"># transform data</span></span><br><span class="line">scaled = scaler.fit_transform(data) </span><br></pre></td></tr></table></figure>
<h4><span id="12-数据标准化">1.2 数据标准化</span></h4>
<p>数据标准化是指通过改变数据的分布得到均值为0，标准差为1的服从标准正态分布的数据。主要目的是为了让不同特征之间具有相同的尺度（Scale），这样更有理化模型训练收敛。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment"># define standard scaler</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"><span class="comment"># transform data</span></span><br><span class="line">scaled = scaler.fit_transform(data) </span><br><span class="line"><span class="built_in">print</span>(scaled)</span><br></pre></td></tr></table></figure>
<h4><span id="13-对数转换">1.3 对数转换</span></h4>
<p><span class="math inline">\(\log\)</span> 函数的定义为 <span class="math inline">\(\log _a\left(\alpha^x\right)=x\)</span>, 其中
<span class="math inline">\(\mathrm{a}\)</span> 是 <span class="math inline">\(\log\)</span> 函数的底数, <span class="math inline">\(\alpha\)</span> 是一个正常数, <span class="math inline">\(x\)</span> 可以是任何正数。由于 <span class="math inline">\(\alpha^0=1\)</span> <span class="math inline">\(a=10\)</span> 时, 函数 <span class="math inline">\(\log _{10}(x)\)</span> 可以将 <span class="math inline">\([1,10]\)</span> 映射到[ <span class="math inline">\([0,1]\)</span>, 将 <span class="math inline">\([1,100]\)</span> 映射到 <span class="math inline">\([1,2]\)</span> 。换句话说,
<strong>log函数压缩了大数的范 围, 扩大了小数的范围</strong>。 <span class="math inline">\(\mathrm{x}\)</span> 越大, <span class="math inline">\(\log (\mathrm{x})\)</span> 增量越慢。 <span class="math inline">\(\log (\mathrm{x})\)</span> 函数的图像如下:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261226003.png" alt="image-20220426154119370" style="zoom: 25%;"></p>
<p><strong>Log函数可以极大压缩数值的范围，相对而言就扩展了小数字的范围。该转换方法适用于长尾分布且值域范围很大的特征，变换后的特征趋向于正态分布。</strong>对数值类型使用对数转换一般有以下几种好处：</p>
<ul>
<li>缩小数据的绝对数值</li>
<li>取对数后，可以将乘法计算转换成加法计算</li>
<li>在数据的整个值域中不同区间的差异带来的影响不同</li>
<li>取对数后不会改变数据的性质和相关关系，但压缩了变量的尺度。</li>
<li>得到的数据易消除异方差问题</li>
</ul>
<h3><span id="二-序数和类别特征处理">二、序数和类别特征处理</span></h3>
<p>本文主要说明特征工程中关于<strong>序数特征</strong>和<strong>类别特征</strong>的常用处理方法。主要包含<strong>LabelEncoder</strong>、<strong>One-Hot编码</strong>、<strong>DummyCoding</strong>、<strong>FeatureHasher</strong>以及要重点介绍的<strong>WOE编码</strong>。</p>
<h4><span id="21-序数特征处理">2.1 序数特征处理</span></h4>
<p><strong>序数特征指的是有序但无尺度的特征</strong>。比如表示‘学历’的特征，'高中'、'本科'、'硕士'，这些特征彼此之间是有顺序关系的，但是特征本身无尺度，并且也可能不是数值类型。在实际应用中，一般是字符类型居多，为了将其转换成模型能处理的形式，通常需要先进行编码，比如LabelEncoding。如果序数特征本身就是数值类型变量，则可不进行该步骤。下面依次介绍序数特征相关的处理方式。</p>
<ul>
<li><h4><span id="label-encoding">Label Encoding</span></h4></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">x = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">encoder = LabelEncoder()</span><br><span class="line">x1 = encoder.fit_transform(x)</span><br><span class="line"></span><br><span class="line">x2 = pd.Series(x).astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">x2.cat.codes.values</span><br><span class="line"><span class="comment"># pandas 因子化</span></span><br><span class="line">x2, uniques = pd.factorize(x)</span><br><span class="line"><span class="comment"># pandas 二值化</span></span><br><span class="line">x2 = pd.Series(x)</span><br><span class="line">x2 = (x2 &gt;= <span class="string">&#x27;b&#x27;</span>).astype(<span class="built_in">int</span>) <span class="comment">#令大于等于&#x27;b&#x27;的都为1</span></span><br></pre></td></tr></table></figure>
<h4><span id="22-类别特征处理">2.2 类别特征处理</span></h4>
<p><strong>类别特征由于没有顺序也没有尺度</strong>，因此处理较为麻烦，但是在CTR等领域却是非常常见的特征。比如<strong>商品的类型，颜色，用户的职业，兴趣</strong>等等。类别变量编码方法中最常使用的就是<strong>One-Hot编码</strong>，接下来结合具体实例来介绍。</p>
<ul>
<li><h4><span id="one-hot编码">One-Hot编码</span></h4></li>
</ul>
<p>One-Hot编码，又称为'独热编码'，其变换后的单列特征值只有一位是1。如下例所示，一个特征中包含3个不同的特征值(a,b,c)，编码转换后变成3个子特征，其中每个特征值中只有一位是有效位1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line"></span><br><span class="line">one_feature = [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">feature = label_encoder.fit_transform(one_feature)</span><br><span class="line">onehot_encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line">onehot_encoder.fit_transform(feature.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="labelbinarizer">LabelBinarizer</span></h4></li>
</ul>
<p>sklearn中的LabelBinarizer也具有同样的作用，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line">feature = np.array([<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">LabelBinarizer().fit_transform(feature)</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="虚拟编码dummy-coding">虚拟编码Dummy Coding</span></h4></li>
</ul>
<p>同样，<strong>pandas中也内置了对应的处理方式,使用起来比Sklearn更加方便</strong>，产生n-1个特征。实例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one_feature = [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">pd.get_dummies(one_feature, prefix=<span class="string">&#x27;test&#x27;</span>) <span class="comment"># 设置前缀test</span></span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="特征哈希feature-hashing"><strong><font color="red">
特征哈希（feature hashing）</font></strong></span></h4></li>
</ul>
<p>按照上述编码方式，如果某个特征具有100个类别值，那么经过编码后将产生100个或99个新特征，这极大地增加了特征维度和特征的稀疏度，同时还可能会出现内存不足的情况。<strong>sklearn中的FeatureHasher接口采用了hash的方法，将不同的值映射到用户指定长度的数组中，使得输出特征的维度是固定的，该方法占用内存少，效率高，可以在多类别变量值中使用，但是由于采用了Hash函数的方式，所以具有冲突的可能，即不同的类别值可能映射到同一个特征变量值中。</strong></p>
<blockquote>
<p>Feature hashing(特征哈希):
https://blog.csdn.net/laolu1573/article/details/79410187</p>
<p>https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/264165760/answer/277634591">如何用通俗的语言解释CTR和推荐系统中常用的<em>Feature</em>
<em>Hashing</em>技术以及其对应的优缺点？</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">h = FeatureHasher(n_features=<span class="number">5</span>, input_type=<span class="string">&#x27;string&#x27;</span>)</span><br><span class="line">test_cat = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;f&#x27;</span>,<span class="string">&#x27;g&#x27;</span>,<span class="string">&#x27;h&#x27;</span>,<span class="string">&#x27;i&#x27;</span>,<span class="string">&#x27;j&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">f = h.transform(test_cat)</span><br><span class="line">f.toarray()</span><br></pre></td></tr></table></figure>
<p><strong>如果hash的目标空间足够大，并且hash函数本身足够散列，不会损失什么特征信息。</strong></p>
<p>feature
hashing简单来说和<strong>kernal的思想</strong>是类似的，就是把输入的特征映射到一个具有一些我们期望的较好性质的空间上去。在feature
hasing这个情况下我们希望目标的空间具有如下的性质：</p>
<ol type="1">
<li><strong>样本无关的维度大小，因为当在线学习，或者数据量非常大，提前对数据观察开销非常大的时候，这可以使得我们能够提前给算法分配存储和切分pattern。大大提高算法的工程友好性</strong>。</li>
<li>这个空间一般来说比输入的特征空间维度小很多。</li>
<li>另外我们假设在原始的特征空间里，样本的分布是非常稀疏的，只有很少一部分子空间是被取值的。</li>
<li><strong>保持内积的无偏</strong>（不变肯定是不可能的，因为空间变小了），否则很多机器学习方法就没法用了。</li>
</ol>
<p><strong>原理</strong>：假设输入特征是一个 <span class="math inline">\(\mathrm{N}\)</span> 维的0/1取值的向量 <span class="math inline">\(\mathrm{x}_{\circ} 一 个
\mathrm{~N}-&gt;\mathrm{M}\)</span> 的哈希函数 <span class="math inline">\(\mathrm{h}\)</span> 。那么 <span class="math inline">\(\phi_j=\sum_{h(i)=j} x_i\)</span></p>
<p><strong>好处：</strong></p>
<ul>
<li>从某种程度上来讲，使得训练样本的特征在对应空间里的<strong>分布更均匀</strong>了。这个好处对于实际训练过程是非常大的，某种程度上起到了<strong>shuffle的作用</strong>。</li>
<li>特征的空间变小了，而且是一个可以预测的大小。比如说加入输入特征里有个东西叫做user_id，那么显然你也不知道到底有多少<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=userid&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%22277634591%22%7D">userid</a>的话，你需要先扫描一遍并且分配足够的空间给到它不然学着学着oom了。你也不能很好地提前优化<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=分片&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A%22277634591%22%7D">分片</a>。</li>
<li>对在线学习非常友好。</li>
</ul>
<p><strong>坏处：</strong></p>
<ul>
<li>会给debug增加困难，为了debug你要保存记录h计算的过程数据，否则如果某个特征有毛病，你怎么知道到底是哪个原始特征呢？</li>
<li>没选好哈希函数的话，<strong>可能会造成碰撞</strong>，如果原始特征很稠密并且碰撞很严重，那可能会带来坏的训练效果。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hashing_vectorizer</span>(<span class="params">features, N</span>):</span><br><span class="line">  	x = [<span class="number">0</span>] * N</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">      	h = <span class="built_in">hash</span>(f)</span><br><span class="line">        idx = h % N</span><br><span class="line">        <span class="keyword">if</span> xt(f)  <span class="number">1</span>: <span class="comment"># xt 2值hash函数减少hash冲突</span></span><br><span class="line">          	x[idx] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">         		x[idx] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="多类别值处理方式-基于统计的编码方法">多类别值处理方式 --
基于统计的编码方法</span></h4></li>
</ul>
<p>当类别值过多时，<strong>One-Hot 编码或者Dummy
Coding都可能导致编码出来的特征过于稀疏</strong>，其次也会占用过多内存。<strong>如果使用FeatureHasher，n_features的设置不好把握，可能会造成过多冲突，造成信息损失</strong>。这里提供一种基于统计的编码方法，包括<strong>基于特征值的统计</strong>或者<strong>基于标签值的统计</strong>——基于标签的编码。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line"><span class="built_in">test</span> = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="built_in">df</span> = pd.DataFrame(<span class="built_in">test</span>, columns=[<span class="string">&#x27;alpha&#x27;</span>])</span><br><span class="line">sns.countplot(<span class="built_in">df</span>[<span class="string">&#x27;alpha&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261226350.png" alt="image-20220426130800412" style="zoom:50%;"></p>
<p>首先我们将每个类别值出现的频数计算出来，比如我们设置阈值为1，那么所有小于阈值1的类别值都会被编码为同一类，大于1的类别值会分别编码，如果出现频数一样的类别值，既可以都统一分为一个类，也可以按照某种顺序进行编码，这个可以根据业务需要自行决定。那么根据上图，可以得到其编码值为：</p>
<p><span class="math display">\[
\left\{a^{\prime}: 0, &#39; c^{\prime}: 1, &#39; e^{\prime}: 2, &#39;
b^{\prime}: 2, &#39; d &#39;: 2\right\}
\]</span>
<strong>即（a,c）分别编码为一个不同的类别，（e,b,d）编码为同一个类别。</strong></p>
<h4><span id="23-二阶">2.3 二阶</span></h4>
<blockquote>
<p>w * h = s</p>
</blockquote>
<h3><span id="三-特征离散化处理方法">三、 特征离散化处理方法</span></h3>
<p><strong>特征离散化指的是将连续特征划分离散的过程</strong>：将原始定量特征的一个区间一一映射到单一的值。离散化过程也被表述成分箱（Binning）的过程。特征离散化常应用于<strong>逻辑回归</strong>和金融领域的评分卡中，同时在规则提取，特征分类中也有对应的应用价值。本文主要介绍几种常见的分箱方法，包括<strong>等宽分箱、等频分箱、信息熵分箱</strong>、<strong>基于决策树分箱、卡方分箱</strong>等。</p>
<p><strong>可以看到在分箱之后，数据被规约和简化，有利于理解和解</strong>释。总来说特征离散化，即
分箱之后会带来如下优势：</p>
<ul>
<li>有助于模型部署和应用，加快模型迭代</li>
<li>增强模型鲁棒性</li>
<li>增加非线性表达能力：连续特征不同区间对模型贡献或者重要程度不一样时，分箱后不同的权重能直接体现这种差异，离散化后的特征再进行特征
交叉衍生能力会进一步加强。</li>
<li>提升模型的泛化能力</li>
<li><strong>扩展数据在不同各类型算法中的应用范围</strong></li>
</ul>
<p>当然特征离散化也有其缺点，总结如下：</p>
<ul>
<li>分箱操作必定会导致一定程度的信息损失</li>
<li>增加流程：建模过程中加入了额外的的离散化步骤</li>
<li>影响模型稳定性：
当一个特征值处于分箱点的边缘时，此时微小的偏差会造成该特征值的归属从一箱跃迁到另外一箱，影响模型的稳定性。</li>
</ul>
<h4><span id="31-等宽分箱equal-widthbinning">3.1 等宽分箱（Equal-Width
Binning)</span></h4>
<p><strong>等宽分箱指的是每个分隔点或者划分点的距离一样，即等宽</strong>。实践中一般指定分隔的箱数，等分计算后得到每个分隔点。例如将数据序列分为n份，则
分隔点的宽度计算公式为： <span class="math display">\[
w=\frac{\max -\min }{n}
\]</span>
这样就将原始数据划分成了n个等宽的子区间，一般情况下，分箱后每个箱内的样本数量是不一致的。使用pandas中的cut函数来实现等宽分箱，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value, cutoff = pd.cut(df[<span class="string">&#x27;mean radius&#x27;</span>], bins=<span class="number">4</span>, retbins=<span class="literal">True</span>, precision=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>等宽分箱计算简单，但是当数值方差较大时，即数据离散程度很大，那么很可能出现没有任何数据的分箱</strong>，这个问题可以通过自适应数据分布的分箱方法--等频分箱来避免</p>
<h4><span id="32-等频分箱equal-frequencybinning">3.2 等频分箱（Equal-Frequency
Binning）</span></h4>
<p><strong>等频分箱理论上分隔后的每个箱内得到数据量大小一致</strong>，但是当某个值出现次数较多时，会出现等<strong>分边界是同一个值</strong>，导致同一数值分到不同的箱内，这是不正确的。具体的实现可以<strong>去除分界处的重复值</strong>，但这也导致每箱的数量不一致。如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1 = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">value, cutoff = pd.qcut(s1, <span class="number">3</span>, retbins=<span class="literal">True</span>)</span><br><span class="line">sns.countplot(value)</span><br></pre></td></tr></table></figure>
<p><strong>上述的等宽和等频分箱容易出现的问题是每箱中信息量变化不大</strong>。例如，等宽分箱不太适合分布不均匀的数据集、离群值；等频方法不太适合特定的值占比过多的数据集，如<strong>长尾分布</strong>。</p>
<h4><span id="33-信息熵分箱有监督">3.3 信息熵分箱【有监督】</span></h4>
<p><strong>如果分箱后箱内样本对y的区分度好，那么这是一个好的分箱</strong>。通过信息论理论，我们可知信息熵衡量了这种区分能力。当特征按照某个分隔点划分为上下两部分后能达到最大的信息增益，那么这就是一个好的分隔点。由上可知，信息熵分箱是有监督的分箱方法。<strong><font color="red">
其实决策树的节点分裂原理也是基于信息熵。</font></strong></p>
<p>首先我们需要明确信息熵和信息增益的计算方式, 分别如下: <span class="math display">\[
\begin{gathered}
\operatorname{Entropy}(y)=-\sum_{i=1}^m p_i \log _2 p_i \\
\operatorname{Gain}(x)=\operatorname{Entropy}(y)-\operatorname{Infos}_{\text
{split }}(x)
\end{gathered}
\]</span> 在二分类问题中, <span class="math inline">\(m=2\)</span>
。信息增益的物理含义表达为： <span class="math inline">\(x\)</span>
的分隔带来的信息对 <span class="math inline">\(y\)</span>
的不确定性带来的增益。 对于二值化的单点分隔,
如果我们找到一个分隔点将数据一分为二, 分成 <span class="math inline">\(P_1\)</span> 和 <span class="math inline">\(P_2\)</span> 两部分, 那么划分后的信息熵
的计算方式为: <span class="math display">\[
\operatorname{Info}_{\text {split }}(x)=P 1_{\text {ratio }}
\operatorname{Entropy}\left(x_{p 1}\right)+P 2_{\text {ratio }}
\operatorname{Entropy}\left(x_{p 2}\right)
\]</span>
同时也可以看出，当分箱后，某个箱中的标签y的类别（0或者1）的比例相等时，其熵值最大，表明此特征划分几
乎没有区分度。而当某个箱中的数据的标签 <span class="math inline">\(y\)</span> 为单个类别时, 那么该箱的熵值达到最小的
0 , 即纯度最纯, 最具区 分度。从结果上来看,
最大信息增益对应分箱后的总熵值最小。</p>
<h4><span id="34-决策树分箱有监督">3.4 决策树分箱【有监督】</span></h4>
<p><strong>由于决策树的结点选择和划分也是根据信息熵来计算的，因此我们其实可以利用决策树算法来进行特征分箱</strong>，具体做法如下：</p>
<p>还是以乳腺癌数据为例，首先取其中‘mean
radius’字段，和标签字段‘target’来拟合一棵决策树，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>, max_depth=<span class="number">3</span>) <span class="comment"># 树最大深度为3</span></span><br><span class="line">dt.fit(df[<span class="string">&#x27;mean radius&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>), df[<span class="string">&#x27;target&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>接着我们取出这课决策树的所有叶节点的分割点的阈值，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qts = dt.tree_.threshold[np.where(dt.tree_.children_left &gt; -<span class="number">1</span>)]</span><br><span class="line">qts = np.sort(qts)</span><br><span class="line">res = [np.<span class="built_in">round</span>(x, <span class="number">3</span>) <span class="keyword">for</span> x <span class="keyword">in</span> qts.tolist()]</span><br></pre></td></tr></table></figure>
<h4><span id="35-卡方分箱-有监督">3.5 卡方分箱 【有监督】</span></h4>
<blockquote>
<p><strong>特征选择之卡方分箱、WOE/IV</strong> - 云水僧的文章 - 知乎
https://zhuanlan.zhihu.com/p/101771771</p>
</blockquote>
<p><strong><font color="red">
卡方检验可以用来评估两个分布的相似性，因此可以将这个特性用到数据分箱的过程中。卡方分箱认为：理想的分箱是在同一个区间内标签的分布是相同的</font>;</strong>
<strong>卡方分布是概率统计常见的一种概率分布，是卡方检验的基础。</strong></p>
<p>布定义为: 若n个独立的随机变量 <span class="math inline">\(Z_1, Z_2,
\ldots, Z_k\)</span> 满足标准正态分布 <span class="math inline">\(N(0,1)\)</span>, 则 <span class="math inline">\(\mathrm{n}\)</span> 个随机变量的平方和 <span class="math inline">\(X=\sum_{i=0}^k Z_i^2\)</span> 为服从自由度为 <span class="math inline">\(\mathrm{k}\)</span> 的卡方分布, 记为 <span class="math inline">\(X \sim \chi^2\)</span> 。参数 <span class="math inline">\(\mathrm{n}\)</span>
称为自由度（样本中独立或能自由变化的自变 量的个数),
不同的自由度是不同的分布。</p>
<p>卡方检验：卡方检验属于非参数假设检验的一种，其本质都是度量频数之间的差异。其假设为：观察频数与期望
频数无差异或者两组变量相互独立不相关。 <span class="math display">\[
\chi^2=\sum \frac{(O-E)^2}{E}
\]</span></p>
<ul>
<li>卡方拟合优度检验：用于检验样本是否来自于某一个分布，比如检验某样本是否为正态分布</li>
<li>独立性卡方检验，查看两组类别变量分布是否有差异或者相关，以列联表的形式比较。以列联表形式的卡方检验中，卡方统计量由上式给出。</li>
</ul>
<h4><span id="步骤">步骤：</span></h4>
<p>卡方分箱是自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。基本思想:
对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p>
<p><strong>理想的分箱是在同一个区间内标签的分布是相同的</strong>。卡方分箱就是不断的计算相邻区间的卡方值（卡方值越小表示分布越相似），将分布相似的区间（卡方值最小的）进行合并，直到相邻区间的分布不同，达到一个理想的分箱结果。下面用一个例子来解释：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261231284.png" alt="image-20220708173638301" style="zoom: 33%;"></p>
<p>由上图，第一轮中初始化是5个区间，分别计算相邻区间的卡方值。找到1.2是最小的，合并2、3区间，为了方便，将合并后的记为第2区间，因此得到4个区间。第二轮中，由于合并了区间，影响该区间与前面的和后面的区间的卡方值，因此重新计算1和2,2和4的卡方值，由于4和5区间没有影响，因此不需要重新计算，这样就得到了新的卡方值列表，找到最小的取值2.5，因此该轮会合并2、4区间，并重复这样的步骤，一直到满足终止条件。</p>
<h4><span id="36-woe编码有监督"><strong><font color="red"> 3.6 WOE编码
【有监督】</font></strong></span></h4>
<blockquote>
<p><strong><font color="red">
风控模型—WOE与IV指标的深入理解应用</font></strong>:
https://zhuanlan.zhihu.com/p/80134853</p>
</blockquote>
<p><strong>WOE (Weight of Evidence, 证据权重）编码利用了标签信息,
属于有监督的编码方式</strong>。该方式广泛用于金融领 域信用风险模型中,
是该领域的经验做法。下面先给出WOE的计算公式: <span class="math display">\[
W O E_i=\ln \left\{\frac{P_{y 1}}{P_{y 0}}\right\}=\ln \left\{\frac{B_i
/ B}{G_i / G}\right\}
\]</span> <span class="math inline">\(W O E_i\)</span> 值可解释为第
<span class="math inline">\(i\)</span>
类别中好坏样本分布比值的对数。其中各个分量的解释如下: - <span class="math inline">\(P_{y 1}\)</span> 表示该类别中坏样本的分布 - <span class="math inline">\(P_{y 0}\)</span> 表示该类别中好样本的分布 - <span class="math inline">\(B_i / B\)</span>
表示该类别中坏样本的数量在总体坏样本中的占比 - <span class="math inline">\(G_i / G\)</span>
表示该类别中好样本的数量在总体好样本中的占比</p>
<p>很明显，如果整个分数的值大于1，那么WOE值为正，否则为负，所以WOE值的取值范围为正负无穷。
<strong>WOE值直观上表示的实际上是“当前分组中坏客户占所有坏客户的比例”和“当前分组中好客户占所有坏客户的比例”的差异。</strong>转化公式以后，也可以理解为：当前这个组中坏客户和好客户的比值，和所有样本中这个比值的差异。这个差异为这两个比值的比值，再取对数来表示的。
WOE越大，这种差异越大，这个分组里的样本坏样本可能性就越大，WOE越小，差异越小，这个分组里的坏样本可能性就越小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 随机生成1000行数据</span></span><br><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;x&#x27;</span>: np.random.choice([<span class="string">&#x27;R&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="string">&#x27;B&#x27;</span>], <span class="number">1000</span>),</span><br><span class="line">    <span class="string">&#x27;y&#x27;</span>: np.random.randint(<span class="number">2</span>, size=<span class="number">1000</span>)</span><br><span class="line">&#125;)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<h3><span id="四-缺失值处理解析">四、缺失值处理解析</span></h3>
<blockquote>
<p>看不懂你打我，史上最全的缺失值解析:
https://zhuanlan.zhihu.com/p/379707046</p>
<p>https://zhuanlan.zhihu.com/p/137175585</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>机器学习模型</th>
<th>是否支持缺失值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>XGBoost</strong></td>
<td>是</td>
</tr>
<tr class="even">
<td><strong>LightGBM</strong></td>
<td>是</td>
</tr>
<tr class="odd">
<td>线性回归</td>
<td>否</td>
</tr>
<tr class="even">
<td>逻辑回归（LR）</td>
<td>否</td>
</tr>
<tr class="odd">
<td>随机森林（RF）</td>
<td>否</td>
</tr>
<tr class="even">
<td>SVM</td>
<td>否</td>
</tr>
<tr class="odd">
<td>因子分解机(FM)</td>
<td>否</td>
</tr>
<tr class="even">
<td>朴实贝叶斯（NB）</td>
<td>否</td>
</tr>
</tbody>
</table>
<h4><span id="41-缺失值的替换">4.1 <strong>缺失值的替换</strong></span></h4>
<p><strong>scikit-learn中填充缺失值的API是Imputer类，使用方法如下：</strong></p>
<p>参数strategy有三个值可选：mean(平均值)，median(中位数)，most_frequent(众数)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rom sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 缺失值填补的时候必须得是float类型</span></span><br><span class="line"><span class="comment"># 缺失值要填充为np.nan，它是浮点型，strategy是填充的缺失值类型，这里填充平均数，axis代表轴，这里第0轴是列</span></span><br><span class="line">im = Imputer(missing_values=<span class="string">&#x27;NaN&#x27;</span>,strategy=<span class="string">&#x27;mean&#x27;</span>,axis=<span class="number">0</span>)</span><br><span class="line">data = im.fit_transform([[<span class="number">1</span>, <span class="number">2</span>], </span><br><span class="line">                         [np.nan, <span class="number">3</span>], </span><br><span class="line">                         [<span class="number">7</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<h4><span id="42-缺失值的删除">4.2 缺失值的删除</span></h4>
<h3><span id="五-异常值处理">五、异常值处理</span></h3>
<h3><span id="数据预处理qampa">数据预处理Q&amp;A</span></h3>
<h4><span id="1-lr为什么要离散化"><strong><font color="red"> 1、LR为什么要离散化？</font></strong></span></h4>
<p>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/122387176">学习]
连续<em>特征的离散化</em>：在什么情况<em>下</em>将连续的<em>特征离散化</em>之后可以获得更好的效果？</a></p>
<p><strong>问题描述：</strong>发现CTR预估一般都是用LR，而且特征都是离散的，为什么一定要用离散特征呢？这样做的好处在哪里？求大拿们解答。</p>
<h5><span id="答案一严林"><strong>答案一（<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=严林&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22122387176%22%7D">严林</a>）：</strong></span></h5>
<p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ol type="1">
<li>离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>【鲁棒性】离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则为0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>【模型假设】<strong>逻辑回归属于广义线性模型，表达能力受限</strong>；单变量离散化为N个后，每个变量有独立的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li>
<li>【特征交叉】离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险；</li>
</ol>
<h5><span id="李沐曾经说过模型是使用离散特征还是连续特征其实是一个海量离散特征简单模型同少量连续特征复杂模型的权衡"><strong><font color="red">
<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=李沐&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22122387176%22%7D">李沐</a>曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型”同“少量连续特征+复杂模型”的权衡。</font></strong></span></h5>
<blockquote>
<p>这里我写下我关于上面某些点的理解，有问题的欢迎指出：</p>
<ol start="0" type="1">
<li>假设目前有两个连续的特征：『年龄』和『收入』，预测用户的『魅力指数』；</li>
</ol>
<p>第三点:
<strong>LR是广义线性模型</strong>，因此如果特征『年龄』不做离散化直接输入，那么只能得到『年龄』和魅力指数的一个线性关系。但是这种线性关系是不准确的，并非年龄越大魅力指一定越大；如果将年龄划分为M段，则可以针对每段有一个对应的权重；这种分段的能力为模型带来类似『折线』的能力，也就是所谓的非线性
<strong>连续变量的划分，naive的可以通过人为先验知识划分，也可以通过训练单特征的决策树桩，根据Information
Gain/Gini系数等来有监督的划分。</strong>
假如『年龄』离散化后，共有N段，『收入』离散化后有M段；此时这两个离散化后的特征类似于<strong>CategoryFeature</strong>，对他们进行<strong>OneHotEncode</strong>，即可以得到
M + N的 01向量；例如： 0 1 0 0， 1 0 0 0 0； 第四点:
<strong>特征交叉</strong>，可以理解为上述两个向量的互相作用，作用的方式可以例如是
&amp;和|操作（这种交叉方式可以产生一个 M * N的01向量；）
上面特征交叉，可以类比于决策树的决策过程。例如进行&amp;操作后，得到一个1，则可以认为产生一个特征
（a &lt; age &lt; b &amp;&amp; c &lt; income &lt;
d）;将特征空间进行的非线性划分，也就是所谓的引入非线性；</p>
</blockquote>
<h5><span id="答案二周开拓"><strong>答案二（周开拓）：</strong></span></h5>
<p><strong><font color="red"> 机器学习里当然并没有free
lunch，一个方法能work，必定是有假设的。如果这个假设和真实的问题及数据比较吻合，就能work。</font></strong></p>
<p>对于LR这类的模型来说，假设基本如下：</p>
<ul>
<li><strong>局部<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=平坦性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22122387176%22%7D">平坦性</a>，或者说连续性</strong>。对于连续特征x来说，在任何一个取值x0的邻域附近，这个特征对预估目标y的影响也在一个足够小的邻域内变化。比如，人年龄对点击率的影响，x0=30岁假设会产生一定的影响，那么x=31或者29岁，这个影响和x0=30岁的影响差距不会太大；</li>
<li><strong>x对y的影响，这个函数虽然局部比较平坦，但是不太规律，如果你知道这个影响是个严格的直线</strong>（或者你有先验知识知道这个影响一定可以近似于一个参数不太多的函数），<strong>显然也没必要去做离散化</strong>。当然这条基本对于绝大多数问题都是成立的，因为基本没有这种好事情。</li>
</ul>
<p>假设一个最简单的问题，binary
classification，y=0/1，x是个连续值。你希望学到一个logloss足够小的y=f(x)。</p>
<p>那么有一种做法就是，在数据轴上切若干段，每一段观察训练样本里y为1的比例，以这个比例作为该段上y=f(x)的值。这个当然不是LR训练的过程，但是就是离散化的思想。你可以发现：</p>
<ul>
<li><strong>如果每一段里面都有足够多的样本，那么在这一段里的y=f(x)值的点估计就比较可信</strong>；</li>
<li><font color="red">如果x在数轴上分布不太均匀，比如是<strong>指数分布或者周期分布</strong>的，这么做可能会有问题，因而你要先<strong>对x取个log，或者去掉周期性</strong></font>；</li>
</ul>
<p>这就告诉了你应该怎么做离散化：<strong><font color="red">
尽可能保证每个分段里面有足够多的样本，尽量让样本的分布在数轴上均匀一些。</font></strong></p>
<p>结语：<strong>本质上连续特征离散化，可以理解为连续信号怎么转化为数字信号，好比我们计算机画一条曲线，也是变成了画一系列线段的问题。</strong>用分段函数来表达一个连续的函数在大多数情况下，都是work的。想取得好的效果需要：</p>
<ul>
<li>你的分段足够小，以使得在每个分段内x对y的影响基本在一个不大的邻域内，或者你可以忍受这个变化的幅度；</li>
<li>你的分段足够大，以使得在每个分段内有足够的样本，以获得可信的f(x)也就是权重；</li>
<li>你的分段策略使得在每个x的分段中，样本的分布尽量均匀（当然这很难），一般会根据先验知识先对x做一些变化以使得变得均匀一些；</li>
<li>如果你有非常强的x对y的先验知识，比如严格线性之类的，也未必做离散化，但是这种先验在计算广告或者推荐系统里一般是不存在的，也许其他领域比如CV之类的里面是可能存在的；</li>
</ul>
<p>最后还有个特别大的<strong>LR用离散特征的好处就是LR的特征是并行的，每个特征是并行同权的</strong>，如果有异常值的情况下，如果这个异常值没见过，那么LR里因为没有这个值的权重，最后对<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=score&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22122387176%22%7D">score</a>的贡献为0，最多效果不够好，但是不会错的太离谱。另外，如果你debug，很容易查出来是哪个段上的权重有问题，比较好定位和解决。</p>
<h4><span id="2-树模型为什么离散化"><strong><font color="red">2、树模型为什么离散化？</font></strong></span></h4>
<h5><span id="cart树的离散化">Cart树的离散化：</span></h5>
<p><strong>分类：</strong></p>
<ul>
<li><p><strong><font color="red">如果特征值是连续值：CART的处理思想与C4.5是相同的，即将连续特征值离散化。唯一不同的地方是度量的标准不一样，</font></strong>
<strong>CART采用基尼指数，而C4.5采用信息增益比</strong>。</p></li>
<li><p>如果当前节点为连续属性，<strong>CART树中该属性（剩余的属性值）后面还可以参与子节点的产生选择过程</strong>。</p></li>
</ul>
<p><strong>回归：</strong></p>
<p><strong>对于连续值的处理, CART
分类树采用基尼系数的大小来度量特征的各个划分点</strong>。在回归模型中,
我们使用常见的和方差度量方式, 对于任意划分特征 <span class="math inline">\(\mathrm{A}\)</span>, 对应的任意划分点 <span class="math inline">\(\mathrm{s}\)</span> 两边划分成的数据集 <span class="math inline">\(D_1\)</span> 和 <span class="math inline">\(D_2\)</span>, 求出使 <span class="math inline">\(D_1\)</span> 和 <span class="math inline">\(D_2\)</span> 各自集合的均方差最小, 同时 <span class="math inline">\(D_1\)</span> 和 <span class="math inline">\(D_2\)</span>
的均方差之和最小所对应的特征和特征值划分点。表达式为: <span class="math display">\[
\min _{a, s}\left[\min _{c_1} \sum_{x_i \in
D_1}\left(y_i-c_1\right)^2+\min _{c_2} \sum_{x_i \in
D_2}\left(y_i-c_2\right)^2\right]
\]</span> 其中, <span class="math inline">\(c_1\)</span> 为 <span class="math inline">\(D_1\)</span> 数据集的样本输出均值, <span class="math inline">\(c_2\)</span> 为 <span class="math inline">\(D_2\)</span> 数据集的样本输出均值。</p>
<h5><span id="lgb直方图算法优点">LGB直方图算法优点：</span></h5>
<p><strong>内存小、复杂度降低、直方图加速【分裂、并行通信、缓存优化】</strong></p>
<ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于
直方图算法，则只需要(1x样本数x维
度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的
bin
值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p></li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为<span class="math inline">\(k\)</span>的树的时间复杂度：对特征所有取值的排序为<span class="math inline">\(O(NlogN)\)</span>，<span class="math inline">\(N\)</span>为样本点数目，若有<span class="math inline">\(D\)</span>维特征，则<span class="math inline">\(O(kDNlogN)\)</span>，而直方图算法需要<span class="math inline">\(O(kD \times bin)\)</span> (bin是histogram
的横轴的数量，一般远小于样本数量<span class="math inline">\(N\)</span>)。</p></li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>两个维度</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的<span class="math inline">\(k\)</span>个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p></li>
<li><p><strong>数据并行优化</strong>，用 histgoram
可以大幅降低通信代价。用 pre-sorted
算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst
在并行的时候也使用 histogram 进行通信。</p></li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost
的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost
提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM
所使用直方图算法对 Cache
天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在
Cache Miss的问题。</p></li>
</ul>
<h4><span id="3-归一化">3、归一化？</span></h4>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li><p><strong>机器学习中的特征工程（四）----
特征离散化处理方法：</strong>https://www.jianshu.com/p/918649ce379a</p></li>
<li><p><strong>机器学习中的特征工程（三）----
序数和类别特征处理方法</strong>：https://www.jianshu.com/p/3d828de72cd4</p></li>
<li><p><strong>机器学习中的特征工程（二）----
数值类型数据处理</strong>：https://www.jianshu.com/p/b0cc0710ef55</p></li>
<li><p>机器学习中的特征工程（一）----
概览：https://www.jianshu.com/p/172677f4ea4c</p></li>
<li><p>特征工程完全手册 -
从预处理、构造、选择、降维、不平衡处理，到放弃：https://zhuanlan.zhihu.com/p/94994902</p></li>
<li><p><strong>这9个特征工程使用技巧，解决90%机器学习问题！</strong> -
Python与数据挖掘的文章 - 知乎
https://zhuanlan.zhihu.com/p/462744763</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2DEZT90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2DEZT90/" class="post-title-link" itemprop="url">特征工程（3）特征选择</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:49:50" itemprop="dateModified" datetime="2023-04-26T14:49:50+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="特征选择">特征选择</span></h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/74198735">【机器学习】特征选择(Feature
Selection)方法汇总</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/306057603"><em>特征选择方法</em>全面总结</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/479948993"><em>特征选择</em>的基本<em>方法</em>总结</a></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261231076.jpg" style="zoom: 67%;"></p>
<p>训练数据包含许多冗余或无用的特征，移除这些特征并不会导致丢失信息。其中冗余是指一个本身很有用的特征与另外一个有用的特征强相关，或它包含的信息能从其它特征推演出来;
特征很多但样本相对较少。</p>
<ul>
<li><p><strong>产生过程</strong>：产生特征或特征子集候选集合；</p></li>
<li><p><strong>评价函数</strong>：衡量特征或特征子集的重要性或者好坏程度，即量化特征变量和目标变量之间的联系以及特征之间的相互联系。为了避免过拟合，可用交叉验证的方式来评估特征的好坏；</p></li>
<li><p><strong>停止准则</strong>：为了减少计算复杂度，需设定一个阈值，当评价函数值达到阈值后搜索停止</p></li>
<li><p><strong>验证过程</strong>：在验证数据集上验证选出来的特征子集的有效性</p></li>
</ul>
<h3><span id="一-特征选择的目的"><strong>一、特征选择的目的</strong></span></h3>
<p>1.<strong>简化模型</strong>，使模型更易于理解：去除不相关的特征会降低学习任务的难度。并且可解释性能对模型效果的稳定性有更多的把握</p>
<p>2.<strong>改善性能</strong>：节省存储和计算开销</p>
<p>3.<strong>改善通用性、降低过拟合风险</strong>：减轻维数灾难，特征的增多会大大增加模型的搜索空间，大多数模型所需要的训练样本随着特征数量的增加而显著增加。特征的增加虽然能更好地拟合训练数据，但也可能增加方差</p>
<h3><span id="二-特征选择常见方法">二、特征选择常见方法</span></h3>
<ul>
<li><strong>Filter(过滤法)</strong>
<ul>
<li><strong>覆盖率</strong></li>
<li><strong>方差选择</strong></li>
<li><strong>Pearson(皮尔森)相关系数</strong></li>
<li><strong>卡方检验</strong></li>
<li><strong>互信息法(KL散度、相对熵)和最大信息系数</strong></li>
<li>Fisher得分</li>
<li>相关特征选择</li>
<li>最小冗余最大相关性</li>
</ul></li>
<li><strong>Wrapper(包装法)</strong>
<ul>
<li>完全搜索</li>
<li>启发搜索</li>
<li>随机搜索</li>
</ul></li>
<li><strong>Embedded(嵌入法)</strong>
<ul>
<li>L1 正则项</li>
<li>树模型选择</li>
<li>不重要性特征选择</li>
</ul></li>
</ul>
<h3><span id="三-filter过滤法特征集">三、<strong>Filter(过滤法)</strong>
【特征集】</span></h3>
<h3><span id></span></h3>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261231951.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h5><span id="定义"><strong>定义</strong></span></h5>
<ul>
<li><strong>过滤法的思想就是不依赖模型，仅从特征的角度来做特征的筛选</strong>，具体又可以分为两种方法，一种是根据特征里面包含的信息量，如方差选择法，如果一列特征的方差很小，每个样本的取值都一样的话，说明这个特征的作用不大，可以直接剔除。另一种是对每一个特征，都计算关于目标特征的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=相关度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22479948993%22%7D">相关度</a>，然后根据这个相关度来筛选特征，只保留高于某个阈值的特征，这里根据相关度的计算方式不同就可以衍生出一下很多种方法。</li>
</ul>
<h5><span id="分类"><strong>分类</strong></span></h5>
<ul>
<li><strong>单变量过滤方法</strong>：不需要考虑特征之间的相互关系，按照特征变量和目标变量之间的相关性或互信息对特征进行排序，过滤掉最不相关的特征变量。优点是计算效率高、不易过拟合。</li>
<li><strong>多变量过滤方法</strong>：考虑特征之间的相互关系，常用方法有基于相关性和一致性的特征选择</li>
</ul>
<h5><span id="覆盖率">覆盖率</span></h5>
<ul>
<li>即特征在训练集中出现的比例。若覆盖率很小，如有10000个样本，但某个特征只出现了5次，则次覆盖率对模型的预测作用不大，可删除</li>
</ul>
<h5><span id="1方差选择法">（1）<strong>方差选择法</strong></span></h5>
<ul>
<li>先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="comment"># 方差选择法，返回值为特征选择后的数据</span></span><br><span class="line"><span class="comment"># 参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>
<h5><span id="2pearson皮尔森相关系数">（2）Pearson皮尔森相关系数</span></h5>
<p><font color="red"><strong>用于度量两个变量X和Y之间的线性相关性</strong></font></p>
<ul>
<li><strong>用于度量两个变量X和Y之间的线性相关性</strong>，结果的取值区间为[-1,
1]，
-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关性</li>
<li>计算方法为两个变量之间的<strong>协方差</strong>和<strong>标准差</strong>的商</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">  <span class="comment"># 选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">  <span class="comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，</span></span><br><span class="line">  <span class="comment"># 输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。</span></span><br><span class="line">  <span class="comment"># 在此为计算相关系数</span></span><br><span class="line">  <span class="comment"># 其中参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, </span><br><span class="line">              k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h5><span id="3卡方检验">（3）<strong>卡方检验</strong></span></h5>
<p><font color="red">自变量对因变量的相关性</font></p>
<p><strong>检验定性自变量对定性因变量的相关性</strong>。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量:
<span class="math display">\[
\chi^2=\sum \frac{(A-E)^2}{E}
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="comment">#选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h5><span id="4psi互信息法kl散度-相对熵和最大信息系数mutual-information-and-maximal-information-coefficient-mic">（4）PSI互信息法(KL散度、相对熵)和最大信息系数
Mutual information and maximal information coefficient (MIC)</span></h5>
<blockquote>
<p><strong><font color="red">
风控模型—群体稳定性指标(PSI)深入理解应用</font></strong>:https://zhuanlan.zhihu.com/p/79682292</p>
</blockquote>
<p>评价定性自变量对定性因变量的相关性，评价类别型变量对类别型变量的相关性，互信息越大表明两个变量相关性越高，互信息为0时，两个变量相互独立。互信息的计算公式为
<span class="math display">\[
I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x)
p(y)}=D_{K L}(p(x, y) \| p(x) p(y))
\]</span></p>
<h5><span id="5fisher得分">（5）<strong>Fisher得分</strong></span></h5>
<p>对于分类问题, <strong>好的特征应该是在同一个类别中的取值比较相似,
而在不同类别之间的取值差异比较大</strong>。因此特征 <span class="math inline">\(\mathrm{i}\)</span> 的重要性可用Fiser得分 <span class="math inline">\(S_i\)</span> 来表示 <span class="math display">\[
S_i=\frac{\sum_{j=1}^K n_j\left(\mu_{i j}-\mu_i\right)^2}{\sum_{j=1}^K
n_j \rho_{i j}^2}
\]</span> 其中, <span class="math inline">\(u_{i j}\)</span> 和 <span class="math inline">\(\rho_{i j}\)</span> 分别是特征i在类别 <span class="math inline">\(j\)</span> 中均值和方差, <span class="math inline">\(\mu_i\)</span> 为特征i的均值, <span class="math inline">\(n_j\)</span>
为类别中中的样本数。<strong>Fisher得分越高,
特征在不同类别之间的差异性越大、在同一类别中的差异性越小，则特征越重要;</strong></p>
<h5><span id="6相关特征选择">（6）<strong>相关特征选择</strong></span></h5>
<p>该方法基于的假设是，好的特征集合包含跟目标变量非常相关的特征，但这些特征之间彼此不相关</p>
<h5><span id="7最小冗余最大相关性mrmr">（7）<strong>最小冗余最大相关性(
mRMR)</strong></span></h5>
<p>由于单变量过滤法只考虑了单特征变量和目标变量之间的相关性，因此选择的特征子集可能过于冗余。mRMR在进行特征时考虑到了特征之间的冗余性，具体做法是对跟已选择特征相关性较高的冗余特征进行惩罚;</p>
<h4><span id="四-wrapper包装法特征集模型">四、Wrapper(包装法)
【特征集+模型】</span></h4>
<p><img src="https://pic4.zhimg.com/v2-bd321ff1e16c011d1a2bce86a5939a17_b.jpg"></p>
<p><strong>定义</strong></p>
<ul>
<li><p>使用<strong>机器学习算法评估特征子集</strong>的效果，可以检测两个或多个特征之间的交互关系，而且选择的特征子集让模型的效果达到最优。</p></li>
<li><p>这是<strong>特征子集搜索</strong>和<strong>评估指标相结合</strong>的方法。前者提供候选的新特征子集，后者基于新特征子集训练一个模型，并用验证集进行评估，为每一组特征子集进行打分。</p></li>
<li><p>最简单的方法是在<strong>每一个特征子集上训练并评估模型</strong>，从而找出最优的特征子集</p></li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>需要对每一组特征子集训练一个模型，<strong>计算量很大</strong></li>
<li>样本不够充分的情况下<strong>容易过拟合</strong></li>
<li>特征变量较多时计算复杂度太高</li>
</ul>
<h5><span id="1完全搜索">（1）完全搜索</span></h5>
<p>即穷举法, 遍历所有可能的组合达到全局最优, 时间复杂度 <span class="math inline">\(2^n\)</span></p>
<h5><span id="2启发式搜索">（2）启发式搜索</span></h5>
<p>序列向前选择: 特征子集从空集开始, 每次只加入一个特征, 时间复杂度为
<span class="math inline">\(O(n+(n-1)+(n-2)+\ldots+1)=O\left(n^2\right)\)</span></p>
<p>序列向后选择: 特征子集从全集开始, 每次删除一个特征, 时间复杂度为
<span class="math inline">\(O\left(n^2\right)\)</span></p>
<h5><span id="3随机搜索">（3）<strong>随机搜索</strong></span></h5>
<p>执行序列向前或向后选择时，随机选择特征子集</p>
<h5><span id="4递归特征消除法">（4）<strong>递归特征消除法</strong></span></h5>
<p>使用一个基模型进行多轮训练，每轮训练后通过学习器返回的<strong>coef</strong>_或者<strong>feature_importances</strong>_消除若干权重较低的特征，再基于新的特征集进行下一轮训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#递归特征消除法，返回特征选择后的数据</span></span><br><span class="line"><span class="comment">#参数estimator为基模型</span></span><br><span class="line"><span class="comment">#参数n_features_to_select为选择的特征个数</span></span><br><span class="line">RFE(estimator=LogisticRegression(), </span><br><span class="line">    n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, </span><br><span class="line">                                          iris.target)</span><br></pre></td></tr></table></figure>
<h3><span id="五-embedded嵌入法">五、Embedded（嵌入法）</span></h3>
<p>将特征选择嵌入到模型的构建过程中，具有包装法与机器学习算法相结合的优点，也具有过滤法计算效率高的优点</p>
<h5><span id="1lasso方法-l1正则项">（1）LASSO方法 L1正则项</span></h5>
<p>通过对回归系数添加惩罚项来防止过拟合，可以让特定的回归系数变为0，从而可以选择一个不包含那些系数的更简单的模型；实际上，L1惩罚项降维的原理是，在多个对实际上，L1惩罚项降维的原理是，在多个对目标值具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#带L1惩罚项的逻辑回归作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(LogisticRegression(</span><br><span class="line">          penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(</span><br><span class="line">               iris.data,iris.target)</span><br></pre></td></tr></table></figure>
<h5><span id="2基于树模型的特征选择方法">（2）基于树模型的特征选择方法</span></h5>
<ul>
<li>在决策树中，深度较浅的节点一般对应的特征分类能力更强(可以将更多的样本区分开)</li>
<li>对于基于决策树的算法，如随机森林，重要的特征更有可能出现在深度较浅的节点，而且出现的次数可能越多</li>
<li>即可基于树模型中特征出现次数等指标对特征进行重要性排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="comment">#GBDT作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(</span><br><span class="line">    GradientBoostingClassifier()).fit_transform(</span><br><span class="line">      iris.data,iris.target)</span><br></pre></td></tr></table></figure>
<h5><span id="3使用特征重要性来筛选特征的缺陷">（3）使用特征重要性来筛选特征的缺陷？</span></h5>
<ul>
<li>特征重要性只能说明哪些特征在训练时起到作用了，并不能说明特征和目标变量之间一定存在依赖关系。举例来说，随机生成一大堆没用的特征，然后用这些特征来训练模型，一样可以得到特征重要性，但是这个特征重要性并不会全是0，这是完全没有意义的。</li>
<li>特征重要性容易高估数值特征和基数高的类别特征的重要性。这个道理很简单，特征重要度是根据决策树分裂前后节点的不纯度的减少量（基尼系数或者MSE）来算的，那么对于数值特征或者基础高的类别特征，不纯度较少相对来说会比较多。</li>
<li>特征重要度在选择特征时需要决定阈值，要保留多少特征、删去多少特征，这些需要人为决定，并且删掉这些特征后模型的效果也不一定会提升。</li>
</ul>
<h5><span id="4non-importance-选择">（4）Non importance 选择</span></h5>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/16MJEZ4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/16MJEZ4/" class="post-title-link" itemprop="url">特征工程（4）不平衡数据集*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:55:00" itemprop="dateCreated datePublished" datetime="2022-03-28T10:55:00+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:51:47" itemprop="dateModified" datetime="2023-04-26T14:51:47+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="不平衡数据问题">不平衡数据问题</span></h2>
<p><strong>实际上，很多时候，数据不平衡并没有啥负面影响，并不是数据不平衡了，就一定要处理。如果你只是为了做而做，我有99%的自信告诉你，你做了也是白做，啥收益都没有。</strong></p>
<h4><span id="为什么很多模型在训练数据不均衡时会出问题">为什么很多模型在训练数据不均衡时会出问题？</span></h4>
<p><strong>本质原因是</strong>：<strong>模型在训练时优化的目标函数和在测试时使用的评价标准不一致。</strong>这种”不一致“可能是训练数据的样本分布与测试数据分布不一致；</p>
<h3><span id="一-不平衡数据集的主要处理方式"><strong>一、不平衡数据集的主要处理方式？</strong></span></h3>
<h4><span id="11-数据的角度">1.1 <strong>数据的角度</strong></span></h4>
<p>主要方法为采样，分为<strong>欠采样</strong>和<strong>过采样</strong>以及对应的一些改进方法。[<strong>python
imblearn库</strong>]<strong><font color="red">尊重真实样本分布，人为主观引入样本权重，反而可能得出错误的结论。</font></strong></p>
<h5><span id="业务角度"><strong><font color="red">
业务角度</font></strong>：</span></h5>
<ul>
<li><strong>时间因素</strong>，对近期样本提高权重，较远样本降低权重。这是考虑近期样本与未来样本之间的“相似度”更高，希望模型学到更多近期样本的模式。</li>
<li><strong>贷款类型</strong>，不同额度、利率、期限的样本赋予不同权重，这需要结合业务未来的发展方向。例如，未来业务模式希望是小额、短期、低利率，那就提高这批样本的权重。</li>
<li><strong>样本分群</strong>，不同群体赋予不同权重。例如，按流量获客渠道，如果未来流量渠道主要来自平台A，那么就提高这批样本权重。</li>
</ul>
<h5><span id="技术角度"><strong><font color="red">
技术角度：</font></strong></span></h5>
<ul>
<li><p><strong>欠采样</strong>：</p>
<ul>
<li><p><strong>EasyEnsemble</strong>：从多数类<span class="math inline">\(S_{max}\)</span>上随机抽取一个子集，与其他类训练一个分类器；重复若干次，多个分类器融合。</p></li>
<li><p><font color="red"><strong>BalanceCascade</strong></font>：从多数类<span class="math inline">\(S_{max}\)</span>上随机抽取一个子集，与其他类训练一个分类器；<strong>剔除能被分类正确的分类器</strong>，重复若干次，多个分类器融合。</p></li>
<li><p><strong>NearMIss</strong>：利用K邻近信息挑选具有代表性的样本。</p></li>
<li><p><strong>one-side Selection</strong>：采用数据清洗技术。</p></li>
</ul></li>
<li><p><strong>过采样</strong>：</p>
<ul>
<li><p><strong>随机采样</strong></p></li>
<li><p><strong>SMOTE算法</strong>：对少数类<span class="math inline">\(S_{min}\)</span>中每个样本x的K近邻随机选取一个样本y，在x，y的连线上随机选取一个点作为新的样本点。</p></li>
<li><p><strong>Borderline-SMOTE、ADASYN改进算法等</strong></p></li>
</ul></li>
<li><h5><span id="分层抽样技术批量训练分类器的分层抽样技术-当面对不平衡类问题时这种技术通过消除批次内的比例差异可使训练过程更加稳定">分层抽样技术：批量训练分类器的「分层抽样」技术。当面对不平衡类问题时，这种技术（通过消除批次内的比例差异）可使训练过程更加稳定。</span></h5></li>
</ul>
<h4><span id="12-算法的角度"><strong>1.2 算法的角度</strong></span></h4>
<p>考虑<strong>不同误分类情况代价的差异性</strong>对算法进行优化，主要是基于<strong>代价敏感学习算法</strong>(Cost-Sensitive
Learning)，代表的算法有<strong>adacost</strong>。<a href="实现基于代价敏感的AdaCost算法">实现基于代价敏感的AdaCost算法</a></p>
<ul>
<li><p><strong>代价函数</strong>：可以增加小类样本的权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。刚开始，可以设置每个类别的权值与样本个数比例的倒数，然后可以使用过采样进行调优。</p>
<blockquote>
<p>这种方法的难点在于设置合理的权重，实际应用中一般让各个分类间的加权损失值近似相等。当然这并不是通用法则，还是需要具体问题具体分析。</p>
</blockquote></li>
<li><h5><span id="xgb自定义损失函数jhwjhw0123imbalance-xgboostfocalloss">XGB自定义损失函数
/<strong><a target="_blank" rel="noopener" href="https://github.com/jhwjhw0123/Imbalance-XGBoost">Imbalance-XGBoost</a></strong>【Focal
Loss】：</span></h5></li>
</ul>
<p><span class="math display">\[
L_w=-\sum_{i=1}^m \hat{y}_i\left(1-y_i\right)^\gamma \log
\left(y_i\right)+\left(1-\hat{y}_i\right) y_i^\gamma \log
\left(1-y_i\right)
\]</span></p>
<h4><span id="13-分类方式">1.3 <strong>分类方式</strong></span></h4>
<p>可以把小类样本作为<strong>异常点</strong>(outliers)，把问题转化为<strong>异常点检测问题(anomaly
detection)</strong>。此时分类器需要学习到大类的决策分界面，即分类器是一个<strong>单个类分类器（One
Class Classifier）</strong>。代表的算法有<font color="red">
<strong>One-class SVM</strong></font>。</p>
<h5><span id="一类分类算法">一类分类算法：</span></h5>
<p>不平衡数据集的一类分类算法:https://machinelearningmastery.com/one-class-classification-algorithms/</p>
<p>一类分类是机器学习的一个领域，它提供了异常值和异常检测的技术,如何使一类分类算法适应具有严重偏斜类分布的不平衡分类,如何拟合和评估
SVM、隔离森林、椭圆包络、局部异常因子等一类分类算法。</p>
<h5><span id="不平数据集的划分方法">不平数据集的划分方法？</span></h5>
<ul>
<li><p>K折交叉验证？</p></li>
<li><p>自助法？</p></li>
</ul>
<h5><span id="不平数据集的评价方法">不平数据集的评价方法？</span></h5>
<p>G-Mean和ROC曲线和AUC。Topk@P</p>
<ul>
<li><strong>AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。</strong></li>
</ul>
<h3><span id="二-类别不平衡如何得到一个不错的分类器">二、「类别不平衡」如何得到一个不错的分类器？</span></h3>
<blockquote>
<p><strong>微调</strong>：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32940093">如何处理数据中的「类别不平衡」？</a></p>
</blockquote>
<p>机器学习中常常会遇到数据的<strong>类别不平衡（class
imbalance）</strong>，也叫数据偏斜（class
skew）。以常见的二分类问题为例，我们希望预测病人是否得了某种罕见疾病。但在历史数据中，阳性的比例可能很低（如百分之0.1）。在这种情况下，学习出好的分类器是很难的，而且在这种情况下得到结论往往也是很具迷惑性的。</p>
<p>以上面提到的场景来说，如果我们的分类器<strong>总是</strong>预测一个人未患病，即预测为反例，那么我们依然有高达99.9%的预测准确率。然而这种结果是没有意义的，这提出了今天的第一个问题，如何有效在类别不平衡的情况下评估分类器？</p>
<p><strong>当然，本文最终希望解决的问题是：在数据偏斜的情况下，如何得到一个不错的分类器？如果可能，是否可以找到一个较为简单的解决方法，而规避复杂的模型、数据处理，降低我们的工作量。</strong></p>
<h4><span id="21类别不平衡下的评估问题"><strong>2.1
类别不平衡下的评估问题</strong>?</span></h4>
<p>而<strong>当类别不平衡时，准确率就非常具有迷惑性</strong>，而且意义不大。给出几种主流的评估方法：</p>
<ul>
<li><strong>ROC</strong>是一种常见的替代方法，全名receiver operating
curve，计算ROC曲线下的面积是一种主流方法</li>
<li><strong>Precision-recall
curve</strong>和ROC有相似的地方，但定义不同，计算此曲线下的面积也是一种方法</li>
<li><strong>Precision@n</strong>是另一种方法，特制将分类阈值设定得到恰好n个正例时分类器的precision</li>
<li>Average
precision也叫做平均精度，主要描述了precision的一般表现，在异常检测中有时候会用</li>
<li>直接使用Precision也是一种想法，但此时的假设是分类器的阈值是0.5，因此意义不大</li>
</ul>
<blockquote>
<p>本文的目的不是介绍一般的分类评估标准，简单的科普可以参看：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/19645541">如何解释召回率与准确率？</a></p>
</blockquote>
<h4><span id="22解决类别不平衡中的奇淫巧技有什么"><strong>2.2
解决类别不平衡中的“奇淫巧技”有什么？</strong></span></h4>
<p>对于类别不平衡的研究已经有很多年了，在资料[1]中就介绍了很多比较复杂的技巧。结合我的了解举几个简单的例子：</p>
<blockquote>
<p>[1] He, H. and Garcia, E.A., 2009. Learning from imbalanced data.
<em>IEEE Transactions on knowledge and data engineering</em>,
<em>21</em>(9), pp.1263-1284.</p>
</blockquote>
<ul>
<li>对数据进行采用的过程中通过相似性同时生成并插样“少数类别数据”，叫做SMOTE算法</li>
<li>对数据先进行聚类，再将大的簇进行随机欠采样或者小的簇进行数据生成</li>
<li>把监督学习变为无监督学习，舍弃掉标签把问题转化为一个无监督问题，如异常检测</li>
<li><strong>先对多数类别进行随机的欠采样，并结合boosting算法进行集成学习</strong></li>
</ul>
<h4><span id="23简单通用的算法有哪些"><strong>2.3
简单通用的算法有哪些？</strong></span></h4>
<ul>
<li>对较多的那个类别进行欠采样(under-sampling)，舍弃一部分数据，使其与较少类别的数据相当</li>
<li>对较少的类别进行过采样(over-sampling)，重复使用一部分数据，使其与较多类别的数据相当</li>
<li><strong>阈值调整（threshold
moving）</strong>，将原本默认为0.5的阈值调整到
较少类别/（较少类别+较多类别）即可</li>
</ul>
<p>当然很明显我们可以看出，<strong>第一种和第二种方法都会明显的改变数据分布，我们的训练数据假设不再是真实数据的无偏表述</strong>。在第一种方法中，我们<strong>浪费了很多数据</strong>。而第二类方法中有无中生有或者重复使用了数据，会导致过拟合的发生。</p>
<p><strong>因此欠采样的逻辑中往往会结合集成学习来有效的使用数据，假设正例数据n，而反例数据m个。我们可以通过欠采样，随机无重复的生成（k=n/m）个反例子集，并将每个子集都与相同正例数据合并生成k个新的训练样本。我们在k个训练样本上分别训练一个分类器，最终将k个分类器的结果结合起来，比如求平均值。这就是一个简单的思路，也就是Easy
Ensemble [5]。</strong></p>
<p>但不难看出，其实这样的过程是需要花时间处理数据和编程的，对于很多知识和能力有限的人来说难度比较大。特此推荐两个简单易行且效果中上的做法：</p>
<ul>
<li>简单的调整阈值，不对数据进行任何处理。此处特指将分类阈值从0.5调整到正例比例</li>
<li>使用现有的集成学习分类器，如随机森林或者xgboost，<strong>并调整分类阈值</strong></li>
</ul>
<p><strong>提出这样建议的原因有很多。首先，简单的阈值调整从经验上看往往比过采样和欠采样有效</strong>
[6]。其次，如果你对统计学知识掌握有限，而且编程能力一般，在集成过程中更容易出错，还不如使用现有的集成学习并调整分类阈值。</p>
<h4><span id="24一个简单但有效的方案"><strong>2.4
一个简单但有效的方案</strong></span></h4>
<p>经过了上文的分析，我认为一个比较靠谱的解决方案是：</p>
<ul>
<li>不对数据进行过采样和欠采样，但使用现有的集成学习模型，如随机森林</li>
<li>输出随机森林的预测概率，<strong>调整阈值得到最终结果</strong></li>
<li><strong>选择合适的评估标准，如precision@n</strong></li>
</ul>
<h3><span id="三-脉脉数据集不平衡应该思考什么">三、脉脉：数据集不平衡应该思考什么</span></h3>
<p>首先, 猜测一下, 你研究的数据存在着较 大的不平衡,
你还是比较关注正类(少数类) 样本的, 比如【想要识别出 有信用风险 的
人】那么就要谈一下你所说的【模型指标还行】这个问题。<strong>auc这种复合指标先不提,
precision代表的是, 你预测的信用风险人群,
其中有多少是真的信用风险人群。recall 代表的是,
"真的信用风险人群"有多少被你识别出来了</strong>；</p>
<ul>
<li>所以, 倘若你比较关注的是【我想要找出
所有"可能有违约风险的人"】宁可错杀也不
放过。那么你应该重点关注的就是召回率 recall。在此基础上,
尽量提高precision。</li>
<li>你把训练集的正负样本控制在64左右, 那 么你是怎么控制的呢,
是单纯用了数据清理技术, 还是单纯生成了一些新的样本, 还是怎么做的？</li>
<li><font color="red"><strong>如果条件允许, 可以查看一下你被错分的 样本,
看看被错分的原因可能是什么, 是因为类重叠,
还是有少数类的分离还是单纯的因为不平衡比太夸张所以使分类器产生偏倚?</strong>
</font></li>
<li><font color="red">不知道你用的什么模型,
但是现在有一些把重采样和分类器结合在一起的集成学习方法,
可以试试看。</font></li>
<li>维度太高的时候, <strong>特征的提取很重要</strong>呀！</li>
<li>当做异常检测问题可能会好一些?</li>
</ul>
<h3><span id="四-样本准备与权重指标">四、样本准备与权重指标</span></h3>
<blockquote>
<p>样本权重对逻辑回归评分卡的影响探讨:
https://zhuanlan.zhihu.com/p/110982479</p>
</blockquote>
<h4><span id="风控业务背景"><strong>风控业务背景</strong></span></h4>
<p><strong>在统计学习建模分析中，样本非常重要，它是我们洞察世界的窗口</strong>。在建立逻辑回归评分卡时，我们也会考虑对样本赋予不同的权重weight，希望模型学习更有侧重点。</p>
<p>诚然，我们可以通过实际数据测试来检验赋权前后的差异，但我们更希望从理论上分析其合理性。毕竟理论可以指导实践。本文尝试探讨样本权重对逻辑回归评分卡的影响，以及从业务和技术角度分析样本权重调整的操作方法。</p>
<h4><span id="part-1样本加权对woe的影响"><strong>Part 1.
样本加权对WOE的影响</strong></span></h4>
<p>在<strong>《</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80134853">WOE与IV指标的深入理解应用</a><strong>》</strong>一文中,
我们介绍了WOE的概念和计算方法。在逻辑回归评分卡中, 其具有
重要意义。其公式定义如下:</p>
<p><span class="math display">\[
W_O=\ln \left(\frac{\text { Good }_i}{\text { Good }_T} / \frac{\text {
Bad }_i}{\operatorname{Bad}_T}\right)=\ln \left(\frac{\text { Good
}_i}{\text { Bad }_i}\right)-\ln \left(\frac{\text { Good
}_T}{\operatorname{Bad}_T}\right)
\]</span>
<strong>现在，我们思考在计算WOE时，是否要考虑样本权重呢?</strong></p>
<p>如图1所示的样本, 我们希望对某些样本加强学习,
因此对年龄在46岁以下的样本赋予权重1.5, 而对46岁以上的样本赋予权重1.0,
也就是加上权重列weight。此时再计算WOE值,
我们发现数值发生变化。这是因为权重的改 变, 既影响了局部bucket中的
odds，也影响了整体的 odds 。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261249252.jpg" alt="img">我们有2种对样本赋权后的模型训练方案，如图2所示。</p>
<ul>
<li>方案1: WOE变换利用原训练集，LR模型训练时利用加权后的样本。</li>
<li>方案2: WOE变换和LR模型训练时，均使用加权后的样本。</li>
</ul>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261249494.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>个人更倾向于第一种方案，原因在于：<strong>WOE变换侧重变量的可解释性，引入样本权重会引起不可解释的困扰。</strong></p>
<h4><span id="part-2采样对lr系数的影响"><strong>Part 2.
采样对LR系数的影响</strong></span></h4>
<p>我们定义特征向量 <span class="math inline">\(\mathbf{x}=x_1, x_2,
\ldots, x_n\)</span> 。记 <span class="math inline">\(G=\operatorname{Good}, B=B a d\)</span>,
那么逻辑回归的公式组成便是: <span class="math display">\[
\begin{aligned}
&amp; \operatorname{Ln}(\text { odds }(G \mid
\mathbf{x}))=\operatorname{Ln}\left(\frac{p_G f(\mathbf{x} \mid G)}{p_B
f(\mathbf{x} \mid
B)}\right)=\operatorname{Ln}\left(\frac{p_G}{p_B}\right)+\operatorname{Ln}\left(\frac{f(\mathbf{x}
\mid G)}{f(\mathbf{x} \mid B)}\right) \\
&amp;
=\operatorname{Ln}\left(\frac{p_G}{p_B}\right)+\operatorname{Ln}\left(\frac{f\left(x_1,
x_2, \ldots, x_n \mid G\right)}{f\left(x_1, x_2, \ldots, x_n \mid
B\right)}\right) \\
&amp;
=\operatorname{Ln}\left(\frac{p_G}{p_B}\right)+\operatorname{Ln}\left(\frac{f\left(x_1
\mid G\right)}{f\left(x_1 \mid
B\right)}\right)+\operatorname{Ln}\left(\frac{f\left(x_2 \mid
G\right)}{f\left(x_2 \mid
B\right)}\right)+\ldots+\operatorname{Ln}\left(\frac{f\left(x_n \mid
G\right)}{f\left(x_n \mid B\right)}\right) \\
&amp; =L n\left(\text { odd } s_{\text {pop
}}\right)+\operatorname{Ln}\left(\text { odd } s_{i n f
o}(\mathbf{x})\right) \\
&amp;
\end{aligned}
\]</span> 其中, 第2行到第3行的变换是基于朴素贝叶斯假设, 即自变量 <span class="math inline">\(x_i\)</span> 之间相互独立。</p>
<ul>
<li><span class="math inline">\(o d d s_{p o p}\)</span> 是指总体
(训练集) 的 <span class="math inline">\(o d d s\)</span>, 指先验信息
<span class="math inline">\(o d d s\)</span> 。</li>
<li><span class="math inline">\(o d d s_{i n f o}(\mathbf{x})\)</span>
是指自变量引起的 <span class="math inline">\(o d d s\)</span> 变化,
我们称为后验信息 <span class="math inline">\(o d d s 。\)</span></li>
</ul>
<p><font color="red">因此，随着观察信息的不断加入，对群体的好坏 <span class="math inline">\(o d d s\)</span> 判断将越来越趋于客观。</font></p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-0021358b05ebb5fea25073cceea1da2d_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h5><span id="样本权重调整直接影响先验项也就是截距-那对系数的影响呢">样本权重调整直接影响先验项，也就是截距。那对系数的影响呢？</span></h5>
<p>接下来，我们以过采样（Oversampling）和欠采样（Undersampling）为例，分析采样对LR系数的影响。如图4所示，对于不平衡数据集，过采样是指对正样本简单复制很多份；欠采样是指对负样本随机抽样。最终，正负样本的比例将达到1:1平衡状态。</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-c49197fdd37023e75daabd7e74feb11b_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>我们同样从贝叶斯角度进行解释:</strong> <span class="math display">\[
\begin{gathered}
P(B \mid \mathbf{x})=\frac{P(\mathbf{x} \mid B)
P(B)}{P(\mathbf{x})}=\frac{P(\mathbf{x} \mid B) P(B)}{P(\mathbf{x} \mid
B) P(B)+P(\mathbf{x} \mid G) P(G)} \\
\Leftrightarrow \frac{1}{P(B \mid \mathbf{x})}=1+\frac{P(\mathbf{x} \mid
G) P(G)}{P(\mathbf{x} \mid B) P(B)} \\
\Leftrightarrow \operatorname{Ln}\left(\frac{P(\mathbf{x} \mid
G)}{P(\mathbf{x} \mid B)}\right)=\operatorname{Ln}\left(\frac{1}{P(B
\mid
\mathbf{x})}-1\right)-\operatorname{Ln}\left(\frac{P(G)}{P(B)}\right) \\
\Leftrightarrow \operatorname{Ln}\left(\frac{P(G \mid \mathbf{x})}{P(B
\mid \mathbf{x})}\right)=\operatorname{Ln}\left(\frac{P(\mathbf{x} \mid
G)}{P(\mathbf{x} \mid
B)}\right)+\operatorname{Ln}\left(\frac{P(G)}{P(B)}\right)
\end{gathered}
\]</span> 假设采样处理后的训练集为 <span class="math inline">\(\mathbf{x}^{\prime}\)</span> 。记 <span class="math inline">\(\# B\)</span> 和 <span class="math inline">\(\#
G\)</span> 分别表示正负样本数, 那么显然: <span class="math display">\[
o d d s=\frac{\# G}{\# B} \neq \frac{\# G^{\prime}}{\# B^{\prime}}=o d d
s^{\prime}
\]</span> 由于 <span class="math inline">\(\operatorname{Ln}\left(\frac{P(G)}{P(B)}\right)=\operatorname{Ln}\left(\frac{\#
G}{\# B}\right)\)</span> ，因此对应截距将发生变化。</p>
<p><font color="red">无论是过采样, 还是欠采样,
处理后的新样本都和原样本服从同样的分布, 即满足</font>: <span class="math display">\[
\begin{aligned}
&amp; P(\mathbf{x} \mid G)=P\left(\mathbf{x}^{\prime} \mid
G^{\prime}\right) \\
&amp; P(\mathbf{x} \mid B)=P\left(\mathbf{x}^{\prime} \mid
B^{\prime}\right)
\end{aligned}
\]</span> 因此, <span class="math inline">\(\operatorname{Ln}\left(\frac{P(\mathbf{x} \mid
G)}{P(\mathbf{x} \mid
B)}\right)=\operatorname{Ln}\left(\frac{P\left(\mathbf{x}^{\prime} \mid
G^{\prime}\right)}{P\left(\mathbf{x}^{\prime} \mid
B^{\prime}\right)}\right)\)</span>, 即系数不发生变化。</p>
<p><strong>实践证明，按照做评分卡的方式，做WOE变换，然后跑LR，单变量下确实只有截距影响。而对于多变量，理想情况下，当各自变量相互独立时，LR的系数是不变的，但实际自变量之间多少存在一定的相关性，所以还是会有一定的变化。</strong></p>
<h4><span id="part-3样本准备与权重指标"><strong><font color="red"> Part 3.
样本准备与权重指标</font></strong></span></h4>
<p><strong>风控建模的基本假设是末来样本和历史样本的分布是一致的</strong>。模型从历史样本中拟合
<span class="math inline">\(X_{\text {old }}\)</span> 和 <span class="math inline">\(y\)</span> 之间的关系，并 根据末来样本的 <span class="math inline">\(X_{\text {new }}\)</span>
进行预测。<font color="red">因此, 我们总是在思考,
如何选择能代表末来样本的训练样本。</font></p>
<p>如图5所示，不同时间段、不同批次的样本总是存在差异，即<strong>服从不同的总体分布</strong>。因此，我们需要<strong>从多个维度来衡量两个样本集之间的相似性。</strong></p>
<p>从迁移学习的角度看，这是一个从源域（source
domain）中学习模式，并应用到目标域（target
domain）的过程。在这里，源域是训练集，目标域指测试集，或者未来样本。</p>
<p>这就会涉及到一些难点：</p>
<ul>
<li>假设测试集OOT与未来总体分布样本基本一致，但未来样本是不可知且总是在发生变化。</li>
<li>面向测试集效果作为评估指标，会出现在测试集上过拟合现象。 <img src="https://pic4.zhimg.com/80/v2-b40bd56da51ab37d01fbffbdb8bc91f7_1440w.jpg" alt="img"></li>
</ul>
<p><strong>那么，建模中是否可以考虑建立一个权重指标体系，即综合多方面因素进行样本赋权？我们采取2种思路来分析如何开展。</strong></p>
<p><strong><font color="red"> 业务角度</font></strong>：</p>
<ol type="1">
<li><strong>时间因素</strong>，对近期样本提高权重，较远样本降低权重。这是考虑近期样本与未来样本之间的“相似度”更高，希望模型学到更多近期样本的模式。</li>
<li><strong>贷款类型</strong>，不同额度、利率、期限的样本赋予不同权重，这需要结合业务未来的发展方向。例如，未来业务模式希望是小额、短期、低利率，那就提高这批样本的权重。</li>
<li><strong>样本分群</strong>，不同群体赋予不同权重。例如，按流量获客渠道，如果未来流量渠道主要来自平台A，那么就提高这批样本权重。</li>
</ol>
<p>结合以上各维度，可得到总体采样权重的一种融合方式为： <span class="math display">\[
w=w_1 * w_2 * w_3
\]</span>
<strong>这种业务角度的方案虽然解释性强，但实际拍定多大的权重显得非常主观，实践中往往需要不断尝试，缺少一些理论指导。</strong></p>
<p><strong><font color="red"> 技术角度：</font></strong></p>
<ol type="1">
<li><strong>过采样、欠采样等，从样本组成上调整正负不平衡</strong>。</li>
<li><strong>代价敏感学习，在损失函数对好坏样本加上不同的代价。比如，坏样本少，分错代价更高。</strong></li>
<li><strong>借鉴Adaboost的做法，对误判样本在下一轮训练时提高权重。</strong></li>
</ol>
<p>在机器学习中，有一个常见的现象——<strong>Covariate
Shift</strong>，是指当训练集的样本分布和测试集的样本分布不一致的时候，训练得到的模型无法具有很好的泛化
(Generalization) 能力。</p>
<p>其中一种做法，既然是希望让训练集尽可能像测试集，那就让模型帮助我们做这件事。如图6所示，将测试集标记为1，训练集标记为0，训练一个LR模型，在训练集上预测，概率越高，说明这个样例属于测试集的可能性越大。以此达到样本权重调整的目的。</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-6a218af9de452a4c58e36a9e8e6d4bb0_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4><span id="part-4常见工具包的样本赋权"><strong>Part 4.
常见工具包的样本赋权</strong></span></h4>
<p>现有Logistic
Regression模块主要来自sklearn和scipy两个包。很不幸，scipy包并不支持直接赋予权重列。这是为什么呢？有统计学家认为，<strong><font color="red">
尊重真实样本分布，人为主观引入样本权重，反而可能得出错误的结论。</font></strong></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-4942078e60d225438f77362c675bec20_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>因此，我们只能选择用scikit-learn。样本权重是如何体现在模型训练过程呢？查看源码后，发现目前主要是体现在<strong>损失函数</strong>中，即<strong>代价敏感学习。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Logistic loss is the negative of the log of the logistic function.</span></span><br><span class="line"><span class="comment"># 添加L2正则项的逻辑回归对数损失函数</span></span><br><span class="line">out = -np.<span class="built_in">sum</span>(sample_weight * log_logistic(yz)) + <span class="number">.5</span> * alpha * np.dot(w, w)</span><br></pre></td></tr></table></figure>
<p><strong><font color="red">
样本权重对决策分割面的影响:</font></strong></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-699ad7ac670c27a5b0963b8b104ad7fc_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以下是scikit-learn包中的逻辑回归参数列表说明，可以发现调节样本权重的方法有两种：</p>
<ul>
<li>在class_weight参数中使用balanced</li>
<li>在调用fit函数时，通过sample_weight调节每个样本权重。</li>
</ul>
<p>如果同时设置上述2个参数，<strong>那么样本的真正权重是class_weight *
sample_weight.</strong></p>
<p><strong>那么，在评估模型的指标时，是否需要考虑抽样权重，即还原真实场景下的模型评价指标？</strong>笔者认为，最终评估还是需要还原到真实场景下。例如，训练集正负比例被调节为1:1，但这并不是真实的<span class="math inline">\(odds\)</span>，在预测时将会偏高。因此，仍需要进行模型校准。</p>
<h4><span id="part-5-总结"><strong>Part 5. 总结</strong></span></h4>
<p>本文系统整理了样本权重的一些观点，但目前仍然没有统一的答案。据笔者所知，目前在实践中还是采取以下几种方案：</p>
<ol type="1">
<li>尊重原样本分布，不予处理，LR模型训练后即为真实概率估计。</li>
<li>结合权重指标综合确定权重，训练完毕模型后再进行校准，还原至真实概率估计。</li>
</ol>
<p>值得指出的是，大环境总是在发生变化，造成样本分布总在偏移。因此，尽可能增强模型的鲁棒性，以及策略使用时根据实际情况灵活调整，两者相辅相成，可能是最佳的使用方法。欢迎大家一起讨论业界的一些做法。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li>机器学习中不平衡数据的预处理：https://capallen.gitee.io/2019/Deal-with-imbalanced-data-in-ML.html</li>
<li>如何处理数据中的「类别不平衡」？：https://zhuanlan.zhihu.com/p/32940093</li>
<li><strong><font color="blue">不平衡数据集处理方法</font></strong>：https://blog.csdn.net/asialee_bird/article/details/83714612</li>
<li><strong><font color="blue">不平衡数据究竟出了什么问题？</font></strong>：https://www.zhihu.com/column/jiqizhixin</li>
<li>数据挖掘时，当正负样本不均，代码如何实现改变正负样本权重? -
十三的回答 - 知乎
https://www.zhihu.com/question/356640889/answer/2299286791</li>
<li><strong>样本权重对逻辑回归评分卡的影响探讨</strong> -
求是汪在路上的文章 - 知乎 https://zhuanlan.zhihu.com/p/110982479</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3FG16R6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3FG16R6/" class="post-title-link" itemprop="url">模型部署（4）DaaS-Client</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-28 10:53:47" itemprop="dateCreated datePublished" datetime="2022-03-28T10:53:47+08:00">2022-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 15:57:46" itemprop="dateModified" datetime="2023-04-26T15:57:46+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" itemprop="url" rel="index"><span itemprop="name">模型部署</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="how自动部署开源ai模型到生产环境">HOW
自动部署开源AI模型到生产环境？</span></h2>
<h3><span id="一-背景介绍">一、背景介绍</span></h3>
<p>AI的广泛应用是由AI在开源技术的进步推动的，利用功能强大的开源模型库，数据科学家们可以很容易的训练一个性能不错的模型。但是因为模型生产环境和开发环境的不同，涉及到不同角色人员：模型训练是数据科学家和数据分析师的工作，但是模型部署是开发和运维工程师的事情，导致模型上线部署却不是那么容易。</p>
<p><strong>DaaS（Deployment-as-a-Service）是AutoDeployAI公司推出的基于Kubernetes的AI模型自动部署系统，提供一键式自动部署开源AI模型生成REST
API，以方便在生产环境中调用</strong>。下面，我们主要演示在DaaS中如何部署经典机器学习模型，包括<strong>Scikit-learn、XGBoost、LightGBM、和PySpark
ML Pipelines</strong>。关于深度学习模型的部署，会在下一章中介绍。</p>
<p><strong>DMatrix 格式</strong>
在xgboost当中运行速度更快，性能更好。</p>
<h3><span id="二-部署准备">二、部署准备</span></h3>
<p>我们使用DaaS提供的Python客户端（DaaS-Client）来部署模型，对于XGBoost和LightGBM，我们同样使用它们的Python
API来作模型训练。在训练和部署模型之前，我们需要完成以下操作。</p>
<ul>
<li><strong>安装Python DaaS-Client</strong></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade git+https://github.com/autodeployai/daas-client.git</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>初始化DaasClient</strong>。使用DaaS系统的URL、账户、密码登陆系统，文本使用的DaaS演示系统安装在本地的Minikube上。完整Jupyter
Notebook，请参考：<a target="_blank" rel="noopener" href="https://github.com/aipredict/ai-deployment/blob/master/deploy-ai-models-in-daas/deploy-sklearn-xgboost-lightgbm-pyspark.ipynb">deploy-sklearn-xgboost-lightgbm-pyspark.ipynb</a></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from daas_client import DaasClient</span><br><span class="line"></span><br><span class="line">client = DaasClient(&#x27;https://192.168.64.3:30931&#x27;, &#x27;username&#x27;, &#x27;password&#x27;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>创建项目</strong>。DaaS使用项目管理用户不同的分析任务，一个项目中可以包含用户的各种分析资产：模型、部署、程序脚本、数据、数据源等。项目创建成功后，设置为当前活动项目，发布的模型和创建的部署都会存储在该项目下。<code>create_project</code>函数接受三个参数：
<ul>
<li><strong>项目名称</strong>：可以是任意有效的Linux文件目录名。</li>
<li><strong>项目路由</strong>：使用在部署的REST
URL中来唯一表示当前项目，只能是小写英文字符(a-z)，数字(0-9)和中横线<code>-</code>，并且<code>-</code>不能在开头和结尾处。</li>
<li><strong>项目说明</strong>（可选）：可以是任意字符。</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">project = <span class="string">&#x27;部署测试&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> client.project_exists(project):</span><br><span class="line">    client.create_project(project, <span class="string">&#x27;deployment-test&#x27;</span>, <span class="string">&#x27;部署测试项目&#x27;</span>)</span><br><span class="line">client.set_project(project)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>初始化数据</strong>。我们使用流行的分类数据集<code>iris</code>来训练不同的模型，并且把数据分割为训练数据集和测试数据集以方便后续使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">seed = <span class="number">123456</span></span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris_target_name = <span class="string">&#x27;Species&#x27;</span></span><br><span class="line">iris_feature_names = iris.feature_names</span><br><span class="line">iris_df = pd.DataFrame(iris.data, columns=iris_feature_names)</span><br><span class="line">iris_df[iris_target_name] = iris.target</span><br><span class="line"></span><br><span class="line">X, y = iris_df[iris_feature_names], iris_df[iris_target_name]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=seed)    </span><br></pre></td></tr></table></figure>
<ul>
<li><strong>模型部署流程。主要包含以下几步</strong>：
<ul>
<li><strong>训练模型</strong>。使用模型库提供的API，在<code>iris</code>数据集上训练模型。</li>
<li><strong>发布模型</strong>。调用<code>publish</code>函数发布模型到DaaS系统。</li>
<li><strong>测试模型</strong>（可选）。调用<code>test</code>函数获取测试API信息，可以使用任意的REST客户端程序测试模型在DaaS中是否工作正常，使用的是DaaS系统模型测试API。第一次执行<code>test</code>会比较慢，因为DaaS系统需要启动测试运行时环境。</li>
<li><strong>部署模型</strong>。发布成功后，调用<code>deploy</code>函数部署部署模型。可以使用任意的REST客户端程序测试模型部署，使用的是DaaS系统正式部署API。</li>
</ul></li>
</ul>
<h3><span id="三-部署scikit-learn模型">三、部署Scikit-learn模型</span></h3>
<ul>
<li><strong>训练一个Scikit-learn分类模型</strong>：SVC</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">model = SVC(probability=<span class="literal">True</span>, random_state=seed)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>发布Scikit-learn模型</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">publish_resp = client.publish(model,</span><br><span class="line">                            name=<span class="string">&#x27;iris&#x27;</span>,</span><br><span class="line">                            mining_function=<span class="string">&#x27;classification&#x27;</span>,</span><br><span class="line">                            X_test=X_test,</span><br><span class="line">                            y_test=y_test,</span><br><span class="line">                            description=<span class="string">&#x27;A SVC model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>test</strong>函数必须要指定前两个参数，第一个<strong>model</strong>是训练的模型对象，第二个是模型名称，其余是可选参数：</p>
<ul>
<li><strong>mining_function</strong>：指定挖掘功能，可以指定为<code>regression</code>（回归）、<code>classification</code>（分类）、和<code>clustering</code>（聚类）。</li>
<li><strong>X_test和y_test</strong>：指定测试训练集，发布时计算模型评估指标，比如针对分类模型，计算正确率（Accuracy），对于回归模型，计算可释方差（explained
Variance）。</li>
<li><strong>data_test</strong>：
同样是指定测试训练集，但是该参数用在Spark模型上，非Spark模型通过<code>X_test</code>和<code>y_test</code>指定。</li>
<li><strong>description</strong>：模型描述。</li>
<li><strong>params</strong>：记录模型参数设置。</li>
</ul>
<p><strong>publish_resp</strong>是一个字典类型的结果，记录了模型名称，和发布的模型版本。该模型是<code>iris</code>模型的第一个版本。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>&#x27;model_name&#x27;<span class="punctuation">:</span> &#x27;iris&#x27;<span class="punctuation">,</span> &#x27;model_version&#x27;<span class="punctuation">:</span> &#x27;<span class="number">1</span>&#x27;<span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>测试Scikit-learn模型</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_resp = client.test(publish_resp[<span class="string">&#x27;model_name&#x27;</span>],</span><br><span class="line">                        model_version=publish_resp[<span class="string">&#x27;model_version&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><code>test_resp</code>是一个字典类型的结果，记录了测试REST
API信息。如下，其中<code>access_token</code>是访问令牌，一个长字符串，这里没有显示出来。<code>endpoint_url</code>指定测试REST
API地址，<code>payload</code>提供了测试当前模型需要输入的请求正文格式。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;access_token&#x27;<span class="punctuation">:</span> &#x27;A-LONG-STRING-OF-BEARER-TOKEN-USED-IN-HTTP-HEADER-AUTHORIZATION&#x27;<span class="punctuation">,</span></span><br><span class="line">			&#x27;endpoint_url&#x27;<span class="punctuation">:</span> &#x27;https<span class="punctuation">:</span><span class="comment">//192.168.64.3:30931/api/v1/test/deployment-test/daas-python37-faas/test&#x27;,</span></span><br><span class="line">			&#x27;payload&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      				&#x27;args&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">              			&#x27;X&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span> &#x27;petal length (cm)&#x27;<span class="punctuation">:</span> <span class="number">1.5</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;petal width (cm)&#x27;<span class="punctuation">:</span> <span class="number">0.4</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;sepal length (cm)&#x27;<span class="punctuation">:</span> <span class="number">5.7</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;sepal width (cm)&#x27;<span class="punctuation">:</span> <span class="number">4.4</span></span><br><span class="line">                          <span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">               &#x27;model_name&#x27;<span class="punctuation">:</span> &#x27;iris&#x27;<span class="punctuation">,</span></span><br><span class="line">               &#x27;model_version&#x27;<span class="punctuation">:</span> &#x27;<span class="number">1</span>&#x27;<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>使用requests调用测试API，这里我们直接使用<strong>test_resp</strong>返回的测试payload，您也可以使用自定义的数据<code>X</code>，但是参数<code>model_name</code>和<code>model_version</code>必须使用上面输出的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">response = requests.post(test_resp[<span class="string">&#x27;endpoint_url&#x27;</span>],</span><br><span class="line">                        headers=&#123;</span><br><span class="line">      <span class="string">&#x27;Authorization&#x27;</span>: <span class="string">&#x27;Bearer &#123;token&#125;&#x27;</span>.<span class="built_in">format</span>(token=test_resp[<span class="string">&#x27;access_token&#x27;</span>])&#125;,</span><br><span class="line">                        json=test_resp[<span class="string">&#x27;payload&#x27;</span>],</span><br><span class="line">                        verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>返回结果，不同于正式部署API，除了预测结果，测试API会同时返回标准控制台输出和标准错误输出内容，以方便用户碰到错误时，查看相关信息。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># response.json()</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;result&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;PredictedValue&#x27;<span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">            &#x27;Probabilities&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">0.8977133931668801</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="number">0.05476023239878367</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="number">0.047526374434336216</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">			&#x27;stderr&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">			&#x27;stdout&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3><span id="四-部署pyspark模型">四、部署PySpark模型</span></h3>
<p><strong>训练一个PySpark分类模型</strong>：RandomForestClassifier。PySpark模型必须是一个<code>PipelineModel</code>，也就是说必须使用Pipeline来建立模型，哪怕只有一个Pipeline节点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.getOrCreate()</span><br><span class="line">df = spark.createDataFrame(iris_df)</span><br><span class="line"></span><br><span class="line">df_train, df_test = df.randomSplit([<span class="number">0.7</span>, <span class="number">0.3</span>], seed=seed)</span><br><span class="line">assembler = VectorAssembler(inputCols=iris_feature_names,</span><br><span class="line">                            outputCol=<span class="string">&#x27;features&#x27;</span>)</span><br><span class="line"></span><br><span class="line">rf = RandomForestClassifier(seed=seed).setLabelCol(iris_target_name)</span><br><span class="line">pipe = Pipeline(stages=[assembler, rf])</span><br><span class="line">model = pipe.fit(df_train)</span><br></pre></td></tr></table></figure>
<p><strong>发布PySpark模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">publish_resp = client.publish(model,</span><br><span class="line">                            name=<span class="string">&#x27;iris&#x27;</span>,</span><br><span class="line">                            mining_function=<span class="string">&#x27;classification&#x27;</span>,</span><br><span class="line">                            data_test=df_test,</span><br><span class="line">                            description=<span class="string">&#x27;A RandomForestClassifier of Spark model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="五-模型部署管理">五、模型部署管理</span></h3>
<p>打开浏览器，登陆DaaS管理系统。进入项目<code>部署测试</code>，切换到<code>模型</code>标签页，有一个<code>iris</code>模型，最新版本是<code>v4</code>，类型是<code>Spark</code>即我们最后发布的模型。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261549637.jpeg" alt="Daas-models" style="zoom: 33%;"></p>
<p>点击模型，进入模型主页（概述）。当前<code>v4</code>是一个Spark
Pipeline模型，正确率是94.23%，并且显示了<code>iris</code>不同版本正确率历史图。下面罗列了模型的输入和输出变量，以及评估结果，当前为空，因为还没有在DaaS中执行任何的模型评估任务。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261549145.jpeg" alt="Daas-model-overview-v4" style="zoom: 50%;"></p>
<p>点击<code>v4</code>，可以自由切换到其他版本。比如，切换到<code>v1</code>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261549781.png" alt="DaaS-model-versions" style="zoom:50%;"></p>
<p><code>v1</code>版本是一个Scikit-learn
SVM分类模型，正确率是98.00%。其他信息与<code>v4</code>类似。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261549645.jpeg" alt="DaaS-model-overview-v1" style="zoom:50%;"></p>
<p>切换到模型<code>部署</code>标签页，有一个我们刚才创建的部署<code>iris-svc</code>，鼠标移动到操作菜单，选择<code>修改设置</code>。可以看到，当前部署服务关联的是模型<code>v1</code>，就是我们刚才通过<code>deploy</code>函数部署的<code>iris</code>第一个版本Scikit-learn模型。选择最新的<code>v4</code>，点击命令<code>保存并且重新部署</code>，该部署就会切换到<code>v4</code>版本。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li>DaaS-Client、Sklearn、XGBoost、LightGBM、和PySpark_关注 AI/ML
模型上线、模型部署-程序员宅基地_</li>
<li>DaaS-Client：https://github.com/autodeployai/daas-client</li>
<li>3万字长文
PySpark入门级学习教程，框架思维:https://zhuanlan.zhihu.com/p/395431025</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/8AJ5QK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/8AJ5QK/" class="post-title-link" itemprop="url">深度学习Q&A</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:13:46" itemprop="dateCreated datePublished" datetime="2022-03-24T14:13:46+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-06 20:39:50" itemprop="dateModified" datetime="2022-07-06T20:39:50+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="残差神经网络为什么可以缓解梯度消失">残差神经网络为什么可以缓解梯度消失？</span></h2>
<blockquote>

</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3FW9EBT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3FW9EBT/" class="post-title-link" itemprop="url">异常检测（3）HBOS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:12:56" itemprop="dateCreated datePublished" datetime="2022-03-24T14:12:56+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 15:24:27" itemprop="dateModified" datetime="2023-04-26T15:24:27+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">异常检测</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-hbos">一、HBOS</span></h3>
<p><strong>HBOS全名为：Histogram-based Outlier
Score</strong>。它是一种单变量方法的组合，不能对特征之间的依赖关系进行建模，但是计算速度较快，对大数据集友好，其基本假设是数据集的每个维度相互独立，然后对<strong>每个维度进行区间(bin)划分，区间的密度越高，异常评分越低。理解了这句话，基本就理解了这个算法。</strong></p>
<h4><span id="11-hbos算法流程"><strong>1.1 HBOS算法流程</strong></span></h4>
<h5><span id="1-静态宽度直方图"><strong>1、静态宽度直方图</strong></span></h5>
<p>标准的直方图构建方法，在值范围内使用k个等宽箱，样本落入每个箱的频率（相对数量）作为密度（箱子高度）的估计，时间复杂度：O(n)</p>
<p><strong>注意：</strong>等宽分箱，每个箱中的数据宽度相同，不是指数据个数相同。例如序列[5,10,11,13,15,35,50,55,72,92,204,215]，数据集中最大值是215，最小值是5，分成3个箱，故每个箱的宽度应该为（215-5）/3=70，所以箱的宽度是70，这就要求箱中数据之差不能超过70，并且要把不超过70的数据全放在一起，最后的分箱结果如下：</p>
<p><strong>箱一：5,10，11,13,15,35,50,55,72；箱二：92；箱三：204,215</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026218.png" alt="图片" style="zoom:48%;"></p>
<h5><span id="2-动态宽度直方图"><strong>2、动态宽度直方图</strong></span></h5>
<p>首先对所有值进行排序，然后固定数量的N/k 个连续值装进一个箱里，其
中N是总实例数，k是箱个数，<strong>直方图中的箱面积表示实例数</strong>，因为箱的宽度是由箱中第一个值和最后一个值决定的，所有箱的面积都一样，因此每一个箱的高度都是可计算的。这意味着跨度大的箱的高度低，即密度小，只有一种情况例外，超过k个数相等，此时允许在同一个箱里超过N/k值，时间复杂度：O(n×log(n))</p>
<p>还是用序列[<strong>5,10,11,13,15</strong>,<strong>35,50,55,72</strong>,92,204,215]举例，也是假如分3箱，那么每箱都是4个，宽度为边缘之差，第一个差为15-5=10，第二差为72-35=37，第三个箱宽为215-92=123，为了保持面积相等，所以导致后面的很矮，前面的比较高，如下图所示（非严格按照规则）：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026012.png" alt="图片" style="zoom:50%;"></p>
<h5><span id="3-算法推导过程"><strong>3、算法推导过程</strong></span></h5>
<p><strong>对每个维度都计算了一个独立的直方图，其中每个箱子的高度表示密度的估计，然后为了使得最大高度为1（确保了每个特征与异常值得分的权重相等），对直方图进行归一化处理。</strong>最后，每一个实例的HBOS值由以下公式计算：
<span class="math display">\[
H B O S(p)=\sum_{i=0}^d \log
\left(\frac{1}{\operatorname{hist}_i(p)}\right)
\]</span> 推导过程: 假设样本 <span class="math inline">\(\mathrm{p}\)</span> 第 <span class="math inline">\(\mathrm{i}\)</span> 个特征的概率密度为 pi ( <span class="math inline">\(p\)</span> ) , 则p的概率密度可以计算为, <span class="math inline">\(d\)</span> 为总的特征的个数： <span class="math display">\[
P(p)=P_1(p) P_2(p) \cdots P_d(p)
\]</span> 两边取对数： <span class="math display">\[
\log (P(p))=\log \left(P_1(p) P_2(p) \cdots P_d(p)\right)=\sum_{i=1}^d
\log \left(P_i(p)\right)
\]</span> 概率密度越大，异常评分越小，为了方便评分，两边乘以“-1”: <span class="math display">\[
-\log (P(p))=-1 \sum_{i=1}^d \log \left(P_t(p)\right)=\sum_{i=1}^d
\frac{1}{\log \left(P_i(p)\right)}
\]</span> 最后可得： <span class="math display">\[
H B O S(p)=-\log (P(p))=\sum_{i=1}^d \frac{1}{\log \left(P_i(p)\right)}
\]</span>
PyOD是一个可扩展的Python工具包，用于检测多变量数据中的异常值。它可以在一个详细记录API下访问大约
20 个离群值检测算法。</p>
<h3><span id="三-xgbod">三、 XGBOD</span></h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349519844">【异常检测】<em>XGBOD</em>：用无监督表示学习改进有监督异常检测</a></p>
</blockquote>
<h3><span id="四-copod用统计机器学习检测异常">四、COPOD：用「统计」+「机器学习」检测异常</span></h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338189299">COPOD：用「统计」+「机器学习」检测异常</a></p>
</blockquote>
<h3><span id="五-more-about-anomalydetection">五、More about Anomaly
Detection</span></h3>
<p>那这个异常检测啊,其实也是另外一门学问,那我们课堂上就没有时间讲了,异常检测不是只能用
Aauto-Encoder 这个技术,Aauto-Encoder
这个技术,只是众多可能方法里面的其中一个,我们拿它来当做 Aauto-Encoder
的作业,因为我相信,你未来有很多的机会用得上异常检测这个技术,那实际上有关异常检测更完整的介绍,我们把过去上课的录影放在这边,给大家参考,</p>
<p>Part 1: https://youtu.be/gDp2LXGnVLQ</p>
<ul>
<li><strong>简介</strong></li>
</ul>
<p>Part 2: https://youtu.be/cYrNjLxkoXs</p>
<ul>
<li><strong>信心分数</strong></li>
</ul>
<p>Part 3: https://youtu.be/ueDlm2FkCnw</p>
<ul>
<li><p>异常检测系统的评估？</p>
<ul>
<li><p>no ACC</p></li>
<li><p>cost loss设计</p></li>
<li><p>RUC</p></li>
</ul></li>
</ul>
<p>Part 4: https://youtu.be/XwkHOUPbc0Q</p>
<p>Part 5: https://youtu.be/Fh1xFBktRLQ</p>
<ul>
<li>无监督</li>
</ul>
<p>Part 6: https://youtu.be/LmFWzmn2rFY</p>
<p>Part 7: https://youtu.be/6W8FqUGYyDo</p>
<p>那以上就是有关 Aauto-Encoder 的部分</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/19/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><span class="page-number current">20</span><a class="page-number" href="/page/21/">21</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/21/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
