<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/23/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/23/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/23/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">267</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">77</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1M5VH4X/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1M5VH4X/" class="post-title-link" itemprop="url">深度学习（6）LSTM*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:45:12" itemprop="dateCreated datePublished" datetime="2022-03-16T21:45:12+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 22:55:53" itemprop="dateModified" datetime="2022-07-13T22:55:53+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN/" itemprop="url" rel="index"><span itemprop="name">RNN</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="lstm和gru算法简单梳理">LSTM和GRU算法简单梳理🍭</span></h2>
<h3><span id="前言-从反向传播推导到梯度消失and爆炸的原因及解决方案"><font color="red">
前言 - 从反向传播推导到梯度消失and爆炸的原因及解决方案？</font></span></h3>
<blockquote>
<ul>
<li>从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导）
- 韦伟的文章 - 知乎 https://zhuanlan.zhihu.com/p/76772734</li>
</ul>
<p><strong>本质上</strong>是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。其实<strong>梯度爆炸和梯度消失问题都是因为网络太深</strong>，网络权值更新不稳定造成的，本质上是因为<strong>梯度反向传播中的连乘效应</strong>。</p>
</blockquote>
<h3><span id="一-反向传播推导到梯度消失and爆炸的原因及解决方案">一、反向传播推导到梯度消失and爆炸的原因及解决方案</span></h3>
<h4><span id="11-反向传播推导">1.1 ==反向传播推导：==</span></h4>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以上图为例开始推起来，先说明几点，i1，i2是输入节点，h1，h2为隐藏层节点，o1，o2为输出层节点，除了输入层，其他两层的节点结构为下图所示：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>举例说明，<img src="https://www.zhihu.com/equation?tex=NET_%7Bo1%7D" alt="[公式]"> 为输出层的输入，也就是隐藏层的输出经过线性变换后的值，
<img src="https://www.zhihu.com/equation?tex=OUT_%7Bo1%7D" alt="[公式]"> 为经过激活函数sigmoid后的值；同理 <img src="https://www.zhihu.com/equation?tex=NET_%7Bh1%7D" alt="[公式]">
为隐藏层的输入，也就是输入层经过线性变换后的值， <img src="https://www.zhihu.com/equation?tex=OUT_%7Bh1%7D" alt="[公式]">
为经过激活函数sigmoid 的值。只有这两层有激活函数，输入层没有。</p>
<blockquote>
<p><strong>定义一下sigmoid的函数：</strong> <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29+%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D" alt="[公式]"> <strong>说一下sigmoid的求导：</strong></p>
</blockquote>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csigma%5E%7B%5Cprime%7D%28z%29+%26%3D%5Cleft%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D%5Cright%29%5E%7B%5Cprime%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7B-z%7D%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B1%2Be%5E%7B-z%7D-1%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B%5Csigma%28z%29%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Csigma%28z%29%281-%5Csigma%28z%29%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>定义一下损失函数，这里的损失函数是均方误差函数，即：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D%5Csum+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget+-+output%7D%29%5E%7B2%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>具体到上图，就是：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget1+-+out_o1%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget2+-+out_o2%7D%29%5E%7B2%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>到这里，所有前提就交代清楚了，前向传播就不推了，默认大家都会，下面推反向传播。</p>
<ul>
<li><strong>第一个反向传播（热身）</strong></li>
</ul>
<p>先来一个简单的热热身，求一下损失函数对W5的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D" alt="[公式]"></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>首先根据链式求导法则写出对W5求偏导的总公式，再把图拿下来对照（如上），可以看出，需要计算三部分的求导【损失函数、激活函数、线性函数】，下面就一步一步来：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_%7B5%7D%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B2%7D%28%7Btarget_1+-+out_%7Bo1%7D%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28+%7Btarget_2+-+out_%7Bo2%7D%7D%29%5E%7B2%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bo1%7D%7D%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+w_5%7D+%3Dout_%7Bh_1%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>综上三个步骤，得到总公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%28out_%7Bo1%7D+-+target_1%29+%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+out_%7Bh_1%7D++%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<ul>
<li><strong>第二个反向传播：</strong></li>
</ul>
<p>接下来，要求损失函数对w1的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D" alt="[公式]"></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>还是把图摆在这，方便看，先写出总公式，对w1求导有个地方要注意，w1的影响不仅来自o1还来自o2，从图上可以一目了然，所以总公式为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D%2B%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo2%7D%7D%7B%5Cpartial+net_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo2%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>所以总共分为左右两个式子，分别又对应5个步骤，详细写一下左边，右边同理：</p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]"></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3Dw_5%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%3D+%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+i_1w_1%2Bi_2w_2%7D%7B%5Cpartial+w_1%7D+%3Di_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>右边也是同理，就不详细写了，写一下总的公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D++%26%3D%5Cleft%28%28out_%7Bo1%7D+-+target_1%29%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+w_5+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29+%5C%5C+%26%2B%7B%5Cleft%28%28out_%7Bo2%7D+-+target_2%29%5Ccdot+%28%5Csigma%28net_%7Bo2%7D%29%281-%5Csigma%28net_%7Bo2%7D%29%29%29%5Ccdot+w_7+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29%7D+%5C%5C+%5Cend%7Baligned%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>这个公式只是对如此简单的一个网络结构的一个节点的偏导，就这么复杂。。亲自推完才深深的意识到。。。</p>
<p>为了后面描述方便，把上面的公式化简一下， <img src="https://www.zhihu.com/equation?tex=out_%7Bo1%7D+-+target_1" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]"> ，
<img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D" alt="[公式]"> ，则：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+C_%7Bo1%7D+%5Ccdot+%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_5+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1+%2B+C_%7Bo2%7D+%5Ccdot+%5Csigma%28net_%7Bo2%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_7+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="12梯度消失爆炸产生原因">1.2
<strong>==梯度消失，爆炸产生原因：==</strong></span></h4>
<p>从上式其实已经能看出来，求和操作其实不影响，主要是是看乘法操作就可以说明问题，可以看出，损失函数对w1的偏导，与
<img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]">
，权重w，sigmoid的导数有关，明明还有输入i为什么不提？因为如果是多层神经网络的中间某层的某个节点，那么就没有输入什么事了。所以产生影响的就是刚刚提的三个因素。</p>
<p>再详细点描述，如图，多层神经网络：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-0f2ded75fbecc449a25bfd58b8c58d35_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25631496">PENG：神经网络训练中的梯度消失与梯度爆炸282
赞同 · 26 评论文章</a></p>
<p>假设（假设每一层只有一个神经元且对于每一层 <img src="https://www.zhihu.com/equation?tex=y_i%3D%5Csigma%5Cleft%28z_i%5Cright%29%3D%5Csigma%5Cleft%28w_ix_i%2Bb_i%5Cright%29" alt="[公式]">，其中<img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]">为sigmoid函数），如图：</p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-ea9beb6c28c7d4e89be89dc5f4cbae2e_1440w.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>则：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+y_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+y_%7B4%7D%7D%7B%5Cpartial+z_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B4%7D%7D%7B%5Cpartial+x_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B4%7D%7D%7B%5Cpartial+z_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B3%7D%7D%7B%5Cpartial+x_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B3%7D%7D%7B%5Cpartial+z_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B2%7D%7D%7B%5Cpartial+x_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B2%7D%7D%7B%5Cpartial+z_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B1%7D%7D%7B%5Cpartial+b_%7B1%7D%7D%7D+%5C%5C+%7B%3DC_%7By4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B4%7D%5Cright%29+w_%7B4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B3%7D%5Cright%29+w_%7B3%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B2%7D%5Cright%29+w_%7B2%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B1%7D%5Cright%29%7D%5Cend%7Barray%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>看一下sigmoid函数的求导之后的样子：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-208a4aa5dc657fe86919f3549d853793_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>发现sigmoid函数求导后最大最大也只能是0.25。</strong></p>
<p>再来看W，一般我们初始化权重参数W时，通常都小于1，用的最多的应该是0，1正态分布吧。</p>
<p><font color="red"><strong>所以 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq0.25" alt="[公式]">
，多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说几乎不更新，这就是梯度消失的根本原因。</strong></font></p>
<p>再来看看<strong>梯度爆炸</strong>的原因，也就是说如果 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cgeq1" alt="[公式]">
时，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。sigmoid的函数是不可能大于1了，上图看的很清楚，那只能是w了，这也就是经常看到别人博客里的一句话，初始权重过大，一直不理解为啥。。现在明白了。</p>
<p>但梯度爆炸的情况一般不会发生，对于sigmoid函数来说， <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29%5E%7B%5Cprime%7D" alt="[公式]"> 的大小也与w有关，因为 <img src="https://www.zhihu.com/equation?tex=z%3Dwx%2Bb" alt="[公式]">
，除非该层的输入值<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">在一直一个比较小的范围内。</p>
<p>其实<strong>梯度爆炸和梯度消失问题都是因为网络太深</strong>，网络权值更新不稳定造成的，本质上是因为<strong>梯度反向传播中的连乘效应</strong>。</p>
<p>==<strong>所以，总结一下，为什么会发生梯度爆炸和消失：</strong>==</p>
<blockquote>
<p>本质上是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。</p>
</blockquote>
<h3><span id="13-梯度消失-爆炸解决方案">1.3 梯度消失、爆炸解决方案？</span></h3>
<blockquote>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33006526">DoubleV：详解深度学习中的梯度消失、爆炸原因及其解决方法</a></p>
<ul>
<li>预训练加微调</li>
<li>梯度剪切、正则</li>
</ul>
</blockquote>
<h4><span id="解决方案一预训练加微调"><strong>解决方案一（预训练加微调）：</strong></span></h4>
<p>提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（<strong>fine-tunning</strong>）。</p>
<p>Hinton在训练深度信念网络（Deep Belief
Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p>
<h4><span id="解决方案二梯度剪切-正则"><strong>解决方案二（梯度剪切、正则）：</strong></span></h4>
<p><strong>梯度剪切</strong>这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p>
<p><strong>正则化</strong>是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss%3D%28y-W%5ETx%29%5E2%2B+%5Calpha+%7C%7CW%7C%7C%5E2%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]">
是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</p>
<p>注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些</p>
<h4><span id="解决方案三改变激活函数"><strong><font color="red">
解决方案三（改变激活函数）：</font></strong></span></h4>
<p>首先说明一点，<strong>tanh激活函数不能有效的改善这个问题</strong>，先来看tanh的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Ctanh+%28x%29%3D%5Cfrac%7Be%5E%7Bx%7D-e%5E%7B-x%7D%7D%7Be%5E%7Bx%7D%2Be%5E%7B-x%7D%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>再来看tanh的导数图像：</p>
<p><img src="https://pic4.zhimg.com/80/v2-66a7e4fcf11a2d85c15e7bf7b88b2d1b_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>发现虽然比sigmoid的好一点，sigmoid的最大值小于0.25，tanh的最大值小于1，但仍是小于1的，所以并不能解决这个问题。</strong></p>
<p><strong>Relu</strong>:思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Coperatorname%7BRe%7D+%5Coperatorname%7Blu%7D%28%5Cmathrm%7Bx%7D%29%3D%5Cmax+%28%5Cmathrm%7Bx%7D%2C+0%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D%7B0%2C+x%3C0%7D+%5C%5C+%7Bx%2C+x%3E0%7D%5Cend%7Barray%7D%5Cright%5C%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-55475ee2d90cd7257a39f62549a65769_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>从上图中，我们可以很容易看出，<strong>relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</strong></p>
<p><strong>relu</strong>的主要贡献在于：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练</li>
</ul>
<p>同时也存在一些<strong>缺点</strong>：</p>
<ul>
<li><strong>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</strong></li>
<li>输出不是以0为中心的</li>
</ul>
<p><strong>leakrelu</strong></p>
<p>leakrelu就是为了解决relu的0区间带来的影响，其数学表达为： <img src="https://www.zhihu.com/equation?tex=leakrelu%3D%5Cbegin%7Bequation%7D+f%28x%29%3D+%5Cbegin%7Bcases%7D+x%2C+%26+%7Bx%5Cgt+0%7D+%5C%5C%5C%5C+x%2Ak%2C+%26+%7Bx%5Cleq+0%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]">
其中k是leak系数，一般选择0.1或者0.2，或者通过学习而来解决死神经元的问题。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3ab1bd8fb85542a0c85eb907b73fa327_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>leakrelu解决了0区间带来的影响，而且包含了relu的所有优点</p>
<p><strong>elu</strong></p>
<p>elu激活函数也是为了解决relu的0区间带来的影响，其数学表达为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7Bx%2C%7D+%26+%7B%5Ctext+%7B+if+%7D+x%3E0%7D+%5C%5C+%7B%5Calpha%5Cleft%28e%5E%7Bx%7D-1%5Cright%29%2C%7D+%26+%7B%5Ctext+%7B+otherwise+%7D%7D%5Cend%7Barray%7D%5Cright.%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其函数及其导数数学形式为：</p>
<p><img src="https://pic3.zhimg.com/80/v2-ec3c80e51129bd76d49cad6e52d449c2_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>但是elu相对于leakrelu来说，计算要更耗时间一些，因为有e。</p>
<h4><span id="解决方案四batchnorm梯度消失"><strong>解决方案四（batchnorm）：</strong>【梯度消失】</span></h4>
<p><strong>Batchnorm</strong>是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch
normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。</p>
<p>具体的batchnorm原理非常复杂，在这里不做详细展开，此部分大概讲一下batchnorm解决梯度的问题上。具体来说就是反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：
正向传播中<img src="https://www.zhihu.com/equation?tex=f_3%3Df_2%28w%5ET%2Ax%2Bb%29" alt="[公式]">，那么反向传播中，<img src="https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cpartial+f_2%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+f_1%7Dw" alt="[公式]">，反向传播式子中有w的存在，所以<img src="https://www.zhihu.com/equation?tex=w" alt="[公式]">的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出做scale和shift的方法，通过一定的规范化手段，<strong>把每层神经网络任意神经元这个输入值的分布【假设原始是正态分布】强行拉回到接近均值为0方差为1的标准正太分布，即严重偏离的分布强制拉回比较标准的分布，<font color="red">
这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生</font>，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<h4><span id="解决方案五残差结构"><strong><font color="red">
解决方案五（残差结构）：</font></strong></span></h4>
<p><img src="https://pic4.zhimg.com/80/v2-3134d24348c47ca2001d37fef1c3f8bf_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>如图，把输入加入到某层中，这样求导时，总会有个1在，这样就不会梯度消失了。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot+%5Cfrac%7B%5Cpartial+x_%7BL%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot%5Cleft%281%2B%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D+F%5Cleft%28x_%7Bi%7D%2C+W_%7Bi%7D%5Cright%29%5Cright%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>式子的第一个因子 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D" alt="[公式]"> 表示的损失函数到达 L
的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p>
<p><code>注：上面的推导并不是严格的证</code>，只为帮助理解</p>
<p>==<strong>解决方案六（LSTM）：</strong>==</p>
<p>在介绍这个方案之前，有必要来推导一下RNN的反向传播，<strong>因为关于梯度消失的含义它跟DNN不一样！不一样！不一样！</strong></p>
<p>先推导再来说，从这copy的：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28687529">沉默中的思索：RNN梯度消失和爆炸的原因565
赞同</a></p>
<p>RNN结构如图：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-ab844e07a86f910d2852198c3117ddb7_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>假设我们的时间序列只有三段， <img src="https://www.zhihu.com/equation?tex=S_%7B0%7D" alt="[公式]">
为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下： <img src="https://www.zhihu.com/equation?tex=S_%7B1%7D%3DW_%7Bx%7DX_%7B1%7D%2BW_%7Bs%7DS_%7B0%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B1%7D%3DW_%7Bo%7DS_%7B1%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B2%7D%3DW_%7Bx%7DX_%7B2%7D%2BW_%7Bs%7DS_%7B1%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B2%7D%3DW_%7Bo%7DS_%7B2%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B3%7D%3DW_%7Bx%7DX_%7B3%7D%2BW_%7Bs%7DS_%7B2%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B3%7D%3DW_%7Bo%7DS_%7B3%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p>假设在t=3时刻，损失函数为 <img src="https://www.zhihu.com/equation?tex=L_%7B3%7D%3D%5Cfrac%7B1%7D%7B2%7D%28Y_%7B3%7D-O_%7B3%7D%29%5E%7B2%7D" alt="[公式]"> 。</p>
<p>则对于一次训练任务的损失函数为 <img src="https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bt%3D0%7D%5E%7BT%7D%7BL_%7Bt%7D%7D" alt="[公式]"> ，即每一时刻损失值的累加。</p>
<p>使用随机梯度下降法训练RNN其实就是对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D+" alt="[公式]"> 、
<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]">
、 <img src="https://www.zhihu.com/equation?tex=W_%7Bo%7D" alt="[公式]"> 以及 <img src="https://www.zhihu.com/equation?tex=b_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=b_%7B2%7D" alt="[公式]">
求偏导，并不断调整它们以使L尽可能达到最小的过程。</p>
<p>现在假设我们我们的时间序列只有三段，t1，t2，t3。</p>
<p><strong>我们只对t3时刻的 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D%E3%80%81W_%7B0%7D" alt="[公式]"> 求偏导（其他时刻类似）：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7B0%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bo%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>可以看出对于 <img src="https://www.zhihu.com/equation?tex=W_%7B0%7D" alt="[公式]">
求偏导并没有长期依赖，但是对于 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导，会随着时间序列产生长期依赖</strong>。因为 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]">
随着时间序列向前传播，而 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]"> 又是
<img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]">的函数。</p>
<p>根据上述求偏导的过程，我们可以得出任意时刻对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导的公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Csum_%7Bk%3D0%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%7B%5Cpartial%7BS_%7Bt%7D%7D%7D%7D%28%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D%29%5Cfrac%7B%5Cpartial%7BS_%7Bk%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>任意时刻对<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]"> 求偏导的公式同上。</p>
<p><font color="red"> 如果加上激活函数， <img src="https://www.zhihu.com/equation?tex=S_%7Bj%7D%3Dtanh%28W_%7Bx%7DX_%7Bj%7D%2BW_%7Bs%7DS_%7Bj-1%7D%2Bb_%7B1%7D%29" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D" alt="[公式]"> = <img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7Btanh%5E%7B%27%7D%7DW_%7Bs%7D" alt="[公式]">激活函数tanh和它的导数图像在上面已经说过了，所以原因在这就不赘述了，还是一样的，激活函数导数小于1。</font></p>
<blockquote>
<p>==<strong>现在来解释一下，为什么说RNN和DNN的梯度消失问题含义不一样？</strong>==</p>
<ol type="1">
<li><strong>先来说DNN中的反向传播：</strong>在上文的DNN反向传播中，我推导了两个权重的梯度，第一个梯度是直接连接着输出层的梯度，求解起来并没有梯度消失或爆炸的问题，因为它没有连乘，只需要计算一步。第二个梯度出现了连乘，也就是说越靠近输入层的权重，梯度消失或爆炸的问题越严重，可能就会消失会爆炸。<strong>一句话总结一下，DNN中各个权重的梯度是独立的，该消失的就会消失，不会消失的就不会消失。</strong></li>
<li><strong>再来说RNN：</strong>RNN的特殊性在于，它的权重是共享的。抛开W_o不谈，因为它在某时刻的梯度不会出现问题（某时刻并不依赖于前面的时刻），但是W_s和W_x就不一样了，每一时刻都由前面所有时刻共同决定，是一个相加的过程，这样的话就有个问题，当距离长了，计算最前面的导数时，最前面的导数就会消失或爆炸，但当前时刻整体的梯度并不会消失，因为它是求和的过程，当下的梯度总会在，只是前面的梯度没了，但是更新时，由于权值共享，所以整体的梯度还是会更新，<strong>通常人们所说的梯度消失就是指的这个，指的是当下梯度更新时，用不到前面的信息了，因为距离长了，前面的梯度就会消失，也就是没有前面的信息了，但要知道，整体的梯度并不会消失，因为当下的梯度还在，并没有消失。</strong></li>
<li><strong>一句话概括：</strong>RNN的梯度不会消失，RNN的梯度消失指的是当下梯度用不到前面的梯度了，但DNN靠近输入的权重的梯度是真的会消失。</li>
</ol>
</blockquote>
<p>说完了RNN的反向传播及梯度消失的含义，终于该说<strong>为什么LSTM可以解决这个问题了</strong>，这里默认大家都懂LSTM的结构，对结构不做过多的描述。<strong>见第三节</strong>。【LSTM通过它的“门控装置”有效的缓解了这个问题，这也就是为什么我们现在都在使用LSTM而非普通RNN。】</p>
<h3><span id="二-lstm-框架结构">二、LSTM 框架结构</span></h3>
<h4><span id="前言">前言：</span></h4>
<blockquote>
<p>LSTM是RNN的一种变体，更高级的RNN，那么它的本质还是一样的，还记得RNN的特点吗，<strong>可以有效的处理序列数据，</strong>当然LSTM也可以，还记得RNN是如何处理有效数据的吗，是不是<strong>每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息</strong>，如图，我们把存每一时刻信息的地方叫做Memory
Cell，中文就是记忆细胞，可以这么理解。</p>
<p><img src="https://pic3.zhimg.com/80/v2-a5590a65bf7a93bbaafc1a6b03cf3862_1440w.png" alt="img" style="zoom:50%;"></p>
<p><strong>RNN什么信息它都存下来，因为它没有挑选的能力，而LSTM不一样，它会选择性的存储信息，因为它能力强，它有门控装置，它可以尽情的选择。</strong>如下图，普通RNN只有中间的Memory
Cell用来存所有的信息，而从下图我们可以看到，<strong>LSTM多了三个Gate</strong>。</p>
<p><img src="https://pic4.zhimg.com/80/v2-5602237fa98e90614cea748aa6a8b6d3_1440w.jpg" alt="img" style="zoom:50%;"></p>
<ul>
<li><strong>Input
Gate</strong>：输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory
Cell。</li>
<li><strong>Output Gate</strong>：输出门，每一时刻是否有信息从Memory
Cell输出取决于这一道门。</li>
<li><strong>Forget Gate</strong>：遗忘门，每一时刻Memory
Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory
Cell里的值清除，也就是遗忘掉。</li>
</ul>
<p>在了解LSTM的内部结构之前，我们需要先回顾一下普通RNN的结构，以免在这里很多读者被搞懵，如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-acee3e085ee62fe162bcac5cd135b54c_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>我们可以看到，左边是为了简便描述RNN的工作原理而画的缩略图，右边是展开之后，每个时间点之间的流程图，<strong>注意，我们接下来看到的LSTM的结构图，是一个时间点上的内部结构，就是整个工作流程中的其中一个时间点，也就是如下图：</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-ccf7b5baee04f3f24bd04637df9bcd3a_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>注意，<strong>上图是普通RNN的一个时间点的内部结构</strong>，上面已经讲过了公式和原理，<strong>LSTM的内部结构更为复杂，不过如果这么类比来学习，我认为也没有那么难</strong>。</p>
<p><img src="https://pic2.zhimg.com/80/v2-1428c54d3ae79cf12616e7051c07799d_1440w.jpg" alt="img" style="zoom:50%;"></p>
<ul>
<li>Cell：memory cell，也就是一个记忆存储的地方，这里就类似于普通RNN的
<img src="https://www.zhihu.com/equation?tex=S_t" alt="[公式]">
，都是用来存储信息的，这里面的信息都会保存到下一时刻，其实标准的叫法应该是
<img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]">
，因为这里对应神经网络里的隐藏层，所以是hidden的缩写，无论普通RNN还是LSTM其实t时刻的记忆细胞里存的信息，都应该被称为
<img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"></li>
<li><img src="https://www.zhihu.com/equation?tex=a" alt="[公式]">
是这一时刻的输出，也就是类似于普通RNN里的 <img src="https://www.zhihu.com/equation?tex=O_t" alt="[公式]"></li>
<li>四个 <img src="https://www.zhihu.com/equation?tex=Z%EF%BC%8CZ_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]"> ，这四个相辅相成，才造就了中间的Memory
Cell里的值，你肯恩要问普通RNN里有个 <img src="https://www.zhihu.com/equation?tex=X_t+" alt="[公式]">
作为输入，那LSTM的输入在哪？别着急，其实这四个 <img src="https://www.zhihu.com/equation?tex=Z%EF%BC%8CZ_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]"> 都有输入向量 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]">
的参与。对了，在解释这四个分别是什么之前，我要先解释一下上图的所有这个符号：<img src="https://pic4.zhimg.com/80/v2-b74b5413f6897f2890b3ede634f60efb_1440w.png" alt="img" style="zoom:50%;">都代表一个激活函数，<strong>LSTM里常用的激活函数有两个，一个是tanh，一个是sigmoid</strong>。</li>
<li><figure>
<img src="https://www.zhihu.com/equation?tex=Z%3Dtanh%28W%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_i%3D%5Csigma%28W_i%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_f%3D%5Csigma%28W_f%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_o%3D%5Csigma%28W_o%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure></li>
<li><img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
<strong>是最为普通的输入</strong>，可以从上图中看到， <img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
是通过该时刻的输入 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]"> 和上一时刻存在memory cell里的隐藏层信息 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
向量拼接，再与权重参数向量 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
点积，得到的值经过激活函数tanh最终会得到一个数值。</li>
<li><img src="https://www.zhihu.com/equation?tex=Z_i" alt="[公式]">
<strong>input gate的缩写i，所以也就是输入门的门控装置</strong>， <img src="https://www.zhihu.com/equation?tex=Z_i+" alt="[公式]">
同样也是通过该时刻的输入 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]">
和上一时刻隐藏状态，也就是上一时刻存下来的信息 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
向量拼接，在与权重参数向量 <img src="https://www.zhihu.com/equation?tex=W_i" alt="[公式]">
点积（注意每个门的权重向量都不一样，这里的下标i代表input的意思，也就是输入门)。得到的值经过激活函数sigmoid的最终会得到一个0-1之间的一个数值，用来作为<strong>输入门的控制信号</strong>。</li>
<li>以此类推，就不详细讲解 <img src="https://www.zhihu.com/equation?tex=Z_f%EF%BC%8CZ_o" alt="[公式]">
了，分别是缩写forget和output的门控装置，原理与上述输入门的门控装置类似。上面说了，只有
<img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
是输入，其他的三个都是门控装置，负责把控每一阶段的信息记录与遗忘，具体是怎样的呢？我们先来看公式：<strong>首先解释一下，经过这个sigmod激活函数后，得到的
<img src="https://www.zhihu.com/equation?tex=Z_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]">
都是在0到1之间的数值，1表示该门完全打开，0表示该门完全关闭</strong>，</li>
</ul>
</blockquote>
<h4><span id="lstm迭代过程">==LSTM迭代过程==</span></h4>
<blockquote>
<p>LSTM和GRU算法简单梳理🍭: https://zhuanlan.zhihu.com/p/72500407</p>
</blockquote>
<p><img src="https://pic3.zhimg.com/80/v2-10f9ec56794c9f89ca2b6ce86c7693ee_1440w.jpg?source=d16d100b" alt="img" style="zoom: 33%;"></p>
<p><img src="https://pic2.zhimg.com/80/v2-867b66c85751aa3bfeeb4956054e0eb8_1440w.jpg?source=d16d100b" alt="img" style="zoom:67%;"></p>
<p><img src="https://www.zhihu.com/equation?tex=h_%7Bt%7D" alt="[公式]"> ：当前序列的隐藏状态、 <img src="https://www.zhihu.com/equation?tex=x_%7Bt%7D" alt="[公式]">
：当前序列的输入数据、 <img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D" alt="[公式]">
：当前序列的细胞状态、 <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]"> ：
<img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
激活函数、 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]"> ： <img src="https://www.zhihu.com/equation?tex=+%5Ctanh" alt="[公式]">
激活函数。</p>
<p><strong>遗忘门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++f_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7Bf%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bf%7D%5Cright%29++%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>输入门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+i_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bi%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bi%7D%5Cright%29+%5C%5C+%5Ctilde%7BC%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W_%7BC%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7BC%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>细胞状态更新：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D%3DC_%7Bt-1%7D%5Codot+f_t%2Bi_t%5Codot+%5Ctilde%7BC_t%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>输出门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D++o_t%26%3D%5Csigma%7B%28W_o%5Ccdot%5Bh_%7Bt-1%7D%2Cx_t%5D%2Bb_o%29%7D%5C%5C+h_t%26%3Do_t%5Codot+%5Ctanh%7B%28C_t%29%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="21-lstm之遗忘门">2.1 LSTM之遗忘门</span></h4>
<p><img src="https://pic3.zhimg.com/80/v2-e616cbe445d60aee532ef3d9db0fb8f6_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>遗忘门是控制是否遗忘的</strong>，在 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]">
中即以一定的概率控制是否遗忘上一层的细胞状态。图中输入的有前一序列的隐藏状态
<img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]"> 和当前序列的输入数据 <img src="https://www.zhihu.com/equation?tex=x_t" alt="[公式]"> ，通过一个
<img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
激活函数得到遗忘门的输出 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]"> 。因为 <img src="https://www.zhihu.com/equation?tex=+sigmoid" alt="[公式]">
函数的取值在 <img src="https://www.zhihu.com/equation?tex=%5B0%2C+1%5D" alt="[公式]"> 之间，所以 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]">
表示的是遗忘前一序列细胞状态的概率，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++f_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7Bf%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bf%7D%5Cright%29++%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="22-lstm之输入门">2.2 LSTM之输入门</span></h4>
<p><img src="https://pic2.zhimg.com/80/v2-1dbddd39ff6a039631b8ff1e4bc294a5_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>输入门是用来决定哪些数据是需要更新的</strong>，由 <img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
层决定；然后，一个 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]"> 层为新的候选值创建一个向量 <img src="https://www.zhihu.com/equation?tex=+%5Ctilde%7BC_t%7D" alt="[公式]"> ，这些值能够加入到当前细胞状态中，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+i_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bi%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bi%7D%5Cright%29+%5C%5C+%5Ctilde%7BC%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W_%7BC%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7BC%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="23-lstm之细胞状态更新">2.3 LSTM之细胞状态更新</span></h4>
<p><img src="https://pic4.zhimg.com/80/v2-82f48ff07dd75ee4346ab44c573fe0b7_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>前面的遗忘门和输入门的结果都会作用于细胞状态</strong> <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
，<strong>在决定需要遗忘和需要加入的记忆之后，就可以更新前一序列的细胞状态
<img src="https://www.zhihu.com/equation?tex=+C_%7Bt-1%7D" alt="[公式]"> 到当前细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
了</strong>，前一序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_%7Bt-1%7D" alt="[公式]">
乘以遗忘门的输出 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]"> 表示决定遗忘的信息， <img src="https://www.zhihu.com/equation?tex=+i_t%5Codot+%5Ctilde%7BC_t%7D" alt="[公式]"> 表示新的记忆信息，数学表达式为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D%3DC_%7Bt-1%7D%5Codot+f_t%2Bi_t%5Codot+%5Ctilde%7BC_t%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="24-lstm之输出门">2.4 LSTM之输出门</span></h4>
<p><img src="https://pic3.zhimg.com/80/v2-6335c8e72a056eeb80970c88988f6bd2_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>在得到当前序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
后，就可以计算当前序列的输出隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"></strong>
了，隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"> 的更新由两部分组成，第一部分是 <img src="https://www.zhihu.com/equation?tex=o_t" alt="[公式]">
，它由前一序列的隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
和当前序列的输入数据 <img src="https://www.zhihu.com/equation?tex=x_t" alt="[公式]"> 通过激活函数 <img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
得到，第二部分由当前序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]"> 经过 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]">
激活函数后的结果组成，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D++o_t%26%3D%5Csigma%7B%28W_o%5Ccdot%5Bh_%7Bt-1%7D%2Cx_t%5D%2Bb_o%29%7D%5C%5C+h_t%26%3Do_t%5Codot+%5Ctanh%7B%28C_t%29%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h3><span id="三-gru-框架结构">三、GRU 框架结构</span></h3>
<p><img src="https://pic3.zhimg.com/80/v2-7547e5c91e1590bf67e0641e9e29ec66_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>循环门单元( <img src="https://www.zhihu.com/equation?tex=Gated%5C+Recurrent%5C+Unit%2C%5C+GRU" alt="[公式]">
)，它组合了遗忘门和输入门到一个单独的更新门当中，也合并了细胞状态 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">
和隐藏状态</strong> <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]">，并且还做了一些其他的改变使得其模型比标准 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]">
模型更简单，其数学表达式式为： <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+z_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bz%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+r_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Br%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+%5Ctilde%7Bh%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W+%5Ccdot%5Cleft%5Br_%7Bt%7D+%5Codot+h_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+h_%7Bt%7D+%26%3D%5Cleft%281-z_%7Bt%7D%5Cright%29+%5Codot+h_%7Bt-1%7D%2Bz_%7Bt%7D+%5Codot+%5Ctilde%7Bh%7D_%7Bt%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C+" alt="[公式]"></p>
<p>首先介绍 <img src="https://www.zhihu.com/equation?tex=GRU" alt="[公式]"> 的两个门，它们分别是重置门 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]"> 和更新门
<img src="https://www.zhihu.com/equation?tex=z_t" alt="[公式]">
，计算方法与 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]"> 中门的计算方法是一致的；然后是计算候选隐藏层 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> ，该候选隐藏层和 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]"> 中的 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BC%7D_t" alt="[公式]"> 类似，都可以看成是当前时刻的新信息，<strong>其中 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]">
用来控制需要保留多少之前的记忆，如果 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=0" alt="[公式]"> 则表示 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> 只保留当前序列的输入信息；最后 <img src="https://www.zhihu.com/equation?tex=+z_t" alt="[公式]">
控制需要从前一序列的隐藏层 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
中遗忘多少信息和需要加入多少当前序列的隐藏层信息</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> ，从而得到当前序列的输出隐藏层信息 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"> ，而 <img src="https://www.zhihu.com/equation?tex=GRU" alt="[公式]">
是没有输出门的。</p>
<p>GRU和LSTM的性能差不多，但GRU参数更少，更简单，所以训练效率更高。但是，如果数据的依赖特别长且数据量很大的话，LSTM的效果可能会稍微好一点，毕竟参数量更多。所以默认推荐使用LSTM。</p>
<h4><span id="参考资料️">参考资料⬇️</span></h4>
<p><a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding
LSTM Networks</a></p>
<h2><span id="lstm-qampa">LSTM Q&amp;A</span></h2>
<h3><span id="1-为什么lstm可以解决梯度消失和梯度爆炸">1、为什么LSTM可以解决梯度消失和梯度爆炸？</span></h3>
<p><img src="https://pic3.zhimg.com/80/v2-10f9ec56794c9f89ca2b6ce86c7693ee_1440w.jpg?source=d16d100b" alt="img" style="zoom: 33%;"></p>
<p><strong>参考（这个老哥说的是最好的）：</strong><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/34878706">LSTM如何来避免梯度弥散和梯度爆炸？</a></p>
<ul>
<li>==<strong>LSTM 中梯度的传播有很多条路径</strong>，<img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+c_t+%3D+f_t%5Codot+c_%7Bt-1%7D+%2B+i_t+%5Codot+%5Chat%7Bc_t%7D" alt="[公式]">
这条路径上只有逐元素相乘和相加的操作，梯度流最稳定==；但是其他路径（例如
<img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+h_%7Bt-1%7D+%5Crightarrow+i_t+%5Crightarrow+c_t" alt="[公式]"> )上梯度流与普通 RNN
类似，照样会发生相同的权重矩阵反复连乘。</li>
<li><strong>LSTM 刚提出时没有遗忘门</strong>，或者说相当于 <img src="https://www.zhihu.com/equation?tex=f_t%3D1" alt="[公式]">
，这时候在 <img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+c_t" alt="[公式]"> 直接相连的短路路径上，<img src="https://www.zhihu.com/equation?tex=dl%2Fdc_t" alt="[公式]">
可以无损地传递给 <img src="https://www.zhihu.com/equation?tex=dl%2Fdc_%7Bt-1%7D" alt="[公式]">
，从而<strong>这条路径</strong>上的梯度畅通无阻，不会消失。类似于 ResNet
中的残差连接。</li>
<li>但是在<strong>其他路径</strong>上，LSTM 的梯度流和普通 RNN
没有太大区别，依然会爆炸或者消失。由于总的远距离梯度 =
各条路径的远距离梯度之和，即便其他远距离路径梯度消失了，只要保证有一条远距离路径（就是上面说的那条高速公路）梯度不消失，总的远距离梯度就不会消失（正常梯度
+ 消失梯度 = 正常梯度）。因此 LSTM
通过改善<strong>一条路径</strong>上的梯度问题拯救了<strong>总体的远距离梯度</strong>。</li>
<li>同样，因为总的远距离梯度 =
各条路径的远距离梯度之和，高速公路上梯度流比较稳定，但其他路径上梯度有可能爆炸，此时总的远距离梯度
= 正常梯度 + 爆炸梯度 = 爆炸梯度，因此 <strong>LSTM
仍然有可能发生梯度爆炸</strong>。不过，<strong>==由于 LSTM
的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于
1），因此 LSTM
发生梯度爆炸的频率要低得多==</strong>。实践中梯度爆炸一般通过梯度裁剪来解决。</li>
<li>对于现在常用的带遗忘门的 LSTM 来说，4 中的分析依然成立，而 3
分为两种情况：其一是遗忘门接近 1（例如模型初始化时会把 forget bias
设置成较大的正数，让遗忘门饱和），这时候远距离梯度不消失；其二是<strong>遗忘门接近
0，但这时模型是故意阻断梯度流的，这不是 bug 而是
feature</strong>（例如情感分析任务中有一条样本 “A，但是
B”，模型读到“但是”后选择把遗忘门设置成 0，遗忘掉内容
A，这是合理的）。当然，常常也存在 f 介于 [0, 1]
之间的情况，在这种情况下只能说 LSTM
改善（而非解决）了梯度消失的状况。</li>
</ul>
<h3><span id="2-为什么lstm模型中既存在sigmoid又存在tanh两种激活函数">2、为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数？</span></h3>
<p>关于激活函数的选取，在LSTM中，遗忘门、输入门和输出门使用
Sigmoid函数作为激活函数;在<strong>生成候选记忆</strong>时，使用双曲正切函数<strong>tanh</strong>作为激活函数。值得注意的是，这两个激活函数都是<strong>饱和</strong>的也就是说在<strong>输入达到一定值的情况下，输出就不会发生明显变化</strong>了。如果是用非饱和的激活图数，例如ReLU，那么将<strong>难以实现门控的效果。</strong></p>
<ul>
<li><p>Sigmoid的输出在0-1之同，符合门控的物理定义，且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关，在生成候选记亿时，</p></li>
<li><p><strong>tanh函数，是因为其输出在-1-1之间，这与大多数场景下特征分布是0中心的吻合</strong>。此外，<strong>tanh函数在输入为0近相比
Sigmoid函数有更大的梯度，通常使模型收敛更快。</strong></p></li>
</ul>
<p>激活函数的选择也不是一成不变的。<strong>例如在原始的LSTM中，使用的激活函数是
Sigmoid函数的变种，h(x)=2sigmoid(x)-1,g(x)＝4
sigmoid(x)-2，这两个函数的范国分别是[-1，1]和[-2，2]</strong>。并且在原始的LSTM中，只有输入门和输出门，没有遗忘门，其中输入经过输入门后是直接与记忆相加的，所以输入门控g(x)的值是0中心的。</p>
<p><strong>后来经过大量的研究和实验，人们发现增加遗忘门对LSTM的性能有很大的提升</strong>且<strong>h(x)使用tanh比2
sigmoid(x)-1要好</strong>，所以现代的LSTM采用
Sigmoid和tanh作为激活函数。<strong>事实上在门控中，使用
Sigmoid函数是几乎所有现代神经网络模块的共同选择</strong>。例如在<strong>门控循环单元和注意力机制</strong>中，也广泛使用
Sigmoid i函数作为门控的激活函数。</p>
<h4><span id="为什么">为什么？</span></h4>
<ol type="1">
<li>门是控制开闭的，全开时值为1，全闭值为０。用于遗忘和保留信息。</li>
<li>对于求值的激活函数无特殊要求。</li>
</ol>
<h4><span id="能更换吗">能更换吗？</span></h4>
<ol type="1">
<li>门是控制开闭的，全开时值为1，全闭值为０。用于遗忘和保留信息。门的激活函数只能是值域为０到１的，最常见的就是sigmoid。</li>
<li>对于求值的激活函数无特殊要求。</li>
</ol>
<h3><span id="3-能不能把tanh换成relu">3、能不能把tanh换成relu？</span></h3>
<p><strong>不行</strong>？对于梯度爆炸的问题用梯度裁剪解决就行了。</p>
<ol type="1">
<li><strong>会造成输出值爆炸</strong>。RNN共享参数矩阵，长程的话相当于多个相乘，最后输出类似于
<img src="https://www.zhihu.com/equation?tex=f%5Cleft%5BW+%5Cldots%5Cleft%5BW+f%5Cleft%5BW+f%5Cleft%5BW+f_%7Bi%7D%5Cright%5D%5Cright%5D%5Cright%5D%5Cright%5D" alt="[公式]"> ，其中是 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 激活函数，如果 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
有一个大于1的特征值，且使用relu激活函数，那最后的输出值会爆炸。但是使用tanh激活函数，能够把输出值限制在-1和1之间。</li>
<li><strong>这里relu并不能解决梯度消失或梯度爆炸的问题</strong>。假设有t=3，最后一项输出反向传播对W求导，
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+W_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+f_%7B2%7D%2B%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D+f_%7B1%7D%2B%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B1%7D%7D%7B%5Cpartial+a_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+a_%7B1%7D%7D%7B%5Cpartial+W_%7B1%7D%7D" alt="[公式]"> 。我们用最后一项做分析，即使使用了<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=relu&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22350613342%22%7D">relu</a>，
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B1%7D%7D%3D1" alt="[公式]"> ，还是会有两个 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
相乘，并不能解决梯度消失或梯度爆炸的问题。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3MJS4P5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3MJS4P5/" class="post-title-link" itemprop="url">深度学习（4）CNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:44:58" itemprop="dateCreated datePublished" datetime="2022-03-16T21:44:58+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 21:22:12" itemprop="dateModified" datetime="2023-04-18T21:22:12+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CNN/" itemprop="url" rel="index"><span itemprop="name">CNN</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="cnn-门控卷积网络">CNN-门控卷积网络</span></h2>
<ul>
<li>https://zhuanlan.zhihu.com/p/59064623</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/FishPotatoChen/article/details/123389289?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123389289-blog-108305491.pc_relevant_paycolumn_v3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123389289-blog-108305491.pc_relevant_paycolumn_v3&amp;utm_relevant_index=2">参数估计</a></li>
<li>**<font color="red"> 卷积神经网络中用1*1
卷积有什么作用或者好处呢？</font>** - 初识CV的回答 - 知乎
https://www.zhihu.com/question/56024942/answer/1850649283</li>
</ul>
<h2><span id="一-参数估计">一、参数估计</span></h2>
<p>卷积操作的本质是<strong>稀疏交互</strong>和<strong>参数共享</strong>。</p>
<p><strong>稀疏交互</strong>：每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重。<strong>物理意义</strong>：通常图像，文字、语音等现实世界中的数据都具有<strong>局部的特征结构</strong>。</p>
<p><strong>参数共享</strong>：卷积核中的每一个元素将作用于每一个局部输入的特定位置上。<strong>物理意义</strong>：平移等变性</p>
<p><strong>池化操作</strong>：针对非重叠区域，均值池化、最大池化。除了能显著降低参数外，还能够保持对平移、伸缩、旋转操作的不变性。</p>
<h4><span id="11-正向传播">1.1 正向传播</span></h4>
<p>这里用输入为 <span class="math inline">\(3 * 3\)</span> 矩阵 <span class="math inline">\(A^{l-1}\)</span>, 步长为 1 , 卷积核为 <span class="math inline">\(2 * 2\)</span> 矩阵 <span class="math inline">\(W^{l}\)</span>, 输出为 <span class="math inline">\(2 * 2\)</span> 矩阵 <span class="math inline">\(Z^{l}\)</span> 的卷积层为 例。矩阵 <span class="math inline">\(A^{l-1}\)</span> 可以是整个神经网络的输入,
也可以是池化层的输出。这个模型简化为输入层 <span class="math inline">\(A^{l-1}\)</span> 经 过卷积计算得到特征图 <span class="math inline">\(Z^{l}, Z^{l}\)</span> 经过激活函数 <span class="math inline">\(\sigma(x)\)</span> 得到输出层 <span class="math inline">\(A^{l}\)</span> (实际上在现实工程中很多时候不用激
活函数)。对于第 <span class="math inline">\(l\)</span> 层, 有下列表达式:
<span class="math display">\[
\left[\begin{array}{lll}
a_{1} &amp; a_{2} &amp; a_{3} \\
a_{4} &amp; a_{5} &amp; a_{6} \\
a_{7} &amp; a_{8} &amp; a_{9}
\end{array}\right]^{l-1} \Rightarrow\left[\begin{array}{cc}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l} \Rightarrow\left[\begin{array}{ll}
z_{1} &amp; z_{2} \\
z_{3} &amp; z_{4}
\end{array}\right]^{l} \Rightarrow\left[\begin{array}{ll}
a_{1} &amp; a_{2} \\
a_{3} &amp; a_{4}
\end{array}\right]^{l}
\]</span></p>
<p><span class="math display">\[
\left\{\begin{aligned}
a_{1}^{l} &amp;=\sum\left(\left[\begin{array}{ll}
a_{1} &amp; a_{2} \\
a_{4} &amp; a_{5}
\end{array}\right]^{l-1} \cdot\left[\begin{array}{ll}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l}\right)=\sigma\left(\omega_{1}
a_{1}^{l-1}+\omega_{2} a_{2}^{l-1}+\omega_{3} a_{3}^{l-1}+\omega_{4}
a_{4}^{l-1}+b^{l}\right) \\
a_{2}^{l} &amp;=\sum\left(\left[\begin{array}{ll}
a_{2} &amp; a_{3} \\
a_{5} &amp; a_{6}
\end{array}\right]^{l-1} \cdot\left[\begin{array}{ll}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l}\right)=\sigma\left(\omega_{1}
a_{2}^{l-1}+\omega_{2} a_{3}^{l-1}+\omega_{3} a_{5}^{l-1}+\omega_{4}
a_{6}^{l-1}+b^{l}\right) \\
a_{3}^{l} &amp;=\sum\left(\left[\begin{array}{ll}
a_{4} &amp; a_{5} \\
a_{7} &amp; a_{8}
\end{array}\right]^{l-1} \cdot\left[\begin{array}{ll}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l}\right)=\sigma\left(\omega_{1}
a_{4}^{l-1}+\omega_{2} a_{5}^{l-1}+\omega_{3} a_{7}^{l-1}+\omega_{4}
a_{8}^{l-1}+b^{l}\right) \\
a_{4}^{l} &amp;=\sum\left(\left[\begin{array}{ll}
a_{5} &amp; a_{6} \\
a_{8} &amp; a_{9}
\end{array}\right]^{l-1} \cdot\left[\begin{array}{ll}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l}\right)=\sigma\left(\omega_{1}
a_{5}^{l-1}+\omega_{2} a_{6}^{l-1}+\omega_{3} a_{8}^{l-1}+\omega_{4}
a_{9}^{l-1}+b^{l}\right)
\end{aligned}\right.
\]</span></p>
<p>简单来说, 卷积过程就是对应的位置代入函数之后相加求和,
不同的函数有不同的参数 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span>, 我们需 要训练的是卷积核参数,
所以这个公式还可以写做 <span class="math inline">\(Z^{l}=W^{l} *
A^{l-1}+b^{l}, \sigma(x)\)</span> 是激活函数,
<strong>我们假设是ReLU函数, 求导比较好求</strong>,
所以我们接下来的计算忽略了对激活层的求导。 <span class="math display">\[
\sigma(x)=\left\{\begin{array}{lc}
0 &amp; , \quad x&lt;0 \\
x &amp; , \quad x&gt;=0
\end{array}\right.
\]</span></p>
<h3><span id="12-反向传播">1.2 反向传播</span></h3>
<h2><span id="二-cnn-李宏毅">二、CNN - 李宏毅</span></h2>
<h2><span id="三-cnn-qampa">三、CNN Q&amp;A</span></h2>
<h4><span id="31卷积神经网络的卷积核大小-个数卷积层数如何确定呢">3.1
卷积神经网络的卷积核大小、个数，卷积层数如何确定呢？</span></h4>
<ul>
<li>https://cloud.tencent.com/developer/article/1462368</li>
</ul>
<p>每一层卷积有多少channel数，以及一共有多少层卷积，这些暂时没有理论支撑，一般都是靠感觉去设置几组候选值，然后通过实验挑选出其中的最佳值。这也是现在深度卷积神经网络虽然效果拔群，但是一直为人诟病的原因之一。</p>
<h4><span id="32卷积神经网络中用11-卷积有什么作用或者好处呢">3.2
卷积神经网络中用1*1 卷积有什么作用或者好处呢？</span></h4>
<ul>
<li>卷积神经网络中用1*1 卷积有什么作用或者好处呢？ - YJango的回答 - 知乎
https://www.zhihu.com/question/56024942/answer/194997553</li>
</ul>
<p><img src="image-20220627212030158.png" alt="image-20220627212030158" style="zoom: 50%;"></p>
<p><strong>降维</strong>：Inception结构可以看出，这些1X1的卷积的作用是为了让网络根据需要能够更灵活的控制数据的depth的。</p>
<p><strong>加入非线性</strong>。卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（
non-linear activation ），提升网络的表达能力；</p>
<p><strong>1 * 1的卷积就是多个feature channels线性叠加，nothing
more!只不过这个组合系数恰好可以看成是一个1 *
1的卷积</strong>。这种表示的好处是，完全可以回到模型中其他常见N*N的框架下，不用定义新的层。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/4YNH5E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/4YNH5E/" class="post-title-link" itemprop="url">特征工程（9）TF-IDF</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:06:42" itemprop="dateCreated datePublished" datetime="2022-03-16T21:06:42+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:54:35" itemprop="dateModified" datetime="2023-04-26T14:54:35+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-tf-idf">一、TF-IDF</span></h3>
<blockquote>
<p>https://blog.csdn.net/u010417185/article/details/87905899</p>
</blockquote>
<p><strong>TF-IDF(Term Frequency-Inverse Document Frequency,
词频-逆文件频率)</strong>是一种用于资讯检索与资讯探勘的常用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加权技术&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2297273457%22%7D">加权技术</a>。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p>
<p>上述引用总结就是, <strong>一个词语在一篇文章中出现次数越多,
同时在所有文档中出现次数越少,
越能够代表该文章。</strong>这也就是TF-IDF的含义。</p>
<h4><span id="11-tf"><strong>1.1 TF</strong></span></h4>
<p><strong>TF(Term
Frequency，词频)</strong>表示词条在文本中出现的频率，这个数字通常会被归一化(一般是词频除以文章总词
数),
以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频,
而不管该词语重要与否）。TF 用公式表示如下: <span class="math display">\[
T F_{i, j}=\frac{n_{i, j}}{\sum_k n_{k, j}}
\]</span> 其中, <span class="math inline">\(n_{i, j}\)</span> 表示词条
<span class="math inline">\(t_i\)</span> 在文档 <span class="math inline">\(d_j\)</span> 中出现的次数, <span class="math inline">\(T F_{i, j}\)</span> 就是表示词条 <span class="math inline">\(t_i\)</span> 在文档 <span class="math inline">\(d_j\)</span> 中出现的频率。</p>
<p><strong>但是, 需要注意, 一些通用的词语对于主题并没有太大的作用,
反倒是一些出现频率较少的词才能够表达文章的主题,
所以单纯使用是TF不合适的</strong>。权重的设计必须满足：一个词预测主题的能力越强,
权重越大, 反之, 权重 越小。所有统计的文章中,
一些词只是在其中很少几篇文章中出现, 那么这样的词对文章的主题的作用很大,
这些 词的权重应该设计的较大。IDF就是在完成这样的工作。</p>
<h4><span id="12-idf"><strong>1.2 IDF</strong></span></h4>
<p><strong>IDF(Inverse Document
Frequency，逆文件频率)</strong>表示关键词的普遍程度。如果包含词条 <span class="math inline">\(i\)</span> 的文档越少, IDF越 大,
则说明该词条具有很好的类别区分能力。某一特定词语的IDF,
可以由总文件数目除以包含该词语之文件的数 目, 再将得到的商取对数得到:
<span class="math display">\[
I D F_i=\log \frac{|D|}{1+\left|j: t_i \in d_j\right|}
\]</span> 其中, <span class="math inline">\(|D|\)</span>
表示所有<strong>文档的数量, <span class="math inline">\(\left|j: t_i \in
d_j\right|\)</span> 表示包含词条 <span class="math inline">\(t_i\)</span> 的文档数量</strong>, 为什么这里要加 1
呢? 主要是<strong>防止包含词条 <span class="math inline">\(t_i\)</span>
的数量为 0 从而导致运算出错的现象发生</strong>。</p>
<p>某一特定文件内的高词语频率, 以及该词语在整个文件集合中的低文件频率,
可以产生出高权重的TF-IDF。因此, TF-IDF倾向于<strong>过滤淖常见的词语,
保留重要的词语,</strong> 表达为 <span class="math display">\[
T F-I D F=T F \cdot I D F
\]</span> 最后在计算完文档中每个字符的tfidf之后, 对其进行归一化,
将值保留在0-1之间, 并保存成稀疏矩阵。</p>
<h3><span id="二-tf-idf-qampa">二、TF-IDF Q&amp;A</span></h3>
<h4><span id="1-究竟应该是对整个语料库进行tf-idf呢还是先对训练集进行tf-idf然后再对xtest进行tf-idf呢两者有什么区别"><strong>1、究竟应该是对整个语料库进行tf-idf呢？还是先对训练集进行tf-idf，然后再对xtest进行tf-idf呢？两者有什么区别？</strong></span></h4>
<h5><span id="fit">fit</span></h5>
<ul>
<li>学习输入的数据有多少个不同的单词，以及每个单词的idf</li>
</ul>
<h5><span id="transform-训练集">transform 训练集</span></h5>
<ul>
<li>回我们一个document-term matrix.</li>
</ul>
<h5><span id="transform-测试集">transform 测试集</span></h5>
<p>transform的过程也很让人好奇。要知道，他是将测试集的数据中的文档数量纳入进来，重新计算每个词的idf呢，还是<strong>直接用训练集学习到的idf去计算测试集里面每一个tf-idf</strong>呢？</p>
<p><strong>如果纳入了测试集新词，就等于预先知道测试集中有什么词，影响了idf的权重。这样预知未来的行为，会导致算法丧失了泛化性。</strong></p>
<h4><span id="2-tf-idf模型加载太慢"><font color="red">2、TF-IDF
模型加载太慢</font></span></h4>
<blockquote>
<p>https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> idfs <span class="keyword">import</span> idfs <span class="comment"># numpy array with our pre-computed idfs</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># subclass TfidfVectorizer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyVectorizer</span>(<span class="title class_ inherited__">TfidfVectorizer</span>):</span><br><span class="line">    <span class="comment"># plug our pre-computed IDFs</span></span><br><span class="line">    TfidfVectorizer.idf_ = idfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiate vectorizer</span></span><br><span class="line">vectorizer = MyVectorizer(lowercase = <span class="literal">False</span>,</span><br><span class="line">                          min_df = <span class="number">2</span>,</span><br><span class="line">                          norm = <span class="string">&#x27;l2&#x27;</span>,</span><br><span class="line">                          smooth_idf = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plug _tfidf._idf_diag</span></span><br><span class="line">vectorizer._tfidf._idf_diag = sp.spdiags(idfs,</span><br><span class="line">                                         diags = <span class="number">0</span>,</span><br><span class="line">                                         m = <span class="built_in">len</span>(idfs),</span><br><span class="line">                                         n = <span class="built_in">len</span>(idfs))</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1FJMY6C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1FJMY6C/" class="post-title-link" itemprop="url">贝叶斯分类器（4）EM算法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 17:16:27" itemprop="dateCreated datePublished" datetime="2022-03-16T17:16:27+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:32:53" itemprop="dateModified" datetime="2023-04-22T16:32:53+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" itemprop="url" rel="index"><span itemprop="name">贝叶斯分类器</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="em期望最大-概率模型">EM——期望最大 [概率模型]</span></h2>
<p><strong>EM 算法通过引入隐含变量，使用
MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM
算法首先会固定其中的第一个参数，然后使用 MLE
计算第二个变量值；接着通过固定第二个变量，再使用 MLE
估测第一个变量值，依次迭代，直至收敛到局部最优解。</strong></p>
<p><strong><font color="red"> EM 算法，全称 Expectation Maximization
Algorithm。期望最大算法是一种迭代算法，用于含有隐变量（Hidden
Variable）的概率参数模型的最大似然估计或极大后验概率估计。</font></strong></p>
<p>本文思路大致如下：先简要介绍其思想，然后举两个例子帮助大家理解，有了感性的认识后再进行严格的数学公式推导。</p>
<h3><span id="1-思想">1. 思想</span></h3>
<p>EM 算法的核心思想非常简单，分为两步：<strong>Expection-Step</strong>
和 <strong>Maximization-Step</strong>。<strong>E-Step
主要通过观察数据和现有模型来估计参数</strong>，然后用这个估计的参数值来计算似然函数的期望值；而
M-Step
是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后<strong>似然函数都会增加</strong>，所以函数最终会收敛。</p>
<p><font color="red"> <span class="math inline">\(E M\)</span>
算法一句话总结就是: <span class="math inline">\(E\)</span> 步固定 <span class="math inline">\(\theta\)</span> 优化 <span class="math inline">\(Q, M\)</span> 步固定 <span class="math inline">\(Q\)</span> 优化 <span class="math inline">\(\theta\)</span> 。</font></p>
<h3><span id="2-例子">2 例子</span></h3>
<h4><span id="21-例子-a">2.1 例子 A</span></h4>
<p>假设有两枚硬币 <span class="math inline">\(\mathrm{A}\)</span> 和
<span class="math inline">\(B\)</span>,
他们的随机抛郑的结果如下图所示:</p>
<p><img src="https://pic4.zhimg.com/80/v2-4e19d89b47e21cf284644b0576e9af0f_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>我们很容易估计出两枚硬币抛出正面的概率： <span class="math display">\[
\begin{aligned}
&amp; \theta_A=24 / 30=0.8 \\
&amp; \theta_B=9 / 20=0.45
\end{aligned}
\]</span> 现在我们加入隐变量, 抺去每轮投郑的硬币标记:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221558453.jpg" alt="img" style="zoom:50%;"></p>
<p>碰到这种情况, 我们该如何估计 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> 的值?</p>
<p>我们多了一个隐变量 <span class="math inline">\(Z=\left(z_1, z_2, z_3,
z_4, z_5\right)\)</span>, 代表每一轮所使用的硬币,
我们需要知道每一轮抛郑所使用的硬币这样才能估计 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> 的值, 但是估计隐变量 <span class="math inline">\(\mathrm{Z}\)</span> 我们又需要知道 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> 的值,
才能用极大似然估计法去估计出 Z。这就陷入了一个鸡生蛋和蛋生鸡的问题。</p>
<p>其解决方法就是先随机初始化 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span>, 然后用去估计 <span class="math inline">\(Z\)</span>, 然后基于 <span class="math inline">\(Z\)</span> 按照最大似然概率去估计新的 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> , 循环至收敛。</p>
<h5><span id="212-计算"><strong>2.1.2 计算</strong></span></h5>
<p>随机初始化 <span class="math inline">\(\theta_A=0.6\)</span> 和 <span class="math inline">\(\theta_B=0.5\)</span></p>
<p>对于第一轮来说, 如果是硬币 <span class="math inline">\(A\)</span>,
得出的 5 正 5 反的概率为: <span class="math inline">\(0.6^5 *
0.4^5\)</span>; 如果是硬币 <span class="math inline">\(B\)</span>,
得出的 5 正 5 反的概率为: <span class="math inline">\(0.5^5 *
0.5^5\)</span> 。我们可以算出使用是硬币 <span class="math inline">\(A\)</span> 和硬币 <span class="math inline">\(B\)</span> 的概率 分别为: <span class="math display">\[
\begin{aligned}
&amp; P_A=\frac{0.6^5 * 0.4^5}{\left(0.6^5 * 0.4^5\right)+\left(0.5^5 *
0.5^5\right)}=0.45 \\
&amp; P_B=\frac{0.5^5 * 0.5^5}{\left(0.6^5 * 0.4^5\right)+\left(0.5^5 *
0.5^5\right)}=0.55
\end{aligned}
\]</span> <img src="https://pic4.zhimg.com/80/v2-b325de65a5bcac196fc0939f346410d7_1440w.jpg" alt="img"></p>
<p>从期望的角度来看, 对于第一轮抛郑, 使用硬币 <span class="math inline">\(A\)</span> 的概率是 0.45 , 使用硬币 <span class="math inline">\(B\)</span> 的概率是 0.55。同理其他轮。这一
步我们实际上是估计出了 Z 的概率分布，这部就是 E-Step。</p>
<p>结合硬币 <span class="math inline">\(A\)</span> 的概率和上一张结果,
我们利用期望可以求出硬币 <span class="math inline">\(A\)</span> 和硬币
<span class="math inline">\(B\)</span> 的贡献。以第二轮硬币 <span class="math inline">\(A\)</span> 为例子, 计算方式为: <span class="math display">\[
\begin{aligned}
&amp; H: 0.80 * 9=7.2 \\
&amp; T: 0.80 * 1=0.8
\end{aligned}
\]</span> 于是我们可以得到：</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-9b6e8c50c0761c6ac19909c26e0a71d4_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>然后用极大似然估计来估计新的 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> 。 <span class="math display">\[
\begin{aligned}
\theta_A &amp; =\frac{21.3}{21.3+8.6}=0.71 \\
\theta_B &amp; =\frac{11.7}{11.7+8.4}=0.58
\end{aligned}
\]</span></p>
<p>这步就对应了
M-Step，重新估计出了参数值。如此反复迭代，我们就可以算出最终的参数值。</p>
<p>上述讲解对应下图：</p>
<p><img src="https://pic3.zhimg.com/v2-6cac968d6500cbca58fc90347c288466_r.jpg" alt="preview" style="zoom:50%;"></p>
<h4><span id="22-例子-b">2.2 例子 B</span></h4>
<p>如果说例子 A 需要计算你可能没那么直观, 那就举更一个简单的例子:</p>
<p>现在一个班里有 50 个男生和 50
个女生，且男女生分开。我们假定男生的身高服从正态分布： <span class="math inline">\(N\left(\mu_1, \sigma_1^2\right)\)</span> ,
女生的身高则服从另一个正态分布： <span class="math inline">\(N\left(\mu_2, \sigma_2^2\right)\)</span>
。这时候我们可以用 极大似然法 (MLE) , 分别通过这 50 个男生和 50
个女生的样本来估计这两个正态分布的参数。</p>
<p>但现在我们让情况复杂一点, 就是这 50 个男生和 50
个女生混在一起了。我们拥有 100 个人的身高数据, 却不知 道这 100
个人每一个是男生还是女生。</p>
<p>这时候情况就有点箩尬, 因为通常来说,
我们只有知道了精确的男女身高的正态分布参数我们才能知道每一个人更
有可能是男生还是女生。但从另一方面去考量,
我们只有知道了每个人是男生还是女生才能尽可能准确地估计男女
各自身高的正态分布的参数。</p>
<p>这个时候有人就想到我们必须从某一点开始,
并用迭代的办法去解决这个问题：<strong>我们先设定男生身高和女生身高分
布的几个参数（初始值）, 然后根据这些参数去判断每一个样本
(人）是男生还是女生, 之后根据标注后的样本再
反过来重新估计参数。之后再多次重复这个过程，直至稳定。这个算法也就是 EM
算法。</strong></p>
<h3><span id="3-推导">3. 推导</span></h3>
<p>给定数据集, 假设样本间相互独立, 我们想要拟合模型 <span class="math inline">\(p(x ; \theta)\)</span>
到数据的参数。根据分布我们可以 得到如下似然函数: <span class="math display">\[
\begin{aligned}
L(\theta) &amp; =\sum_{i=1}^n \log p\left(x_i ; \theta\right) \\
&amp; =\sum_{i=1}^n \log \sum_z p\left(x_i, z ; \theta\right)
\end{aligned}
\]</span></p>
<p>第一步是<strong>对极大似然函数取对数</strong>，第二步是对每个样本的每个可能的类别
z 求<strong>联合概率分布之和</strong>。如果这个 z
是已知的数，那么使用极大似然法会很容易。但如果 z 是隐变量，我们就需要用
EM
算法来求。<strong>事实上，隐变量估计问题也可以通过梯度下降等优化算法，但事实由于求和项将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而
EM 算法则可看作一种非梯度优化方法。</strong></p>
<h4><span id="31-求解含有隐变量的概率模型">3.1 求解含有隐变量的概率模型</span></h4>
<p>为了求解含有隐变量 <span class="math inline">\(z\)</span> 的概率模型
<span class="math inline">\(\hat{\theta}=\underset{\theta}{\arg \max }
\sum_{i=1}^m \log \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ;
\theta\right)\)</span> 需要一些特殊的技巧, 通过引入隐变量<span class="math inline">\(z^{(i)}\)</span> 的概率分布为 <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span>, 因为 <span class="math inline">\(\log (x)\)</span>
是凹函数故结合凹函数形式下的詹森不等式进行放缩处理 <span class="math display">\[
\begin{aligned}
\sum_{i=1}^m \log \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ; \theta\right)
&amp; =\sum_{i=1}^m \log \sum_{z^{(i)}} Q_i\left(z^{(i)}\right)
\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_i\left(z^{(i)}\right)}
\\
&amp; =\sum_{i=1}^m \log \mathbb{E}\left(\frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{Q_i\left(z^{(i)}\right)}\right) \\
&amp; \left.\geq \sum_{i=1}^m \mathbb{E}\left[\log \frac{p\left(x^{(i)},
z^{(i)} ; \theta\right)}{Q_i\left(z^{(i)}\right)}\right)\right] \\
&amp; =\sum_{i=1}^m \sum_{z^{(i)}} Q_i\left(z^{(i)}\right) \log
\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_i\left(z^{(i)}\right)}
\end{aligned}
\]</span> 其中由概率分布的充要条件 <span class="math inline">\(\sum_{z^{(i)}} Q_i\left(z^{(i)}\right)=1 、
Q_i\left(z^{(i)}\right) \geq 0\)</span> 可看成下述关于 <span class="math inline">\(z\)</span> 函数分布列的形式:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221618376.jpg" alt="img" style="zoom:50%;"></p>
<p>这个过程可以看作是对 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 求了下界, 假设 <span class="math inline">\(\theta\)</span> 已经给定那么 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 的值就取决于 <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span> 和 <span class="math inline">\(p\left(x^{(i)}, z^{(i)}\right)\)</span> 了, 因
此可以通过调整这两个概率使下界不断上升, 以逼近 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 的真实值,
当不等式变成等式时说明调整后的概率能够 等价于 <span class="math inline">\(\mathcal{L}(\theta)\)</span>
，所以必须找到使得等式成立的条件, 即寻找 <span class="math display">\[
\left.\mathbb{E}\left[\log \frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{Q_i\left(z^{(i)}\right)}\right)\right]=\log
\mathbb{E}\left[\frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{Q_i\left(z^{(i)}\right)}\right]
\]</span> 由期望得性质可知当 <span class="math display">\[
\frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{Q_i\left(z^{(i)}\right)}=C, \quad C \in \mathbb{R}
\quad(*)
\]</span> 等式成立，对上述等式进行变形处理可得 <span class="math display">\[
\begin{aligned}
&amp; p\left(x^{(i)}, z^{(i)} ; \theta\right)=C Q_i\left(z^{(i)}\right)
\\
&amp; \Leftrightarrow \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ;
\theta\right)=C \sum_{z^{(i)}} Q_i\left(z^{(i)}\right)=C \\
&amp; \Leftrightarrow \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ;
\theta\right)=C
\end{aligned}
\]</span> 把 <span class="math inline">\((* *)\)</span> 式带入 <span class="math inline">\((*)\)</span> 化简可知 <span class="math display">\[
\begin{aligned}
Q_i\left(z^{(i)}\right) &amp; =\frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{\sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ; \theta\right)}
\\
&amp; =\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{p\left(x^{(i)} ;
\theta\right)} \\
&amp; =p\left(z^{(i)} \mid x^{(i)} ; \theta\right)
\end{aligned}
\]</span> 至此, 可以推出在固定参数 <span class="math inline">\(\theta\)</span> 后, <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span>
的计算公式就是后验概率, 解决了 <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span>
如何选择得问题。这一步 称为 <span class="math inline">\(E\)</span> 步,
建立 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 得下界;
接下来得 <span class="math inline">\(M\)</span> 步, 就是在给定 <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span> 后, 调整 <span class="math inline">\(\theta\)</span> 去极大化 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 的下界即 <span class="math display">\[
\begin{aligned}
&amp; \underset{\theta}{\arg \max } \sum_{i=1}^m \log p\left(x^{(i)} ;
\theta\right) \\
&amp; \Leftrightarrow \underset{\theta}{\arg \max } \sum_{i=1}^m
\sum_{z^{(i)}} Q_i\left(z^{(i)}\right) \log \frac{p\left(x^{(i)},
z^{(i)} ; \theta\right)}{Q_i\left(z^{(i)}\right)} \\
&amp; \Leftrightarrow \underset{\theta}{\arg \max } \sum_{i=1}^m
\sum_{z^{(i)}} Q_i\left(z^{(i)}\right)\left[\log p\left(x^{(i)}, z^{(i)}
; \theta\right)-\log Q_i\left(z^{(i)}\right)\right] \\
&amp; \Leftrightarrow \underset{\theta}{\arg \max } \sum_{i=1}^m
\sum_{z^{(i)}} Q_i\left(z^{(i)}\right) \log p\left(x^{(i)}, z^{(i)} ;
\theta\right)
\end{aligned}
\]</span> 因此EM算法的迭代形式为：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221619542.jpg" alt="img" style="zoom:50%;"></p>
<p><img src="https://pic3.zhimg.com/80/v2-2f7fc5ca144d2f85f14d46e88055dd86_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>这张图的意思就是: 首先我们固定 <span class="math inline">\(\theta\)</span>, 调整 <span class="math inline">\(Q(z)\)</span> 使下界 <span class="math inline">\(J(z, Q)\)</span> 上升至与 <span class="math inline">\(L(\theta)\)</span> 在此点 <span class="math inline">\(\theta\)</span> 处相等 (绿色曲线到 蓝色曲线) ,
然后固定 <span class="math inline">\(Q(z)\)</span>, 调整 <span class="math inline">\(\theta\)</span> 使下界 <span class="math inline">\(J(z, Q)\)</span> 达到最大值 <span class="math inline">\(\left(\theta_t\right.\)</span> 到 <span class="math inline">\(\left.\theta_{t+1}\right)\)</span> ，然后再固定
<span class="math inline">\(\theta\)</span>, 调整 <span class="math inline">\(Q(z)\)</span> , 一直到收敛到似然函数 <span class="math inline">\(L(\theta)\)</span> 的最大值处的 <span class="math inline">\(\theta\)</span> 。</p>
<p><strong><font color="red"> EM 算法通过引入隐含变量，使用
MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM
算法首先会固定其中的第一个参数，然后使用 MLE
计算第二个变量值；接着通过固定第二个变量，再使用 MLE
估测第一个变量值，依次迭代，直至收敛到局部最优解。</font></strong></p>
<h4><span id="32-em算法的收敛性">3.2 EM算法的收敛性</span></h4>
<p>不妨假设 <span class="math inline">\(\theta^{(k)}\)</span> 和 <span class="math inline">\(\theta^{(k+1)}\)</span> 是EM算法第 <span class="math inline">\(k\)</span> 次迭代和第 <span class="math inline">\(k+1\)</span> 次迭代的结果, 要确保 <span class="math inline">\(E M\)</span> 算法收敛那么等价于证明 <span class="math inline">\(\mathcal{L}\left(\theta^{(k)}\right) \leq
\mathcal{L}\left(\theta^{(k+1)}\right)\)</span>
也就是说极大似然估计单调增加,
那么算法最终会迭代到极大似然估计的最大值。在选定 <span class="math inline">\(\theta^{(k)}\)</span> 后可以得到 <span class="math inline">\(E\)</span> 步 <span class="math inline">\(Q_i^{(k)}\left(z^{(i)}\right)=p\left(z^{(i)} \mid
x^{(i)} ; \theta^{(k)}\right)\)</span>, 这一步保证了在给定 <span class="math inline">\(\theta^{(k)}\)</span> 时, 詹森不等式中的等式成立即
<span class="math inline">\(\mathcal{L}\left(\theta^{(k)}\right)=\sum_{i=1}^m
\sum_{z^{(i)}} Q_i^{(k)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)},
z^{(i)} ; \theta^{(k)}\right)}{Q_i\left(z^{(i)}\right)}\)</span></p>
<p>然后再进行 <span class="math inline">\(M\)</span> 步, 固定 <span class="math inline">\(Q_i^{(k)}\left(z^{(i)}\right)\)</span> 并将 <span class="math inline">\(\theta^{(k)}\)</span> 视作变量, 对上式 <span class="math inline">\(\mathcal{L}\left(\theta^{(k)}\right)\)</span>
求导后得到 <span class="math inline">\(\theta^{(k+1)}\)</span>
因此有如下式子成立 <span class="math display">\[
\begin{aligned}
\mathcal{L}\left(\theta^{(k)}\right) &amp; =\sum_{i=1}^m \sum_{z^{(i)}}
Q_i^{(k)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ;
\theta^{(k)}\right)}{Q_i\left(z^{(i)}\right)} \\
&amp; \leq \sum_{i=1}^m \sum_{z^{(i)}} Q_i^{(k)}\left(z^{(i)}\right)
\log \frac{p\left(x^{(i)}, z^{(i)} ;
\theta^{(k)}\right)}{Q_i\left(z^{(i)}\right)} \\
&amp; \leq \mathcal{L}\left(\theta^{(k+1)}\right)
\end{aligned}
\]</span> 首先 (a) 式是前面 <span class="math inline">\(E\)</span>
步所保证詹森不等式中的等式成立的条件, (a) 到 (b) 是 <span class="math inline">\(M\)</span> 步的定义, (b) 到 (c) 对任意 参数都成立,
而其等式的条件是固定 <span class="math inline">\(\theta\)</span>
并调整好 <span class="math inline">\(Q\)</span> 时成立, <span class="math inline">\((b)\)</span> 到 <span class="math inline">\((c)\)</span> 只是固定 <span class="math inline">\(Q\)</span> 调整 <span class="math inline">\(\theta\)</span>, 在得到 <span class="math inline">\(\theta^{(k+1)}\)</span> 时, 只是最大化 <span class="math inline">\(\mathcal{L}\left(\theta^{(k)}\right)\)</span>,
也就是 <span class="math inline">\(\mathcal{L}\left(\theta^{(k+1)}\right)\)</span>
的一个下界而没有使等式成立。</p>
<h3><span id="4-另一种理解">4. 另一种理解</span></h3>
<p>坐标上升法（Coordinate ascent）：</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221621244.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>途中直线为迭代优化路径，因为每次只优化一个变量，所以可以看到它没走一步都是平行与坐标轴的。</p>
<p>EM 算法类似于坐标上升法，E 步：固定参数，优化 Q；M 步：固定
Q，优化参数。交替将极值推向最大。</p>
<h3><span id="5-应用">5. 应用</span></h3>
<p>EM 的应用有很多，比如、混合高斯模型、聚类、HMM 等等。其中 <strong>EM
在 K-means 中的用处</strong>，我将在介绍 K-means 中的给出。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ol type="1">
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/27976634">怎么通俗易懂地解释 EM
算法并且举个例子?</a></p></li>
<li><p><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/zouxy09/article/details/8537620">从最大似然到
EM 算法浅解</a></p></li>
<li><h5><span id="em算法"></span></h5></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/5BWKZW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/5BWKZW/" class="post-title-link" itemprop="url">度量学习（1）KNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:48" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:48+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 19:45:25" itemprop="dateModified" datetime="2023-04-26T19:45:25+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-什么是knnkd树-siftbbf算法">一、什么是KNN【KD树 +
SIFT+BBF算法】</span></h3>
<h4><span id="11-knn的通俗解释">1.1 KNN的通俗解释</span></h4>
<p>何谓K近邻算法，即K-Nearest Neighbor
algorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。</p>
<p>用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，<strong>在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。</strong></p>
<p>​ <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067"><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232118382.jpeg" alt="img"></a></p>
<p>​ <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67"><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232118849.png" alt="img"></a></p>
<p>如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），KNN就是解决这个问题的。</p>
<p>如果<strong>K=3</strong>，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>红色</strong>的三角形一类。</p>
<p>如果<strong>K=5</strong>，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>蓝色</strong>的正方形一类。</p>
<p><strong>于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</strong></p>
<h4><span id="12-近邻的距离度量">1.2 近邻的距离度量</span></h4>
<p>我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。</p>
<p><strong>有哪些距离度量的表示法</strong>(普及知识点，可以跳过)：</p>
<h5><span id="121-欧式距离">1.2.1 欧式距离</span></h5>
<p>欧氏距离, 最常见的两点之间或多点之间的距离表示法,
又称之为欧几里得度量, 它定义于欧几里得空间中, 如点 <span class="math inline">\(x=(x 1, \ldots, x n)\)</span> 和 <span class="math inline">\(y=(y 1, \ldots, y n)\)</span> 之间的距离为: <span class="math display">\[
d(x,
y)=\sqrt{\left(x_1-y_1\right)^2+\left(x_2-y_2\right)^2+\ldots+\left(x_n-y_n\right)^2}=\sqrt{\sum_{i=1}^n\left(x_i-y_i\right)^2}
\]</span> 二维平面上两点 <span class="math inline">\(a(x 1, y
1)\)</span> 与 <span class="math inline">\(b(x 2, y 2)\)</span>
间的欧氏距离: <span class="math display">\[
d_{12}=\sqrt{\left(x_1-x_2\right)^2+\left(y_1-y_2\right)^2}
\]</span> 三维空间两点 <span class="math inline">\(a(x 1, y 1, z
1)\)</span> 与 <span class="math inline">\(b(x 2, y 2, z 2)\)</span>
间的欧氏距离: <span class="math display">\[
d_{12}=\sqrt{\left(x_1-x_2\right)^2+\left(y_1-y_2\right)^2+\left(z_1-z_2\right)^2}
\]</span> 两个n维向量 <span class="math inline">\(a(x 11, \times 12,
\ldots, x 1 n)\)</span> 与 <span class="math inline">\(b(\times 21,
\times 22, \ldots, \times 2 n)\)</span> 间的欧氏距离: <span class="math display">\[
d_{12}=\sqrt{\sum_{k=1}^n\left(x_{1 k}-x_{2 k}\right)^2}
\]</span> 也可以用表示成向量运算的形式:</p>
<h5><span id="122-曼哈顿距离">1.2.2 曼哈顿距离</span></h5>
<p>曼哈顿距离, 我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离,
也就是在欧几里得空间的固定
直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上, 坐标
<span class="math inline">\((x 1, y 1)\)</span> 的点P1与坐标 <span class="math inline">\((\mathrm{x} 2, \mathrm{y} 2)\)</span>
的点P2的曼哈顿距离为: <span class="math inline">\(\left|x_1-x_2\right|+\left|y_1-y_2\right|\)</span>,
要注意的是, 曼哈顿距离依赖座标系统的转度,
而非系统在座标轴上的平移或映射。</p>
<p>通俗来讲, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口,
驾驶距离是两点间的直线距离吗? 显 然不是,
除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”,
此即曼哈顿距离名称的来源, 同时, 曼 哈顿距离也称为城市街区距离(City Block
distance)。</p>
<p>二维平面两点 <span class="math inline">\(a(x 1, y 1)\)</span> 与
<span class="math inline">\(b(x 2, y 2)\)</span> 间的曼哈顿距离 <span class="math display">\[
d_{12}=\left|x_1-x_2\right|+\left|y_1-y_2\right|
\]</span> 两个 <span class="math inline">\(n\)</span> 维向量 <span class="math inline">\(a(x 11, x 12, \ldots, x 1 n)\)</span> 与 <span class="math inline">\(b(x 21, x 22, \ldots, x 2 n)\)</span>
间的曼哈顿距离 <span class="math display">\[
d_{12}=\sum_{k=1}^n\left|x_{1 k}-x_{2 k}\right|
\]</span></p>
<h5><span id="123-切比雪夫距离">1.2.3 切比雪夫距离</span></h5>
<p>切比雪夫距离, 若二个向量或二个点 <span class="math inline">\(\mathrm{p} 、\)</span> and <span class="math inline">\(\mathrm{q}\)</span>, 其座标分别为 <span class="math inline">\(\mathrm{P}\)</span> 及 <span class="math inline">\(\mathrm{qi}\)</span>,
则两者之间的切比雪夫距离定义如 下: <span class="math inline">\(D_{C h e
b y s h e v}(p, q)=\max _i\left(\left|p_i-q_i\right|\right)\)</span></p>
<p>这也等于以下Lp度量的极值： <span class="math inline">\(\lim _{x
\rightarrow \infty}\left(\sum_{i=1}^n\left|p_i-q_i\right|^k\right)^{1 /
k}\)</span>, 因此切比雪夫距离也称为 <span class="math inline">\(\infty\)</span> 度量。</p>
<p>以数学的观点来看, 切比雪夫距离是由一致范数 (uniform norm)
(或称为上确界范数）所衍生的度量, 也 是超凸度量（injective metric
space）的一种。</p>
<p>在平面几何中, 若二点 <span class="math inline">\(\mathrm{p}
\mathrm{q}\)</span> 的直角坐标系坐标为 <span class="math inline">\((x 1,
y 1)\)</span> 及 <span class="math inline">\((x 2, y 2)\)</span>,
则切比雪夫距离为: <span class="math inline">\(D_{C h e s s}=\max
\left(\left|x_2-x_1\right|,\left|y_2-y_1\right|\right)\)</span></p>
<p>玩过国际象棋的朋友或许知道，国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)
走到格子(x2,y2)最少需要多少步? 。你会发现最少步数总是 <span class="math inline">\(\max (|x 2-x 1| ，|y 2-y 1|)\)</span> 步
。有一种类似的 一种距离度量方法叫切比雪夫距离。</p>
<p>二维平面两点 <span class="math inline">\(a(x 1, y 1)\)</span> 与
<span class="math inline">\(b(x 2, y 2)\)</span> 间的切比雪夫距离 :
<span class="math display">\[
d_{12}=\max \left(\left|x_2-x_1\right|,\left|y_2-y_1\right|\right)
\]</span> 两个 <span class="math inline">\(n\)</span> 维向量 <span class="math inline">\(a(x 11, x 12, \ldots, x 1 n)\)</span> 与 <span class="math inline">\(b(x 21, x 22, \ldots, x 2 n)\)</span>
间的切比雪夫距离 : <span class="math display">\[
d_{12}=\max _i\left(\left|x_{1 i}-x_{2 i}\right|\right)
\]</span></p>
<p><strong>简单说来，各种“距离”的应用场景简单概括为：</strong></p>
<ul>
<li><strong>空间：欧氏距离</strong>，</li>
<li><strong>路径：曼哈顿距离，国际象棋国王：切比雪夫距离</strong>，</li>
<li>以上三种的统一形式:闵可夫斯基距离，</li>
<li>加权：标准化欧氏距离，</li>
<li>排除量纲和依存：马氏距离，</li>
<li>向量差距：夹角余弦，</li>
<li><strong>编码差别：汉明距离</strong>，</li>
<li>集合近似度：杰卡德类似系数与距离，</li>
<li>相关：相关系数与相关距离。</li>
</ul>
<h4><span id="13-k值选择">1.3 K值选择</span></h4>
<ol type="1">
<li>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></li>
<li>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且<strong>K值的增大就意味着整体的模型变得简单。</strong></li>
<li>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</li>
</ol>
<p>在实际应用中，K值一般取一个比较小的数值，<strong>例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</strong></p>
<h4><span id="14-knn最近邻分类算法的过程">1.4 KNN最近邻分类算法的过程</span></h4>
<ol type="1">
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前 k 个最小距离的样本；</li>
<li>根据这 k 个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<h3><span id="关于knn的一些问题">关于KNN的一些问题</span></h3>
<ol type="1">
<li><p>在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用<strong>曼哈顿距离</strong>？</p>
<p><strong>答：</strong>我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向的运动。</p></li>
<li><p>KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?</p>
<p>答：极大的节约了时间成本．点线距离如果
&gt;　最小点，无需回溯上一层，如果&lt;,则再上一层寻找。</p></li>
</ol>
<h3><span id="参考文献">参考文献</span></h3>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272"><font color="blue">KNN与KD树</font></a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23966698"><font color="blue">【数学】kd
树算法之详细篇 - 椰了的文章 - 知乎</font></a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/475072467/answer/2027766449"><font color="blue"><strong>KNN是生成式模型还是判别式的</strong>，为什么？
- 风控算法小白的回答 - 知乎</font></a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3BS4T2Z/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3BS4T2Z/" class="post-title-link" itemprop="url">降维（1）PCA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:27:10" itemprop="dateModified" datetime="2023-04-22T16:27:10+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>数据降维算法: https://www.zhihu.com/column/c_1194552337170214912</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221528241.jpg" alt="【机器学习】降维——PCA（非常详细）" style="zoom: 50%;"></p>
<h2><span id="一-pca">一、 PCA</span></h2>
<p><strong><font color="red"> 降维问题的优化目标：将一组 <span class="math inline">\(\mathrm{N}\)</span> 维向量降为 <span class="math inline">\(\mathrm{K}\)</span> 维，其目标是选择 <span class="math inline">\(\mathrm{K}\)</span>
个单位正交基，使得原始数据变换到这组基上 后，各变量两两间协方差为 0
，而变量方差则尽可能大（在正交的约束下，取最大的 <span class="math inline">\(\mathrm{K}\)</span> 个方差）。</font></strong></p>
<p>要找的 <span class="math inline">\(\mathbf{P}\)</span>
是能让<strong>原始协方差矩阵对角化</strong>的 <span class="math inline">\(\mathbf{P}\)</span> 。换句话说,
优化目标变成了寻找一个矩阵 <span class="math inline">\(\mathbf{P}\)</span>, <strong>满足 <span class="math inline">\(P C P^T\)</span> 是一个对
角矩阵，并且对角元素按从大到小依次排列，那么 <span class="math inline">\(P\)</span> 的前 <span class="math inline">\(K\)</span> 行就是要寻找的基，用 <span class="math inline">\(P\)</span> 的前 <span class="math inline">\(K\)</span> 行组成的矩阵乘以 <span class="math inline">\(X\)</span> 就使得 X 从 N 维降到了 K
维并满足上述优化条件。</strong></p>
<p><strong>PCA (Principal Component Analysis) 是一种常见的数据分析方式,
常用于高维数据的降维，可用于提取数 据的主要特征分量。</strong>PCA
的数学推导可以从<strong>最大可分型</strong>和<strong>最近重构性</strong>两方面进行,
前者的优化条件为划分后方差 最大,
后者的优化条件为点到划分平面距离最小，这里我将从最大可分性的角度进行证明。</p>
<h3><span id="1-向量表示与基变换">1. 向量表示与基变换</span></h3>
<p>我们先来介绍些线性代数的基本知识。</p>
<h4><span id="111-内积">1.1.1 内积</span></h4>
<p><strong>两个向量的 A 和 B 内积</strong>我们知道形式是这样的： <span class="math display">\[
\left(a_1, a_2, \cdots, a_n\right) \cdot\left(b_1, b_2, \cdots,
b_n\right)^{\top}=a_1 b_1+a_2 b_2+\cdots+a_n b_n
\]</span>
内积运算将两个向量映射为实数，其计算方式非常容易理解，但我们无法看出其物理含义。接下来我们从几何角度
来分析，为了简单起见，我们假设 <span class="math inline">\(A\)</span> 和
<span class="math inline">\(B\)</span> 均为二维向量，则： <span class="math display">\[
A=\left(x_1, y_1\right), \quad B=\left(x_2, y_2\right) A \cdot B=|A||B|
\cos (\alpha)
\]</span> 其几何表示见下图：</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221530907.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们看出 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(B\)</span> 的内积等于 <span class="math inline">\(A\)</span> 到 <span class="math inline">\(B\)</span> 的投影长度乘以 <span class="math inline">\(B\)</span> 的模。</p>
<p>如果假设 <span class="math inline">\(\mathrm{B}\)</span> 的模为 1 ，
即让 <span class="math inline">\(|B|=1\)</span> ，那么就变成了: <span class="math display">\[
A \cdot B=|A| \cos (a)
\]</span> 也就是说, <strong>A 与 <span class="math inline">\(B\)</span>
的内积值等于 A 向 <span class="math inline">\(B\)</span>
所在直线投影的标量大小。</strong></p>
<h4><span id="12-基">1.2 基</span></h4>
<p>在我们常说的坐标系种, 向量 <span class="math inline">\((3,2)\)</span>
其实隐式引入了一个定义：以 <span class="math inline">\(x\)</span> 轴和
<span class="math inline">\(y\)</span> 轴上正方向长度为 1
的向量为标准。向 量 <span class="math inline">\((3,2)\)</span>
实际是说在 <span class="math inline">\(x\)</span> 轴投影为 3 而 <span class="math inline">\(y\)</span> 轴的投影为 2。注意投影是一个标量,
所以可以为负。</p>
<p>所以, 对于向量 <span class="math inline">\((3,2)\)</span> 来说,
如果我们想求它在 <span class="math inline">\((1,0),(0,1)\)</span>
这组基下的坐标的话, 分别内积即可。当然, 内积完 了还是 <span class="math inline">\((3,2)\)</span> 。</p>
<p>所以, 我们大致可以得到一个结论, 我们<strong>要准确描述向量,
首先要确定一组基, 然后给出在基所在的各个直线上的 投影值,
就可以了</strong>。为了方便求坐标, 我们希望这组基向量模长为
1。因为向量的内积运算, 当模长为 1 时, 内积
可以直接表示投影。然后还需要这组基是线性无关的, 我们一般用正交基,
非正交的基也是可以的, 不过正交基有 较好的性质。</p>
<h4><span id="13-基变换的矩阵表示">1.3 基变换的矩阵表示</span></h4>
<p>这里我们先做一个练习：对于向量 <span class="math inline">\((3,2)\)</span> 这个点来说, 在 <span class="math inline">\(\left(\frac{1}{\sqrt{2}},
\frac{1}{\sqrt{2}}\right)\)</span> 和 <span class="math inline">\(\left(-\frac{1}{\sqrt{2}},
\frac{1}{\sqrt{2}}\right)\)</span> 这组基下的坐标是多少? 我们拿 <span class="math inline">\((3,2)\)</span> 分别与之内积, 得到 <span class="math inline">\(\left(\frac{5}{\sqrt{2}},-\frac{1}{\sqrt{2}}\right)\)</span>
这个新坐标。</p>
<p>我们可以用矩阵相乘的形式简洁的表示这个变换： <span class="math display">\[
\left(\begin{array}{cc}
1 / \sqrt{2} &amp; 1 / \sqrt{2} \\
-1 / \sqrt{2} &amp; 1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{l}
3 \\
2
\end{array}\right)=\left(\begin{array}{c}
5 / \sqrt{2} \\
-1 / \sqrt{2}
\end{array}\right)
\]</span> 左边矩阵的两行分别为两个基, 乘以原向量,
其结果刚好为新基的坐标。推广一下, 如果我们有 <span class="math inline">\(m\)</span> 个二维向量,
只要将二维向量按列排成一个两行 <span class="math inline">\(m\)</span>
列矩阵, 然后用“基矩阵”乘以这个矩阵就可以得到了所有这些向量在新基下
的值。例如对于数据点 <span class="math inline">\((1,1),(2,2),(3,3)\)</span> 来说,
想变换到刚才那组基上, 则可以这样表示: <span class="math display">\[
\left(\begin{array}{cc}
1 / \sqrt{2} &amp; 1 / \sqrt{2} \\
-1 / \sqrt{2} &amp; 1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{lll}
1 &amp; 2 &amp; 3 \\
1 &amp; 2 &amp; 3
\end{array}\right)=\left(\begin{array}{ccc}
2 / \sqrt{2} &amp; 4 / \sqrt{2} &amp; 6 / \sqrt{2} \\
0 &amp; 0 &amp; 0
\end{array}\right)
\]</span> 我们可以把它写成通用的表示形式： <span class="math display">\[
\left(\begin{array}{c}
p_1 \\
p_2 \\
\vdots \\
p_R
\end{array}\right)\left(\begin{array}{llll}
a_1 &amp; a_2 &amp; \cdots &amp; a_M
\end{array}\right)=\left(\begin{array}{cccc}
p_1 a_1 &amp; p_1 a_2 &amp; \cdots &amp; p_1 a_M \\
p_2 a_1 &amp; p_2 a_2 &amp; \cdots &amp; p_2 a_M \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
p_R a_1 &amp; p_R a_2 &amp; \cdots &amp; p_R a_M
\end{array}\right)
\]</span> 其中 <span class="math inline">\(p_i\)</span> 是一个行向量,
表示第 <span class="math inline">\(\mathrm{i}\)</span> 个基, <span class="math inline">\(a_j\)</span> 是一个列向量, 表示第 <span class="math inline">\(\mathrm{j}\)</span>
个原始数据记录。实际上也就是做了一个向 量矩阵化的操作。</p>
<p><font color="red"> 上述分析给矩阵相乘找到了一种物理解释:
两个矩阵相乘的意义是将右边矩阵中的每一列向量 <span class="math inline">\(a_i\)</span> 变换到左边矩阵
中以每一行行向量为基所表示的空间中去。也就是说一个矩阵可以表示一种线性变换。</font></p>
<h3><span id="2-最大可分性">2. 最大可分性</span></h3>
<p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示,
<strong>如果基的数量少于向量本身的维数, 则可以达
到降维的效果。</strong></p>
<p><strong>但是我们还没回答一个最关键的问题:
如何选择基才是最优的。或者说, 如果我们有一组 <span class="math inline">\(\mathbf{N}\)</span> 维向量, 现在要将其 降到 K
维（K 小于 N），那么我们应该如何选择 <span class="math inline">\(\mathrm{K}\)</span>
个基才能最大程度保留原有的信息？</strong></p>
<p>一种直观的看法是： <font color="red"> 希望投影后的投影值尽可能分散,
因为如果重叠就会有样本消失。当然这个也可以从樀的角
度进行理解，樀越大所含信息越多。</font></p>
<h4><span id="21-方差">2.1 方差</span></h4>
<p>我们知道数值的分散程度,
可以用数学上的方差来表述。<strong>一个变量的方差可以看做是每个元素与变量均值的差的平
方和的均值</strong>, 即: <span class="math display">\[
\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^m\left(a_i-\mu\right)^2
\]</span> <strong>为了方便处理，我们将每个变量的均值都化为 0</strong> ,
因此方差可以直接用每个元素的平方和除以元素个数表示: <span class="math display">\[
\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^m a_i^2
\]</span>
于是上面的问题被形式化表述为：<strong>寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大</strong>。</p>
<h4><span id="22-协方差">2.2 协方差</span></h4>
<p>在一维空间中我们可以用方差来表示数据的分散程度。而对于高维数据，我们用协方差进行约束，<strong>协方差可以表示两个变量的相关性</strong>。<strong><font color="red">
为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性</font></strong>，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。</p>
<p>协方差公式为：</p>
<p><span class="math display">\[
\operatorname{Cov}(a, b)=\frac{1}{m-1}
\sum_{i=1}^m\left(a_i-\mu_a\right)\left(b_i-\mu_b\right)
\]</span></p>
<p>由于均值为 0，所以我们的协方差公式可以表示为：</p>
<p><span class="math display">\[
\operatorname{Cov}(a, b)=\frac{1}{m} \sum_{i=1}^m a_i b_i
\]</span> 当样本数较大时，不必在意其是 m 还是
m-1，为了方便计算，我们分母取 m。</p>
<p><strong><font color="red"> 协方差为 0
时，表示两个变量完全不相关</font></strong>。为了让协方差为
0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。</p>
<p>（<strong>补充</strong>：协方差为 0
时，两个变量只是线性不相关。完全独立是有问题的，才疏学浅，还望见谅。）</p>
<p><strong><font color="red"> 至此，我们得到了降维问题的优化目标：将一组
N 维向量降为 K 维，其目标是选择 K
个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为
0，而变量方差则尽可能大（在正交的约束下，取最大的 K
个方差）。</font></strong></p>
<h4><span id="23-协方差矩阵">2.3 协方差矩阵</span></h4>
<p>针对我们给出的优化目标，接下来我们将从数学的角度来给出优化目标。我们看到，最终要达到的目的与<strong>变量内方差及变量间协方差</strong>有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们有：</p>
<p>假设我们只有 <span class="math inline">\(\mathrm{a}\)</span> 和 <span class="math inline">\(\mathrm{b}\)</span> 两个变量,
那么我们将它们按行组成矩阵 <span class="math inline">\(\mathrm{X}\)</span> : <span class="math display">\[
X=\left(\begin{array}{cccc}
a_1 &amp; a_2 &amp; \cdots &amp; a_m \\
b_1 &amp; b_2 &amp; \cdots &amp; b_m
\end{array}\right)
\]</span> 然后: <span class="math display">\[
\frac{1}{m} X X^{\top}=\left(\begin{array}{cc}
\frac{1}{m} \sum_{i=1}^m a_i^2 &amp; \frac{1}{m} \sum_{i=1}^m a_i b_i \\
\frac{1}{m} \sum_{i=1}^m a_i b_i &amp; \frac{1}{m} \sum_{i=1}^m b_i^2
\end{array}\right)=\left(\begin{array}{cc}
\operatorname{Cov}(a, a) &amp; \operatorname{Cov}(a, b) \\
\operatorname{Cov}(b, a) &amp; \operatorname{Cov}(b, b)
\end{array}\right)
\]</span> 我们可以看到这个矩阵对角线上的分别是两个变量的方差,
而其它元素是 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span> 的协方差。两者被统一到了一个
矩阵里。</p>
<p>设我们有 <span class="math inline">\(\mathrm{m}\)</span> 个 <span class="math inline">\(\mathrm{n}\)</span> 维数据记录, 将其排列成矩阵
<span class="math inline">\(X_{n, m}\)</span>, 设 <span class="math inline">\(C=\frac{1}{m} X X^T\)</span>, 则 <span class="math inline">\(\mathrm{C}\)</span> 是一个对称矩阵, 其对角线分别
对应各个变量的方差, 而第 <span class="math inline">\(\mathrm{i}\)</span>
行 <span class="math inline">\(\mathrm{j}\)</span> 列和 <span class="math inline">\(\mathrm{j}\)</span> 行 <span class="math inline">\(\mathrm{i}\)</span> 列元素相同, 表示 <span class="math inline">\(\mathrm{i}\)</span> 和 <span class="math inline">\(\mathrm{j}\)</span> 两个变量的协方差。</p>
<h4><span id="24-矩阵对角化">2.4 矩阵对角化</span></h4>
<p>根据我们的优化条件，<strong>我们需要将除对角线外的其它元素化为
0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大）</strong>，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系。</p>
<p>设原始数据矩阵 <span class="math inline">\(X\)</span>
对应的协方差矩阵为 <span class="math inline">\(C\)</span>, 而 <span class="math inline">\(P\)</span> 是一组基按行组成的矩阵, 设 <span class="math inline">\(Y=P X\)</span>, 则 <span class="math inline">\(Y\)</span> 为 <span class="math inline">\(X\)</span> 对 <span class="math inline">\(P\)</span> 做基变换后的 数据。设 <span class="math inline">\(Y\)</span> 的协方差矩阵为 <span class="math inline">\(D\)</span>, 我们推导一下 <span class="math inline">\(D\)</span> 与 <span class="math inline">\(C\)</span> 的关系: <span class="math display">\[
\begin{aligned}
D &amp; =\frac{1}{m} Y Y^T \\
&amp; =\frac{1}{m}(P X)(P X)^T \\
&amp; =\frac{1}{m} P X X^T P^T \\
&amp; =P\left(\frac{1}{m} X X^T\right) P^T \\
&amp; =P C P^T
\end{aligned}
\]</span> 这样我们就看清楚了, 我们要找的 <span class="math inline">\(\mathrm{P}\)</span> 是能让原始协方差矩阵对角化的
<span class="math inline">\(\mathrm{P}\)</span> 。换句话说,
优化目标变成了<strong>寻找一个矩 阵 <span class="math inline">\(\mathbf{P}\)</span>, 满足 <span class="math inline">\(P C P^T\)</span>
是一个对角矩阵，并且对角元素按从大到小依次排列，那么 <span class="math inline">\(\mathbf{P}\)</span> 的前 <span class="math inline">\(\mathbf{K}\)</span> 行就是要寻找的基, 用 <span class="math inline">\(\mathbf{P}\)</span> 的前 <span class="math inline">\(\mathrm{K}\)</span> 行组成的矩阵乘以 <span class="math inline">\(\mathrm{X}\)</span> 就使得 <span class="math inline">\(\mathrm{X}\)</span> 从 <span class="math inline">\(\mathrm{N}\)</span> 维降到了 <span class="math inline">\(\mathrm{K}\)</span>
维并满足上述优化条件。</strong></p>
<p>至此, 我们离 PCA 还有仅一步之遥, 我们还需要完成对角化。</p>
<p><strong>由上文知道，协方差矩阵 C
是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质:</strong></p>
<ol type="1">
<li>实对称矩阵不同特征值对应的特征向量必然正交。</li>
<li>设特征向量 <span class="math inline">\(\lambda\)</span> 重数为 <span class="math inline">\(r\)</span>, 则必然存在 <span class="math inline">\(r\)</span> 个线性无关的特征向量对应于 <span class="math inline">\(\lambda\)</span> ，因此可以将这 <span class="math inline">\(r\)</span> 个特征向量单位正 交化。</li>
</ol>
<p>由上面两条可知, 一个 <span class="math inline">\(\mathrm{n}\)</span>
行 <span class="math inline">\(\mathrm{n}\)</span>
列的实对称矩阵一定可以找到 <span class="math inline">\(\mathrm{n}\)</span> 个单位正交特征向量, 设这 <span class="math inline">\(\mathrm{n}\)</span> 个特征向量为 <span class="math inline">\(e_1, e_2, \cdots, e_n\)</span>,
我们将其按列组成矩阵: <span class="math inline">\(E=\left(e_1, e_2,
\cdots, e_n\right)\)</span> 。</p>
<p>则对协方差矩阵 C 有如下结论: <span class="math display">\[
E^T C E=\Lambda=\left(\begin{array}{llll}
\lambda_1 &amp; &amp; &amp; \\
&amp; \lambda_2 &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \lambda_n
\end{array}\right)
\]</span> 其中 <span class="math inline">\(\Lambda\)</span> 为对角矩阵,
其对角元素为各特征向量对应的特征值（可能有重复）。到这里,
我们发现我们已经找到了 需要的矩阵 P: <span class="math inline">\(P=E^{\top}\)</span> 。</p>
<p><strong><span class="math inline">\(P\)</span>
是协方差矩阵的特征向量单位化后按行排列出的矩阵</strong>，其中每一行都是
<span class="math inline">\(C\)</span> 的一个特征向量。如果设 <span class="math inline">\(P\)</span> 按照 <span class="math inline">\(\Lambda\)</span> 中 特征值的从大到小,
将特征向量从上到下排列, 则用 <span class="math inline">\(P\)</span> 的前
<span class="math inline">\(K\)</span> 行组成的矩阵乘以原始数据矩阵
<span class="math inline">\(X\)</span>, 就得到了我们
需要的降维后的数据矩阵 <span class="math inline">\(Y\)</span> 。</p>
<blockquote>
<p><strong>拉格朗日乘子法证明</strong>:<strong>方差就是协方差矩阵的特征值</strong></p>
</blockquote>
<h4><span id="25-最近重构性-思路">2.5 最近重构性-思路</span></h4>
<p>以上的证明思路主要是基于最大可分性的思想，<strong>通过一条直线使得样本点投影到该直线上的方差最大</strong>。除此之外，我们还可以<strong>将其转换为线型回归问题，其目标是求解一个线性函数使得对应直线能够更好地拟合样本点集合</strong>。这就<strong>使得我们的优化目标从方差最大转化为平方误差最小</strong>，因为映射距离越短，丢失的信息也会越小。区别于最大可分性，这是从最近重构性的角度进行论证。</p>
<h3><span id="3-求解步骤">3. 求解步骤</span></h3>
<p><strong>总结一下 PCA 的算法步骤：设有 <span class="math inline">\(m\)</span> 条 <span class="math inline">\(n\)</span> 维数据。</strong></p>
<ol start="3" type="1">
<li>将原始数据按列组成 <span class="math inline">\(\mathbf{n}\)</span>
行 <span class="math inline">\(\mathbf{m}\)</span> 列矩阵 <span class="math inline">\(\mathbf{X}\)</span>;</li>
<li>将 <span class="math inline">\(\mathrm{X}\)</span>
的每一行进行零均值化，即减去这一行的均值；【零均值化】【方差、协方差好计算】</li>
<li>求出协方差矩阵 <span class="math inline">\(C=\frac{1}{m} X
X^{\top}\)</span>;</li>
<li>求出协方差矩阵的特征值及对应的特征向量;</li>
<li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 <span class="math inline">\(\mathbf{k}\)</span> 行组成矩阵 <span class="math inline">\(\mathbf{P}\)</span>;</li>
<li><span class="math inline">\(Y=P X\)</span> 即为降维到 <span class="math inline">\(\mathbf{k}\)</span> 维后的数据。</li>
</ol>
<h3><span id="4性质维度灾难-降噪-过拟合-特征独立">4.
性质【维度灾难、降噪、过拟合、特征独立】</span></h3>
<ol type="1">
<li><strong>缓解维度灾难</strong>：PCA
算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；</li>
<li><strong>降噪</strong>：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；</li>
<li><strong>过拟合</strong>：PCA
保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以
PCA 也可能加剧了过拟合；</li>
<li><strong>特征独立</strong>：PCA
不仅将数据压缩到低维，它也使得<strong>降维之后的数据各特征相互独立</strong>；</li>
</ol>
<h3><span id="5-细节">5. 细节</span></h3>
<h4><span id="51-零均值化">5.1 零均值化</span></h4>
<p>当对训练集进行 PCA
降维时，也需要对验证集、测试集执行同样的降维。而<strong>对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来</strong>，不能使用验证集或者测试集的中心向量。</p>
<p>其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。</p>
<p>另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现
Variance Shift 的问题。</p>
<h4><span id="52-svd-的对比">5.2 SVD 的对比</span></h4>
<p>这是两个不同的数学定义。我们先给结论：<strong>特征值和特征向量是针对方阵</strong>才有的，而<strong>对任意形状的矩阵都可以做奇异值分解</strong>。</p>
<p><strong>PCA</strong>：<strong>方阵的特征值分解</strong>，对于一个方阵
A。其中，Q 是这个矩阵 A 的特征向量组成的矩阵， <span class="math inline">\(\Lambda\)</span>
是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列)。也就是说矩阵
A 的信息可以由其特征值和特征向量表示。</p>
<p><strong>SVD：矩阵的奇异值分解其实就是对于矩阵 <span class="math inline">\(\mathrm{A}\)</span> 的协方差矩阵 <span class="math inline">\(A^T A\)</span> 和 <span class="math inline">\(A
A^T\)</span> 做特征值分解推导出来的</strong>: <span class="math display">\[
A_{m, n}=U_{m, m} \Lambda_{m, n} V_{n, n}^T \approx U_{m, k} \Lambda_{k,
k} V_{k, n}^T
\]</span> 其中: <span class="math inline">\(U，V\)</span> 都是正交矩阵,
有 <span class="math inline">\(U^T U=I_m, V^T V=I_n\)</span>
。这里的约等于是因为 <span class="math inline">\(\Lambda\)</span> 中有
<span class="math inline">\(\mathrm{n}\)</span> 个奇异值, 但是由于排在
后面的很多接近 0, 所以我们可以仅保留比较大的 <span class="math inline">\(\mathrm{k}\)</span> 个奇异值。 <span class="math display">\[
\begin{aligned}
&amp; A^T A=\left(U \Lambda V^T\right)^T U \Lambda V^T=V \Lambda^T U^T U
\Lambda V^T=V \Lambda^2 V^T \\
&amp; A A^T=U \Lambda V^T\left(U \Lambda V^T\right)^T=U \Lambda V^T V
\Lambda^T U^T=U \Lambda^2 U^T
\end{aligned}
\]</span> 所以, <span class="math inline">\(V \cup\)</span>
两个矩阵分别是 <span class="math inline">\(A^T A\)</span> 和 <span class="math inline">\(A A^T\)</span> 的特征向量,
中间的矩阵对角线的元素是 <span class="math inline">\(A^T A\)</span> 和
<span class="math inline">\(A A^T\)</span> 的特征值。我 们也很容易看出
<span class="math inline">\(\mathrm{A}\)</span> 的奇异值和 <span class="math inline">\(A^T A\)</span> 的特征值之间的关系。</p>
<p>PCA 需要对协方差矩阵 <span class="math inline">\(C=\frac{1}{m} X
X^T\)</span> 。进行特征值分解; SVD 也是对 <span class="math inline">\(A^T A\)</span> 进行特征值分解。如果取 <span class="math inline">\(A=\frac{X^T}{\sqrt{m}}\)</span>
则两者基本等价。所以 PCA 问题可以转换成 SVD 求解。</p>
<p><strong>而实际上 Sklearn 的 PCA 就是用 SVD
进行求解的</strong>，原因有以下几点：</p>
<ol type="1">
<li>当样本维度很高时，协方差矩阵计算太慢；</li>
<li>方阵特征值分解计算效率不高；</li>
<li><strong>SVD
除了特征值分解这种求解方式外，还有更高效更准确的迭代求解方式，避免了<span class="math inline">\(A^T A\)</span>的计算；</strong></li>
<li><strong>其实 PCA 与 SVD 的右奇异向量的压缩效果相同</strong>。</li>
</ol>
<h3><span id="参考链接">参考链接</span></h3>
<ol type="1">
<li>《机器学习》周志华</li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.codinglabs.org/articles/pca-tutorial.html">PCA
的数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm">Singular
Value Decomposition (SVD) tutorial</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">机器学习中的数学（4）——线性判别分析（LDA）,
主成分分析（PCA）</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//my.oschina.net/findbill/blog/535044">从SVD到PCA——奇妙的数学游戏</a></li>
<li>scikit-learn：降维算法PCA和SVD
https://blog.csdn.net/HHG20171226/article/details/102981822</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2Y0FFGH/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2Y0FFGH/" class="post-title-link" itemprop="url">降维（2）LDA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:37:33" itemprop="dateModified" datetime="2023-04-22T16:37:33+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-线性判别分析lda监督">一、线性判别分析（LDA）【监督】</span></h2>
<blockquote>
<p><strong>“投影后类内方差最小，类间方差最大”</strong></p>
<ul>
<li>https://blog.csdn.net/liuweiyuxiang/article/details/78874106</li>
</ul>
</blockquote>
<h3><span id="11-概念">1.1 概念</span></h3>
<p><strong>线性判别分析（Linear Discriminant
Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。</strong></p>
<p><strong>LDA分类思想简单总结如下：</strong></p>
<ol type="1">
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。</li>
<li>对于训练数据，设法将多维数据投影到一条直线上，<strong>同类数据的投影点尽可能接近，异类数据点尽可能远离</strong>。</li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</li>
</ol>
<p><strong><font color="red">
如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</font></strong></p>
<p>假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221634387.png" alt="image-20220526135646769"></p>
<p>从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。
以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3><span id="12-原理">1.2 原理</span></h3>
<p>LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(<a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Linear_classifier">Linear
Classifier</a>)：因为LDA是一种线性分类器。对于<strong>K-分类的一个分类问题，会有K个线性函数</strong>：</p>
<p><span class="math display">\[
y_k(x)=w_k^T x+w_{k 0}
\]</span></p>
<p>当满足条件：对于所有的j，都有Yk &gt;
Yj,的时候，我们就说x属于类别k。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的就是所属的分类了。</p>
<p>上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221635862.gif" alt="clip_image002" style="zoom:67%;"></p>
<p>红色的方形的点为0类的原始点、蓝色的方形点为1类的原始点，经过原点的那条线就是投影的直线，从图上可以清楚的看到，红色的点和蓝色的点被<strong>原点</strong>明显的分开了，这个数据只是随便画的，如果在高维的情况下，看起来会更好一点。下面我来推导一下二分类LDA问题的公式：假设用来区分二分类的直线（投影函数)为：
<span class="math display">\[
y_k(x)=w_k^T x+w_{k 0}
\]</span> 当满足条件：对于所有的j, 都有 <span class="math inline">\(Y
k&gt;Y j\)</span>,的时候, 我们就说 <span class="math inline">\(x\)</span> 属于类别 <span class="math inline">\(k\)</span> 。对于每一个分类, 都有一个公式去算一个
分值，在所有的公式得到的分值中, 找一个最大的就是所属的分类了。</p>
<p>上式实际上就是一种投影, 是将一个高维的点投影到一条高维的直线上,
LDA最求的目标是, 给出一个标注了类别 的数据集, 投影到了一条直线之后,
能够使得点尽量的按类别区分开, 当 <span class="math inline">\(k=2\)</span> 即二分类问题的时候, 如下图所示:</p>
<p>红色的方形的点为 0 类的原始点、蓝色的方形点为 1
类的原始点，经过原点的那条线就是投影的直线，从图上可以 清楚的看到,
红色的点和蓝色的点被原点明显的分开了, 这个数据只是随便画的,
如果在高维的情况下, 看起来会
更好一点。下面我来推导一下二分类LDA问题的公式：假设用来区分二分类的直线（投影函数)为：
<span class="math display">\[
y=w^T x
\]</span> <strong>LDA分类的一个目标是使得不同类别之间的距离越远越好,
同一类别之中的距离越近越好</strong>, 所以我们需要定义几 个关键的值。</p>
<ul>
<li><strong>类别的原始中心点为</strong>：(Di表示属于类别的点)</li>
</ul>
<p><span class="math display">\[
m_i=\frac{1}{n_i} \sum_{x \in D_i} x
\]</span></p>
<ul>
<li>类别投影后的中心点为:</li>
</ul>
<p><span class="math display">\[
\widetilde{m_i}=w^T m_i
\]</span></p>
<ul>
<li><strong>衡量类别i投影后, 类别点之间的分散程度 (方差)
为</strong>:</li>
</ul>
<p><span class="math display">\[
\tilde{s_i}=\sum_{y \in Y_i}\left(y-\tilde{m_i}\right)^2
\]</span></p>
<ul>
<li><strong>最终我们可以得到一个下面的公式,
表示LDA投影到w后的损失函数</strong>:</li>
</ul>
<p><span class="math display">\[
J(w)=\frac{\left|\widetilde{m_1}-\widetilde{m_2}\right|^2}{\widetilde{s}_1^2+\widetilde{s}_2^2}
\]</span></p>
<p>我们<strong>分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。</strong>分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。</p>
<p>我们定义一个<strong>投影前的各类别分散程度的矩阵</strong>，这个矩阵看起来有一点麻烦，其实意思是，如果某一个分类的<strong>输入点集Di里面的点距离这个分类的中心店mi越近</strong>，则Si里面元素的值就越小，如果分类的点都紧紧地围绕着mi，则Si里面的元素值越更接近0.
<span class="math display">\[
S_{i}=\sum_{x \in D_{i}}\left(x-m_{i}\right)\left(x-m_{i}\right)^{T}
\]</span> 带入 <span class="math inline">\(\mathrm{Si}\)</span>, 将
<span class="math inline">\(\mathrm{J}(\mathrm{w})\)</span>
分母化为:</p>
<p><span class="math inline">\(\tilde{s}_{i}=\sum_{x \in
D_{i}}\left(w^{T} x-w^{T} m_{i}\right)^{2}=\sum_{x \in D_{i}}
w^{T}\left(x-m_{i}\right)\left(x-m_{i}\right)^{T} w=w^{T} S_{i}
w\)</span></p>
<p><span class="math display">\[{\tilde{S_{1}}}^{2}+{\tilde{S_{2}}}^{2}=w^{T}\left(S_{1}+S_{2}\right)
w=w^{T} S_{w} w\]</span></p>
<p>同样的将 <span class="math inline">\(\mathrm{J}(\mathrm{w})\)</span>
分子化为: <span class="math display">\[
\left|\widetilde{m_{1}}-\widetilde{m_{2}}\right|^{2}=w^{T}\left(m_{1}-m_{2}\right)\left(m_{1}-m_{2}\right)^{T}
w=w^{T} S_{B} w
\]</span> 这样<strong>损失函数</strong>可以化成下面的形式: <span class="math display">\[
J(w)=\frac{w^{T} S_{B} w}{w^{T} S_{w} w}
\]</span> 这样就可以用最喜欢的<strong>拉格朗日乘子法</strong>了,
但是还有一个问题, 如果分子、分母是都可以取任意值的, 那就会 使得有无穷解,
我们将分母限制为长度为 1, 并作为拉格朗日乘子法的限制条件, 带入得到:
<span class="math display">\[
\begin{aligned}
&amp;c(w)=w^{T} S_{B} w-\lambda\left(w^{T} S_{w} w-1\right) \\
&amp;\Rightarrow \frac{d c}{d w}=2 S_{B} w-2 \lambda S_{w} w=0 \\
&amp;\Rightarrow S_{B} w=\lambda S_{w} w
\end{aligned}
\]</span> <strong>这样的式子就是一个求特征值的问题了。</strong> 对于
<span class="math inline">\(N(N&gt;2)\)</span> 分类的问题,
我就直接写出下面的结论了: <span class="math display">\[
\begin{aligned}
&amp;S_{W}=\sum_{i=1}^{c} S_{i} \\
&amp;S_{B}=\sum_{i=1}^{c}
n_{i}\left(m_{i}-m\right)\left(m_{i}-m\right)^{T} \\
&amp;S_{B} w_{i}=\lambda S_{w} w_{i}
\end{aligned}
\]</span>
这同样是一个求特征值的问题，我们求出的第i大的特征向量，就是对应的Wi了。</p>
<blockquote>
<p>这里想多谈谈特征值，特征值在纯数学、量子力学、固体力学、计算机等等领域都有广泛的应用，特征值表示的是矩阵的性质，当我们取到矩阵的前N个最大的特征值的时候，我们可以说提取到的矩阵主要的成分（这个和之后的PCA相关，但是不是完全一样的概念）。在机器学习领域，不少的地方都要用到特征值的计算，比如说图像识别、pagerank、LDA、还有之后将会提到的PCA等等。</p>
</blockquote>
<p><strong>优缺点</strong></p>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 90%">
</colgroup>
<thead>
<tr class="header">
<th>优缺点</th>
<th>简要说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>优点</td>
<td>1. 可以使用类别的先验知识； 2.
以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr class="even">
<td>缺点</td>
<td>1. LDA不适合对非高斯分布样本进行降维； 2.
<strong>LDA降维最多降到分类数k-1维</strong>； 3.
LDA在样本分类信息依赖方差而不是均值时，降维效果不好； 4.
LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/24K7V34/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/24K7V34/" class="post-title-link" itemprop="url">降维（3）t-SNE</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:29:43" itemprop="dateModified" datetime="2023-04-22T16:29:43+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-t-sne-高维数据可视化">一、t-SNE 高维数据可视化</span></h3>
<blockquote>
<p>高维数据可视化之t-SNE算法🌈:https://zhuanlan.zhihu.com/p/57937096</p>
</blockquote>
<p><strong>T-SNE算法是用于可视化的算法中效果最好的算法之一</strong>，相信大家也对T-SNE算法略有耳闻，本文参考T-SNE作者<strong>Laurens
van der
Maaten</strong>给出的源代码自己实现T-SNE算法代码，以此来加深对T-SNE的理解。先简单介绍一下T-SNE算法，T-SNE将数据点变换映射到概率分布上。</p>
<h4><span id="11-t-sne数据算法的目的">1.1 t-SNE数据算法的目的</span></h4>
<p><strong>主要是将数据从高维数据转到低维数据，并在低维空间里也保持其在高维空间里所携带的信息（比如高维空间里有的清晰的分布特征，转到低维度时也依然存在）。</strong></p>
<p><strong><font color="red">
t-SNE将欧氏距离距离转换为条件概率，来表达点与点之间的相似度，再优化两个分布之间的距离-KL散度，从而保证点与点之间的分布概率不变。</font></strong></p>
<h4><span id="12-sne原理">1.2 SNE原理</span></h4>
<p><span class="math inline">\(S N E\)</span>
是<strong>通过仿射变换将数据点映射到相应概率分布上</strong>,
主要包括下面两个步骤: 1. 通过在高维空间中构建数据点之间的概率分布 <span class="math inline">\(P\)</span>, 使得相似的数据点有更高的概率被选择, 而
不相似的数据点有较低的概率被选择; 2.
然后在低维空间里重构这些点的概率分布 <span class="math inline">\(Q\)</span>, 使得这两个概率分布尽可能相似。</p>
<p>令输入空间是 <span class="math inline">\(X \in
\mathbb{R}^{n}\)</span>, 输出空间为 <span class="math inline">\(Y \in
\mathbb{R}^{t}(t \ll n)\)</span> 。不妨假设含有 <span class="math inline">\(m\)</span> 个样本数据 <span class="math inline">\(\left\{x^{(1)}, x^{(2)}, \cdots,
x^{(m)}\right\}\)</span>, 其中 <span class="math inline">\(x^{(i)} \in
X\)</span>, 降维后的数据为 <span class="math inline">\(\left\{y^{(1)},
y^{(2)}, \cdots, y^{(m)}\right\}, y^{(i)} \in Y\)</span> 。 <span class="math inline">\(S N E\)</span>
是<strong>先将欧几里得距离转化为条件概率来表达点与点之间的相似度</strong>,
即首先是计算条件概 率 <span class="math inline">\(p_{j \mid i}\)</span>,
其正比于 <span class="math inline">\(x^{(i)}\)</span> 和 <span class="math inline">\(x^{(j)}\)</span> 之间的相似度, <span class="math inline">\(p_{j \mid i}\)</span> 的计算公式为: <span class="math display">\[
p_{j \mid i}=\frac{\exp
\left(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^{2}}{2
\sigma_{i}^{2}}\right)}{\sum_{k \neq i} \exp
\left(-\frac{\left\|x^{(i)}-x^{(k)}\right\|^{2}}{2
\sigma_{i}^{2}}\right)}
\]</span> 在这里引入了一个参数 <span class="math inline">\(\sigma_{i}\)</span>, 对于不同的数据点 <span class="math inline">\(x^{(i)}\)</span> 取值亦不相同,
因为我们关注的是不同数据 点两两之间的相似度, 故可设置 <span class="math inline">\(p_{i \mid i}=0\)</span> 。对于低维度下的数据点
<span class="math inline">\(y^{(i)}\)</span>, 通过条件概率 <span class="math inline">\(q_{j \mid i}\)</span> 来 刻画 <span class="math inline">\(y^{(i)}\)</span> 与 <span class="math inline">\(y^{(j)}\)</span> 之间的相似度, <span class="math inline">\(q_{j \mid i}\)</span> 的计算公式为: <span class="math display">\[
q_{j \mid i}=\frac{\exp
\left(-\left\|y^{(i)}-y^{(j)}\right\|^{2}\right)}{\sum_{k \neq i} \exp
\left(-\left\|y^{(i)}-y^{(k)}\right\|^{2}\right)}
\]</span> 同理, 设置 <span class="math inline">\(q_{i \mid i}=0\)</span>
。 如果降维的效果比较好, 局部特征保留完整, 那么有 <span class="math inline">\(p_{i \mid j}=q_{i \mid j}\)</span> 成立,
因此通过优化两个分布之 间的 <strong><span class="math inline">\(K
L\)</span> 散度构造出的损失函数为</strong>: <span class="math display">\[
C\left(y^{(i)}\right)=\sum_{i} K L\left(P_{i} \| Q_{i}\right)=\sum_{i}
\sum_{j} p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}}
\]</span> 这里的 <span class="math inline">\(P_{i}\)</span>
表示在给定高维数据点 <span class="math inline">\(x^{(i)}\)</span> 时,
其他所有数据点的条件概率分布; <span class="math inline">\(Q_{i}\)</span>
则表示在给定 低维数据点 <span class="math inline">\(y^{(i)}\)</span> 时,
其他所有数据点的条件概率分布。从损失函数可以看出, 当 <span class="math inline">\(p_{j \mid i}\)</span> 较大 <span class="math inline">\(q_{j \mid i}\)</span> 较小时, 惩罚较高; 而 <span class="math inline">\(p_{j \mid i}\)</span> 较小 <span class="math inline">\(q_{j \mid i}\)</span> 较大时,
惩罚较低。换句话说就是高维空间中两个数据点距 离较近时,
若映射到低维空间后距离较远, 那么将得到一个很高的惩罚; 反之,
高维空间中两个数 据点距离较远时, 若映射到低维空间距离较近,
将得到一个很低的惩罚值。也就是说, <strong><span class="math inline">\(S
N E\)</span> 的 损失函数更关注于局部特征,
而忽视了全局结构</strong>。</p>
<h4><span id="13-目标函数求解">1.3 目标函数求解</span></h4>
<h4><span id="14-对称性-sne">1.4 对称性-SNE</span></h4>
<p><strong>优化 <span class="math inline">\(K L(P \| Q)\)</span>
的一种替换思路是使用联合概率分布来替换条件概率分布</strong>, 即 <span class="math inline">\(P\)</span> 是高维空间里数据点的联合概 率分布,
<span class="math inline">\(Q\)</span>
是低维空间里数据点的联合概率分布，此时的损失函数为: <span class="math display">\[
C\left(y^{(i)}\right)=K L(P \| Q)=\sum_i \sum_j p_{i j} \log \frac{p_{i
j}}{q_{i j}}
\]</span> 同样的 <span class="math inline">\(p_{i i}=q_{i i}=0\)</span>
，这种改进下的 <span class="math inline">\(S N E\)</span> 称为对称 <span class="math inline">\(S N E\)</span> ，因为它的先验假设为对 <span class="math inline">\(\forall i\)</span> 有 <span class="math inline">\(p_{i j}=p_{j i}, q_{i j}=q_{j i}\)</span>
成立，故概率分布可以改写成: <span class="math display">\[
p_{i j}=\frac{\exp \left(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^2}{2
\sigma^2}\right)}{\sum_{k \neq l} \exp
\left(-\frac{\left\|x^{(k)}-x^{(l)}\right\|^2}{2 \sigma^2}\right)} \quad
q_{i j}=\frac{\exp
\left(-\left\|y^{(i)}-y^{(j)}\right\|^2\right)}{\sum_{k \neq l} \exp
\left(-\left\|y^{(k)}-y^{(l)}\right\|^2\right)}
\]</span> 这种改进方法使得表达式简洁很多, 但是容易受到异常点数据的影响,
为了解决这个问题通过对联合概率分布定义修正为： <span class="math inline">\(p_{i j}=\frac{p_{j \mid i}+p_{i \mid
j}}{2}\)</span>, 这保证了 <span class="math inline">\(\sum_j p_{i
j}&gt;\frac{1}{2 m}\)</span> ，使得每个点对于损失函数都会有贡献。对称
<span class="math inline">\(S N E\)</span> 最大的 优点是简化了梯度计算,
梯度公式改写为: <span class="math display">\[
\frac{\partial C\left(y^{(i)}\right)}{\partial y^{(i)}}=4
\sum_j\left(p_{i j}-q_{i j}\right)\left(y^{(i)}-y^{(j)}\right)
\]</span> 研究表明, 对称 <span class="math inline">\(S N E\)</span> 和
<span class="math inline">\(S N E\)</span> 的效果差不多,
有时甚至更好一点。</p>
<h4><span id="15-t-sne">1.5 t-SNE</span></h4>
<p><span class="math inline">\(t-S N E\)</span> 在对称 <span class="math inline">\(S N E\)</span> 的改进是,
首先通过在高维空间中使用高斯分布将距离转换为概率分布，然后<strong>在低维空间
中，使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度空间中的中低等距离在映射后能够有一个
较大的距离</strong>。</p>
<p><img src="https://pic4.zhimg.com/80/v2-928a3ada308128f26b719d510a728fbb_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>从图中可以看到，在没有异常点时, <span class="math inline">\(t\)</span>
分布与高斯分布的拟合结果基本一致。而在第二张图中, 出现了部分异常点,
由于高斯分布的尾部较低, 对异常点比较敏感, 为了照顾这些异常点,
高斯分布的拟合结果偏离了大多数样本所在位置,
方差也较大。<strong>相比之下, <span class="math inline">\(t\)</span>
分布的尾部较高, 对异常点不敏感, 保证了其鲁棒性, 因此拟合结果更为合理,
较好的捕获了数据的全局特征。</strong></p>
<p>使用 <span class="math inline">\(t\)</span> 分布替换高斯分布之后
<span class="math inline">\(q_{i j}\)</span> 的变化如下: <span class="math display">\[
q_{i
j}=\frac{\left(1+\left\|y^{(i)}-y^{(j)}\right\|^2\right)^{-1}}{\sum_{k
\neq l}\left(1+\left\|y^{(i)}-y^{(j)}\right\|^2\right)^{-1}}
\]</span> 此外，随着自由度的逐渐增大, <span class="math inline">\(t\)</span>
分布的密度函数逐渐接近标准正态分布，因此在计算梯度方面会简单很多, 优
化后的梯度公式如下: <span class="math display">\[
\frac{\partial C\left(y^{(i)}\right)}{\partial y^{(i)}}=4
\sum_j\left(p_{i j}-q_{i
j}\right)\left(y^{(i)}-y^{(j)}\right)\left(1+\left\|y^{(i)}-y^{(j)}\right\|^2\right)^{-1}
\]</span> 总的来说, <span class="math inline">\(t-S N E\)</span>
的梯度更新具有以下两个优势： -
<strong>对于低维空间中不相似的数据点，用一个较小的距离会产生较大的梯度让这些数据点排斥开来;</strong>
-
<strong>这种排斥又不会无限大，因此避免了不相似的数据点距离太远。</strong></p>
<h5><span id="t-s-n-e-算法其实就是在-s-n-e-算法的基础上增加了两个改进"><span class="math inline">\(t-S N E\)</span> 算法其实就是在 <span class="math inline">\(S N E\)</span> 算法的基础上增加了两个改进:</span></h5>
<ul>
<li>把 <span class="math inline">\(S N E\)</span> 修正为对称 <span class="math inline">\(S N E\)</span> ，提高了计算效率,
效果稍有提升;</li>
<li>在低维空间中采用了 <span class="math inline">\(t\)</span>
分布替换原来的高斯分布，解决了高维空间映射到低维空间所产生的拥挤问题,
优化 了 <span class="math inline">\(S N E\)</span>
过于关注局部特征而忽略全局特征的问题。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/F3RYP7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/F3RYP7/" class="post-title-link" itemprop="url">降维（4）AutoEncoder</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:30:11" itemprop="dateModified" datetime="2023-04-22T16:30:11+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>430</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="autoencoder">AutoEncoder</span></h3>
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80377698">【全】一文带你了解自编码器（<em>AutoEncoder</em>）</a></li>
</ul>
</blockquote>
<p>理解为：（下图）高维数据（左测蓝色）通过某种网络变成低位数据（中间红色）后，又经过某种网络变回高维数据（右侧蓝色）。数据经过该模型前后没有变化，而中间的低维数据完全具有输入输出的高维数据的全部信息，所以可以用低维数据代表高维数据。</p>
<p>之所以叫AutoEncoder，而不叫AutoEncoderDecoder，是因为训练好之后只有encoder部分有用，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=decoder&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22157482881%22%7D">decoder</a>部分就不用了。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221552166.jpg" alt="img" style="zoom: 67%;"></p>
<p>进入深度学习的思路之后，编码的网络是开放的，可以自由设计的。一个思路是端到端，将网络的输出设为你任务要的结果（如类别、序列等），<strong>过程中的某层嵌入都可以作为降维的低维结果</strong>。当然，这种低维结果其实是模型的副产品，因为任务已经解决。比如bert模型得到（中文的）字嵌入。</p>
<h4><span id="优点">优点：</span></h4>
<ul>
<li>能够学习到非线性特性</li>
<li>降低数据维度</li>
</ul>
<h4><span id="缺点">缺点：</span></h4>
<ul>
<li>训练的<strong>计算成本高</strong></li>
<li><strong>可解释性较差</strong></li>
<li>背后的数学知识复杂</li>
<li>容易产生<strong>过度拟合</strong>的问题，尽管可以通过引入正则化策略缓解</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1CZDQSE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1CZDQSE/" class="post-title-link" itemprop="url">理论基础（3）参数估计</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:58:04" itemprop="dateCreated datePublished" datetime="2022-03-15T22:58:04+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 11:34:01" itemprop="dateModified" datetime="2023-04-26T11:34:01+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="机器学习优化方法optimization">机器学习优化方法
(Optimization)</span></h3>
<blockquote>
<p>机器学习与优化基础（Machine Learning and
Optimization）:https://zhuanlan.zhihu.com/p/169835477</p>
<p>最优化方法复习笔记：https://github.com/LSTM-Kirigaya/OptimizeNote</p>
<p><strong>FreeWill</strong>：<a target="_blank" rel="noopener" href="https://plushunter.github.io/2017/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8825%EF%BC%89%EF%BC%9A%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/">机器学习算法系列（25）：最速下降法、牛顿法、拟牛顿法</a></p>
</blockquote>
<h3><span id="一-什么是凸优化">一、什么是凸优化</span></h3>
<p><strong>凸函数</strong>的严格定义为, 函数 <span class="math inline">\(L(\cdot)\)</span>
是凸函数当且仅当对定义域中的任意两点 <span class="math inline">\(x,
y\)</span> 和任意实数 <span class="math inline">\(\lambda
\in[0,1]\)</span> 总有: <span class="math display">\[
L(\lambda x+(1-\lambda) y) \leq \lambda L(x)+(1-\lambda) L(y)
\]</span> 该不等式的一个直观解释是, 凸函数曲面上任意两点连接而成的线段,
其上的任 意一点都不会处于该函数曲面的 下方，如下图所示所示。</p>
<p>该不等式的一个直观解释是，凸函数曲面上任意两点连接而成的线段，其上的任
意一点都不会处于该函数曲面的下方，如下图所示所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304252130605.jpeg" alt="img" style="zoom:67%;"></p>
<p>凸优化问题的例子包括支持向量机、线性回归等
线性模型，非凸优化问题的例子包括低秩模型（如矩阵分解）、深度神经网络模型等。</p>
<h3><span id="二-正则化项">二、正则化项</span></h3>
<p>使用正则化项，也就是给loss
function加上一个参数项，正则化项有<strong>L1正则化、L2正则化、ElasticNet</strong>。加入这个正则化项好处：</p>
<ul>
<li>控制参数幅度，不让模型“无法无天”。</li>
<li>限制参数搜索空间</li>
<li>解决欠拟合与过拟合的问题。</li>
</ul>
<h3><span id="三-常见的几种最优化方法">三、常见的几种最优化方法</span></h3>
<p><font color="red">下面以线性模型损失函数为例：</font></p>
<h4><span id="31-梯度下降法"><strong>3.1 梯度下降法</strong></span></h4>
<p>梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当<strong>目标函数是凸函数时，梯度下降法的解是全局解</strong>。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。
<span class="math display">\[
\begin{gathered}\frac{\partial}{\partial \theta_j}
J(\theta)=\frac{\partial}{\partial \theta_j}
\frac{1}{2}\left(h_\theta(x)-y\right)^2 \\ =2 \cdot
\frac{1}{2}\left(h_\theta(x)-y\right) \frac{\partial}{\partial
\theta_j}\left(h_\theta(x)-y\right) \\ =\left(h_\theta(x)-y\right)
x_j\end{gathered}
\]</span>
梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的搜索迭代示意图如下图所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304252127619.png" alt="img" style="zoom: 67%;"></p>
<ul>
<li><strong>批量梯度下降法（BGD）</strong></li>
</ul>
<p><strong>参数θ的值每更新一次都要遍历样本集中的所有的样本</strong>，得到新的θj，看是否满足阈值要求，若满足，则迭代结束，根据此值就可以得到；否则继续迭代。【易受极小值影响】</p>
<ul>
<li><h5><span id="随机梯度下降算法sgd单样本增量梯度下降">随机梯度下降算法（SGD）【单样本增量梯度下降】</span></h5></li>
</ul>
<p>每次更新只用到一个训练样本，若根据当前严格不能进行迭代得到一个，此时会得到一个，有新样本进来之后，在此基础上继续迭代，又得到一组新的和，以此类推。</p>
<p>缺点：靠近极小值时收敛速度减慢；直线搜索时可能会产生一些问题；可能会“之字形”地下降。</p>
<h4><span id="32-牛顿法">3.2 牛顿法</span></h4>
<p>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数 <span class="math inline">\(\mathrm{f}(\mathrm{x})\)</span>
的泰勒级数的前面几项来寻找方程 <span class="math inline">\((x)=0\)</span>
的根。牛顿法最大的特点就在于它的收敛速度很快。具体步骤：</p>
<ul>
<li>首先, 选择一个接近函数 <span class="math inline">\(f(x)\)</span>
零点的 <span class="math inline">\(x 0\)</span>, 计算相应的 <span class="math inline">\(f(x 0)\)</span> 和切线斜率 <span class="math inline">\(f^{\prime}(x 0)\)</span> (这里 <span class="math inline">\(f^{\prime}\)</span> 表示函数 <span class="math inline">\(f\)</span> 的导 数)</li>
<li>然后我们计算穿过点 <span class="math inline">\((x 0, f(x
0))\)</span> 并且斜率为 <span class="math inline">\(f^{\prime}(x
0)\)</span> 的直线和 <span class="math inline">\(x\)</span> 轴的交点的
<span class="math inline">\(x\)</span> 坐标, 也就是求如下方程的解: <span class="math inline">\(x *
f^{\prime}\left(x_0\right)+f\left(x_0\right)-x_0 *
f^{\prime}\left(x_0\right)=0\)</span></li>
<li>我们将新求得的点的 <span class="math inline">\(x\)</span> 坐标命名为
<span class="math inline">\(x 1\)</span>, 通常 <span class="math inline">\(x 1\)</span> 会比 <span class="math inline">\(x
0\)</span> 更接近方程 <span class="math inline">\(f(x)=0\)</span>
的解。因此我们现在可以利用 <span class="math inline">\(x 1\)</span>
开始下一轮迭代。</li>
</ul>
<p>由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法搜索动态示例图：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304252126567.gif" alt="img" style="zoom: 67%;"></p>
<p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。<strong>缺点：</strong></p>
<ul>
<li>牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
<ul>
<li>在高维情况下这个矩阵非常大，计算和存储都是问题。</li>
</ul></li>
<li>在小批量的情况下，牛顿法对于二阶导数的估计噪声太大。
<ul>
<li>目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。</li>
</ul></li>
</ul>
<h4><span id="33-拟牛顿法">3.3 拟牛顿法</span></h4>
<p>拟牛顿法是求解非线性优化问题最有效的方法之一，<strong>本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</strong>拟牛顿法和梯度下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于梯度下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>
<p><strong>DFP、BFGS、L-BFGS:</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261133926.png" alt="image-20220509171547743" style="zoom:50%;"></p>
<h4><span id="34-共轭梯度法">3.4 共轭梯度法</span></h4>
<p>共轭梯度法是介于梯度下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。
在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p>
<p>具体的实现步骤请参加wiki百科<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB">共轭梯度法</a>。下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304252126043.jpeg" alt="img" style="zoom:67%;"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/22/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><span class="page-number current">23</span><a class="page-number" href="/page/24/">24</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/24/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
