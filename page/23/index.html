<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/23/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/23/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/23/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">239</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3VQ3FZD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3VQ3FZD/" class="post-title-link" itemprop="url">集成学习（5）LightGBM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:13:30" itemprop="dateCreated datePublished" datetime="2022-03-11T21:13:30+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 21:19:58" itemprop="dateModified" datetime="2023-04-21T21:19:58+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-lightgbm">一、LightGBM</span></h2><blockquote>
<ul>
<li>《<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=Lightgbm%3A+A+highly+efficient+gradient+boosting+decision+tree&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">Lightgbm: A highly efficient gradient boosting decision tree</a>》</li>
<li>《A communication-efficient <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=parallel+algorithm+for+decision+tree&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">parallel algorithm for decision tree</a>》</li>
</ul>
</blockquote>
<p>LightGBM 由微软提出，主要用于解决 GDBT 在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中。从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有<strong>训练速度快、内存占用低</strong>的特点。下图分别显示了 XGBoost、XGBoost_hist（利用梯度直方图的 XGBoost） 和 LightGBM 三者之间针对不同数据集情况下的内存和训练时间的对比：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304212057167.jpg" alt="img"></p>
<p>那么 LightGBM 到底如何做到<strong>更快的训练速度和更低的内存</strong>使用的呢？</p>
<p><strong><font color="red"> LightGBM 为了解决这些问题提出了以下几点解决方案：</font></strong></p>
<ol>
<li><p><strong>【减小内存、最优分类点】直方图算法</strong>；【特征离散化 + 内存占用 + 方差减少】</p>
</li>
<li><p><strong>【样本维度】 单边梯度抽样算法</strong>；</p>
<ul>
<li><p>【根据样本梯度来对梯度小的这边样本进行采样，一部分大梯度和随机分布】</p>
</li>
<li><p>一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。</p>
</li>
</ul>
</li>
<li><p><strong>【特征维度】互斥特征捆绑算法</strong>；【特征稀疏行优化 +分箱 】</p>
</li>
<li><p><strong>【分裂算法】基于最大深度的 Leaf-wise 的垂直生长算法</strong>；【深度限制的最大分裂收益的叶子】</p>
</li>
<li><p><strong>类别特征最优分割</strong>；</p>
</li>
<li><p><strong>特征并行和数据并行</strong>；</p>
</li>
<li><p><strong>缓存优化。</strong></p>
</li>
</ol>
<h3><span id="11-数学原理">1.1 数学原理</span></h3><h4><span id="111-直方图算法"><strong>1.1.1 直方图算法</strong></span></h4><h5><span id="1-直方图算法"><strong>(1) 直方图算法</strong></span></h5><p><strong><font color="red"> 直方图算法的基本思想是将连续的特征离散化为 k （默认256, 1字节）个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。</font></strong></p>
<p>我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）：</p>
<ul>
<li><strong>内存占用更小：</strong>XGBoost 需要用 32 位的浮点数去存储特征值，并用 32 位的整形去存储排序索引，而 LightGBM 只需要用 8 位去存储直方图，相当于减少了 1/8；</li>
<li><strong>计算代价更小：</strong>计算特征分裂增益时，XGBoost 需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k 次，直接将时间复杂度从代价是O( feature <em> distinct_values_of_the_feature); 而 histogram 只需要计算 bins次, 代价是( feature </em> bins)。distinct_values_of_the_feature &gt;&gt; bins</li>
</ul>
<p><strong>（2）直方图优化算法流程:</strong></p>
<ol>
<li><strong>直方图优化算法需要在训练前预先把特征值转化为bin value</strong>，也就是对每个特征的取值做个分段函数，将所有样本在该特征上的取值划分到某一段（bin）中。最终把特征取值从连续值转化成了离散值。需要注意得是：feature value对应的bin value在整个训练过程中是不会改变的。</li>
<li><strong>最外面的 for 循环表示的意思是对当前模型下所有的叶子节点处理</strong>，需要遍历所有的特征，来找到增益最大的特征及其划分值，以此来分裂该叶子节点。</li>
<li>在某个叶子上，第二个 for 循环就开始遍历所有的特征了。<strong>对于每个特征，首先为其创建一个直方图 (new Histogram() )</strong>。这个直方图存储了两类信息，分别是<strong><font color="red"> 每个bin中样本的梯度之和 $H[ f.bins[i] ].g$ </font></strong>，还有就是<strong>每个bin中样本数量</strong>$（H[f.bins[i]].n）$</li>
<li>第三个 for 循环遍历所有样本，累积上述的两类统计值到样本所属的bin中。即直方图的每个 bin 中包含了一定的样本，在此计算每个 bin 中的样本的梯度之和并对 bin 中的样本记数。</li>
<li>最后一个for循环, 遍历所有bin, 分别以当前bin作为分割点, 累加其左边的bin至当前bin的梯度和（ $\left.S<em>{L}\right)$ 以及样本数量 $\left(n</em>{L}\right)$, 并与父节点上的总梯度和 $\left(S<em>{p}\right)$ 以及总样本数量 $\left(n</em>{p}\right)$ 相减, 得到右边 所有bin的梯度和 $\left(S<em>{R}\right)$ 以及样本数量 $\left(n</em>{R}\right)$, 带入公式, 计算出增益, 在遍历过程中取最大的增 益, 以此时的特征和bin的特征值作为分裂节点的特征和分裂特征取值。</li>
</ol>
<h5><span id="3-源码分析">(3) 源码分析</span></h5><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/anshuai_aw1/article/details/83040541">https://blog.csdn.net/anshuai_aw1/article/details/83040541</a></p>
<p>  <strong><font color="red"> 『我爱机器学习』集成学习（四）LightGBM</font></strong>：<a target="_blank" rel="noopener" href="https://www.hrwhisper.me/machine-learning-lightgbm/">https://www.hrwhisper.me/machine-learning-lightgbm/</a></p>
</blockquote>
<p>问题一：<strong>如何将特征映射到bin呢？即如何分桶？对于连续特征和类别特征分别怎么样处理？</strong></p>
<p>问题二：<strong>如何构建直方图？直方图算法累加的g是什么？难道没有二阶导数h吗？</strong></p>
<h5><span id="特征分桶">特征分桶：</span></h5><blockquote>
<p>  <strong>特征分桶的源码</strong>在<strong>bin.cpp</strong>文件和<strong>bin.h</strong>文件中。由于LGBM可以处理类别特征，因此对连续特征和类别特征的处理方式是不一样的。</p>
</blockquote>
<h5><span id="连续特征">连续特征:</span></h5><p>在<strong>bin.cpp</strong>中，我们可以看到<strong>GreedyFindBin</strong>函数和<strong>FindBinWithZeroAsOneBin</strong>函数，这两个函数得到了数值型特征取值（负数，0，正数）的各个bin的切分点，即bin_upper_bound。</p>
<h5><span id="greedyfindbin-数值型根据特征不同取值的个数划分类别型">GreedyFindBin: 数值型根据特征不同取值的个数划分，类别型？？</span></h5><ul>
<li><em>特征取值计数的数组</em>、<em>特征的不同的取值的数组</em>、<em>特征有多少个不同的取值</em></li>
<li><strong>bin_upper_bound就是记录桶分界的数组</strong></li>
<li>特征取值数比max_bin数量少，直接取distinct_values的中点放置</li>
<li>特征取值数比max_bin来得大，说明几个特征值要共用一个bin<ul>
<li>如果一个特征值的数目比mean_bin_size大，那么这些特征需要单独一个bin</li>
<li>剩下的特征取值的样本数平均每个剩下的bin：mean size for one bin</li>
</ul>
</li>
</ul>
<h5><span id="构建直方图">构建直方图：</span></h5><p>给定一个特征的值，我们现在已经可以转化为对应的bin了。现在我们就可以构建直方图了。</p>
<h5><span id="constructhistogram"><strong>ConstructHistogram</strong>：</span></h5><ul>
<li><strong>累加了一阶、二阶梯度和还有个数</strong></li>
<li>当然还有其它的版本，当is_constant_hessianis_constant_hessian为true的时候是不用二阶梯度的</li>
</ul>
<h5><span id="寻找最优切分点-缺失值处理-gain和xgb一样">寻找最优切分点 : 缺失值处理 + Gain和XGB一样</span></h5><h5><span id="4直方图算法优点"><strong><font color="red"> （4）直方图算法优点：</font></strong></span></h5><ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于 直方图算法，则只需要(1x样本数x维 度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的 bin 值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p>
</li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为$k$的树的时间复杂度：对特征所有取值的排序为$O(NlogN)$，$N$为样本点数目，若有$D$维特征，则$O(kDNlogN)$，而直方图算法需要$O(kD \times bin)$ (bin是histogram 的横轴的数量，一般远小于样本数量$N$)。</p>
</li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>两个维度</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的$k$个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
</li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM 所使用直方图算法对 Cache 天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</p>
</li>
<li><p><strong>数据并行优化</strong>，用 histgoram 可以大幅降低通信代价。用 pre-sorted 算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst 在并行的时候也使用 histogram 进行通信。</p>
</li>
</ul>
<h5><span id="5直方图算法缺点">（5）直方图算法缺点：</span></h5><p><strong>当然，直方图算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。</strong>但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；<strong>较粗的分割点也有正则化的效果，可以有效地防止过拟合</strong>；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（GradientBoosting）的框架下没有太大的影响。</p>
<h4><span id="112-单边梯度抽样算法"><strong>1.1.2 单边梯度抽样算法</strong></span></h4><font color="red"> **直方图算法仍有优化的空间**，建立直方图的复杂度为O(**feature × data**)，如果能**降低特征数**或者**降低样本数**，训练的时间会大大减少。</font>

<p><strong>GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法</strong>（Gradient-based One-Side Sampling, GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算锅中只需关注梯度高的样本，极大的减少了计算量。</p>
<ol>
<li>根据<strong>梯度的绝对值</strong>将样本进行<strong>降序</strong>排序</li>
<li>选择前a×100%的样本，这些样本称为A</li>
<li>剩下的数据(1−a)×100的数据中，随机抽取b×100%的数据，这些样本称为B</li>
<li>在计算增益的时候，放大样本B中的梯度 (1−a)/b 倍</li>
<li>关于g，在具体的实现中是一阶梯度和二阶梯度的乘积，见Github的实现（LightGBM/src/boosting/goss.hpp）</li>
</ol>
<blockquote>
<p>  a%（大梯度）+ (1-a)/ b * b % 的大梯度</p>
</blockquote>
<p><strong>使用GOSS进行采样, 使得训练算法更加的关注没有充分训练(under-trained)的样本, 并且只会稍微的改变原有的数据分布</strong>。原有的在特征值为 $\mathrm{d}$ 处分数据带来的增益可以定义为：</p>
<script type="math/tex; mode=display">
V_{j \mid O}(d)=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in O: x_{i j} \leq d} g_{i}\right)^{2}}{n_{l \mid O}^{j}(d)}+\frac{\left(\sum_{x_{i} \in O: x_{i j}>d} g_{i}\right)^{2}}{n_{r \mid O}^{j}(d)}\right)</script><p>其中:</p>
<ul>
<li>O为在决策树待分裂节点的训练集</li>
<li>$n<em>{o}=\sum I\left(x</em>{i} \in O\right)$</li>
<li>$n<em>{l \mid O}^{j}(d)=\sum I\left[x</em>{i} \in O: x<em>{i j} \leq d\right]$ and $n</em>{r \mid O}^{j}(d)=\sum I\left[x<em>{i} \in O: x</em>{i j}&gt;d\right]$</li>
</ul>
<p><strong>而使用GOSS后, 增益定义为：</strong></p>
<script type="math/tex; mode=display">
V_{j \mid O}(d)=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in A_{l}} g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}} g_{i}\right)^{2}}{n_{l}^{j}(d)}+\frac{\left(\sum_{x_{i} \in A_{r}} g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}} g_{r}\right)^{2}}{n_{r}^{j}(d)}\right)</script><p>其中:</p>
<ul>
<li>$A<em>{l}=\left{x</em>{i} \in A: x<em>{i j} \leq d\right}, A</em>{r}=\left{x<em>{i} \in A: x</em>{i j}&gt;d\right}$</li>
<li>$B<em>{l}=\left{x</em>{i} \in B: x<em>{i j} \leq d\right}, B</em>{r}=\left{x<em>{i} \in B: x</em>{i j}&gt;d\right}$</li>
</ul>
<p>实验表明，该做法并没有降低模型性能，反而还有一定提升。究其原因，应该是采样也会增加弱学习器的多样性，从而潜在地提升了模型的泛化能力，稍微有点像深度学习的dropout。</p>
<h4><span id="113-互斥特征捆绑算法冲突小的特征可能与多个特征包组合特征集合"><strong>1.1.3 互斥特征捆绑算法</strong>【冲突小的特征可能与多个特征包组合】[特征集合]</span></h4><blockquote>
<p>  <strong>互斥指的是一些特征很少同时出现非0值</strong>【<strong>类似one-hot特征</strong>】</p>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/366234433">详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）</a></p>
</blockquote>
<p><strong><font color="red"> 互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行合并，则可以降低特征数量。</font></strong>高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。</p>
<p><strong>1）首先介绍如何判定哪些特征应该捆绑在一起？</strong></p>
<p>EFB算法采用<strong>构图（build graph）</strong>的思想，将特征作为节点，不互斥的特征之间进行连边，然后从图中找出所有的捆绑特征集合。其实学过数据结构里的图算法就了解过，这个问题基本就是<a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%9B%BE%E7%9D%80%E8%89%B2%E9%97%AE%E9%A2%98/8928655%3Ffr%3Daladdin">图着色问题</a>。但是图着色问题是一个<strong>NP-hard问题</strong>，不可能在多项式时间里找到最优解。</p>
<p>因此EFB采用了一种近似的贪心策略解决办法。<strong>它允许特征之间存在少数的样本点并不互斥（比如某些对应的样本 点之间并不同时为非 0 ）</strong>, 并设置一个最大冲突阈值 $K$ 。我们选择合适的 $K$ 值, 可以在准确度和训绩效率上获 得很好的trade-off (均衡)。</p>
<p><strong>下面给出EFB的特征捆绑的贪心策略流程：</strong></p>
<blockquote>
<p>  （1）将特征作为图的顶点，对于<strong>不互斥的特征进行相连</strong>（存在同时不为0的样本），特征同时不为0的样本个数作为边的权重；<br>  （2）根据顶点的度对特征进行降序排序，度越大表明特征与其他特征的冲突越大（越不太可能与其他特征进行捆绑）；【<strong>入度排序，转化为非零值个数排序</strong>】<br>  （3）设置<strong>最大冲突阈值K</strong>，外层循环先对每一个上述排序好的特征，遍历已有的特征捆绑簇，如果发现该特征加入到该特征簇中的冲突数不会超过最大阈值K，则将该特征加入到该簇中。否则新建一个特征簇，将该特征加入到新建的簇中。</p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-743681d9fd6cebee11f0dcc607f2f687_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>上面时间的复杂度为 $O(N^2)$，n为特征的数量，时间其实主要花费在建图上面，两两特征计算互斥程度的时间较长（2层for循环）。对于百万级别的特征数量来说，该复杂度仍是<strong>不可行的</strong>。为了提高效率，可以不再构建图，将特征直接按照非零值个数排序，将特征<strong>非零值个数</strong>类比为节点的度（即冲突程度)，因为更多的非零值更容易引起冲突。只是改进了排序策略，不再构建图，下面的for循环是一样的。</p>
<p><strong>2）如何将特征捆绑簇里面的所有特征捆绑（合并）为一个特征？</strong>【<strong>直方图偏移</strong>】</p>
<p>如何进行合并，最关键的是如何能将原始特征从合并好的特征进行分离出来。EFB采用的是加入一个<strong>偏移常量</strong>（offset）来解决。</p>
<blockquote>
<p>  举个例子，我们绑定两个特征A和B，A取值范围为[0, 10)，B取值范围为[0, 20)。则我们可以加入一个偏移常量10，即将B的取值范围变为[10,30），然后合并后的特征范围就是[0, 30)，并且能很好的分离出原始特征~</p>
</blockquote>
<p>因为lgb中<strong>直方图算法</strong>对特征值进行了<strong>分桶</strong>（bin）操作，导致合并互斥特征变得更为简单。从上面伪码看到偏移常量offset直接对每个特征桶的数量累加就行，然后放入偏移常数数组（binRanges）中。</p>
<h4><span id="114-带深度限制的-leaf-wise-算法"><strong>1.1.4 带深度限制的 Leaf-wise 算法</strong></span></h4><h5><span id="level-wise">Level-wise</span></h5><p>大多数GBDT框架使用的按层生长 (level-wise) 的决策树生长策略，Level-wise遍历一次数据可以同时分裂同一层的叶子，容易进行<strong>多线程优化</strong>，也好<strong>控制模型复杂度，不容易过拟合</strong>。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
<h5><span id="leaf-wise">Leaf-wise</span></h5><p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<p><img src="https://pic2.zhimg.com/80/v2-76f2f27dd24fc452a9a65003e5cdd305_1440w.jpg" alt="img"></p>
<h4><span id="115-lightgbm类别特征最优分割"><strong>1.1.5 LightGBM类别特征最优分割</strong></span></h4><blockquote>
<p>  LightGBM中只需要提前将类别映射到非负整数即可(<code>integer-encoded categorical features</code>)</p>
</blockquote>
<p><strong>我们知道，LightGBM可以直接处理类别特征，而不需要对类别特征做额外的one-hot encoding。那么LGB是如何实现的呢？</strong></p>
<p>类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足, LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。<strong>LightGBM 采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分</strong>。假设某维 特征有 k 个类别，则有$2^k - 1$种可能, 时间复杂度为$o(2^k)$ ,LightGBM 基于 Fisher的 《<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=On+Grouping+For+Maximum+Homogeneity&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">On Grouping For Maximum Homogeneity</a>》论文实现了 O(klogk) 的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=时间复杂度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">时间复杂度</a>。</p>
<p><strong>算法流程如下图所示</strong>，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序; 然后按照排序的结果依次枚举最优分割点。从下图可以看到, $\frac{\operatorname{Sum}(y)}{\operatorname{Count}(y)}$为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。</p>
<p><img src="https://pic1.zhimg.com/v2-0f1b7024e9da8f09c75b7f8e436a5d24_b.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>在Expo数据集上的实验结果表明，相比0/1展开的方法，使用LightGBM支持的类别特征可以使训练速度加速8倍，并且精度一致。</strong>更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。</p>
<h3><span id="12-工程实现-并行计算">1.2 工程实现 - 并行计算</span></h3><h4><span id="121-特征并行优化-最优划分点"><strong>1.2.1 特征并行</strong>【优化 最优划分点】</span></h4><p>传统的特征并行算法在于对数据进行垂直划分，然后使用<strong>不同机器找到不同特征的最优分裂点</strong>，<strong>基于通信整合得到最佳划分点</strong>，然后基于通信告知其他机器划分结果。在本小节中，<strong>工作的节点称为worker</strong></p>
<h5><span id="传统"><strong>传统：</strong></span></h5><ul>
<li>垂直划分数据<strong>（对特征划分）</strong>，<strong>不同的worker有不同的特征集</strong></li>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li><strong>具有全局最佳切分点的worker进行节点分裂，然后广播切分后的结果</strong>（<strong>左右子树的instance indices</strong>）</li>
<li>其它worker根据收到的instance indices也进行划分</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-b0d10c5cd832402e4503e2c1220f7376_r.jpg" alt="preview" style="zoom: 67%;"></p>
<p><strong>传统的特征并行方法有个很大的缺点</strong>：</p>
<ul>
<li><strong>需要告知每台机器最终划分结果，增加了额外的复杂度</strong>（因为对数据进行垂直划分，每台机器所含数据不同，划分结果需要通过通信告知）；</li>
<li>无法加速split的过程，该过程复杂度为O(#data)O(#data)，当数据量大的时候效率不高；</li>
</ul>
<h5><span id="lightgbm"><strong>LightGBM</strong></span></h5><p><strong>LightGBM 则不进行数据垂直划分，每台机器都有训练集完整数据</strong>，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。</p>
<ul>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li>每个worker根据全局最佳切分点进行节点分裂</li>
</ul>
<h5><span id="缺点">缺点：</span></h5><ul>
<li>split过程的复杂度仍是O(#data)，当数据量大的时候效率不高</li>
<li><strong>每个worker保存所有数据，存储代价高</strong></li>
</ul>
<h4><span id="122-数据并行"><strong>1.2.2 数据并行</strong></span></h4><h5><span id="传统方法">传统方法：</span></h5><p>数据并行目标是并行化整个决策学习的过程：</p>
<ul>
<li>水平切分数据，<strong>不同的worker拥有部分数据</strong></li>
<li>每个worker根据本地数据构建局部直方图</li>
<li>合并所有的局部直方图得到全部直方图</li>
<li>根据全局直方图找到最优切分点并进行分裂</li>
</ul>
<p><img src="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-data-parallelization.png" alt="LightGBM-data-parallelization"></p>
<p>在第3步中，有两种合并的方式：</p>
<ul>
<li>采用点对点方式(point-to-point communication algorithm)进行通讯，每个worker通讯量为$O(machine <em>  feature </em>  bin $)。</li>
<li>采用collective communication algorithm(如“All Reduce”)进行通讯（相当于有一个中心节点，通讯后在返回结果），每个worker的通讯量为$O(machine <em>  feature </em>  bin $)。</li>
</ul>
<h5><span id="lightgbm中的数据并行">LightGBM中的数据并行</span></h5><ol>
<li><strong>使用“Reduce Scatter”将不同worker的不同特征的直方图合并，然后workers在局部合并的直方图中找到局部最优划分，最后同步全局最优划分。</strong></li>
<li>前面提到过，可以通过直方图作差法得到兄弟节点的直方图，因此只需要通信一个节点的直方图。</li>
</ol>
<p>传统的数据并行策略主要为水平划分数据，然后本地构建直方图并整合成全局直方图，最后在全局直方图中找出最佳划分点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 $O(machine <em>  feature </em>  bin $); 如果使用集成的通信, 则通讯开销为 $O(2 <em> feature </em>  b i n)$.</p>
<p><strong>LightGBM 采用分散规约（Reduce scatter）的方式将直方图整合的任务分摊到不同机器上，从而降低通信代价，并通过直方图做差进一步降低不同机器间的通信。</strong></p>
<h4><span id="123-投票并行"><strong>1.2.3 投票并行</strong></span></h4><p>LightGBM采用一种称为<strong>PV-Tree</strong>的算法进行投票并行(Voting Parallel)，其实这本质上也是一种<strong>数据并行</strong>。PV-Tree和普通的决策树差不多，只是在寻找最优切分点上有所不同。</p>
<p>其算法伪代码描述如下：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304212108095.png" alt="LightGBM-pv-tree" style="zoom:50%;"></p>
<ol>
<li>水平切分数据，不同的worker拥有部分数据。</li>
<li>Local voting: <strong>每个worker构建直方图，找到top-k个最优的本地划分特征。</strong></li>
<li>Global voting: <strong>中心节点聚合得到最优的top-2k个全局划分特征（top-2k是看对各个worker选择特征的个数进行计数，取最多的2k个）。</strong></li>
<li><strong>Best Attribute Identification</strong>： <strong>中心节点向worker收集这top-2k个特征的直方图，并进行合并，然后计算得到全局的最优划分。</strong></li>
<li>中心节点将全局最优划分广播给所有的worker，worker进行本地划分。</li>
</ol>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304212107125.png" alt="LightGBM-voting-parallelization" style="zoom: 50%;"></p>
<p><strong>可以看出，PV-tree将原本需要 $O(machine <em>  feature </em>  bin $) 变为了 $O(2 <em> feature </em>  b i n)$，通信开销得到降低。此外，可以证明，当每个worker的数据足够多的时候，top-2k个中包含全局最佳切分点的概率非常高。</strong></p>
<h4><span id="124-缓存优化"><strong>1.2.4 缓存优化</strong></span></h4><p>上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。</p>
<p>而 LightGBM 所使用直方图算法对 Cache 天生友好：</p>
<ol>
<li>首先，<strong>所有的特征都采用相同的方法获得梯度</strong>（区别于不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中；</li>
<li>其次，因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3TFM6N7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3TFM6N7/" class="post-title-link" itemprop="url">线性模型（1）线性回归</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-08 18:56:01" itemprop="dateCreated datePublished" datetime="2022-03-08T18:56:01+08:00">2022-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-20 02:49:51" itemprop="dateModified" datetime="2023-04-20T02:49:51+08:00">2023-04-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">线性模型</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="一、线性回归"><a href="#一、线性回归" class="headerlink" title="一、线性回归"></a>一、线性回归</h2><p><strong>线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。</strong>这样就可以表达特征与结果之间的非线性关系。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/3TFM6N7/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1J1QH0W/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1J1QH0W/" class="post-title-link" itemprop="url">线性模型（2）逻辑回归</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-08 18:56:01" itemprop="dateCreated datePublished" datetime="2022-03-08T18:56:01+08:00">2022-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-20 19:08:22" itemprop="dateModified" datetime="2023-04-20T19:08:22+08:00">2023-04-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">线性模型</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>19k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>35 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>Logistic Regression</strong> 虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic Regression 因其简单、可并行化、可解释强深受工业界喜爱。<strong>Logistic 回归的本质是：假设数据服从这个Logistic分布，然后使用极大似然估计做参数的估计。</strong></p>
<p><strong>逻辑回归的思路</strong>是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/1J1QH0W/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3BVGDDE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3BVGDDE/" class="post-title-link" itemprop="url">深度学习-NLP（2）Word2vec*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-08 17:34:12" itemprop="dateCreated datePublished" datetime="2022-03-08T17:34:12+08:00">2022-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-19 19:28:10" itemprop="dateModified" datetime="2022-07-19T19:28:10+08:00">2022-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-word2vec">一、Word2Vec</span></h2><blockquote>
<ul>
<li><strong>nlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert</strong>：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56382372">https://zhuanlan.zhihu.com/p/56382372</a></li>
<li>Word2Vec算法梳理🔥 - 杨航锋的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58290018">https://zhuanlan.zhihu.com/p/58290018</a><ul>
<li><a target="_blank" rel="noopener" href="https://plushunter.github.io/2018/02/14/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9AWord2Vec/">Free will：Word2Vec</a></li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26306795">[NLP] 秒懂词向量<em>Word2vec</em>的本质</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/61635013"><em>Word2Vec</em>详解</a></p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53425736"><em>word2vec</em>详解（CBOW，skip-gram，负采样，分层Softmax）</a></strong></li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89637281">快速入门词嵌入之<em>word2vec</em></a></p>
<p><strong>word2vec 相比之前的 Word Embedding 方法好在什么地方？</strong></p>
</li>
<li><p><strong>极快的训练速度</strong>。以前的语言模型优化的目标是MLE，只能说词向量是其副产品。Mikolov应该是第一个提出抛弃MLE（和困惑度）指标，就是要学习一个好的词嵌入。如果不追求MLE，模型就可以大幅简化，去除隐藏层。再利用HSoftmax以及负采样的加速方法，可以使得训练在小时级别完成。而原来的语言模型可能需要几周时间。</p>
<ul>
<li><strong>一个很酷炫的man-woman=king-queen的示例</strong>。这个示例使得人们发现词嵌入还可以这么玩，并促使词嵌入学习成为了一个研究方向，而不再仅仅是神经网络中的一些参数。</li>
</ul>
</li>
<li><strong>word2vec里有大量的tricks，比如噪声分布如何选？如何采样？如何负采样？</strong>等等。这些tricks虽然摆不上台面，但是对于得到一个好的词向量至关重要。</li>
</ul>
</blockquote>
<p>谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种<strong>浅层的神经网络模型</strong>，它有两种网络结构，分别是<strong>连续词袋</strong>（CBOW）和<strong>跳字</strong>(Skip-Gram)模型。</p>
<blockquote>
<p>  CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好</p>
</blockquote>
<h3><span id="11-介绍cbow">1.1 介绍CBOW</span></h3><p>CBOW，全称Continuous Bag-of-Word，中文叫做连续词袋模型：<strong>以上下文来预测当前词</strong> $w<em>t$ 。CBOW模型的目的是预测 $P(w_t| w</em>{t-k}, \cdots, w<em>{t-1}, w</em>{t+1}, \cdots, w_{t+k}) $</p>
<p><img src="https://pic4.zhimg.com/v2-27f3e577618f84c0026968d273d823ef_b.jpg" alt="img"></p>
<h4><span id="前向传播过程">前向传播过程</span></h4><ul>
<li><p><strong>输入层</strong>: 输入C个单词$x$： $x<em>{1k}, \cdots, x</em>{Ck} $，并且每个 $x$ 都是用 <strong>One-hot</strong> 编码表示，每一个 $x$ 的维度为 V（词表长度）。</p>
</li>
<li><p><strong>输入层到隐层</strong></p>
<ul>
<li>首先，共享矩阵为 $W_{V \times N}$ ，<strong>V表示词表长度</strong>，W的每一行表示的就是一个N维的向量（训练结束后，W的每一行就表示一个词的词向量）。</li>
<li><p>然后，我们把所有<strong>输入的词转$x$化为对应词向量</strong>，然后<strong>取平均值</strong>，这样我们就得到了隐层输出值 ( 注意，隐层中无激活函数，也就是说这里是线性组合)。 其中，隐层输出 $h$ 是一个N维的向量 。</p>
<script type="math/tex; mode=display">
h = \frac{1}{C} W^T(x_1 + x_2 + \cdots + x_c)</script></li>
</ul>
</li>
<li><p><strong>隐层到输出层</strong>：隐层的输出为N维向量 $h$ ， 隐层到输出层的权重矩阵为  $W’_{N \times V}$ 。然后，通过矩阵运算我们得到一个 $V \times 1 $ 维向量</p>
<script type="math/tex; mode=display">
  u = W'^{T} * h</script></li>
</ul>
<p>其中，向量 $u$  的第 $i$  行表示词汇表中第 $i$  个词的可能性，然后我们的目的就是取可能性最高的那个词。<strong>因此，在最后的输出层是一个softmax 层获取分数最高的词</strong>，那么就有我们的最终输出：</p>
<script type="math/tex; mode=display">
P(w_j| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})}</script><h4><span id="损失函数">损失函数</span></h4><p>我们假定 $j^*$ 是真实单词在词汇表中的下标，那么根据极大似然法，则目标函数定义如下：</p>
<script type="math/tex; mode=display">
E = -log \, p(W_O |W_I) = -log \, \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})} =  log  \sum_{k \in V} exp(u_{k})  -u_j</script><h3><span id="12-skip-gram模型">1.2 Skip-gram模型</span></h3><p>Skip-Gram的基本思想是：<strong>通过当前词 $w<em>t$ 预测其上下文 $w</em>{t-i}, \cdots , w_{t+i}$</strong> ，模型如下图所示：</p>
<p><img src="https://pic2.zhimg.com/v2-42ef75691c18a03cfda4fa85a8409e19_b.jpg" alt="img"></p>
<h4><span id="前向传播过程">前向传播过程</span></h4><ul>
<li><strong>输入层</strong>：   输入的是一个单词，其表示形式为 <strong>One-hot</strong> ，我们将其表示为V维向量 $x<em>k$ ，其中 $V$ 为词表大小。然后，通过词向量矩阵 $W</em>{V \times N}$ 我们得到一个N维向量<script type="math/tex; mode=display">
  h = W^T * x_k = v^{T}_{w_I}</script></li>
</ul>
<ul>
<li><p><strong>隐层</strong>： 而隐层中没有激活函数，也就是说输入=输出，因此隐藏的输出也是 $h$ 。</p>
</li>
<li><p><strong>隐层到输出层</strong>：</p>
<ul>
<li><p><strong>首先</strong>，因为要输出C个单词，因此我们此时的输出有C个分布： <strong>$y_1, \cdots y_C $，且每个分布都是独立的</strong>，我们需要单独计算， 其中 $y_i$  表示窗口的第 $i$  个单词的分布。 【<strong>独立性假设</strong>】</p>
</li>
<li><p><strong>其次</strong>， 因为矩阵 $W’<em>{N \times V}$ 是共享的，因此我们得到的 $V \times 1$ 维向量 $u$ 其实是相同的，也就是有 $u</em>{c,j} = u_j$ ，这里 $u$ 的每一行同 CBOW 中一样，表示的也是评分。</p>
</li>
<li><p><strong>最后</strong>，每个分布都经过一个 softmax 层，不同于 CBOW，我们此处产生的是第 $i$ 个单词的分布（共有C个单词），如下：</p>
<script type="math/tex; mode=display">
P(w_{i,j}| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})}</script></li>
</ul>
</li>
</ul>
<h4><span id="损失函数">损失函数</span></h4><p>假设 $j^<em>$ 是真实单词在词汇表中的下标，那么根据<em>*极大似然法</em></em>，则目标函数定义如下：</p>
<script type="math/tex; mode=display">
\begin{split} E &= - log \, p(w_1, w_2, \cdots, w_C | w_I)   \\ &= - log \prod_{c=1}^C P(w_c|w_i) \\ &= - log  \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \\ &= - \sum_{c=1}^C u_{j^*_c} + C \cdot log \sum_{k=1}^{V} exp(u_k) \end{split}</script><h2><span id="二-word2vec-优化">二、Word2Vec 优化</span></h2><p>以上我们讨论的模型（二元模型，CBOW和skip-gram）都是他们的原始形式，没有加入任何优化技巧。对于这些模型，每个单词存在两类向量表达：<strong>输入向量</strong><img src="https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%7D" alt="[公式]">，<strong>输出向量</strong><img src="https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%27%7D" alt="[公式]">（这也是为什么word2vec的名称由来：1个单词对应2个向量表示)。学习得到输入向量比较简单；但<strong>要学习输出向量是很困难</strong>的。</p>
<h3><span id="21-hierarchical-softmax">==2.1 Hierarchical Softmax==</span></h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56139075">https://zhuanlan.zhihu.com/p/56139075</a></p>
</blockquote>
<p>Hierarchical Softmax对原模型的改进主要有两点，第一点是从输入层到隐藏层的映射，没有采用原先的与矩阵W相乘然后相加求平均的方法，而是<strong>直接对所有输入的词向量求和</strong>。假设输入的词向量为（0，1，0，0）和（0,0,0,1），那么隐藏层的向量为（0,1,0,1）。</p>
<p><strong>Hierarchical Softmax</strong>的第二点改进是采用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=哈夫曼树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;61635013&quot;}">哈夫曼树</a>来替换了原先的从隐藏层到输出层的矩阵W’。<strong>哈夫曼树的叶节点个数为词汇表的单词个数V</strong>，一个叶节点代表一个单词，而从根节点到该叶节点的路径确定了这个单词最终输出的词向量。</p>
<p><img src="https://pic4.zhimg.com/v2-3db7e66f36db0a9e6e6bc2f348dece47_b.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>具体来说，这棵哈夫曼树除了根结点以外的所有非叶节点中都含有一个由参数θ确定的sigmoid函数，不同节点中的θ不一样</strong>。训练时==<strong>隐藏层的向量</strong>==与这个<strong>==sigmoid函数==</strong>进行运算，根据结果进行分类，若分类为负类则沿左子树向下传递，编码为0；若分类为正类则沿右子树向下传递，编码为1。</p>
<p><strong>每个叶子节点代表语料库中的一个词</strong>，<strong>于是每个词语都可以被01唯一的编码，并且其编码序列对应一个事件序列，于是我们可以计算条件概率 <img src="https://www.zhihu.com/equation?tex=p%28w+%7C+C+o+n+t+e+x+t%28w%29%29" alt="[公式]"></strong> 。</p>
<p><strong>在开始计算之前，还是得引入一些符号：</strong></p>
<ol>
<li><p><img src="https://www.zhihu.com/equation?tex=p%5E%7Bw%7D" alt="[公式]"> :从根结点出发到达w对应叶子结点的路径</p>
</li>
<li><p><img src="https://www.zhihu.com/equation?tex=l%5E%7Bw%7D" alt="[公式]"> :路径中包含结点的个数</p>
</li>
<li><p><img src="https://www.zhihu.com/equation?tex=p_%7B1%7D%5E%7Bw%7D%2C+p_%7B2%7D%5E%7Bw%7D%2C+%5Ccdots%2C+p_%7Bl%5E%7Bw%7D%7D%5E%7Bw%7D" alt="[公式]"> :路径 <img src="https://www.zhihu.com/equation?tex=p%5E%7Bw%7D" alt="[公式]"> 中的各个节点</p>
</li>
<li><p><img src="https://www.zhihu.com/equation?tex=d_%7B2%7D%5E%7Bw%7D%2C+d_%7B3%7D%5E%7Bw%7D%2C+%5Ccdots%2C+d_%7Bl+w%7D%5E%7Bw%7D+%5Cin%5C%7B0%2C1%5C%7D" alt="[公式]"> :词w的编码， <img src="https://www.zhihu.com/equation?tex=d_%7Bj%7D%5E%7Bw%7D" alt="[公式]"> 表示路径 <img src="https://www.zhihu.com/equation?tex=p%5E%7Bw%7D" alt="[公式]"> 第j个节点对应的编码（根节点无编码）</p>
</li>
<li><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7B1%7D%5E%7Bw%7D%2C+%5Ctheta_%7B2%7D%5E%7Bw%7D%2C+%5Ccdots%2C+%5Ctheta_%7Bl%5E%7Bw%7D-1%7D%5E%7Bw%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm%7D" alt="[公式]"> :路径 <img src="https://www.zhihu.com/equation?tex=p%5E%7Bw%7D" alt="[公式]"> 中非叶节点对应的<strong>参数向量</strong></p>
</li>
</ol>
<p>于是可以给出w的条件概率：</p>
<p><img src="https://www.zhihu.com/equation?tex=p%28w+%7C+C+o+n+t+e+x+t%28w%29%29%3D%5Cprod_%7Bj%3D2%7D%5E%7Bl%5E%7Bw%7D%7D+p%5Cleft%28d_%7Bj%7D%5E%7Bw%7D+%7C+%5Cmathbf%7Bx%7D_%7Bw%7D%2C+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29" alt="[公式]"></p>
<p><strong>这是个简单明了的式子，从根节点到叶节点经过了 <img src="https://www.zhihu.com/equation?tex=l%5E%7Bw%7D-1" alt="[公式]"> 个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1)。</strong></p>
<p>其中，每一项是一个<strong>逻辑斯谛回归</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=p%5Cleft%28d_%7Bj%7D%5E%7Bw%7D+%7C+%5Cmathbf%7Bx%7D_%7Bw%7D%2C+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%7B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%2C%7D+%26+%7Bd_%7Bj%7D%5E%7Bw%7D%3D0%7D+%5C%5C+%7B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%2C%7D+%26+%7Bd_%7Bj%7D%5E%7Bw%7D%3D1%7D%5Cend%7Barray%7D%5Cright." alt="[公式]"></p>
<p>考虑到d只有0和1两种取值，我们可以用指数形式方便地将其写到一起：</p>
<p><img src="https://www.zhihu.com/equation?tex=p%5Cleft%28d_%7Bj%7D%5E%7Bw%7D+%7C+%5Cmathbf%7Bx%7D_%7Bw%7D%2C+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%3D%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5E%7B1-d_%7Bj%7D%5E%7Bw%7D%7D+%5Ccdot%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5E%7Bd%5E%7Bw%7D%7D" alt="[公式]"></p>
<p><strong>我们的目标函数取对数似然</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D%5Csum_%7Bw+%5Cin+%5Cmathcal%7BC%7D%7D+%5Clog+p%28w+%7C+C+o+n+t+e+x+t%28w%29%29" alt="[公式]"></p>
<p>将 <img src="https://www.zhihu.com/equation?tex=p%28w+%7C+C+o+n+t+e+x+t%28w%29%29" alt="[公式]"> 代入上式，有:</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D%5Csum_%7Bw+%5Cin+%5Cmathcal%7BC%7D%7D+%5Clog+%5Cprod_%7Bj%3D2%7D%5E%7Bl%5E%7Bw%7D%7D%5Cleft%5C%7B%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5E%7B1-d_%7Bj%7D%5E%7Bw%7D%7D+%5Ccdot%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5E%7Bd_%7Bj%7D%5E%7Bw%7D%7D%5Cright%5C%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%3D%5Csum_%7Bw+%5Cin+%5Cmathcal%7BC%7D%7D+%5Csum_%7Bj%3D2%7D%5E%7Bl%5E%7Bw%7D%7D%5Cleft%5C%7B%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29+%5Ccdot+%5Clog+%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%2Bd_%7Bj%7D%5E%7Bw%7D+%5Ccdot+%5Clog+%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5Cright%5C%7D" alt="[公式]"></p>
<p>这也很直白，连乘的对数换成求和。不过还是有点长，我们把每一项简记为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28w%2C+j%29%3D%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29+%5Ccdot+%5Clog+%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%2Bd_%7Bj%7D%5E%7Bw%7D+%5Ccdot+%5Clog+%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D" alt="[公式]"></p>
<h4><span id="wordvec-极大化化目标函数使用的算法是是随机梯度上升法"><strong><font color="red"> <em>WordVec</em> 极大化化目标函数使用的算法是是随机梯度上升法</font></strong></span></h4><p>每一项有两个参数，一个是每个节点的参数向量 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"> ，另一个是输出层的输入 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> ，我们分别对其求偏导数：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D%28w%2C+j%29%7D%7B%5Cpartial+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%7D%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%7D%5Cleft%5C%7B%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29+%5Ccdot+%5Clog+%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%2Bd_%7Bj%7D%5E%7Bw%7D+%5Ccdot+%5Clog+%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5Cright%5C%7D" alt="[公式]"></p>
<p>因为sigmoid函数的导数有个很棒的形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B%5Cprime%7D%28x%29%3D%5Csigma%28x%29%5B1-%5Csigma%28x%29%5D" alt="[公式]"></p>
<p>于是代入上上式得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D+%5Cmathbf%7Bx%7D_%7Bw%7D-d_%7Bj%7D%5E%7Bw%7D+%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29+%5Cmathbf%7Bx%7D_%7Bw%7D" alt="[公式]"></p>
<p><strong>合并同类项得到：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cleft%5B1-d_%7Bj%7D%5E%7Bw%7D-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D+%5Cmathbf%7Bx%7D_%7Bw%7D" alt="[公式]"></p>
<p>于是 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]">的更新表达式就得到了：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D+%3A%3D%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%2B%5Ceta%5Cleft%5B1-d_%7Bj%7D%5E%7Bw%7D-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D+%5Cmathbf%7Bx%7D_%7Bw%7D" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Ceta" alt="[公式]"> 是学习率，通常取0-1之间的一个值。学习率越大训练速度越快，但目标函数容易在局部区域来回抖动。</p>
<p><strong>再来 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 的偏导数</strong>，注意到 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28w%2C+j%29%3D%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29+%5Ccdot+%5Clog+%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%2Bd_%7Bj%7D%5E%7Bw%7D+%5Ccdot+%5Clog+%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D" alt="[公式]"> 中 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"> 是对称的，所有直接将 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"> 的偏导数中的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"> 替换为 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> ，得到关于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 的偏导数：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D%28w%2C+j%29%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D_%7Bw%7D%7D%3D%5Cleft%5B1-d_%7Bj%7D%5E%7Bw%7D-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"></p>
<p><strong>不过 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 是上下文的词向量的和，不是上下文单个词的词向量。怎么把这个更新量应用到单个词的词向量上去呢？word2vec采取的是直接将 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 的更新量整个应用到每个单词的词向量上去</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D%28%5Cwidetilde%7Bw%7D%29+%3A%3D%5Cmathbf%7Bv%7D%28%5Cwidetilde%7Bw%7D%29%2B%5Ceta+%5Csum_%7Bj%3D2%7D%5E%7Bl%5E%7Bw%7D%7D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D%28w%2C+j%29%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D_%7Bw%7D%7D%2C+%5Cquad+%5Cwidetilde%7Bw%7D+%5Cin+%5Ctext+%7B+Context+%7D%28w%29" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D%28%5Cwidetilde%7Bw%7D%29" alt="[公式]"> 代表上下文中某一个单词的词向量。我认为应该也可以将其平均后更新到每个词向量上去，无非是学习率的不同，欢迎指正。</p>
<h3><span id="22-negative-sampling">2.2 Negative Sampling</span></h3><blockquote>
<p>  Negative Sampling - 素轻的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56106590">https://zhuanlan.zhihu.com/p/56106590</a></p>
<p>  <strong>为了解决数量太过庞大的输出向量的更新问题，我们就不更新全部向量，而只更新他们的一个样本</strong>。</p>
<p>  训练神经网络 意味着输入一个训练样本调整weight，让它预测这个训练样本更准。换句话说，每个训练样本将会影响网络中所有的weight。<strong>Negative sampling 解决了这个问题，每次我们就修改了其中一小部分weight，而不是全部。</strong></p>
</blockquote>
<p><strong>负采样是另一种用来提高Word2Vec效率的方法</strong>，它是基于这样的观察：训练一个神经网络意味着使用一个训练样本就要稍微调整一下神经网络中所有的权重，这样才能够确保预测训练样本更加精确，如果能设计一种方法每次只更新一部分权重，那么计算复杂度将大大降低。</p>
<p>如果 vocabulary 大小为10000时， 当输入样本 ( “fox”, “quick”) 到神经网络时， <strong>“ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出 0</strong>。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为 negative word.   negative sampling 的想法也很直接 ，<strong>将随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。</strong></p>
<p><strong>假设原来模型每次运行都需要300×10,000(其实没有减少数量，但是运行过程中，减少了需要载入的数量。) 现在只要300×(1+10)减少了好多。</strong></p>
<h4><span id="问题来了如何选择10个negative-sample呢">==问题来了，如何选择10个negative sample呢？==</span></h4><p><strong>negative sample也是根据他们出现概率来选的，而这个概率又和他们出现的频率有关。更常出现的词，更容易被选为negative sample。</strong></p>
<p>这个概率用一个公式表示，每个词给了一个和它频率相关的权重。这个概率公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28w_i%29+%3D+%5Cfrac%7Bf%28w_i%29%5E%7B0.75%7D+%7D%7B+%5Csum_%7Bj%3D0%7D%5E%7Bn%7D%28f%28w_j%29%5E%7B0.75%7D%7D%29" alt="[公式]"></p>
<p>在paper中说0.75这个超参是试出来的，这个函数performance比其他函数好。</p>
<p><strong><font color="red"> 负采样算法实际上就是一个带权采样过程，负例的选择机制是和单词词频联系起来的。</font></strong>具体做法是以 <code>N+1</code> 个点对区间 <code>[0,1]</code> 做非等距切分，并引入的一个在区间 <code>[0,1]</code> 上的 <code>M</code> 等距切分，其中 <code>M &gt;&gt; N。</code>源码中取 M = 10^8。然后对两个切分做投影，得到映射关系：采样时，每次生成一个 [1, M-1] 之间的整数 i，则 Table(i) 就对应一个样本；当采样到正例时，跳过（<strong>拒绝采样</strong>）。</p>
<p><img src="https://pic4.zhimg.com/80/v2-36547a4cd05365292830ad4b22ba4c93_1440w.png" alt="img"></p>
<p><img src="https://pic3.zhimg.com/80/v2-a788595cc2611b0bfdac9e039a2e82fe_1440w.png" alt="img"></p>
<p><img src="https://pic1.zhimg.com/80/v2-cfe67c913af37a9435f3331139abeab8_1440w.jpg" alt="img"></p>
<h2><span id="三-word2vec-qampa">三、Word2vec Q&amp;A</span></h2><h3><span id="31-word2vec与lda的区别">3.1 Word2Vec与LDA的区别</span></h3><ol>
<li><p>LDA是利用文档中<strong>单词的共现关系</strong>来对单词按<strong>主题聚类</strong>，也可以理解为对“<strong>文档-单词</strong>”矩阵进行<strong>分解</strong>，得到“<strong>文档-主题</strong>”和“<strong>主题-单词</strong>”两个<strong>概率分布</strong>。</p>
</li>
<li><p>Word2Vec是利用<strong>上下文-单词</strong>“矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说，如果两个单词所对应的word2vec向量相似度较高，那么它们很可能经常在同样的上下文中出现。</p>
</li>
<li><p>LDA模型是一种基于<strong>概率图模型</strong>的<strong>生成式模型</strong>，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题）；</p>
</li>
<li><p>而Word2Vec模型一般表达为<strong>神经网络</strong>的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。</p>
</li>
</ol>
<h3><span id="32-word2vec存在的问题是什么">3.2 Word2Vec存在的问题是什么？</span></h3><ul>
<li>对每个local context window单独训练，没有利用包 含在global co-currence矩阵中的统计信息。</li>
<li>对多义词无法很好的表示和处理，因为使用了唯一的词向量</li>
</ul>
<h3><span id="32-ood-of-word2vec">3.2  OOD of word2vec</span></h3><p>其它单词认定其为Unknow，编号为0</p>
<h3><span id="34-项目中的word2vec">3.4 项目中的word2vec</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">feature_asm2vec</span>(<span class="params">data_type, inter_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Feature engineering for asm2vec feature.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> data_type == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">        <span class="comment"># TODO : 模型空判断</span></span><br><span class="line">        <span class="comment"># Train a Word2vec model by mixing traing set and test set</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;------------------------ 训练asm2vec模型 ------------------------&quot;</span>)</span><br><span class="line">        sentences = PathLineSentences(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/semantic/&quot;</span>)</span><br><span class="line">        model = Word2Vec(sentences=sentences, vector_size=<span class="number">1024</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, workers=<span class="number">5</span>)</span><br><span class="line">        model.wv.save_word2vec_format(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/models/asm2vec.bin&quot;</span>, binary=<span class="literal">True</span>, sort_attr=<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load the trained Word2vec model</span></span><br><span class="line">    model_wv = KeyedVectors.load_word2vec_format(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/models/asm2vec.bin&quot;</span>, binary=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------------------------ 生成asm2vec特征 ------------------------&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/<span class="subst">&#123;data_type&#125;</span>_filename.txt&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        filename = fp.read().split()</span><br><span class="line">    <span class="comment"># Feature engineering for generating string vector features</span></span><br><span class="line">    obj = StringVector()</span><br><span class="line">    arr = np.zeros((<span class="built_in">len</span>(filename), obj.dim))</span><br><span class="line">    <span class="keyword">with</span> tqdm(total=<span class="built_in">len</span>(filename), ncols=<span class="number">80</span>, desc=obj.name) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> i, file <span class="keyword">in</span> <span class="built_in">enumerate</span>(filename):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/semantic/<span class="subst">&#123;file&#125;</span>.txt&quot;</span>, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                stringz = f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">            lines = <span class="string">&#x27; &#x27;</span>.join(stringz.split(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line">            raw_words = <span class="built_in">list</span>(<span class="built_in">set</span>(lines.split()))</span><br><span class="line">            arr[i, :] = obj.feature_vector((model_wv, raw_words))</span><br><span class="line">            pbar.update(<span class="number">1</span>)</span><br><span class="line">    arr[np.isnan(arr)] = <span class="number">0</span></span><br><span class="line">    np.save(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/feature/<span class="subst">&#123;data_type&#125;</span>_semantic.npy&quot;</span>, arr)</span><br></pre></td></tr></table></figure>
<h3><span id="35-tf-idf-word2vec和bert-比较">3.5 Tf-Idf、Word2Vec和BERT 比较</span></h3><blockquote>
<p>  从算法本质来说 word2vec 一旦训练好了是<strong>没法处理未登录词（OOV）</strong>的，一般的做法是给OOV一个默认的向量，下面是一个类的封装（仅列出核心部分）</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://www.leiphone.com/category/yanxishe/TbZAzc3CJAMs815p.html">https://www.leiphone.com/category/yanxishe/TbZAzc3CJAMs815p.html</a></p>
<ul>
<li><strong>词袋法</strong>：用scikit-learn进行特征工程、特征选择以及机器学习，测试和评估，用lime解释。</li>
<li><strong>词嵌入法</strong>：用gensim拟合Word2Vec，用tensorflow/keras进行特征工程和深度学习，测试和评估，用Attention机制解释。</li>
<li><strong>语言模型</strong>：用transformers进行特征工程，用transformers和tensorflow/keras进行预训练BERT的迁移学习，测试和评估。</li>
</ul>
<h4><span id="概要">概要</span></h4><p>在本文中，我将使用NLP和Python来解释3种不同的文本多分类策略：老式的词袋法（tf-ldf），著名的词嵌入法（Word2Vec）和最先进的语言模型（BERT）。</p>
<p><img src="https://static.leiphone.com/uploads/new/images/20200930/5f73ddf75200f.png?imageView2/2/w/740" alt="NLP之文本分类：「Tf-Idf、Word2Vec和BERT」三种模型比较"></p>
<p>NLP（自然语言处理）是人工智能的一个领域，它研究计算机和人类语言之间的交互作用，特别是如何通过计算机编程来处理和分析大量的自然语言数据。NLP常用于文本数据的分类。文本分类是指根据文本数据内容对其进行分类的问题。</p>
<p>我们有多种技术从原始文本数据中提取信息，并用它来训练分类模型。本教程比较了传统的<strong>词袋法</strong>（与简单的机器学习算法一起使用）、流行的<strong>词嵌入模型</strong>（与深度学习神经网络一起使用）和最先进的语言模型（和基于<strong>attention</strong>的<strong>transformers</strong>模型中的<strong>迁移学习</strong>一起使用），语言模型彻底改变了NLP的格局。</p>
<p>我将介绍一些有用的Python代码，这些代码可以轻松地应用在其他类似的案例中（仅需复制、粘贴、运行），并对代码逐行添加注释，以便你能复现这个例子（下面是全部代码的链接）。</p>
<p><strong>词袋法</strong>：文件越多，词汇表越大，因此特征矩阵将是一个巨大的稀疏矩阵。</p>
<h4><span id="bert比之word2vec有哪些进步呢">Bert比之Word2Vec,有哪些进步呢？</span></h4><ul>
<li><p><strong>静态到动态：一词多义问题</strong></p>
</li>
<li><p><strong>词的多层特性</strong>：一个好的语言表示出了建模一词多义现象以外，还需要能够体现词的复杂特性，包括语法 (syntax)、语义 (semantics) 等。</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/7ATN5F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/7ATN5F/" class="post-title-link" itemprop="url">决策树（3）CART</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-25 17:29:37" itemprop="dateCreated datePublished" datetime="2022-02-25T17:29:37+08:00">2022-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:28:07" itemprop="dateModified" datetime="2023-04-22T19:28:07+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/" itemprop="url" rel="index"><span itemprop="name">决策树</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-cart">一、CART</span></h2><p>ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。</p>
<h3><span id="11-思想">1.1 思想</span></h3><p>CART 包含的基本过程有分裂，剪枝和树选择。 </p>
<ul>
<li><strong>分裂：</strong>分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去； </li>
<li><strong>剪枝：</strong>采用<strong>代价复杂度剪枝</strong>，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树； </li>
<li><strong>树选择：</strong>用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。</li>
</ul>
<p>CART 在 C4.5 的基础上进行了很多提升。 </p>
<ul>
<li>C4.5 为多叉树，运算速度慢，CART 为<strong>二叉树</strong>，运算速度快； </li>
<li>C4.5 只能分类，CART 既可以分类也可以<strong>回归</strong>； </li>
<li>CART 使用 <strong>Gini 系数作为变量的不纯度量</strong>，减少了<strong>大量的对数运算</strong>； </li>
<li>CART 采用<strong>代理测试来估计缺失值</strong>，而 C4.5 以不同概率划分到不同节点中； </li>
<li>CART 采用<strong>“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法</strong>。</li>
</ul>
<h3><span id="12-划分标准">1.2 划分标准</span></h3><p><strong>熵模型拥有大量耗时的对数运算</strong>，基尼指数在简化模型的同时还保留了熵模型的优点。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。</p>
<script type="math/tex; mode=display">
\begin{aligned} \operatorname{Gini}(D) &=\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|}\left(1-\frac{\left|C_{k}\right|}{|D|}\right) =1- \sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}  &\operatorname{Gini}(D \mid A) =\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \operatorname{Gini}\left(D_{i}\right) \end{aligned}</script><p><strong>基尼指数</strong>反映了从<strong>数据集中随机抽取两个样本，其类别标记不一致的概率</strong>。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等，<strong>基尼指数可以理解为熵模型的一阶泰勒展开。</strong></p>
<blockquote>
<p>  <strong><em>基尼指数是信息熵中﹣logP在P=1处一阶泰勒展开后的结果！所以两者都可以用来度量数据集的纯度</em></strong></p>
</blockquote>
<h3><span id="13-缺失值处理">1.3 缺失值处理</span></h3><p>上文说到，模型对于缺失值的处理会分为两个子问题：</p>
<ul>
<li><strong>如何在特征值缺失的情况下进行划分特征的选择？</strong></li>
</ul>
<p>对于问题 1，<strong>CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响</strong>（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。</p>
<ul>
<li><strong>选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？</strong></li>
</ul>
<p>对于问题 2，CART 算法的机制是为树的每个节点都找到<strong>代理分裂器</strong>，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是<strong>代替缺失值特征作为划分特征的特征</strong>），<strong>当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理</strong>，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。</p>
<h3><span id="14-剪枝策略">1.4 剪枝策略</span></h3><p><strong>基于代价复杂度的剪枝</strong>:<a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv11066239">https://www.bilibili.com/read/cv11066239</a></p>
<p>采用一种<strong>“基于代价复杂度的剪枝</strong>”方法进行<strong>后剪枝</strong>，这种方法会生成一系列树，每<strong>个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点</strong>。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。<strong>这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树</strong>。</p>
<blockquote>
<p>  从完整子树 $T0$ 开始， 通过在 $Ti$ 子树序列中裁剪真实误差最小【考虑叶子节点的个数】的子树，得到 $Ti+1$。 </p>
<script type="math/tex; mode=display">
  $\alpha=\frac{R(t)-R(T)}{N(T)-1}$</script><p>  【剪枝之后的误差 - 剪枝前的误差 / 叶子节点数 - 1】</p>
<p>  每次误差增加率最小的节点，得到一系列的子树，从中选择效果最好的【独立剪枝数据集】和【K折交叉验证】</p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304211501941.png" alt="image-20220320215056933" style="zoom: 33%;"></p>
<p>我们来看具体看一下代价复杂度剪枝算法：</p>
<p>首先我们将最大树称为 $T_0$, 我们希望减少树的大小来防止过拟合, 但又担心去掉节点后预测误差会增大, 所以我 们定义了一个损失函数来达到这两个变量之间的平衡。损失函数定义如下：</p>
<script type="math/tex; mode=display">
C_\alpha(T)=C(T)+\alpha|T|</script><p>$T$ 为任意子树, $C(T)$ 为预测误差, $|T|$ 为子树 $T$ 的叶子节点个数, $\alpha$ 是参数, $C(T)$ 衡量训练数据的拟合 程度, $|T|$ 衡量树的复杂度, $\alpha$ <strong>权衡拟合程度与树的复杂度</strong>。</p>
<h3><span id="15-类别不平衡">1.5 类别不平衡</span></h3><font color="red"> CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其子冻消除不需要建模人员采取其他操作。</font>

<p>CART 使用了一种先验机制，其作用相当于对类别进行加权。这种先验机制嵌入于 CART 算法判断分裂优劣的运算 里, 在 CART 默认的分类模式中, 总是要计算每个节点关于根节点的类别频率的比值, 这就相当于对数据自动重加 权, 对类别进行均衡。</p>
<p>对于一个二分类问题，节点 node 被分成类别 1 当且仅当:</p>
<script type="math/tex; mode=display">
\frac{N_1(\text { node })}{N_1(\text { root })}>\frac{N_0(\text { node })}{N_0(\text { root })}</script><p>比如二分类，根节点属于 1 类和 0 类的分别有 20 和 80 个。在子节点上有 30 个样本，其中属于 1 类和 0 类的分 别是 10 和 20 个。如果 10/20&gt;20/80，该节点就属于 1 类。</p>
<p>通过这种计算方式就无需管理数据真实的类别分布。假设有 $\mathrm{K}$ 个目标类别，就可以确保根节点中每个类别的概率 都是 $1 / \mathrm{K}$ 。这种默认的模式被称为“先验相等”。</p>
<p>先验设置和加权不同之处在于先验不影响每个节点中的各类别样本的数量或者份额。先验影响的是每个节点的类别 赋值和树生长过程中分裂的选择。</p>
<h3><span id="16-连续值处理">1.6 连续值处理</span></h3><h4><span id="161-分类树">1.6.1 分类树</span></h4><ul>
<li><p><strong><font color="red">如果特征值是连续值：CART的处理思想与C4.5是相同的，即将连续特征值离散化。唯一不同的地方是度量的标准不一样，</font></strong> <strong>CART采用基尼指数，而C4.5采用信息增益比</strong>。</p>
</li>
<li><p>如果当前节点为连续属性，<strong>CART树中该属性（剩余的属性值）后面还可以参与子节点的产生选择过程</strong>。</p>
</li>
</ul>
<h3><span id="17-回归树">1.7 回归树</span></h3><p><strong>CART（Classification and Regression Tree，分类回归树），从名字就可以看出其不仅可以用于分类，也可以应用于回归</strong>。其回归树的建立算法上与分类树部分相似，这里简单介绍下不同之处。</p>
<h5><span id="连续值处理rss残差平方和"><strong>连续值处理</strong>：RSS<strong>残差平方和</strong></span></h5><p><strong>对于连续值的处理, CART 分类树采用基尼系数的大小来度量特征的各个划分点。在回归模型中, 我们使用常见的 和方差度量方式</strong>, 对于任意划分特征 $\mathrm{A}$, 对应的任意划分点 $\mathrm{s}$ 两边划分成的数据集 $D_1$ 和 $D_2$, 求出使 $D_1$ 和 $D_2$ 各自<strong>集合的均方差最小</strong>, 同时 $D_1$ 和 $D_2$ 的均方差之和最小所对应的特征和特征值划分点。表达式为:</p>
<script type="math/tex; mode=display">
\min _{a, s}\left[\min _{c_1} \sum_{x_i \in D_1}\left(y_i-c_1\right)^2+\min _{c_2} \sum_{x_i \in D_2}\left(y_i-c_2\right)^2\right]</script><p>其中, $c_1$ 为 $D_1$ 数据集的样本输出均值, $c_2$ 为 $D_2$ 数据集的样本输出均值。</p>
<h5><span id="预测方式"><strong>预测方式</strong></span></h5><p>对于决策树建立后做预测的方式，上面讲到了 CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。</p>
<h4><span id="171-cart分类树建模时预测变量中存在连续和离散时会自动分别进行处理吗">1.7.1 CART分类树建模时，预测变量中存在连续和离散时，会自动分别进行处理吗？</span></h4><blockquote>
<p>  在使用sklearn的决策树CART建模时，预测变量中存在连续和离散时，会自动分别进行处理吗？ - 月来客栈的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/472579561/answer/2002434993">https://www.zhihu.com/question/472579561/answer/2002434993</a></p>
</blockquote>
<p><strong>对于这种连续型的特征变量，Sklearn中的具体做法（包括ID3、CART、随机森林等）是先对连续型特征变量进行排序处理</strong>，<strong><font color="red"> 然后取所有连续两个值的均值来离散化整个连续型特征变量。</font></strong></p>
<p>假设现在某数据集其中一个特征维度为:</p>
<script type="math/tex; mode=display">
[0.5,0.2,0.8,0.9,1.2,2.1,3.2,4.5]</script><p>则首先需要对其进行排序处理, 排序后的结果为:</p>
<script type="math/tex; mode=display">
[0.2,0.5,0.8,0.9,1.2,2.1,3.2,4.5]</script><p>接着再计算所有连续两个值之间的平均值：</p>
<script type="math/tex; mode=display">
[0.35,0.65,0.85,1.05,1.65,2.65,3.85]</script><p>这样，便得到了该特征离散化后的结果。最后在构造决策树时，只需要使用式最后离散化后的特征进行划分指标的计算即可。同时，值得一说的地方是<strong>目前Sklearn在实际处理时，会把所有的特征均看作连续型变量进行处理</strong>。</p>
<p>下图所示为iris数据集根据sklearn中CART算法所建模的决策树的可视化结果：</p>
<p><img src="https://picx.zhimg.com/v2-9081bc3cd5f2ec069212b79d5c5ff7d3_b.jpg" alt="img" style="zoom:50%;"></p>
<p>从图中可以看到，<code>petal width</code>这个特征在前两次分类时的分割点分别为0.8和1.75。下面先来看看原始特征<code>petal width</code>的取值情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span>  <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">1.4</span> <span class="number">2.5</span> <span class="number">1.3</span> <span class="number">2.1</span> <span class="number">1.5</span> <span class="number">0.2</span> <span class="number">2.</span>  <span class="number">1.</span>  <span class="number">0.2</span> <span class="number">0.3</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">0.2</span> <span class="number">0.5</span> <span class="number">1.3</span> <span class="number">0.2</span> <span class="number">1.2</span> <span class="number">2.2</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">2.</span>  <span class="number">0.2</span> <span class="number">1.8</span> <span class="number">1.9</span> <span class="number">1.</span>  <span class="number">1.5</span> <span class="number">2.3</span> <span class="number">1.3</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">1.9</span> <span class="number">0.2</span> <span class="number">0.2</span> <span class="number">1.1</span> <span class="number">1.7</span> <span class="number">0.2</span> <span class="number">2.4</span> <span class="number">0.2</span> <span class="number">0.6</span> <span class="number">1.8</span> <span class="number">1.1</span> <span class="number">2.3</span> <span class="number">1.6</span> <span class="number">1.4</span> <span class="number">2.3</span> <span class="number">1.3</span> <span class="number">0.2</span> <span class="number">0.1</span> <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">0.3</span> <span class="number">0.2</span> <span class="number">1.5</span> <span class="number">2.4</span> <span class="number">0.3</span> <span class="number">2.1</span> <span class="number">2.5</span> <span class="number">0.2</span> <span class="number">1.4</span> <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">1.4</span> <span class="number">2.3</span> <span class="number">0.2</span> <span class="number">2.1</span> <span class="number">1.5</span> <span class="number">2.</span>  <span class="number">1.</span>  <span class="number">1.4</span> <span class="number">1.4</span> <span class="number">0.3</span> <span class="number">1.3</span> <span class="number">1.2</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">1.8</span> <span class="number">2.1</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">2.5</span> <span class="number">1.6</span> <span class="number">0.1</span> <span class="number">2.4</span> <span class="number">0.2</span> <span class="number">1.5</span> <span class="number">1.9</span> <span class="number">1.8</span> <span class="number">1.3</span> <span class="number">1.8</span> <span class="number">1.3</span> <span class="number">1.3</span> <span class="number">2.</span>  <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">1.7</span> <span class="number">0.2</span> <span class="number">1.2</span> <span class="number">2.1</span>]</span><br></pre></td></tr></table></figure>
<p>可以发现上面并没有0.8和1.75这两个取值。接着按上面的方法先排序，再取相邻两个值的平均作为离散化的特征，其结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0.1</span>, <span class="number">0.15000000000000002</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, </span><br><span class="line"><span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.25</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.35</span>, <span class="number">0.4</span>, <span class="number">0.4</span>,</span><br><span class="line"> <span class="number">0.45</span>, <span class="number">0.55</span>, <span class="number">0.8</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.05</span>, <span class="number">1.1</span>, <span class="number">1.15</span>, <span class="number">1.2</span>, <span class="number">1.2</span>, <span class="number">1.25</span>, <span class="number">1.3</span>,</span><br><span class="line"> <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.35</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.45</span>, <span class="number">1.5</span>, </span><br><span class="line"><span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.55</span>, <span class="number">1.6</span>, <span class="number">1.65</span>, <span class="number">1.7</span>, <span class="number">1.75</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, </span><br><span class="line"><span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.85</span>, <span class="number">1.9</span>, <span class="number">1.9</span>, <span class="number">1.95</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.05</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, </span><br><span class="line"><span class="number">2.1500000000000004</span>, <span class="number">2.25</span>, <span class="number">2.3</span>, <span class="number">2.3</span>, <span class="number">2.3</span>, <span class="number">2.3499999999999996</span>, <span class="number">2.4</span>, <span class="number">2.4</span>, <span class="number">2.45</span>, <span class="number">2.5</span>, <span class="number">2.5</span>]</span><br></pre></td></tr></table></figure>
<h2><span id="二-总结">二、 总结</span></h2><p>最后通过总结的方式对比下 ID3、C4.5 和 CART 三者之间的差异。</p>
<p>除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：</p>
<ul>
<li><strong>划分标准的差异：</strong>ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li>
<li><strong>使用场景的差异：</strong>ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li>
<li><strong>样本数据的差异：</strong>ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li>
<li><strong>样本特征的差异：</strong>ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征（连续型）；</li>
<li><strong>剪枝策略的差异：</strong>ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2J7SQ0E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2J7SQ0E/" class="post-title-link" itemprop="url">决策树（1）ID3</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-25 17:29:37" itemprop="dateCreated datePublished" datetime="2022-02-25T17:29:37+08:00">2022-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:27:50" itemprop="dateModified" datetime="2023-04-22T19:27:50+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/" itemprop="url" rel="index"><span itemprop="name">决策树</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>
<h2><span id="一-id3删特征">一、ID3【删特征】</span></h2><p>ID3 算法是建立在奥卡姆剃刀[<strong>“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情”</strong>]（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。</p>
<!--more-->
<h3><span id="11-思想">1.1 思想</span></h3><p>从信息论的知识中我们知道：信息熵越大，从而样本纯度越低，。ID3 算法的核心思想就是以<strong>信息增益</strong>来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。 其大致步骤为：</p>
<ol>
<li>初始化特征集合和数据集合；</li>
<li>计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；</li>
<li>更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；</li>
<li>重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。</li>
</ol>
<h3><span id="12-划分标准">1.2 划分标准</span></h3><p>ID3 使用的分类标准是信息增益，它表示得知特征 A 的信息而使得样本集合不确定性减少的程度。</p>
<p>数据集的<strong>信息熵</strong>：</p>
<script type="math/tex; mode=display">
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}</script><p>其中$C_{k}$表示集合 D 中属于第 k 类样本的样本子集。针对某个特征 A，对于数据集 D 的条件熵 $H(D \mid A)$为：</p>
<script type="math/tex; mode=display">
\begin{aligned} H(D \mid A) &=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right) \\ &=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|}\left(\sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}\right) \end{aligned}</script><p><strong>信息增益</strong> = 信息熵 - 条件熵。信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</p>
<script type="math/tex; mode=display">
\operatorname{Gain}(D, A)=H(D)-H(D \mid A)</script><p>信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</p>
<h3><span id="13-缺点没有剪枝-特征偏好-缺失值">1.3 缺点【没有剪枝、特征偏好、缺失值】</span></h3><ul>
<li>ID3 没有剪枝策略，容易过拟合；</li>
<li>信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；</li>
<li>只能用于处理离散分布的特征；</li>
<li>没有考虑缺失值。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1WP1250/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1WP1250/" class="post-title-link" itemprop="url">决策树（4）总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-25 17:29:37" itemprop="dateCreated datePublished" datetime="2022-02-25T17:29:37+08:00">2022-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:28:17" itemprop="dateModified" datetime="2023-04-22T19:28:17+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/" itemprop="url" rel="index"><span itemprop="name">决策树</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="决策树——ID3、C4-5、CART"><a href="#决策树——ID3、C4-5、CART" class="headerlink" title="决策树——ID3、C4.5、CART"></a>决策树——ID3、C4.5、CART</h2><p><strong>决策树</strong>是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>ID3（分类）</th>
<th>C4.5（分类）</th>
<th>CART（分类和回归）</th>
</tr>
</thead>
<tbody>
<tr>
<td>思想</td>
<td>奥卡姆剃刀：越是小型的决策树越优于大的决策树;ID3 算法的核心思想就是以<strong>信息增益</strong>来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间。</td>
<td>C4.5 算法最大的特点是<strong>克服了 ID3 对特征数目的偏重</strong>这一缺点，引入<strong>信息增益率</strong>来作为分类标准。</td>
<td>CART 算法的二分法可以<strong>简化决策树的规模</strong>，提高生成决策树的效率。CART 包含的基本过程有<strong>分裂</strong>，<strong>剪枝</strong>和<strong>树选择</strong>。</td>
</tr>
<tr>
<td><strong>划分标准</strong></td>
<td><strong>信息增益</strong>  =  类别熵 - 特征类别熵                                <strong>类别熵</strong>：$H(D)=-\sum_{k=1}^{K} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>} \log _{2} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}$                 <strong>特征类别熵</strong>：$H(D \mid A)=\sum_{i=1}^{n} \frac{\left</td>
<td>D_{i}\right</td>
<td>}{</td>
<td>D</td>
<td>} H\left(D_{i}\right)$</td>
<td>先从候选划分特征中找到信息增益高于平均值的特征，再从中选择<strong>增益率</strong>最高的。</td>
<td><strong>Gini 系数</strong>作为变量的<strong>不纯度量</strong>，<strong>减少了大量的对数运算</strong>；$G i n i(D)=\sum_{k=1}^{K} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}\left(1-\frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}\right)$</td>
</tr>
<tr>
<td>剪枝策略</td>
<td><strong>无</strong></td>
<td><strong>悲观剪枝策略</strong></td>
<td>基于<strong>代价复杂度剪枝</strong></td>
</tr>
<tr>
<td>数据差异</td>
<td><strong>离散</strong>数据且<strong>缺失值</strong>敏感</td>
<td><strong>离散</strong>、<strong>连续特征离散化</strong>；【排序+离散化】</td>
<td><strong>连续型、离散型</strong></td>
</tr>
<tr>
<td><strong>连续值处理</strong></td>
<td>无</td>
<td><strong>排序</strong>并取相邻两样本值的<strong>平均数</strong>。</td>
<td><strong>排序</strong>并取相邻两样本值的<strong>平均数</strong>。<strong>CART 分类树</strong>【<strong>基尼系数</strong>】。<strong>回归树</strong>【<strong>和方差度量</strong>】。</td>
</tr>
<tr>
<td>缺失值处理</td>
<td><strong>无</strong></td>
<td>1、有缺失值特征，用没有缺失的样本子集所占比重来折算；2、将样本同时划分到所有子节点</td>
<td><strong>代理测试</strong>来估计缺失值</td>
</tr>
<tr>
<td>类别不平衡</td>
<td><strong>无</strong></td>
<td><strong>无</strong></td>
<td><strong>先验机制</strong>：其作用相当于对数据自动重加权，对类别进行均衡。</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>1、ID3 没有剪枝策略，容易过拟合；2、信息增益准则对可取值<strong>数目较多的特征有所偏好</strong>，类似“编号”的特征其信息增益接近于 1； 3、只能用于处理离散分布的特征； 没有考虑缺失值。</td>
<td>1、<strong>多叉树</strong>。2、<strong>只能用于分类</strong>。3、熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>。4、驻留于内存的数据集。</td>
<td>熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>。</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>划分标准的差异：</strong>ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li>
<li><strong>使用场景的差异：</strong>ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li>
<li><strong>样本数据的差异：</strong>ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li>
<li><strong>样本特征的差异：</strong>ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；</li>
<li><strong>剪枝策略的差异：</strong>ID3 没有剪枝策略，C4.5 是通过<strong>悲观剪枝策略</strong>来修正树的准确性，而 CART 是通过<strong>代价复杂度</strong>剪枝。</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/1WP1250/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/12ZJKX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/12ZJKX/" class="post-title-link" itemprop="url">决策树（2）C4-5</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-25 17:29:37" itemprop="dateCreated datePublished" datetime="2022-02-25T17:29:37+08:00">2022-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:27:58" itemprop="dateModified" datetime="2023-04-22T19:27:58+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/" itemprop="url" rel="index"><span itemprop="name">决策树</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-c45">一、C4.5</span></h2><p>C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。</p>
<p>C4.5 相对于 ID3 的缺点对应有以下改进方式： </p>
<ul>
<li>引入<strong>悲观剪枝策略进行后剪枝</strong>； </li>
<li>引入<strong>信息增益率</strong>作为划分标准； </li>
<li><strong>将连续特征离散化</strong>，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； </li>
<li>对于<strong>缺失值的处理</strong>可以分为两个子问题：</li>
<li>问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）<ul>
<li>C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；</li>
</ul>
</li>
<li>问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里） <ul>
<li>C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</li>
</ul>
</li>
</ul>
<h3><span id="12-划分标准">1.2 划分标准</span></h3><p>利用信息增益率可以克服信息增益的缺点，其公式为:</p>
<script type="math/tex; mode=display">
\operatorname{Gain}_{\text {ratio }}(D, A) =\frac{\operatorname{Gain}(D, A)}{H_{A}(D)}     \\</script><script type="math/tex; mode=display">
H_{A}(D)=-\sum_{i=1}^{n}   \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}</script><p>信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个<strong>启发式方法</strong>：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。</p>
<h3><span id="13-剪枝策略">1.3 剪枝策略</span></h3><p>为什么要剪枝：<strong>过拟合的树在泛化能力的表现非常差。</strong></p>
<p><strong>预剪枝和悲观剪枝</strong></p>
<h4><span id="131-预剪枝"><strong>1.3.1 预剪枝</strong></span></h4><p>在节点划分前来确定是否继续增长，及早停止增长的主要方法有：</p>
<ul>
<li>节点内数据样本低于<strong>某一阈值</strong>；</li>
<li>所有节点特征都已分裂；</li>
<li>节点划分前准确率比划分后准确率高。</li>
</ul>
<p>预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。</p>
<h4><span id="132-后剪枝悲观剪枝方法-httpgitlinuxnet2019-06-04-c45"><strong>1.3.2 后剪枝</strong>【悲观剪枝方法】  </span></h4><p>在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。</p>
<p>C4.5 采用的<strong>悲观剪枝方法</strong>，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。<strong>C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率</strong>。</p>
<p>后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。</p>
<h3><span id="14-缺点">1.4 缺点</span></h3><ul>
<li><strong>剪枝策略</strong>可以再优化；</li>
<li>C4.5 用的是<strong>多叉树</strong>，用二叉树效率更高；</li>
<li>C4.5 只能用于<strong>分类</strong>；</li>
<li>C4.5 使用的熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>；</li>
<li>C4.5 在构造树的过程中，<strong>对数值属性值需要按照其大小进行排序</strong>，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/DFA75A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/DFA75A/" class="post-title-link" itemprop="url">安全场景-软件供应链及物联网安全 </a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-09-18 18:52:43" itemprop="dateCreated datePublished" datetime="2021-09-18T18:52:43+08:00">2021-09-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-05-20 16:06:52" itemprop="dateModified" datetime="2022-05-20T16:06:52+08:00">2022-05-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/" itemprop="url" rel="index"><span itemprop="name">应用场景</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="供应链及物理网安全">供应链及物理网安全</span></h3><blockquote>
<h5><span id="1-apk分析">1、</span></h5><p>  考察点：逆向分析、文档收集、数据分析</p>
<h5><span id="2-软件供应链安全分析">2、软件供应链安全分析</span></h5><ul>
<li>MaMaDroid【13】</li>
<li><a target="_blank" rel="noopener" href="https://github.com/MLDroid/drebin">Drebin</a> 【14】</li>
<li><p>AppContext【20】</p>
<p>==考察点：二进制代码分析，二进制函数特征提取，补丁比较，源代码分析==</p>
</li>
<li><p>如V. Livshits 等使用静态分析的方法在Java 源代码中进行脆弱性(vulnerability)检测的策略[47]。</p>
</li>
<li>G. Grieco 和G. Grinblat 等使用机器学习方法根据软件的内存错误信息训练测试模型, 检测软件漏洞的研究[49] <strong><em>Toward Large-Scale Vulnerability Discovery Using Machine Learning</em></strong></li>
<li>彭小详等人的研究针对加壳技术, 提出了对恶意程序进行自动脱壳的方法[59]。<strong><em>Research of Malicious Code in Automatic Unpacking</em></strong></li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_34168880/article/details/86753392">「功守道」软件供应链安全大赛·C源代码赛季启示录</a></p>
<h5><span id="3-powershell反混淆">3、</span></h5><p>考察点：代码分析， 混淆语句定位、还原</p>
</li>
</ul>
</blockquote>
<ul>
<li>XShell污染</li>
<li>CCleaner投毒</li>
</ul>
<h5><span id="混淆原理">混淆原理</span></h5><p>网上帖子太多了，可以参考下看雪论坛用户发的翻译贴：<br><a target="_blank" rel="noopener" href="https://bbs.pediy.com/thread-248034.htm">https://bbs.pediy.com/thread-248034.htm</a></p>
<h5><span id="自动化反混淆工具">自动化反混淆工具</span></h5><p>Unit42安全团队编写的PowerShellProfiler.py：<br>github地址：<br><a target="_blank" rel="noopener" href="https://github.com/pan-unit42/public_tools/tree/master/powershellprofiler">https://github.com/pan-unit42/public_tools/tree/master/powershellprofiler</a><br>用法及原理参考：<br><a target="_blank" rel="noopener" href="https://www.freebuf.com/sectool/219057.html">https://www.freebuf.com/sectool/219057.html</a> </p>
<blockquote>
<h5><span id="4-物联网漏洞挖掘">4、物联网漏洞挖掘</span></h5><p>  考察点：<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1543779">常见漏洞原理</a>，二进制逆向工程，自动化程序分析</p>
</blockquote>
<h4><span id="相关论文">相关论文</span></h4><blockquote>
<p>  DroidMD: an efficient and scalable Android malware detection approach at source code level (2021 C&amp;S)</p>
<p>  Detection of Repackaged Android Malware with Code-Heterogeneity Features.(2020 TDSC)</p>
</blockquote>
<h5><span id="11-软件供应链">1.1 软件供应链</span></h5><p><a target="_blank" rel="noopener" href="https://www.freebuf.com/sectool/276951.html">软件供应链安全工具DependencyTrack的使用</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bonelee/p/13768901.html">https://www.cnblogs.com/bonelee/p/13768901.html</a></p>
<h5><span id="12-物联网安全">1.2 物联网安全</span></h5><h3><span id="常见恶意行为">常见恶意行为</span></h3><blockquote>
<p>  <em>标注为单点确定性恶意行为，*</em>标注为二阶段恶意行为中的上游或下游行为，#标注为复合恶意行为</p>
</blockquote>
<p>* <strong>敏感信息异常采集</strong>:针对生产环境，最大的威胁不是造成应用执行异常，而是在无形中泄漏关键敏感数据，包括可能造成机器控制权丧失的系统相关配置数据，关键的应用存储的用户数据等。</p>
<ul>
<li>口令与秘钥类型文件直接操作 *</li>
<li>系统敏感配置文件绕过API直接读取 *</li>
<li>典型服务端应用敏感配置文件直接读取 *</li>
<li>系统账户操作历史相关信息读取 **</li>
<li>典型服务端应用管理账户和用户数据读取 **</li>
<li>系统一般描述性信息采集 **</li>
<li>软件供应链上游特定资源数据探测、获取和泄漏（如源码遍历泄漏） #</li>
</ul>
<p>* <strong>关键数据篡改</strong>:任何需要在生产环境上，修改、写入数据或代码从而实现恶意打击的行为，我们统一归纳到这一类里面，较为泛化。</p>
<ul>
<li>覆盖、篡改或插入口令秘钥类型文件用以账户植入 *</li>
<li>系统、用户环境变量和关键配置文件修改 **</li>
<li>自动执行脚本/用户操作历史篡改 **</li>
<li>典型服务端应用配置文件和关键数据文件绕过API方式篡改 *</li>
<li>系统/典型应用重要位置的脚本/可执行文件置换 **</li>
<li>开发、测试等环境系统默认工具链篡改替换 *</li>
<li>开发、测试等环境特定类型源文件/资源文件篡改污染 #</li>
</ul>
<p><strong>*不可信数据传入渠道</strong>：以上两者重点考察了隐形的软件供应链本地恶意行为。在涉及到网络和交互的场景下，通过从供应链上进行污染，一种比较直接且有持续后效的恶意行为就是撕开一个口子，供后续入侵进场。</p>
<ul>
<li>下载敏感类型文件到临时目录 **</li>
<li>关键可执行文件（系统应用/关键服务端应用/关键库）下载/释放 **</li>
<li>网络传入指令/地址类型数据且无校验执行/访问 *</li>
</ul>
<p>* <strong>不可信信息外传渠道</strong>。对应于上面的传入。敏感数据的采集后，需要搭配对应的下游传出才能形成完整恶意行为链路，常规可能的渠道形式分为两类。</p>
<ul>
<li>上游数据未脱敏形式的网络传出（TCP/UDP/ICMP） **</li>
<li>上游数据未脱敏形式的本地确定位置落盘 **</li>
</ul>
<p>* <strong>其它典型木马后门行为</strong>。在上述行为框架之外，在生产环境上具有非单纯破坏效果的恶意行为，划分为此类，包括但不限于：</p>
<ul>
<li><p>键盘hook等输入监控行为 *</p>
</li>
<li><p>网络劫持行为 *</p>
</li>
<li><p>全局挂钩注入行为 **</p>
</li>
<li><p>远程控制 #</p>
</li>
</ul>
<h3><span id="经典赛题事例">经典赛题事例</span></h3><p><strong>#1：thttpd后门陷阱</strong></p>
<p>从基础软件或应用上面入手，稳定可控的后门是最佳选择。而在一个无关应用中突兀地出现网络连接，隐蔽性总归很差；在thttpd当中，以很袖珍的代码实现稳定的后门，是这里首先要呈现的一个题目。</p>
<p>在thttpd项目，恶意代码嵌入到libhttpd.c文件中，上下游恶意代码相关上下文：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">*** /thttpd/libhttpd.c</span><br><span class="line"></span><br><span class="line">--- malware/libhttpd.c</span><br><span class="line"></span><br><span class="line">*************** <span class="title function_">httpd_parse_request</span><span class="params">( httpd_conn* hc )</span></span><br><span class="line"></span><br><span class="line">*** 2102,2107 ****</span><br><span class="line"></span><br><span class="line">--- 2102,2113 ----</span><br><span class="line"></span><br><span class="line">cp += <span class="built_in">strspn</span>( cp, <span class="string">&quot; \t&quot;</span> );</span><br><span class="line"></span><br><span class="line">hc-&gt;useragent = cp;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">+ <span class="keyword">else</span> <span class="keyword">if</span> ( strncasecmp( buf, <span class="string">&quot;TE:&quot;</span>, <span class="number">3</span> ) == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">+ &#123;</span><br><span class="line"></span><br><span class="line">+ cp = &amp;buf[<span class="number">3</span>];</span><br><span class="line"></span><br><span class="line">+ cp += <span class="built_in">strspn</span>( cp, <span class="string">&quot; \t&quot;</span>);</span><br><span class="line"></span><br><span class="line">+ hc-&gt;hs-&gt;cgi_pattern = cp;</span><br><span class="line"></span><br><span class="line">+ &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> ( strncasecmp( buf, <span class="string">&quot;Host:&quot;</span>, <span class="number">5</span> ) == <span class="number">0</span> )</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">cp = &amp;buf[<span class="number">5</span>];</span><br><span class="line"></span><br><span class="line">*************** cgi_child( httpd_conn* hc )</span><br><span class="line"></span><br><span class="line">*** <span class="number">3560</span>,<span class="number">3565</span> ****</span><br><span class="line"></span><br><span class="line">--- <span class="number">3566</span>,<span class="number">3576</span> ----</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span> <span class="comment">/* HAVE_SIGSET */</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Run the program. */</span></span><br><span class="line"></span><br><span class="line">+ <span class="keyword">if</span> ( <span class="built_in">strstr</span>( hc-&gt;acceptl, <span class="string">&quot;en;q=1.1&quot;</span>) != (<span class="type">char</span>*)<span class="number">0</span> )</span><br><span class="line"></span><br><span class="line">+ &#123;</span><br><span class="line"></span><br><span class="line">+ binary = argp[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">+ argp++;</span><br><span class="line"></span><br><span class="line">+ &#125;</span><br><span class="line"></span><br><span class="line">(<span class="type">void</span>) execve( binary, argp, envp );</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Something went wrong. */</span></span><br></pre></td></tr></table></figure>
<p>后门会根据http头判断是否开启cgi功能，并根据http头Accept-Language决定解析执行文件的路径。上述代码段中，首先通过http头TE:设置开启cgi功能（对应上述代码中，httpd_parse_request函数中插入的else if ( strncasecmp( buf, “TE:”, 3 ) == 0) {…}代码块）。而下游代码同样巧妙，指定特殊的Accept-Language: en;q=1.1决定是否执行指定的系统命令（即cgi_child函数插入的if ( strstr( hc-&gt;acceptl, “en;q=1.1”) != (char*)0 ) {…}代码块）。</p>
<p>本例恶意行为的主要特点：</p>
<ul>
<li>该后门的嵌入，新增代码量极小（共7行），巧妙借用了thttpd处理用户请求、cgi的原本逻辑，借用了execve的调用，没有任何新增的API调用等行为，可以躲避有意识的行为特征匹配检测。</li>
<li>该后门在代码中的插入，分布在了存在逻辑关联的上下游两个位置，在源代码分析领域，属于过程间代码扫描问题，对于基于语义的源代码静态扫描方案也提出了很高的要求。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2GBZRTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2GBZRTM/" class="post-title-link" itemprop="url">恶意软件检测（14）MALWARE综述</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-08-12 16:29:26" itemprop="dateCreated datePublished" datetime="2021-08-12T16:29:26+08:00">2021-08-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-19 16:31:01" itemprop="dateModified" datetime="2023-04-19T16:31:01+08:00">2023-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/" itemprop="url" rel="index"><span itemprop="name">学术前沿</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>26k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>47 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="malware">MALWARE</span></h1><p>[TOC]</p>
<h2><span id="0补充">0.补充</span></h2><blockquote>
<p>  综述：</p>
<p>  Malconv</p>
<p>  数据集</p>
</blockquote>
<h2><span id="1-survey-overview">1. Survey Overview</span></h2><blockquote>
<p>  Period :2014-2021</p>
</blockquote>
<ul>
<li><h5><span id="platform">Platform</span></h5><ul>
<li>Windows [13,32]</li>
<li>Android [1-3,11,14,16,18,23,25,33,35-37,40]</li>
<li>Linux</li>
</ul>
</li>
<li><h5><span id="direction">Direction</span></h5><ul>
<li>Malware features from various aspects [1,9,24,28,31,40]</li>
<li>Malware propagation(传播) [2,25]</li>
<li>System mechanisms or services against malware [3,37]</li>
<li>Malware behaviors [5,15]<ul>
<li>Obfuscation [8,37]</li>
<li>Packing [8]</li>
<li>Stealth technologies [3,6]</li>
<li>Hook</li>
<li>Evasion from dynamic analysis [10,17,20,31,62]</li>
</ul>
</li>
<li>Dataset challenges, such as aging problem [21,23,41,51]</li>
<li>Performance metrics[14,23]</li>
<li>Specific malware：such as IoT malware[25,26,39], fileless[30] and PDF malware[43,54]</li>
<li>Visualization [15]</li>
<li>Graph representation [22]</li>
<li><strong>Detection Methods</strong> [3-4,8,9,11,12,14,16,19,31,33,36]<ul>
<li>ML based techniques [13,18,21,29,38,40]</li>
<li>DL based techniques [22,29,35]</li>
</ul>
</li>
<li><strong>APT</strong>(Advanced Persistent Threats) [20]</li>
<li><strong>Adversarial malware example generation</strong> [27,32]</li>
<li>ML/DL flaws [7,28]</li>
<li>ML/DL interpretability [34]</li>
</ul>
</li>
</ul>
<h2><span id="2-android-malware-detection">2. Android Malware detection</span></h2><h3><span id="21-behavior-detection-6364">2.1 Behavior detection [63,64]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Malton:Towards On-Device Non-Invasive Mobile Malware Analysis for ART</td>
<td>2017</td>
<td>Toprovide a comprehensive view of malware’s behaviors</td>
<td>Detectingeffectively</td>
<td>multi-layermonitoring &amp; information flow tracking</td>
</tr>
<tr>
<td>CopperDroid:Automatic Reconstruction of Android Malware Behaviors</td>
<td>2015</td>
<td>Toidentify OS- and high-level Android-specific behaviors.</td>
<td>Toreconstruct the behaviors of Android malware</td>
<td>VMI-baseddynamic analysis</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="22-signature-based-6566">2.2 Signature based [65,66]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>EnMobile: Entity-based Characterization and Analysis of Mobile</td>
<td>2018</td>
<td>Tocharacaterize malware comprehensively</td>
<td>Detectingeffectively</td>
<td>entity-based characterization and static analysis; signature based approach</td>
</tr>
<tr>
<td>Screening smartphone applications using malware family signatures</td>
<td>2015</td>
<td>Toimprove the robustness of signature matching</td>
<td>Toautomaticly extract family signature and matching</td>
<td>family signature</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="23-rule-based6768">2.3 Rule based[67,68]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Toward a more dependable hybrid analysis of android malware using aspect-oriented programming</td>
<td>2018</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>dataflowanalysis, detection of resource abuse;rule based</td>
</tr>
<tr>
<td>DroidNative: Automating and optimizing detection of Android native code malware variants</td>
<td>2017</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>specific control flow patterns;rule based</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="24-similarity-based">2.4 Similarity based</span></h3><h4><span id="241-model-similarity69-73">2.4.1 Model similarity[69-73]</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>An HMM and structural entropy based detector for Android malware: An empirical study</td>
<td>2016</td>
<td>Todefeat hiding</td>
<td>Detectingeffectively</td>
<td>HiddenMarkov Model, structural entropy.</td>
</tr>
<tr>
<td>Scalable and robust unsupervised android malware fingerprinting using community-based network partitioning</td>
<td>2020</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>maliciouscommunity</td>
</tr>
<tr>
<td>On the use of artificial malicious patterns for android malware detection</td>
<td>2020</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>malwarepatterns; Genetic Algorithm (GA); Apriori algorithm</td>
</tr>
<tr>
<td>Andro-Dumpsys: Anti-malware system based on the similarity of malware creator and malware centric information</td>
<td>2016</td>
<td>Todefeat packing, dynamic loading etc.</td>
<td>Detectingeffectively</td>
<td>similarity matching of malware creator-centric</td>
</tr>
<tr>
<td>Bayesian Active Malware Analysis</td>
<td>2020</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>the Markov chain models</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="242-graph-similarity74-79">2.4.2 Graph similarity[74-79]</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>PermPair: Android Malware Detection Using Permission Pairs</td>
<td>2020</td>
<td>Tomake use of permission information</td>
<td>Todetect Android malware</td>
<td>The comparasion of the graph of permission pairs.</td>
</tr>
<tr>
<td>Apposcopy: Semantics-Based Detection of Android Malware through Static Analysis</td>
<td>2014</td>
<td>Toimprove signature based methods</td>
<td>Detectingeffectively</td>
<td>combination of static taint analysis and program representation called Inter-Component Call Graph</td>
</tr>
<tr>
<td>Profiling user-trigger dependence for Android malware detection</td>
<td>2015</td>
<td>Tocapture stealthily launch operation</td>
<td>Detectingeffectively</td>
<td>Graphcomparision</td>
</tr>
<tr>
<td>Identifying Android Malware Using Network-Based Approaches</td>
<td>2019</td>
<td>Tomake use of network information</td>
<td>Detectingeffectively</td>
<td>aweighted network to compare closeness</td>
</tr>
<tr>
<td>Cypider: Building Community-Based Cyber-Defense Infrastructure for Android Malware Detection</td>
<td>2016</td>
<td>Todeal with endless new malware</td>
<td>Detectingeffectively</td>
<td>scalablesimilarity network infrastructure;malicious community</td>
</tr>
<tr>
<td>Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs</td>
<td>2014</td>
<td>Tocharacaterize malware from program semantics</td>
<td>Detectingeffectively</td>
<td>a weighted contextual API dependency graph as program semantics;graphsimilarity metrics</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="25-ml-based-6080-101">2.5 ML based [60,80-101]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MAMADROID:Detecting Android Malware by Building Markov Chains of Behavioral Models</strong></td>
<td>2017</td>
<td>Todesign robust malware mitigation techniques</td>
<td>Constructinga classifier</td>
<td>BuildingMarkov Chains of Behavioral Models;Random Forests , Nearest Neighbor (1-NN) ,3-Nearest Neighbor (3-NN) ,and Support Vector Machines (SVM)</td>
</tr>
<tr>
<td><strong>Drebin:Effective and Explainable Detection of Android Malware in Your Pocket</strong></td>
<td>2014</td>
<td>Tomitigate the influence on limited resources in Android platform</td>
<td>To propose a lightweight method to detect malware at run-time</td>
<td>Staticanalysis and SVM</td>
</tr>
<tr>
<td>MakeEvasion Harder: An Intelligent Android Malware Detection System</td>
<td>2018</td>
<td>Todetect evolving Android malware</td>
<td>Higherdetection rate</td>
<td>APIcalls and higher-level semantics; SVM</td>
</tr>
<tr>
<td>UsingLoops For Malware Classification Resilient to Feature-unaware Perturbations</td>
<td>2018</td>
<td>Tosolve feature-unaware perturbation</td>
<td>Todetect malware resilient to feature-unaware perturbation</td>
<td>Looplocating and random forest</td>
</tr>
<tr>
<td>SemanticModelling of Android Malware for Effective Malware Comprehension, Detection,and Classification</td>
<td>2016</td>
<td>Tomake use of semantic information</td>
<td>Todetect Android malware</td>
<td>Semanticmodel; Random forest</td>
</tr>
<tr>
<td>Detecting Android Malware Leveraging Text Semantics of Network Flows</td>
<td>2018</td>
<td>Tomake use of network information</td>
<td>Todetect Android malware</td>
<td>Usingthe text semantics of network traffic; SVM</td>
</tr>
<tr>
<td>Improving Accuracy of Android Malware Detection with Lightweight Contextual Awareness</td>
<td>2018</td>
<td>Toreduce redundant metadata in modeling</td>
<td>ImprovingAccuracy of Android Malware Detection</td>
<td>KNN;RF;MLP</td>
</tr>
<tr>
<td>MalScan: Fast Market-Wide Mobile Malware Scanning by Social-Network Centrality Analysis</td>
<td>2019</td>
<td>Toreduce the cost of semantic analysis</td>
<td>To propose a lightweight method to detect malware</td>
<td>social-network-basedcentrality analysis; kNN and random forest</td>
</tr>
<tr>
<td>PIndroid: A novel Android malware detection system using ensemble learning methods</td>
<td>2017</td>
<td>Tofight against covert technique of malware</td>
<td>Detectingeffectively</td>
<td>Permissionsand Intents based framework supplemented with Ensemble methods:Nave Bayesian,Decision Tree, Decision Table, Random Forest, Sequential Minimal Optimization and Multi Lateral Perceptron(MLP)</td>
</tr>
<tr>
<td>A pragmatic android malware detection procedure</td>
<td>2017</td>
<td>Todesign a new ML model</td>
<td>Detectingeffectively</td>
<td>Atomic Naive Bayes classifiers used as inputs for the Support Vector Machine ensemble.</td>
</tr>
<tr>
<td>ICCDetector: ICC-Based Malware Detection on Android</td>
<td>2016</td>
<td>Tocapture communication among components or cross boundaries to supplymentfeatures</td>
<td>Detectingeffectively</td>
<td>SVM</td>
</tr>
<tr>
<td>A Probabilistic Discriminative Model for Android Malware Detection with Decompiled Source Code</td>
<td>2015</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>the 2-class Naive Bayes with Prior (2-PNB) and a discriminative model,the regularized logistic regression</td>
</tr>
<tr>
<td>DroidCat: Effective Android Malware Detection and Categorization via App-Level Profiling</td>
<td>2019</td>
<td>Tofight against systemcall obfuscation</td>
<td>Detectingeffectively</td>
<td>Dynamicanalysis based on method calls and inter-component communication; RandomForest</td>
</tr>
<tr>
<td>MADAM: Effective and Efficient Behavior-based Android Malware Detection and Prevention</td>
<td>2018</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>KNN</td>
</tr>
<tr>
<td>==Android Malware Detection via (Somewhat) Robust Irreversible Feature Transformations==</td>
<td>2020</td>
<td>Toavoid ML classifier evading</td>
<td>Transferingfeatures to a new feature domain</td>
<td>Classifiers used:(1) Bernoulli Naive Bayes, (2) Random Forest, (3) NearestNeighbors, (4) Logistic Regression, (5) Gaussian Naive Bayes, (6) AdaBoost Classifier, (7) Gradient Boosting Decision Tree, (8) XGB Classifier and (9)SVM.</td>
</tr>
<tr>
<td>Leveraging ontologies and machine-learning techniques for malware analysis into Android permissions ecosystems</td>
<td>2018</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>ontology-basedframework;random forest</td>
</tr>
<tr>
<td>Lightweight, Obfuscation-Resilient Detection and Family Identification of Android Malware</td>
<td>2018</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>familyidentification;linear SVM</td>
</tr>
<tr>
<td>A multi-view context-aware approach to Android malware detection and malicious code localization</td>
<td>2018</td>
<td>To characaterize malware comprehensively</td>
<td>Detectingeffectively</td>
<td>multipleviews of apps;SVM</td>
</tr>
<tr>
<td>DroidFusion: A Novel Multilevel Classifier Fusion Approach for Android Malware Detection</td>
<td>2019</td>
<td>Toimprove classifier</td>
<td>Detectingeffectively</td>
<td>CLASSIFIER FUSION:J48, REPTree, voted perceptron, and random tree</td>
</tr>
<tr>
<td>DL-Droid: Deep learning based android malware detection using real devices</td>
<td>2020</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>input generation;MLP</td>
</tr>
<tr>
<td>JOWMDroid: Android malware detection based on feature weighting with joint optimization of weight-mapping and classifier parameters</td>
<td>2021</td>
<td>Tocharacaterize malware from feature importance</td>
<td>Detectingeffectively</td>
<td>featureweighting with the joint optimization of weight-mapping;SVM, LR, MLP</td>
</tr>
<tr>
<td>Towards using unstructured user input request for malware detection</td>
<td>2020</td>
<td>Todefeat privacy analysis evading</td>
<td>Detectingeffectively</td>
<td>decision tree</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="26-dl-based-102-109">2.6 DL based [102-109]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Toward s an interpretable deep learning model for mobile malware detection and family identification</td>
<td>2021</td>
<td>Topropose a interpretable DL model</td>
<td>Detectingreasonablely</td>
<td>DL:Grad-CAM</td>
</tr>
<tr>
<td>AMalNet: A deep learning framework based on graph convolutional networks for malware detection</td>
<td>2020</td>
<td>Tohave a lower cost</td>
<td>Detectingeffectively</td>
<td>DL:GCNsand IndRNN</td>
</tr>
<tr>
<td>Disentangled Representation Learning in Heterogeneous Information Network for Large-scale Android Malware Detection in the COVID-19 Era and Beyond</td>
<td>2021</td>
<td>Tosolve the problem that society relys on the complex cyberspace</td>
<td>Detectingeffectively</td>
<td>heterogeneousinformation network (HIN);DNN</td>
</tr>
<tr>
<td>A Multimodal Deep Learning Method for Android Malware Detection Using Various Features</td>
<td>2019</td>
<td>Tocharacaterize malware comprehensively</td>
<td>Detectingeffectively</td>
<td>multimodaldeep learning method;DNN</td>
</tr>
<tr>
<td>Android Fragmentation in Malware Detection</td>
<td>2019</td>
<td>Todeal with multiple Android version</td>
<td>Detectingeffectively</td>
<td>Deep Neural Network</td>
</tr>
<tr>
<td>An Image-inspired and CNN-based Android Malware Detection Approach</td>
<td>2019</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>CNN</td>
</tr>
<tr>
<td>A Performance-Sensitive Malware Detection System Using Deep Learning on Mobile Devices</td>
<td>2021</td>
<td>Toreduce time cost of download and upload</td>
<td>Detectingfastly</td>
<td>customized DNN</td>
</tr>
<tr>
<td>Byte-level malware classification based on markov images and deep learning</td>
<td>2020</td>
<td>Toimprove the accuracy of gray image based methods</td>
<td>Detectingeffectively</td>
<td>deep convolutional neural network</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="3-windows-malware-detection">3 Windows Malware detection</span></h2><h3><span id="31-behavior-detection-110111">3.1 Behavior detection [110,111]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>API Chaser: Anti-analysis Resistant Malware Analyzer</td>
<td>2013</td>
<td>API call feature capture</td>
</tr>
<tr>
<td>MalViz: An Interactive Visualization Tool for Tracing Malware</td>
<td>2018</td>
<td>Behavior visualization</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="32-signature-based-112">3.2 Signature based [112]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>CloudEyes: Cloud-based malware detection with reversible sketch for resource-constrained internet of things (IoT) devices</td>
<td>2017</td>
<td>Based on cloud</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="33-rule-based113">3.3 Rule based[113]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>A fast malware detection algorithm based on objective-oriented association mining</td>
<td>2013</td>
<td>API selection</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="34-similarity-based">3.4 Similarity based</span></h3><h4><span id="341-model-similarity114-122">3.4.1 Model similarity[114-122]</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>PoMMaDe: Pushdown Model-checking for Malware Detection</td>
<td>2013</td>
<td>model checking</td>
</tr>
<tr>
<td>Growing Grapes in Your Computer to Defend Against Malware</td>
<td>2014</td>
<td>clustering and template matching</td>
</tr>
<tr>
<td>Hypervisor-based malware protection with AccessMiner</td>
<td>2015</td>
<td>system-centric behavioral detector</td>
</tr>
<tr>
<td>Probabilistic Inference on Integrity for Access Behavior Based Malware Detection</td>
<td>2015</td>
<td>probabilistic model of integrity</td>
</tr>
<tr>
<td>Probabilistic analysis of dynamic malware traces</td>
<td>2018</td>
<td>1.Features of system interaction 2. interpretability</td>
</tr>
<tr>
<td>A malware detection method based on family behavior graph</td>
<td>2018</td>
<td>common behavior graph</td>
</tr>
<tr>
<td>Malware classification using self organising feature maps and machine activity data</td>
<td>2018</td>
<td>1.The improvement of ML. to reduce over-fitting 2. Self Organizing Feature Maps</td>
</tr>
<tr>
<td>Volatile memory analysis using the MinHash method for efficient and secured detection of malware in private cloud</td>
<td>2019</td>
<td>Based on memory features</td>
</tr>
<tr>
<td>A dynamic Windows malware detection and prediction method based on contextual understanding of API call sequence</td>
<td>2020</td>
<td>1.Contextual relationship between API call features 2. Marcovchain</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="342-graph-similarity123-127">3.4.2 Graph similarity[123-127]</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deriving common malware behavior through graph clustering</td>
<td>2013</td>
<td>common behavior graph</td>
</tr>
<tr>
<td>Enhancing the detection of metamorphic malware using call graphs</td>
<td>2014</td>
<td>API call graph matching</td>
</tr>
<tr>
<td>Minimal contrast frequent pattern mining for malware detection</td>
<td>2016</td>
<td>Graph matching</td>
</tr>
<tr>
<td><strong>Heterogeneous Graph Matching Networks for Unknown Malware Detection</strong></td>
<td>2019</td>
<td>Graph matching similarity of benign software</td>
</tr>
<tr>
<td>Random CapsNet for est model for imbalanced malware type classification task</td>
<td>2021</td>
<td>The improvement of the Model</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="35-ml-based-128-143">3.5 ML based [128-143]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Scalable Approach for Malware Detection through Bounded Feature Space Behavior Modeling</td>
<td>2013</td>
<td>Scalable feature space</td>
</tr>
<tr>
<td>SigMal: A Static Signal Processing Based Malware Triage</td>
<td>2013</td>
<td>noise-resistant similarity signatures</td>
</tr>
<tr>
<td>Unsupervised Anomaly-Based Malware Detection Using Hardware Features</td>
<td>2014</td>
<td>hardware supported lower-level features</td>
</tr>
<tr>
<td>Control flow-based opcode behavior analysis for Malware detection</td>
<td>2014</td>
<td>Based on control flow method features</td>
</tr>
<tr>
<td>Employing Program Semantics for Malware Detection</td>
<td>20152021</td>
<td>Extracting information-rich call sequence based on AEPThe improvement of the Model</td>
</tr>
<tr>
<td>AMAL: High-fidelity, behavior-based automated malware analysis and classification</td>
<td>2015</td>
<td>Based on behavior analysis</td>
</tr>
<tr>
<td>Optimized Invariant Representation of Network Traffic for Detecting Unseen Malware Variants</td>
<td>2016</td>
<td>Network features</td>
</tr>
<tr>
<td>DYNAMINER: Leveraging Offline Infection Analytics for On-the-Wire Malware Detection</td>
<td>2017</td>
<td>Network features</td>
</tr>
<tr>
<td>Security importance assessment for system objects and malware detection</td>
<td>2017</td>
<td>Based on importance of system objects</td>
</tr>
<tr>
<td>From big data to knowledge: A spatiotemporal approach to malware detection</td>
<td>2018</td>
<td>cloud based security service features</td>
</tr>
<tr>
<td>From big data to knowledge: A spatiotemporal approach to malware detection</td>
<td>2018</td>
<td>cloud based security service features</td>
</tr>
<tr>
<td>MalDAE: Detecting and explaining malware based on correlation and fusion of static and dynamic characteristics</td>
<td>2019</td>
<td>fusion of static and dynamic API sequence features</td>
</tr>
<tr>
<td>Leveraging Compression-Based Graph Mining for Behavior-Based Malware Detection</td>
<td>2019</td>
<td>Based on data flow graph</td>
</tr>
<tr>
<td>Advanced Windows Methods on Malware Detection and Classification</td>
<td>2020</td>
<td>API based Features extraction.</td>
</tr>
<tr>
<td>Sub-curve HMM: A malware detection approach based on partial analysis of API call sequences</td>
<td>2020</td>
<td>1.Subset of API call feature 2. HMM</td>
</tr>
<tr>
<td>Multiclass malware classification via first- and second-order texture statistics</td>
<td>2020</td>
<td>visualization</td>
</tr>
<tr>
<td>Catch them alive: A malware detection approach through memory forensics, manifoldlearning and computer vision</td>
<td>2021</td>
<td>Visualization</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="36-dl-based-144-156">3.6 DL based [144-156]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Auto-detection of sophisticated malware using lazy-binding control flow graph and deep learning</td>
<td>2018</td>
<td>1.The improvement of CFG 2. Visualizaiton</td>
</tr>
<tr>
<td>Malware identification using visualization images and deep learning</td>
<td>2018</td>
<td>1.SimHash of features 2. Visualization</td>
</tr>
<tr>
<td>Classification of Malware by Using Structural Entropy on Convolutional Neural Networks</td>
<td>2018</td>
<td>visual similarity</td>
</tr>
<tr>
<td>Classifying Malware Represented as Control Flow Graphs using Deep Graph Convolutional Neural Network</td>
<td>2019</td>
<td>The improvement of CFG</td>
</tr>
<tr>
<td><strong>Neurlux: Dynamic Malware Analysis Without Feature Engineering</strong></td>
<td>2019</td>
<td>Based on dynamic analysis reports</td>
</tr>
<tr>
<td>A feature-hybrid malware variants detection using CNN based opcode embedding and BPNN based API embedding</td>
<td>2019</td>
<td>Hybrid features</td>
</tr>
<tr>
<td>Effective analysis of malware detection in cloud computing</td>
<td>2019</td>
<td>The improvement of the DL.</td>
</tr>
<tr>
<td>Recurrent neural network for detecting malware</td>
<td>2020</td>
<td>The improvement of RNN</td>
</tr>
<tr>
<td><strong>Dynamic Malware Analysis with Feature Engineering and Feature Learning</strong></td>
<td>2020</td>
<td>Feature hashing to encode API call info.</td>
</tr>
<tr>
<td>An improved two-hidden-layer extreme learning machine for malware hunting</td>
<td>2020</td>
<td>Improvement of the DL.</td>
</tr>
<tr>
<td>HYDRA: A multimodal deep learning framework for malware classification</td>
<td>2020</td>
<td>Hybrid features</td>
</tr>
<tr>
<td>A novel method for malware detection on ML-based visualization technique</td>
<td>2020</td>
<td>visualization</td>
</tr>
<tr>
<td>Image-Based malware classification using ensemble of CNN architectures (IMCEC)</td>
<td>2020</td>
<td>visualization</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="4-mldl-flaws-overview">4. ML/DL flaws Overview</span></h2><ul>
<li>Ensemble classifier evasion [42]</li>
<li>Performance degradation [42,46,53,54]</li>
<li>Adversarial example generation [43,44,45,48,55,56,57,58]</li>
<li>Poisoning Attack [47]</li>
<li>Feature weights [49]</li>
<li>Cost analysis [50]</li>
<li>ML bias from dataset [51]</li>
<li>Influence of packing [52]</li>
<li>Methods reproduction [59]</li>
</ul>
<h2><span id="5-references">5. References</span></h2><ol>
<li><p>2014 A Survey of Android Malware Characterisitics and Mitigation Techniques</p>
</li>
<li><p>2014 Smartphone Malware and Its Propagation Modeling:A Survey</p>
</li>
<li><p>2015 Android Security: A Survey of Issues, Malware Penetration, and Defenses</p>
</li>
<li><p>2014 Evolution and Detection of Polymorphic and Metamorphic Malwares: A Survey</p>
</li>
<li><p>2015 Kernel Malware Core Implementation: A Survey </p>
</li>
<li><p>2016 A Survey of Stealth Malware Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions</p>
</li>
<li><p>2016 On the Security of Machine Learning in Malware C&amp;C Detection: A Survey</p>
</li>
<li><p>2017 Malware Methodologies and Its Future: A Survey</p>
</li>
<li><p>2017 A Survey on Malware Detection Using Data Mining Techniques</p>
</li>
<li><p>2018 Malware Dynamic Analysis Evasion Techniques: A Survey</p>
</li>
<li><p>2018 Android Malware Detection: A Survey</p>
</li>
<li>2018 A Survey on Metamorphic Malware Detection based on Hidden Markov Model</li>
<li>2018 Machine Learning Aided Static Malware Analysis: A Survey and Tutorial</li>
<li>2018 A survey on dynamic mobile malware detection</li>
<li>2018 A survey of malware behavior description and analysis</li>
<li>2019 A Survey on Android Malware Detection Techniques Using Machine Learning Algorithms</li>
<li>2019 Dynamic Malware Analysis in the Modern Era—A State of the Art Survey</li>
<li>2019 Data-Driven Android Malware Intelligence: A Survey</li>
<li>2019 A survey of zero-day malware attacks and itsdetection methodology</li>
<li>2019 A Survey on malware analysis and mitigation techniques</li>
<li><strong>2019 Survey of machine learning techniques for malware analysis</strong></li>
<li>2020 Deep Learning and Open Set Malware Classification: A Survey</li>
<li>2020 A Comprehensive Survey on Machine Learning Techniques for Android Malware Detection</li>
<li>2015 A Survey on Mining Program-Graph Features for Malware Analysis</li>
<li>2020 Stochastic Modeling of IoT Botnet Spread: A Short Survey on Mobile Malware Spread Modeling</li>
<li>2020 A survey of IoT malware and detection methods based on static features</li>
<li>2020 A survey on practical adversarial examples for malware classifiers</li>
<li>2020 A Survey of Machine Learning Methods and Challenges for Windows Malware Classification</li>
<li>2020 A Survey on Malware Detection with Deep Learning</li>
<li>2020 An emerging threat Fileless malware: a survey and research challenges</li>
<li>2021 Malware classification and composition analysis: A survey of recent developments</li>
<li><strong>2021 Adversarial EXEmples: A Survey and Experimental Evaluation of Practical Attacks on Machine Learning for Windows Malware Detection</strong></li>
<li>2020 A Survey on Mobile Malware Detection Techniques</li>
<li>2021 Towards interpreting ML-based automated malware detection models: a survey</li>
<li>2021 A Survey of Android Malware Detection with Deep Neural Models</li>
<li>2021 A survey of malware detection in Android apps: Recommendations and perspectives for future research</li>
<li>2021 A survey of android application and malware hardening</li>
<li>2021 A survey on machine learning-based malware detection in executable files</li>
<li>2021 The evolution of IoT Malwares, from 2008 to 2019: Survey, taxonomy, process simulator and perspectives</li>
<li>2021 A Survey of Android Malware Static Detection Technology Based on Machine Learning</li>
<li>2016 Empirical assessment of machine learning-based malware detectors for Android Measuring the gap between in-the-lab and in-the-wild validation scenarios </li>
<li>2016 When a Tree Falls: Using Diversity in Ensemble Classifiers to Identify Evasion in Malware Detectors</li>
<li><strong>2016 Automatically Evading Classifiers A Case Study on PDF Malware Classifiers</strong></li>
<li>2017 SecureDroid: Enhancing Security of Machine Learning-based Detection against Adversarial Android Malware Attacks</li>
<li>2017 How to defend against adversarial attack</li>
<li><strong>2017 Transcend: Detecting Concept Drift in Malware Classification Models</strong></li>
<li><strong>2018 Automated poisoning attacks and defenses in malware detection systems: An adversarial machine learning approach</strong></li>
<li><strong>2018 Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers</strong></li>
<li><strong>==2019 Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection==</strong></li>
<li>2019 A cost analysis of machine learning using dynamic runtime opcodes for malware detection</li>
<li><strong>2019 TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time</strong></li>
<li>2020 When Malware is Packin’ Heat; Limits of Machine Learning Classifiers Based on Static Analysis Features</li>
<li>2020 Assessing and Improving Malware Detection Sustainability through App Evolution Studies</li>
<li>2020 On Training Robust PDF Malware Classifiers</li>
<li>2020 Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection</li>
<li>2020 Intriguing Properties of Adversarial ML Attacks in the Problem Space Fabio</li>
<li>2020 Query-Efficient Black-Box Attack Against Sequence-Based Malware Classifiers</li>
<li>2020 Enhancing State-of-the-art Classifiers with API Semantics to Detect Evolved Android Malware</li>
<li>2021 Lessons Learnt on Reproducibility in Machine Learning Based Android Malware Detection</li>
<li>2016 Semantics-Based Online Malware Detection: Towards Efficient Real-Time Protection Against Malware</li>
<li>2018 Understanding Linux Malware</li>
<li>2017 Droid-AntiRM: Taming Control Flow Anti-analysis to Support Automated Dynamic Analysis of Android Malware</li>
<li>2017 Malton: Towards On-Device Non-Invasive Mobile Malware Analysis for ART</li>
<li>2015 CopperDroid: Automatic Reconstruction of Android Malware Behaviors</li>
<li>2018 EnMobile: Entity-based Characterization and Analysis of Mobile</li>
<li>2015 Screening smartphone applications using malware family signatures</li>
<li>2018 Toward a more dependable hybrid analysis of android malware using aspect-oriented programming</li>
<li>2017 DroidNative: Automating and optimizing detection of Android native code malware variants</li>
<li>2016 An HMM and structural entropy based detector for Android malware: An empirical study</li>
<li>2020 Scalable and robust unsupervised android malware fingerprinting using community-based network partitioning </li>
<li>2020 On the use of artificial malicious patterns for android malware detection </li>
<li>2016 Andro-Dumpsys: Anti-malware system based on the similarity of malware creator and malware centric information</li>
<li>2020 Bayesian Active Malware Analysis </li>
<li>2020 PermPair: Android Malware Detection Using Permission Pairs</li>
<li>2014 Apposcopy: Semantics-Based Detection of Android Malware through Static Analysis</li>
<li>2015 Profiling user-trigger dependence for Android malware detection</li>
<li>2019 Identifying Android Malware Using Network-Based Approaches</li>
<li>2016 Cypider: Building Community-Based Cyber-Defense Infrastructure for Android Malware Detection</li>
<li>2014 Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs </li>
<li>2017 MAMADROID: Detecting Android Malware by Building Markov Chains of Behavioral Models</li>
<li>2014 Drebin: Effective and Explainable Detection of Android Malware in Your Pocket</li>
<li>2018 Make Evasion Harder: An Intelligent Android Malware Detection System</li>
<li>2018 Using Loops For Malware Classification Resilient to Feature-unaware Perturbations</li>
<li>2016 Semantic Modelling of Android Malware for Effective Malware Comprehension, Detection, and Classification</li>
<li>2018 Detecting Android Malware Leveraging Text Semantics of Network Flows</li>
<li>2018 Improving Accuracy of Android Malware Detection with Lightweight Contextual Awareness</li>
<li>2019 MalScan: Fast Market-Wide Mobile Malware Scanning by Social-Network Centrality Analysis</li>
<li>2017    PIndroid: A novel Android malware detection system using ensemble learning methods</li>
<li>2017    A pragmatic android malware detection procedure</li>
<li>2016    ICCDetector: ICC-Based Malware Detection on Android</li>
<li>2015    A Probabilistic Discriminative Model for Android Malware Detection with Decompiled Source Code</li>
<li>2019    DroidCat: Effective Android Malware Detection and Categorization via App-Level Profiling</li>
<li>2018    MADAM: Effective and Efficient Behavior-based Android Malware Detection and Prevention</li>
<li>2020    Android Malware Detection via (Somewhat) Robust Irreversible Feature Transformations</li>
<li>2018    Leveraging ontologies and machine-learning techniques for malware analysis into Android permissions ecosystems </li>
<li>2018    Lightweight, Obfuscation-Resilient Detection and Family Identification of Android Malware</li>
<li>2018    A multi-view context-aware approach to Android malware detection and malicious code localization</li>
<li>2019    DroidFusion: A Novel Multilevel Classifier Fusion Approach for Android Malware Detection</li>
<li>2020    DL-Droid: Deep learning based android malware detection using real devices </li>
<li>2021    JOWMDroid: Android malware detection based on feature weighting with joint optimization of weight-mapping and classifier parameters</li>
<li>2020    Towards using unstructured user input request for malware detection</li>
<li>2021 Toward s an interpretable deep learning model for mobile malware detection and family identification</li>
<li>2020 AMalNet: A deep learning framework based on graph convolutional networks for malware detection </li>
<li>2021 Disentangled Representation Learning in Heterogeneous Information Network for Large-scale Android Malware Detection in the COVID-19 Era and Beyond</li>
<li>2019 A Multimodal Deep Learning Method for Android Malware Detection Using Various Features</li>
<li>2019 Android Fragmentation in Malware Detection</li>
<li>2019 An Image-inspired and CNN-based Android Malware Detection Approach</li>
<li>2021 A Performance-Sensitive Malware Detection System Using Deep Learning on Mobile Devices</li>
<li>2020 Byte-level malware classification based on markov images and deep learning</li>
<li>2013 API Chaser: Anti-analysis Resistant Malware Analyzer</li>
<li>2018 MalViz: An Interactive Visualization Tool for Tracing Malware</li>
<li>2017 CloudEyes: Cloud-based malware detection with reversible sketch for resource-constrained internet of things (IoT) devices</li>
<li>2013 A fast malware detection algorithm based on objective-oriented association mining</li>
<li>2013 PoMMaDe: Pushdown Model-checking for Malware Detection</li>
<li>2014 Growing Grapes in Your Computer to Defend Against Malware</li>
<li>2015 Hypervisor-based malware protection with AccessMiner</li>
<li>2015 Probabilistic Inference on Integrity for Access Behavior Based Malware Detection</li>
<li>2018 Probabilistic analysis of dynamic malware traces</li>
<li>2018 A malware detection method based on family behavior graph</li>
<li>2018 Malware classification using self organising feature maps and machine activity data</li>
<li>2019 Volatile memory analysis using the MinHash method for efficient and secured detection of malware in private cloud</li>
<li>2020 A dynamic Windows malware detection and prediction method based on contextual understanding of API call sequence</li>
<li>2013 Deriving common malware behavior through graph clustering</li>
<li>2014 Enhancing the detection of metamorphic malware using call graphs</li>
<li>2016 Minimal contrast frequent pattern mining for malware detection</li>
<li>2019 Heterogeneous Graph Matching Networks for Unknown Malware Detection</li>
<li>2021 Random CapsNet for est model for imbalanced malware type classification task</li>
<li>2013 A Scalable Approach for Malware Detection through Bounded Feature Space Behavior Modeling</li>
<li>2013 SigMal: A Static Signal Processing Based Malware Triage</li>
<li>2014 Unsupervised Anomaly-Based Malware Detection Using Hardware Features</li>
<li>2014 Control flow-based opcode behavior analysis for Malware detection</li>
<li>2015 Employing Program Semantics for Malware Detection</li>
<li>2015 AMAL: High-fidelity, behavior-based automated malware analysis and classification</li>
<li>2016 Optimized Invariant Representation of Network Traffic for Detecting Unseen Malware Variants</li>
<li>2017 DYNAMINER: Leveraging Offline Infection Analytics for On-the-Wire Malware Detection</li>
<li>2017 Security importance assessment for system objects and malware detection</li>
<li>2018 From big data to knowledge: A spatiotemporal approach to malware detection</li>
<li>2019 MalDAE: Detecting and explaining malware based on correlation and fusion of static and dynamic characteristics</li>
<li>2019    Leveraging Compression-Based Graph Mining for Behavior-Based Malware Detection</li>
<li>2020    Advanced Windows Methods on Malware Detection and Classification</li>
<li>2020    Sub-curve HMM: A malware detection approach based on partial analysis of API call sequences </li>
<li>2020 Multiclass malware classification via first- and second-order texture statistics </li>
<li>2021 Catch them alive: A malware detection approach through memory forensics, manifold learning and computer vision</li>
<li>2018 Auto-detection of sophisticated malware using lazy-binding control flow graph and deep learning</li>
<li>2018 Malware identification using visualization images and deep learning</li>
<li>2018 Classification of Malware by Using Structural Entropy on Convolutional Neural Networks</li>
<li>2019 Classifying Malware Represented as Control Flow Graphs using Deep Graph Convolutional Neural Network</li>
<li>2019 Neurlux: Dynamic Malware Analysis Without Feature Engineering</li>
<li>2019 A feature-hybrid malware variants detection using CNN based opcode embedding and BPNN based API embedding </li>
<li>2019 Effective analysis of malware detection in cloud computing </li>
<li>2020 Recurrent neural network for detecting malware</li>
<li>2020 Dynamic Malware Analysis with Feature Engineering and Feature Learning</li>
<li>2020 An improved two-hidden-layer extreme learning machine for malware hunting</li>
<li>2020 HYDRA: A multimodal deep learning framework for malware classification</li>
<li>2020 A novel method for malware detection on ML-based visualization technique</li>
<li>2020 Image-Based malware classification using ensemble of CNN architectures (IMCEC)</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/22/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><span class="page-number current">23</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/24/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
