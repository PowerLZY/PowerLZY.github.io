<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/23/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/23/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/23/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">263</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/5BWKZW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/5BWKZW/" class="post-title-link" itemprop="url">度量学习（1）KNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:48" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:48+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 19:45:25" itemprop="dateModified" datetime="2023-04-26T19:45:25+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-什么是knnkd树-siftbbf算法">一、什么是KNN【KD树 +
SIFT+BBF算法】</span></h3>
<h4><span id="11-knn的通俗解释">1.1 KNN的通俗解释</span></h4>
<p>何谓K近邻算法，即K-Nearest Neighbor
algorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。</p>
<p>用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，<strong>在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。</strong></p>
<p>​ <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067"><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232118382.jpeg" alt="img"></a></p>
<p>​ <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67"><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232118849.png" alt="img"></a></p>
<p>如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），KNN就是解决这个问题的。</p>
<p>如果<strong>K=3</strong>，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>红色</strong>的三角形一类。</p>
<p>如果<strong>K=5</strong>，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>蓝色</strong>的正方形一类。</p>
<p><strong>于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</strong></p>
<h4><span id="12-近邻的距离度量">1.2 近邻的距离度量</span></h4>
<p>我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。</p>
<p><strong>有哪些距离度量的表示法</strong>(普及知识点，可以跳过)：</p>
<h5><span id="121-欧式距离">1.2.1 欧式距离</span></h5>
<p>欧氏距离, 最常见的两点之间或多点之间的距离表示法,
又称之为欧几里得度量, 它定义于欧几里得空间中, 如点 <span class="math inline">\(x=(x 1, \ldots, x n)\)</span> 和 <span class="math inline">\(y=(y 1, \ldots, y n)\)</span> 之间的距离为: <span class="math display">\[
d(x,
y)=\sqrt{\left(x_1-y_1\right)^2+\left(x_2-y_2\right)^2+\ldots+\left(x_n-y_n\right)^2}=\sqrt{\sum_{i=1}^n\left(x_i-y_i\right)^2}
\]</span> 二维平面上两点 <span class="math inline">\(a(x 1, y
1)\)</span> 与 <span class="math inline">\(b(x 2, y 2)\)</span>
间的欧氏距离: <span class="math display">\[
d_{12}=\sqrt{\left(x_1-x_2\right)^2+\left(y_1-y_2\right)^2}
\]</span> 三维空间两点 <span class="math inline">\(a(x 1, y 1, z
1)\)</span> 与 <span class="math inline">\(b(x 2, y 2, z 2)\)</span>
间的欧氏距离: <span class="math display">\[
d_{12}=\sqrt{\left(x_1-x_2\right)^2+\left(y_1-y_2\right)^2+\left(z_1-z_2\right)^2}
\]</span> 两个n维向量 <span class="math inline">\(a(x 11, \times 12,
\ldots, x 1 n)\)</span> 与 <span class="math inline">\(b(\times 21,
\times 22, \ldots, \times 2 n)\)</span> 间的欧氏距离: <span class="math display">\[
d_{12}=\sqrt{\sum_{k=1}^n\left(x_{1 k}-x_{2 k}\right)^2}
\]</span> 也可以用表示成向量运算的形式:</p>
<h5><span id="122-曼哈顿距离">1.2.2 曼哈顿距离</span></h5>
<p>曼哈顿距离, 我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离,
也就是在欧几里得空间的固定
直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上, 坐标
<span class="math inline">\((x 1, y 1)\)</span> 的点P1与坐标 <span class="math inline">\((\mathrm{x} 2, \mathrm{y} 2)\)</span>
的点P2的曼哈顿距离为: <span class="math inline">\(\left|x_1-x_2\right|+\left|y_1-y_2\right|\)</span>,
要注意的是, 曼哈顿距离依赖座标系统的转度,
而非系统在座标轴上的平移或映射。</p>
<p>通俗来讲, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口,
驾驶距离是两点间的直线距离吗? 显 然不是,
除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”,
此即曼哈顿距离名称的来源, 同时, 曼 哈顿距离也称为城市街区距离(City Block
distance)。</p>
<p>二维平面两点 <span class="math inline">\(a(x 1, y 1)\)</span> 与
<span class="math inline">\(b(x 2, y 2)\)</span> 间的曼哈顿距离 <span class="math display">\[
d_{12}=\left|x_1-x_2\right|+\left|y_1-y_2\right|
\]</span> 两个 <span class="math inline">\(n\)</span> 维向量 <span class="math inline">\(a(x 11, x 12, \ldots, x 1 n)\)</span> 与 <span class="math inline">\(b(x 21, x 22, \ldots, x 2 n)\)</span>
间的曼哈顿距离 <span class="math display">\[
d_{12}=\sum_{k=1}^n\left|x_{1 k}-x_{2 k}\right|
\]</span></p>
<h5><span id="123-切比雪夫距离">1.2.3 切比雪夫距离</span></h5>
<p>切比雪夫距离, 若二个向量或二个点 <span class="math inline">\(\mathrm{p} 、\)</span> and <span class="math inline">\(\mathrm{q}\)</span>, 其座标分别为 <span class="math inline">\(\mathrm{P}\)</span> 及 <span class="math inline">\(\mathrm{qi}\)</span>,
则两者之间的切比雪夫距离定义如 下: <span class="math inline">\(D_{C h e
b y s h e v}(p, q)=\max _i\left(\left|p_i-q_i\right|\right)\)</span></p>
<p>这也等于以下Lp度量的极值： <span class="math inline">\(\lim _{x
\rightarrow \infty}\left(\sum_{i=1}^n\left|p_i-q_i\right|^k\right)^{1 /
k}\)</span>, 因此切比雪夫距离也称为 <span class="math inline">\(\infty\)</span> 度量。</p>
<p>以数学的观点来看, 切比雪夫距离是由一致范数 (uniform norm)
(或称为上确界范数）所衍生的度量, 也 是超凸度量（injective metric
space）的一种。</p>
<p>在平面几何中, 若二点 <span class="math inline">\(\mathrm{p}
\mathrm{q}\)</span> 的直角坐标系坐标为 <span class="math inline">\((x 1,
y 1)\)</span> 及 <span class="math inline">\((x 2, y 2)\)</span>,
则切比雪夫距离为: <span class="math inline">\(D_{C h e s s}=\max
\left(\left|x_2-x_1\right|,\left|y_2-y_1\right|\right)\)</span></p>
<p>玩过国际象棋的朋友或许知道，国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)
走到格子(x2,y2)最少需要多少步? 。你会发现最少步数总是 <span class="math inline">\(\max (|x 2-x 1| ，|y 2-y 1|)\)</span> 步
。有一种类似的 一种距离度量方法叫切比雪夫距离。</p>
<p>二维平面两点 <span class="math inline">\(a(x 1, y 1)\)</span> 与
<span class="math inline">\(b(x 2, y 2)\)</span> 间的切比雪夫距离 :
<span class="math display">\[
d_{12}=\max \left(\left|x_2-x_1\right|,\left|y_2-y_1\right|\right)
\]</span> 两个 <span class="math inline">\(n\)</span> 维向量 <span class="math inline">\(a(x 11, x 12, \ldots, x 1 n)\)</span> 与 <span class="math inline">\(b(x 21, x 22, \ldots, x 2 n)\)</span>
间的切比雪夫距离 : <span class="math display">\[
d_{12}=\max _i\left(\left|x_{1 i}-x_{2 i}\right|\right)
\]</span></p>
<p><strong>简单说来，各种“距离”的应用场景简单概括为：</strong></p>
<ul>
<li><strong>空间：欧氏距离</strong>，</li>
<li><strong>路径：曼哈顿距离，国际象棋国王：切比雪夫距离</strong>，</li>
<li>以上三种的统一形式:闵可夫斯基距离，</li>
<li>加权：标准化欧氏距离，</li>
<li>排除量纲和依存：马氏距离，</li>
<li>向量差距：夹角余弦，</li>
<li><strong>编码差别：汉明距离</strong>，</li>
<li>集合近似度：杰卡德类似系数与距离，</li>
<li>相关：相关系数与相关距离。</li>
</ul>
<h4><span id="13-k值选择">1.3 K值选择</span></h4>
<ol type="1">
<li>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></li>
<li>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且<strong>K值的增大就意味着整体的模型变得简单。</strong></li>
<li>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</li>
</ol>
<p>在实际应用中，K值一般取一个比较小的数值，<strong>例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</strong></p>
<h4><span id="14-knn最近邻分类算法的过程">1.4 KNN最近邻分类算法的过程</span></h4>
<ol type="1">
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前 k 个最小距离的样本；</li>
<li>根据这 k 个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<h3><span id="关于knn的一些问题">关于KNN的一些问题</span></h3>
<ol type="1">
<li><p>在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用<strong>曼哈顿距离</strong>？</p>
<p><strong>答：</strong>我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向的运动。</p></li>
<li><p>KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?</p>
<p>答：极大的节约了时间成本．点线距离如果
&gt;　最小点，无需回溯上一层，如果&lt;,则再上一层寻找。</p></li>
</ol>
<h3><span id="参考文献">参考文献</span></h3>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272"><font color="blue">KNN与KD树</font></a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23966698"><font color="blue">【数学】kd
树算法之详细篇 - 椰了的文章 - 知乎</font></a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/475072467/answer/2027766449"><font color="blue"><strong>KNN是生成式模型还是判别式的</strong>，为什么？
- 风控算法小白的回答 - 知乎</font></a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3BS4T2Z/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3BS4T2Z/" class="post-title-link" itemprop="url">降维（1）PCA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:27:10" itemprop="dateModified" datetime="2023-04-22T16:27:10+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>数据降维算法: https://www.zhihu.com/column/c_1194552337170214912</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221528241.jpg" alt="【机器学习】降维——PCA（非常详细）" style="zoom: 50%;"></p>
<h2><span id="一-pca">一、 PCA</span></h2>
<p><strong><font color="red"> 降维问题的优化目标：将一组 <span class="math inline">\(\mathrm{N}\)</span> 维向量降为 <span class="math inline">\(\mathrm{K}\)</span> 维，其目标是选择 <span class="math inline">\(\mathrm{K}\)</span>
个单位正交基，使得原始数据变换到这组基上 后，各变量两两间协方差为 0
，而变量方差则尽可能大（在正交的约束下，取最大的 <span class="math inline">\(\mathrm{K}\)</span> 个方差）。</font></strong></p>
<p>要找的 <span class="math inline">\(\mathbf{P}\)</span>
是能让<strong>原始协方差矩阵对角化</strong>的 <span class="math inline">\(\mathbf{P}\)</span> 。换句话说,
优化目标变成了寻找一个矩阵 <span class="math inline">\(\mathbf{P}\)</span>, <strong>满足 <span class="math inline">\(P C P^T\)</span> 是一个对
角矩阵，并且对角元素按从大到小依次排列，那么 <span class="math inline">\(P\)</span> 的前 <span class="math inline">\(K\)</span> 行就是要寻找的基，用 <span class="math inline">\(P\)</span> 的前 <span class="math inline">\(K\)</span> 行组成的矩阵乘以 <span class="math inline">\(X\)</span> 就使得 X 从 N 维降到了 K
维并满足上述优化条件。</strong></p>
<p><strong>PCA (Principal Component Analysis) 是一种常见的数据分析方式,
常用于高维数据的降维，可用于提取数 据的主要特征分量。</strong>PCA
的数学推导可以从<strong>最大可分型</strong>和<strong>最近重构性</strong>两方面进行,
前者的优化条件为划分后方差 最大,
后者的优化条件为点到划分平面距离最小，这里我将从最大可分性的角度进行证明。</p>
<h3><span id="1-向量表示与基变换">1. 向量表示与基变换</span></h3>
<p>我们先来介绍些线性代数的基本知识。</p>
<h4><span id="111-内积">1.1.1 内积</span></h4>
<p><strong>两个向量的 A 和 B 内积</strong>我们知道形式是这样的： <span class="math display">\[
\left(a_1, a_2, \cdots, a_n\right) \cdot\left(b_1, b_2, \cdots,
b_n\right)^{\top}=a_1 b_1+a_2 b_2+\cdots+a_n b_n
\]</span>
内积运算将两个向量映射为实数，其计算方式非常容易理解，但我们无法看出其物理含义。接下来我们从几何角度
来分析，为了简单起见，我们假设 <span class="math inline">\(A\)</span> 和
<span class="math inline">\(B\)</span> 均为二维向量，则： <span class="math display">\[
A=\left(x_1, y_1\right), \quad B=\left(x_2, y_2\right) A \cdot B=|A||B|
\cos (\alpha)
\]</span> 其几何表示见下图：</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221530907.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们看出 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(B\)</span> 的内积等于 <span class="math inline">\(A\)</span> 到 <span class="math inline">\(B\)</span> 的投影长度乘以 <span class="math inline">\(B\)</span> 的模。</p>
<p>如果假设 <span class="math inline">\(\mathrm{B}\)</span> 的模为 1 ，
即让 <span class="math inline">\(|B|=1\)</span> ，那么就变成了: <span class="math display">\[
A \cdot B=|A| \cos (a)
\]</span> 也就是说, <strong>A 与 <span class="math inline">\(B\)</span>
的内积值等于 A 向 <span class="math inline">\(B\)</span>
所在直线投影的标量大小。</strong></p>
<h4><span id="12-基">1.2 基</span></h4>
<p>在我们常说的坐标系种, 向量 <span class="math inline">\((3,2)\)</span>
其实隐式引入了一个定义：以 <span class="math inline">\(x\)</span> 轴和
<span class="math inline">\(y\)</span> 轴上正方向长度为 1
的向量为标准。向 量 <span class="math inline">\((3,2)\)</span>
实际是说在 <span class="math inline">\(x\)</span> 轴投影为 3 而 <span class="math inline">\(y\)</span> 轴的投影为 2。注意投影是一个标量,
所以可以为负。</p>
<p>所以, 对于向量 <span class="math inline">\((3,2)\)</span> 来说,
如果我们想求它在 <span class="math inline">\((1,0),(0,1)\)</span>
这组基下的坐标的话, 分别内积即可。当然, 内积完 了还是 <span class="math inline">\((3,2)\)</span> 。</p>
<p>所以, 我们大致可以得到一个结论, 我们<strong>要准确描述向量,
首先要确定一组基, 然后给出在基所在的各个直线上的 投影值,
就可以了</strong>。为了方便求坐标, 我们希望这组基向量模长为
1。因为向量的内积运算, 当模长为 1 时, 内积
可以直接表示投影。然后还需要这组基是线性无关的, 我们一般用正交基,
非正交的基也是可以的, 不过正交基有 较好的性质。</p>
<h4><span id="13-基变换的矩阵表示">1.3 基变换的矩阵表示</span></h4>
<p>这里我们先做一个练习：对于向量 <span class="math inline">\((3,2)\)</span> 这个点来说, 在 <span class="math inline">\(\left(\frac{1}{\sqrt{2}},
\frac{1}{\sqrt{2}}\right)\)</span> 和 <span class="math inline">\(\left(-\frac{1}{\sqrt{2}},
\frac{1}{\sqrt{2}}\right)\)</span> 这组基下的坐标是多少? 我们拿 <span class="math inline">\((3,2)\)</span> 分别与之内积, 得到 <span class="math inline">\(\left(\frac{5}{\sqrt{2}},-\frac{1}{\sqrt{2}}\right)\)</span>
这个新坐标。</p>
<p>我们可以用矩阵相乘的形式简洁的表示这个变换： <span class="math display">\[
\left(\begin{array}{cc}
1 / \sqrt{2} &amp; 1 / \sqrt{2} \\
-1 / \sqrt{2} &amp; 1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{l}
3 \\
2
\end{array}\right)=\left(\begin{array}{c}
5 / \sqrt{2} \\
-1 / \sqrt{2}
\end{array}\right)
\]</span> 左边矩阵的两行分别为两个基, 乘以原向量,
其结果刚好为新基的坐标。推广一下, 如果我们有 <span class="math inline">\(m\)</span> 个二维向量,
只要将二维向量按列排成一个两行 <span class="math inline">\(m\)</span>
列矩阵, 然后用“基矩阵”乘以这个矩阵就可以得到了所有这些向量在新基下
的值。例如对于数据点 <span class="math inline">\((1,1),(2,2),(3,3)\)</span> 来说,
想变换到刚才那组基上, 则可以这样表示: <span class="math display">\[
\left(\begin{array}{cc}
1 / \sqrt{2} &amp; 1 / \sqrt{2} \\
-1 / \sqrt{2} &amp; 1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{lll}
1 &amp; 2 &amp; 3 \\
1 &amp; 2 &amp; 3
\end{array}\right)=\left(\begin{array}{ccc}
2 / \sqrt{2} &amp; 4 / \sqrt{2} &amp; 6 / \sqrt{2} \\
0 &amp; 0 &amp; 0
\end{array}\right)
\]</span> 我们可以把它写成通用的表示形式： <span class="math display">\[
\left(\begin{array}{c}
p_1 \\
p_2 \\
\vdots \\
p_R
\end{array}\right)\left(\begin{array}{llll}
a_1 &amp; a_2 &amp; \cdots &amp; a_M
\end{array}\right)=\left(\begin{array}{cccc}
p_1 a_1 &amp; p_1 a_2 &amp; \cdots &amp; p_1 a_M \\
p_2 a_1 &amp; p_2 a_2 &amp; \cdots &amp; p_2 a_M \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
p_R a_1 &amp; p_R a_2 &amp; \cdots &amp; p_R a_M
\end{array}\right)
\]</span> 其中 <span class="math inline">\(p_i\)</span> 是一个行向量,
表示第 <span class="math inline">\(\mathrm{i}\)</span> 个基, <span class="math inline">\(a_j\)</span> 是一个列向量, 表示第 <span class="math inline">\(\mathrm{j}\)</span>
个原始数据记录。实际上也就是做了一个向 量矩阵化的操作。</p>
<p><font color="red"> 上述分析给矩阵相乘找到了一种物理解释:
两个矩阵相乘的意义是将右边矩阵中的每一列向量 <span class="math inline">\(a_i\)</span> 变换到左边矩阵
中以每一行行向量为基所表示的空间中去。也就是说一个矩阵可以表示一种线性变换。</font></p>
<h3><span id="2-最大可分性">2. 最大可分性</span></h3>
<p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示,
<strong>如果基的数量少于向量本身的维数, 则可以达
到降维的效果。</strong></p>
<p><strong>但是我们还没回答一个最关键的问题:
如何选择基才是最优的。或者说, 如果我们有一组 <span class="math inline">\(\mathbf{N}\)</span> 维向量, 现在要将其 降到 K
维（K 小于 N），那么我们应该如何选择 <span class="math inline">\(\mathrm{K}\)</span>
个基才能最大程度保留原有的信息？</strong></p>
<p>一种直观的看法是： <font color="red"> 希望投影后的投影值尽可能分散,
因为如果重叠就会有样本消失。当然这个也可以从樀的角
度进行理解，樀越大所含信息越多。</font></p>
<h4><span id="21-方差">2.1 方差</span></h4>
<p>我们知道数值的分散程度,
可以用数学上的方差来表述。<strong>一个变量的方差可以看做是每个元素与变量均值的差的平
方和的均值</strong>, 即: <span class="math display">\[
\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^m\left(a_i-\mu\right)^2
\]</span> <strong>为了方便处理，我们将每个变量的均值都化为 0</strong> ,
因此方差可以直接用每个元素的平方和除以元素个数表示: <span class="math display">\[
\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^m a_i^2
\]</span>
于是上面的问题被形式化表述为：<strong>寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大</strong>。</p>
<h4><span id="22-协方差">2.2 协方差</span></h4>
<p>在一维空间中我们可以用方差来表示数据的分散程度。而对于高维数据，我们用协方差进行约束，<strong>协方差可以表示两个变量的相关性</strong>。<strong><font color="red">
为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性</font></strong>，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。</p>
<p>协方差公式为：</p>
<p><span class="math display">\[
\operatorname{Cov}(a, b)=\frac{1}{m-1}
\sum_{i=1}^m\left(a_i-\mu_a\right)\left(b_i-\mu_b\right)
\]</span></p>
<p>由于均值为 0，所以我们的协方差公式可以表示为：</p>
<p><span class="math display">\[
\operatorname{Cov}(a, b)=\frac{1}{m} \sum_{i=1}^m a_i b_i
\]</span> 当样本数较大时，不必在意其是 m 还是
m-1，为了方便计算，我们分母取 m。</p>
<p><strong><font color="red"> 协方差为 0
时，表示两个变量完全不相关</font></strong>。为了让协方差为
0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。</p>
<p>（<strong>补充</strong>：协方差为 0
时，两个变量只是线性不相关。完全独立是有问题的，才疏学浅，还望见谅。）</p>
<p><strong><font color="red"> 至此，我们得到了降维问题的优化目标：将一组
N 维向量降为 K 维，其目标是选择 K
个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为
0，而变量方差则尽可能大（在正交的约束下，取最大的 K
个方差）。</font></strong></p>
<h4><span id="23-协方差矩阵">2.3 协方差矩阵</span></h4>
<p>针对我们给出的优化目标，接下来我们将从数学的角度来给出优化目标。我们看到，最终要达到的目的与<strong>变量内方差及变量间协方差</strong>有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们有：</p>
<p>假设我们只有 <span class="math inline">\(\mathrm{a}\)</span> 和 <span class="math inline">\(\mathrm{b}\)</span> 两个变量,
那么我们将它们按行组成矩阵 <span class="math inline">\(\mathrm{X}\)</span> : <span class="math display">\[
X=\left(\begin{array}{cccc}
a_1 &amp; a_2 &amp; \cdots &amp; a_m \\
b_1 &amp; b_2 &amp; \cdots &amp; b_m
\end{array}\right)
\]</span> 然后: <span class="math display">\[
\frac{1}{m} X X^{\top}=\left(\begin{array}{cc}
\frac{1}{m} \sum_{i=1}^m a_i^2 &amp; \frac{1}{m} \sum_{i=1}^m a_i b_i \\
\frac{1}{m} \sum_{i=1}^m a_i b_i &amp; \frac{1}{m} \sum_{i=1}^m b_i^2
\end{array}\right)=\left(\begin{array}{cc}
\operatorname{Cov}(a, a) &amp; \operatorname{Cov}(a, b) \\
\operatorname{Cov}(b, a) &amp; \operatorname{Cov}(b, b)
\end{array}\right)
\]</span> 我们可以看到这个矩阵对角线上的分别是两个变量的方差,
而其它元素是 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span> 的协方差。两者被统一到了一个
矩阵里。</p>
<p>设我们有 <span class="math inline">\(\mathrm{m}\)</span> 个 <span class="math inline">\(\mathrm{n}\)</span> 维数据记录, 将其排列成矩阵
<span class="math inline">\(X_{n, m}\)</span>, 设 <span class="math inline">\(C=\frac{1}{m} X X^T\)</span>, 则 <span class="math inline">\(\mathrm{C}\)</span> 是一个对称矩阵, 其对角线分别
对应各个变量的方差, 而第 <span class="math inline">\(\mathrm{i}\)</span>
行 <span class="math inline">\(\mathrm{j}\)</span> 列和 <span class="math inline">\(\mathrm{j}\)</span> 行 <span class="math inline">\(\mathrm{i}\)</span> 列元素相同, 表示 <span class="math inline">\(\mathrm{i}\)</span> 和 <span class="math inline">\(\mathrm{j}\)</span> 两个变量的协方差。</p>
<h4><span id="24-矩阵对角化">2.4 矩阵对角化</span></h4>
<p>根据我们的优化条件，<strong>我们需要将除对角线外的其它元素化为
0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大）</strong>，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系。</p>
<p>设原始数据矩阵 <span class="math inline">\(X\)</span>
对应的协方差矩阵为 <span class="math inline">\(C\)</span>, 而 <span class="math inline">\(P\)</span> 是一组基按行组成的矩阵, 设 <span class="math inline">\(Y=P X\)</span>, 则 <span class="math inline">\(Y\)</span> 为 <span class="math inline">\(X\)</span> 对 <span class="math inline">\(P\)</span> 做基变换后的 数据。设 <span class="math inline">\(Y\)</span> 的协方差矩阵为 <span class="math inline">\(D\)</span>, 我们推导一下 <span class="math inline">\(D\)</span> 与 <span class="math inline">\(C\)</span> 的关系: <span class="math display">\[
\begin{aligned}
D &amp; =\frac{1}{m} Y Y^T \\
&amp; =\frac{1}{m}(P X)(P X)^T \\
&amp; =\frac{1}{m} P X X^T P^T \\
&amp; =P\left(\frac{1}{m} X X^T\right) P^T \\
&amp; =P C P^T
\end{aligned}
\]</span> 这样我们就看清楚了, 我们要找的 <span class="math inline">\(\mathrm{P}\)</span> 是能让原始协方差矩阵对角化的
<span class="math inline">\(\mathrm{P}\)</span> 。换句话说,
优化目标变成了<strong>寻找一个矩 阵 <span class="math inline">\(\mathbf{P}\)</span>, 满足 <span class="math inline">\(P C P^T\)</span>
是一个对角矩阵，并且对角元素按从大到小依次排列，那么 <span class="math inline">\(\mathbf{P}\)</span> 的前 <span class="math inline">\(\mathbf{K}\)</span> 行就是要寻找的基, 用 <span class="math inline">\(\mathbf{P}\)</span> 的前 <span class="math inline">\(\mathrm{K}\)</span> 行组成的矩阵乘以 <span class="math inline">\(\mathrm{X}\)</span> 就使得 <span class="math inline">\(\mathrm{X}\)</span> 从 <span class="math inline">\(\mathrm{N}\)</span> 维降到了 <span class="math inline">\(\mathrm{K}\)</span>
维并满足上述优化条件。</strong></p>
<p>至此, 我们离 PCA 还有仅一步之遥, 我们还需要完成对角化。</p>
<p><strong>由上文知道，协方差矩阵 C
是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质:</strong></p>
<ol type="1">
<li>实对称矩阵不同特征值对应的特征向量必然正交。</li>
<li>设特征向量 <span class="math inline">\(\lambda\)</span> 重数为 <span class="math inline">\(r\)</span>, 则必然存在 <span class="math inline">\(r\)</span> 个线性无关的特征向量对应于 <span class="math inline">\(\lambda\)</span> ，因此可以将这 <span class="math inline">\(r\)</span> 个特征向量单位正 交化。</li>
</ol>
<p>由上面两条可知, 一个 <span class="math inline">\(\mathrm{n}\)</span>
行 <span class="math inline">\(\mathrm{n}\)</span>
列的实对称矩阵一定可以找到 <span class="math inline">\(\mathrm{n}\)</span> 个单位正交特征向量, 设这 <span class="math inline">\(\mathrm{n}\)</span> 个特征向量为 <span class="math inline">\(e_1, e_2, \cdots, e_n\)</span>,
我们将其按列组成矩阵: <span class="math inline">\(E=\left(e_1, e_2,
\cdots, e_n\right)\)</span> 。</p>
<p>则对协方差矩阵 C 有如下结论: <span class="math display">\[
E^T C E=\Lambda=\left(\begin{array}{llll}
\lambda_1 &amp; &amp; &amp; \\
&amp; \lambda_2 &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \lambda_n
\end{array}\right)
\]</span> 其中 <span class="math inline">\(\Lambda\)</span> 为对角矩阵,
其对角元素为各特征向量对应的特征值（可能有重复）。到这里,
我们发现我们已经找到了 需要的矩阵 P: <span class="math inline">\(P=E^{\top}\)</span> 。</p>
<p><strong><span class="math inline">\(P\)</span>
是协方差矩阵的特征向量单位化后按行排列出的矩阵</strong>，其中每一行都是
<span class="math inline">\(C\)</span> 的一个特征向量。如果设 <span class="math inline">\(P\)</span> 按照 <span class="math inline">\(\Lambda\)</span> 中 特征值的从大到小,
将特征向量从上到下排列, 则用 <span class="math inline">\(P\)</span> 的前
<span class="math inline">\(K\)</span> 行组成的矩阵乘以原始数据矩阵
<span class="math inline">\(X\)</span>, 就得到了我们
需要的降维后的数据矩阵 <span class="math inline">\(Y\)</span> 。</p>
<blockquote>
<p><strong>拉格朗日乘子法证明</strong>:<strong>方差就是协方差矩阵的特征值</strong></p>
</blockquote>
<h4><span id="25-最近重构性-思路">2.5 最近重构性-思路</span></h4>
<p>以上的证明思路主要是基于最大可分性的思想，<strong>通过一条直线使得样本点投影到该直线上的方差最大</strong>。除此之外，我们还可以<strong>将其转换为线型回归问题，其目标是求解一个线性函数使得对应直线能够更好地拟合样本点集合</strong>。这就<strong>使得我们的优化目标从方差最大转化为平方误差最小</strong>，因为映射距离越短，丢失的信息也会越小。区别于最大可分性，这是从最近重构性的角度进行论证。</p>
<h3><span id="3-求解步骤">3. 求解步骤</span></h3>
<p><strong>总结一下 PCA 的算法步骤：设有 <span class="math inline">\(m\)</span> 条 <span class="math inline">\(n\)</span> 维数据。</strong></p>
<ol start="3" type="1">
<li>将原始数据按列组成 <span class="math inline">\(\mathbf{n}\)</span>
行 <span class="math inline">\(\mathbf{m}\)</span> 列矩阵 <span class="math inline">\(\mathbf{X}\)</span>;</li>
<li>将 <span class="math inline">\(\mathrm{X}\)</span>
的每一行进行零均值化，即减去这一行的均值；【零均值化】【方差、协方差好计算】</li>
<li>求出协方差矩阵 <span class="math inline">\(C=\frac{1}{m} X
X^{\top}\)</span>;</li>
<li>求出协方差矩阵的特征值及对应的特征向量;</li>
<li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 <span class="math inline">\(\mathbf{k}\)</span> 行组成矩阵 <span class="math inline">\(\mathbf{P}\)</span>;</li>
<li><span class="math inline">\(Y=P X\)</span> 即为降维到 <span class="math inline">\(\mathbf{k}\)</span> 维后的数据。</li>
</ol>
<h3><span id="4性质维度灾难-降噪-过拟合-特征独立">4.
性质【维度灾难、降噪、过拟合、特征独立】</span></h3>
<ol type="1">
<li><strong>缓解维度灾难</strong>：PCA
算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；</li>
<li><strong>降噪</strong>：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；</li>
<li><strong>过拟合</strong>：PCA
保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以
PCA 也可能加剧了过拟合；</li>
<li><strong>特征独立</strong>：PCA
不仅将数据压缩到低维，它也使得<strong>降维之后的数据各特征相互独立</strong>；</li>
</ol>
<h3><span id="5-细节">5. 细节</span></h3>
<h4><span id="51-零均值化">5.1 零均值化</span></h4>
<p>当对训练集进行 PCA
降维时，也需要对验证集、测试集执行同样的降维。而<strong>对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来</strong>，不能使用验证集或者测试集的中心向量。</p>
<p>其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。</p>
<p>另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现
Variance Shift 的问题。</p>
<h4><span id="52-svd-的对比">5.2 SVD 的对比</span></h4>
<p>这是两个不同的数学定义。我们先给结论：<strong>特征值和特征向量是针对方阵</strong>才有的，而<strong>对任意形状的矩阵都可以做奇异值分解</strong>。</p>
<p><strong>PCA</strong>：<strong>方阵的特征值分解</strong>，对于一个方阵
A。其中，Q 是这个矩阵 A 的特征向量组成的矩阵， <span class="math inline">\(\Lambda\)</span>
是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列)。也就是说矩阵
A 的信息可以由其特征值和特征向量表示。</p>
<p><strong>SVD：矩阵的奇异值分解其实就是对于矩阵 <span class="math inline">\(\mathrm{A}\)</span> 的协方差矩阵 <span class="math inline">\(A^T A\)</span> 和 <span class="math inline">\(A
A^T\)</span> 做特征值分解推导出来的</strong>: <span class="math display">\[
A_{m, n}=U_{m, m} \Lambda_{m, n} V_{n, n}^T \approx U_{m, k} \Lambda_{k,
k} V_{k, n}^T
\]</span> 其中: <span class="math inline">\(U，V\)</span> 都是正交矩阵,
有 <span class="math inline">\(U^T U=I_m, V^T V=I_n\)</span>
。这里的约等于是因为 <span class="math inline">\(\Lambda\)</span> 中有
<span class="math inline">\(\mathrm{n}\)</span> 个奇异值, 但是由于排在
后面的很多接近 0, 所以我们可以仅保留比较大的 <span class="math inline">\(\mathrm{k}\)</span> 个奇异值。 <span class="math display">\[
\begin{aligned}
&amp; A^T A=\left(U \Lambda V^T\right)^T U \Lambda V^T=V \Lambda^T U^T U
\Lambda V^T=V \Lambda^2 V^T \\
&amp; A A^T=U \Lambda V^T\left(U \Lambda V^T\right)^T=U \Lambda V^T V
\Lambda^T U^T=U \Lambda^2 U^T
\end{aligned}
\]</span> 所以, <span class="math inline">\(V \cup\)</span>
两个矩阵分别是 <span class="math inline">\(A^T A\)</span> 和 <span class="math inline">\(A A^T\)</span> 的特征向量,
中间的矩阵对角线的元素是 <span class="math inline">\(A^T A\)</span> 和
<span class="math inline">\(A A^T\)</span> 的特征值。我 们也很容易看出
<span class="math inline">\(\mathrm{A}\)</span> 的奇异值和 <span class="math inline">\(A^T A\)</span> 的特征值之间的关系。</p>
<p>PCA 需要对协方差矩阵 <span class="math inline">\(C=\frac{1}{m} X
X^T\)</span> 。进行特征值分解; SVD 也是对 <span class="math inline">\(A^T A\)</span> 进行特征值分解。如果取 <span class="math inline">\(A=\frac{X^T}{\sqrt{m}}\)</span>
则两者基本等价。所以 PCA 问题可以转换成 SVD 求解。</p>
<p><strong>而实际上 Sklearn 的 PCA 就是用 SVD
进行求解的</strong>，原因有以下几点：</p>
<ol type="1">
<li>当样本维度很高时，协方差矩阵计算太慢；</li>
<li>方阵特征值分解计算效率不高；</li>
<li><strong>SVD
除了特征值分解这种求解方式外，还有更高效更准确的迭代求解方式，避免了<span class="math inline">\(A^T A\)</span>的计算；</strong></li>
<li><strong>其实 PCA 与 SVD 的右奇异向量的压缩效果相同</strong>。</li>
</ol>
<h3><span id="参考链接">参考链接</span></h3>
<ol type="1">
<li>《机器学习》周志华</li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.codinglabs.org/articles/pca-tutorial.html">PCA
的数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm">Singular
Value Decomposition (SVD) tutorial</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">机器学习中的数学（4）——线性判别分析（LDA）,
主成分分析（PCA）</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//my.oschina.net/findbill/blog/535044">从SVD到PCA——奇妙的数学游戏</a></li>
<li>scikit-learn：降维算法PCA和SVD
https://blog.csdn.net/HHG20171226/article/details/102981822</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2Y0FFGH/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2Y0FFGH/" class="post-title-link" itemprop="url">降维（2）LDA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:37:33" itemprop="dateModified" datetime="2023-04-22T16:37:33+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-线性判别分析lda监督">一、线性判别分析（LDA）【监督】</span></h2>
<blockquote>
<p><strong>“投影后类内方差最小，类间方差最大”</strong></p>
<ul>
<li>https://blog.csdn.net/liuweiyuxiang/article/details/78874106</li>
</ul>
</blockquote>
<h3><span id="11-概念">1.1 概念</span></h3>
<p><strong>线性判别分析（Linear Discriminant
Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。</strong></p>
<p><strong>LDA分类思想简单总结如下：</strong></p>
<ol type="1">
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。</li>
<li>对于训练数据，设法将多维数据投影到一条直线上，<strong>同类数据的投影点尽可能接近，异类数据点尽可能远离</strong>。</li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</li>
</ol>
<p><strong><font color="red">
如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</font></strong></p>
<p>假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221634387.png" alt="image-20220526135646769"></p>
<p>从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。
以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3><span id="12-原理">1.2 原理</span></h3>
<p>LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(<a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Linear_classifier">Linear
Classifier</a>)：因为LDA是一种线性分类器。对于<strong>K-分类的一个分类问题，会有K个线性函数</strong>：</p>
<p><span class="math display">\[
y_k(x)=w_k^T x+w_{k 0}
\]</span></p>
<p>当满足条件：对于所有的j，都有Yk &gt;
Yj,的时候，我们就说x属于类别k。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的就是所属的分类了。</p>
<p>上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221635862.gif" alt="clip_image002" style="zoom:67%;"></p>
<p>红色的方形的点为0类的原始点、蓝色的方形点为1类的原始点，经过原点的那条线就是投影的直线，从图上可以清楚的看到，红色的点和蓝色的点被<strong>原点</strong>明显的分开了，这个数据只是随便画的，如果在高维的情况下，看起来会更好一点。下面我来推导一下二分类LDA问题的公式：假设用来区分二分类的直线（投影函数)为：
<span class="math display">\[
y_k(x)=w_k^T x+w_{k 0}
\]</span> 当满足条件：对于所有的j, 都有 <span class="math inline">\(Y
k&gt;Y j\)</span>,的时候, 我们就说 <span class="math inline">\(x\)</span> 属于类别 <span class="math inline">\(k\)</span> 。对于每一个分类, 都有一个公式去算一个
分值，在所有的公式得到的分值中, 找一个最大的就是所属的分类了。</p>
<p>上式实际上就是一种投影, 是将一个高维的点投影到一条高维的直线上,
LDA最求的目标是, 给出一个标注了类别 的数据集, 投影到了一条直线之后,
能够使得点尽量的按类别区分开, 当 <span class="math inline">\(k=2\)</span> 即二分类问题的时候, 如下图所示:</p>
<p>红色的方形的点为 0 类的原始点、蓝色的方形点为 1
类的原始点，经过原点的那条线就是投影的直线，从图上可以 清楚的看到,
红色的点和蓝色的点被原点明显的分开了, 这个数据只是随便画的,
如果在高维的情况下, 看起来会
更好一点。下面我来推导一下二分类LDA问题的公式：假设用来区分二分类的直线（投影函数)为：
<span class="math display">\[
y=w^T x
\]</span> <strong>LDA分类的一个目标是使得不同类别之间的距离越远越好,
同一类别之中的距离越近越好</strong>, 所以我们需要定义几 个关键的值。</p>
<ul>
<li><strong>类别的原始中心点为</strong>：(Di表示属于类别的点)</li>
</ul>
<p><span class="math display">\[
m_i=\frac{1}{n_i} \sum_{x \in D_i} x
\]</span></p>
<ul>
<li>类别投影后的中心点为:</li>
</ul>
<p><span class="math display">\[
\widetilde{m_i}=w^T m_i
\]</span></p>
<ul>
<li><strong>衡量类别i投影后, 类别点之间的分散程度 (方差)
为</strong>:</li>
</ul>
<p><span class="math display">\[
\tilde{s_i}=\sum_{y \in Y_i}\left(y-\tilde{m_i}\right)^2
\]</span></p>
<ul>
<li><strong>最终我们可以得到一个下面的公式,
表示LDA投影到w后的损失函数</strong>:</li>
</ul>
<p><span class="math display">\[
J(w)=\frac{\left|\widetilde{m_1}-\widetilde{m_2}\right|^2}{\widetilde{s}_1^2+\widetilde{s}_2^2}
\]</span></p>
<p>我们<strong>分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。</strong>分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。</p>
<p>我们定义一个<strong>投影前的各类别分散程度的矩阵</strong>，这个矩阵看起来有一点麻烦，其实意思是，如果某一个分类的<strong>输入点集Di里面的点距离这个分类的中心店mi越近</strong>，则Si里面元素的值就越小，如果分类的点都紧紧地围绕着mi，则Si里面的元素值越更接近0.
<span class="math display">\[
S_{i}=\sum_{x \in D_{i}}\left(x-m_{i}\right)\left(x-m_{i}\right)^{T}
\]</span> 带入 <span class="math inline">\(\mathrm{Si}\)</span>, 将
<span class="math inline">\(\mathrm{J}(\mathrm{w})\)</span>
分母化为:</p>
<p><span class="math inline">\(\tilde{s}_{i}=\sum_{x \in
D_{i}}\left(w^{T} x-w^{T} m_{i}\right)^{2}=\sum_{x \in D_{i}}
w^{T}\left(x-m_{i}\right)\left(x-m_{i}\right)^{T} w=w^{T} S_{i}
w\)</span></p>
<p><span class="math display">\[{\tilde{S_{1}}}^{2}+{\tilde{S_{2}}}^{2}=w^{T}\left(S_{1}+S_{2}\right)
w=w^{T} S_{w} w\]</span></p>
<p>同样的将 <span class="math inline">\(\mathrm{J}(\mathrm{w})\)</span>
分子化为: <span class="math display">\[
\left|\widetilde{m_{1}}-\widetilde{m_{2}}\right|^{2}=w^{T}\left(m_{1}-m_{2}\right)\left(m_{1}-m_{2}\right)^{T}
w=w^{T} S_{B} w
\]</span> 这样<strong>损失函数</strong>可以化成下面的形式: <span class="math display">\[
J(w)=\frac{w^{T} S_{B} w}{w^{T} S_{w} w}
\]</span> 这样就可以用最喜欢的<strong>拉格朗日乘子法</strong>了,
但是还有一个问题, 如果分子、分母是都可以取任意值的, 那就会 使得有无穷解,
我们将分母限制为长度为 1, 并作为拉格朗日乘子法的限制条件, 带入得到:
<span class="math display">\[
\begin{aligned}
&amp;c(w)=w^{T} S_{B} w-\lambda\left(w^{T} S_{w} w-1\right) \\
&amp;\Rightarrow \frac{d c}{d w}=2 S_{B} w-2 \lambda S_{w} w=0 \\
&amp;\Rightarrow S_{B} w=\lambda S_{w} w
\end{aligned}
\]</span> <strong>这样的式子就是一个求特征值的问题了。</strong> 对于
<span class="math inline">\(N(N&gt;2)\)</span> 分类的问题,
我就直接写出下面的结论了: <span class="math display">\[
\begin{aligned}
&amp;S_{W}=\sum_{i=1}^{c} S_{i} \\
&amp;S_{B}=\sum_{i=1}^{c}
n_{i}\left(m_{i}-m\right)\left(m_{i}-m\right)^{T} \\
&amp;S_{B} w_{i}=\lambda S_{w} w_{i}
\end{aligned}
\]</span>
这同样是一个求特征值的问题，我们求出的第i大的特征向量，就是对应的Wi了。</p>
<blockquote>
<p>这里想多谈谈特征值，特征值在纯数学、量子力学、固体力学、计算机等等领域都有广泛的应用，特征值表示的是矩阵的性质，当我们取到矩阵的前N个最大的特征值的时候，我们可以说提取到的矩阵主要的成分（这个和之后的PCA相关，但是不是完全一样的概念）。在机器学习领域，不少的地方都要用到特征值的计算，比如说图像识别、pagerank、LDA、还有之后将会提到的PCA等等。</p>
</blockquote>
<p><strong>优缺点</strong></p>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 90%">
</colgroup>
<thead>
<tr class="header">
<th>优缺点</th>
<th>简要说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>优点</td>
<td>1. 可以使用类别的先验知识； 2.
以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr class="even">
<td>缺点</td>
<td>1. LDA不适合对非高斯分布样本进行降维； 2.
<strong>LDA降维最多降到分类数k-1维</strong>； 3.
LDA在样本分类信息依赖方差而不是均值时，降维效果不好； 4.
LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/24K7V34/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/24K7V34/" class="post-title-link" itemprop="url">降维（3）t-SNE</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:29:43" itemprop="dateModified" datetime="2023-04-22T16:29:43+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-t-sne-高维数据可视化">一、t-SNE 高维数据可视化</span></h3>
<blockquote>
<p>高维数据可视化之t-SNE算法🌈:https://zhuanlan.zhihu.com/p/57937096</p>
</blockquote>
<p><strong>T-SNE算法是用于可视化的算法中效果最好的算法之一</strong>，相信大家也对T-SNE算法略有耳闻，本文参考T-SNE作者<strong>Laurens
van der
Maaten</strong>给出的源代码自己实现T-SNE算法代码，以此来加深对T-SNE的理解。先简单介绍一下T-SNE算法，T-SNE将数据点变换映射到概率分布上。</p>
<h4><span id="11-t-sne数据算法的目的">1.1 t-SNE数据算法的目的</span></h4>
<p><strong>主要是将数据从高维数据转到低维数据，并在低维空间里也保持其在高维空间里所携带的信息（比如高维空间里有的清晰的分布特征，转到低维度时也依然存在）。</strong></p>
<p><strong><font color="red">
t-SNE将欧氏距离距离转换为条件概率，来表达点与点之间的相似度，再优化两个分布之间的距离-KL散度，从而保证点与点之间的分布概率不变。</font></strong></p>
<h4><span id="12-sne原理">1.2 SNE原理</span></h4>
<p><span class="math inline">\(S N E\)</span>
是<strong>通过仿射变换将数据点映射到相应概率分布上</strong>,
主要包括下面两个步骤: 1. 通过在高维空间中构建数据点之间的概率分布 <span class="math inline">\(P\)</span>, 使得相似的数据点有更高的概率被选择, 而
不相似的数据点有较低的概率被选择; 2.
然后在低维空间里重构这些点的概率分布 <span class="math inline">\(Q\)</span>, 使得这两个概率分布尽可能相似。</p>
<p>令输入空间是 <span class="math inline">\(X \in
\mathbb{R}^{n}\)</span>, 输出空间为 <span class="math inline">\(Y \in
\mathbb{R}^{t}(t \ll n)\)</span> 。不妨假设含有 <span class="math inline">\(m\)</span> 个样本数据 <span class="math inline">\(\left\{x^{(1)}, x^{(2)}, \cdots,
x^{(m)}\right\}\)</span>, 其中 <span class="math inline">\(x^{(i)} \in
X\)</span>, 降维后的数据为 <span class="math inline">\(\left\{y^{(1)},
y^{(2)}, \cdots, y^{(m)}\right\}, y^{(i)} \in Y\)</span> 。 <span class="math inline">\(S N E\)</span>
是<strong>先将欧几里得距离转化为条件概率来表达点与点之间的相似度</strong>,
即首先是计算条件概 率 <span class="math inline">\(p_{j \mid i}\)</span>,
其正比于 <span class="math inline">\(x^{(i)}\)</span> 和 <span class="math inline">\(x^{(j)}\)</span> 之间的相似度, <span class="math inline">\(p_{j \mid i}\)</span> 的计算公式为: <span class="math display">\[
p_{j \mid i}=\frac{\exp
\left(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^{2}}{2
\sigma_{i}^{2}}\right)}{\sum_{k \neq i} \exp
\left(-\frac{\left\|x^{(i)}-x^{(k)}\right\|^{2}}{2
\sigma_{i}^{2}}\right)}
\]</span> 在这里引入了一个参数 <span class="math inline">\(\sigma_{i}\)</span>, 对于不同的数据点 <span class="math inline">\(x^{(i)}\)</span> 取值亦不相同,
因为我们关注的是不同数据 点两两之间的相似度, 故可设置 <span class="math inline">\(p_{i \mid i}=0\)</span> 。对于低维度下的数据点
<span class="math inline">\(y^{(i)}\)</span>, 通过条件概率 <span class="math inline">\(q_{j \mid i}\)</span> 来 刻画 <span class="math inline">\(y^{(i)}\)</span> 与 <span class="math inline">\(y^{(j)}\)</span> 之间的相似度, <span class="math inline">\(q_{j \mid i}\)</span> 的计算公式为: <span class="math display">\[
q_{j \mid i}=\frac{\exp
\left(-\left\|y^{(i)}-y^{(j)}\right\|^{2}\right)}{\sum_{k \neq i} \exp
\left(-\left\|y^{(i)}-y^{(k)}\right\|^{2}\right)}
\]</span> 同理, 设置 <span class="math inline">\(q_{i \mid i}=0\)</span>
。 如果降维的效果比较好, 局部特征保留完整, 那么有 <span class="math inline">\(p_{i \mid j}=q_{i \mid j}\)</span> 成立,
因此通过优化两个分布之 间的 <strong><span class="math inline">\(K
L\)</span> 散度构造出的损失函数为</strong>: <span class="math display">\[
C\left(y^{(i)}\right)=\sum_{i} K L\left(P_{i} \| Q_{i}\right)=\sum_{i}
\sum_{j} p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}}
\]</span> 这里的 <span class="math inline">\(P_{i}\)</span>
表示在给定高维数据点 <span class="math inline">\(x^{(i)}\)</span> 时,
其他所有数据点的条件概率分布; <span class="math inline">\(Q_{i}\)</span>
则表示在给定 低维数据点 <span class="math inline">\(y^{(i)}\)</span> 时,
其他所有数据点的条件概率分布。从损失函数可以看出, 当 <span class="math inline">\(p_{j \mid i}\)</span> 较大 <span class="math inline">\(q_{j \mid i}\)</span> 较小时, 惩罚较高; 而 <span class="math inline">\(p_{j \mid i}\)</span> 较小 <span class="math inline">\(q_{j \mid i}\)</span> 较大时,
惩罚较低。换句话说就是高维空间中两个数据点距 离较近时,
若映射到低维空间后距离较远, 那么将得到一个很高的惩罚; 反之,
高维空间中两个数 据点距离较远时, 若映射到低维空间距离较近,
将得到一个很低的惩罚值。也就是说, <strong><span class="math inline">\(S
N E\)</span> 的 损失函数更关注于局部特征,
而忽视了全局结构</strong>。</p>
<h4><span id="13-目标函数求解">1.3 目标函数求解</span></h4>
<h4><span id="14-对称性-sne">1.4 对称性-SNE</span></h4>
<p><strong>优化 <span class="math inline">\(K L(P \| Q)\)</span>
的一种替换思路是使用联合概率分布来替换条件概率分布</strong>, 即 <span class="math inline">\(P\)</span> 是高维空间里数据点的联合概 率分布,
<span class="math inline">\(Q\)</span>
是低维空间里数据点的联合概率分布，此时的损失函数为: <span class="math display">\[
C\left(y^{(i)}\right)=K L(P \| Q)=\sum_i \sum_j p_{i j} \log \frac{p_{i
j}}{q_{i j}}
\]</span> 同样的 <span class="math inline">\(p_{i i}=q_{i i}=0\)</span>
，这种改进下的 <span class="math inline">\(S N E\)</span> 称为对称 <span class="math inline">\(S N E\)</span> ，因为它的先验假设为对 <span class="math inline">\(\forall i\)</span> 有 <span class="math inline">\(p_{i j}=p_{j i}, q_{i j}=q_{j i}\)</span>
成立，故概率分布可以改写成: <span class="math display">\[
p_{i j}=\frac{\exp \left(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^2}{2
\sigma^2}\right)}{\sum_{k \neq l} \exp
\left(-\frac{\left\|x^{(k)}-x^{(l)}\right\|^2}{2 \sigma^2}\right)} \quad
q_{i j}=\frac{\exp
\left(-\left\|y^{(i)}-y^{(j)}\right\|^2\right)}{\sum_{k \neq l} \exp
\left(-\left\|y^{(k)}-y^{(l)}\right\|^2\right)}
\]</span> 这种改进方法使得表达式简洁很多, 但是容易受到异常点数据的影响,
为了解决这个问题通过对联合概率分布定义修正为： <span class="math inline">\(p_{i j}=\frac{p_{j \mid i}+p_{i \mid
j}}{2}\)</span>, 这保证了 <span class="math inline">\(\sum_j p_{i
j}&gt;\frac{1}{2 m}\)</span> ，使得每个点对于损失函数都会有贡献。对称
<span class="math inline">\(S N E\)</span> 最大的 优点是简化了梯度计算,
梯度公式改写为: <span class="math display">\[
\frac{\partial C\left(y^{(i)}\right)}{\partial y^{(i)}}=4
\sum_j\left(p_{i j}-q_{i j}\right)\left(y^{(i)}-y^{(j)}\right)
\]</span> 研究表明, 对称 <span class="math inline">\(S N E\)</span> 和
<span class="math inline">\(S N E\)</span> 的效果差不多,
有时甚至更好一点。</p>
<h4><span id="15-t-sne">1.5 t-SNE</span></h4>
<p><span class="math inline">\(t-S N E\)</span> 在对称 <span class="math inline">\(S N E\)</span> 的改进是,
首先通过在高维空间中使用高斯分布将距离转换为概率分布，然后<strong>在低维空间
中，使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度空间中的中低等距离在映射后能够有一个
较大的距离</strong>。</p>
<p><img src="https://pic4.zhimg.com/80/v2-928a3ada308128f26b719d510a728fbb_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>从图中可以看到，在没有异常点时, <span class="math inline">\(t\)</span>
分布与高斯分布的拟合结果基本一致。而在第二张图中, 出现了部分异常点,
由于高斯分布的尾部较低, 对异常点比较敏感, 为了照顾这些异常点,
高斯分布的拟合结果偏离了大多数样本所在位置,
方差也较大。<strong>相比之下, <span class="math inline">\(t\)</span>
分布的尾部较高, 对异常点不敏感, 保证了其鲁棒性, 因此拟合结果更为合理,
较好的捕获了数据的全局特征。</strong></p>
<p>使用 <span class="math inline">\(t\)</span> 分布替换高斯分布之后
<span class="math inline">\(q_{i j}\)</span> 的变化如下: <span class="math display">\[
q_{i
j}=\frac{\left(1+\left\|y^{(i)}-y^{(j)}\right\|^2\right)^{-1}}{\sum_{k
\neq l}\left(1+\left\|y^{(i)}-y^{(j)}\right\|^2\right)^{-1}}
\]</span> 此外，随着自由度的逐渐增大, <span class="math inline">\(t\)</span>
分布的密度函数逐渐接近标准正态分布，因此在计算梯度方面会简单很多, 优
化后的梯度公式如下: <span class="math display">\[
\frac{\partial C\left(y^{(i)}\right)}{\partial y^{(i)}}=4
\sum_j\left(p_{i j}-q_{i
j}\right)\left(y^{(i)}-y^{(j)}\right)\left(1+\left\|y^{(i)}-y^{(j)}\right\|^2\right)^{-1}
\]</span> 总的来说, <span class="math inline">\(t-S N E\)</span>
的梯度更新具有以下两个优势： -
<strong>对于低维空间中不相似的数据点，用一个较小的距离会产生较大的梯度让这些数据点排斥开来;</strong>
-
<strong>这种排斥又不会无限大，因此避免了不相似的数据点距离太远。</strong></p>
<h5><span id="t-s-n-e-算法其实就是在-s-n-e-算法的基础上增加了两个改进"><span class="math inline">\(t-S N E\)</span> 算法其实就是在 <span class="math inline">\(S N E\)</span> 算法的基础上增加了两个改进:</span></h5>
<ul>
<li>把 <span class="math inline">\(S N E\)</span> 修正为对称 <span class="math inline">\(S N E\)</span> ，提高了计算效率,
效果稍有提升;</li>
<li>在低维空间中采用了 <span class="math inline">\(t\)</span>
分布替换原来的高斯分布，解决了高维空间映射到低维空间所产生的拥挤问题,
优化 了 <span class="math inline">\(S N E\)</span>
过于关注局部特征而忽略全局特征的问题。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/F3RYP7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/F3RYP7/" class="post-title-link" itemprop="url">降维（4）AutoEncoder</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:30:11" itemprop="dateModified" datetime="2023-04-22T16:30:11+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>430</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="autoencoder">AutoEncoder</span></h3>
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80377698">【全】一文带你了解自编码器（<em>AutoEncoder</em>）</a></li>
</ul>
</blockquote>
<p>理解为：（下图）高维数据（左测蓝色）通过某种网络变成低位数据（中间红色）后，又经过某种网络变回高维数据（右侧蓝色）。数据经过该模型前后没有变化，而中间的低维数据完全具有输入输出的高维数据的全部信息，所以可以用低维数据代表高维数据。</p>
<p>之所以叫AutoEncoder，而不叫AutoEncoderDecoder，是因为训练好之后只有encoder部分有用，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=decoder&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22157482881%22%7D">decoder</a>部分就不用了。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221552166.jpg" alt="img" style="zoom: 67%;"></p>
<p>进入深度学习的思路之后，编码的网络是开放的，可以自由设计的。一个思路是端到端，将网络的输出设为你任务要的结果（如类别、序列等），<strong>过程中的某层嵌入都可以作为降维的低维结果</strong>。当然，这种低维结果其实是模型的副产品，因为任务已经解决。比如bert模型得到（中文的）字嵌入。</p>
<h4><span id="优点">优点：</span></h4>
<ul>
<li>能够学习到非线性特性</li>
<li>降低数据维度</li>
</ul>
<h4><span id="缺点">缺点：</span></h4>
<ul>
<li>训练的<strong>计算成本高</strong></li>
<li><strong>可解释性较差</strong></li>
<li>背后的数学知识复杂</li>
<li>容易产生<strong>过度拟合</strong>的问题，尽管可以通过引入正则化策略缓解</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1CZDQSE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1CZDQSE/" class="post-title-link" itemprop="url">理论基础（3）参数估计</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:58:04" itemprop="dateCreated datePublished" datetime="2022-03-15T22:58:04+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 11:34:01" itemprop="dateModified" datetime="2023-04-26T11:34:01+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="机器学习优化方法optimization">机器学习优化方法
(Optimization)</span></h3>
<blockquote>
<p>机器学习与优化基础（Machine Learning and
Optimization）:https://zhuanlan.zhihu.com/p/169835477</p>
<p>最优化方法复习笔记：https://github.com/LSTM-Kirigaya/OptimizeNote</p>
<p><strong>FreeWill</strong>：<a target="_blank" rel="noopener" href="https://plushunter.github.io/2017/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8825%EF%BC%89%EF%BC%9A%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/">机器学习算法系列（25）：最速下降法、牛顿法、拟牛顿法</a></p>
</blockquote>
<h3><span id="一-什么是凸优化">一、什么是凸优化</span></h3>
<p><strong>凸函数</strong>的严格定义为, 函数 <span class="math inline">\(L(\cdot)\)</span>
是凸函数当且仅当对定义域中的任意两点 <span class="math inline">\(x,
y\)</span> 和任意实数 <span class="math inline">\(\lambda
\in[0,1]\)</span> 总有: <span class="math display">\[
L(\lambda x+(1-\lambda) y) \leq \lambda L(x)+(1-\lambda) L(y)
\]</span> 该不等式的一个直观解释是, 凸函数曲面上任意两点连接而成的线段,
其上的任 意一点都不会处于该函数曲面的 下方，如下图所示所示。</p>
<p>该不等式的一个直观解释是，凸函数曲面上任意两点连接而成的线段，其上的任
意一点都不会处于该函数曲面的下方，如下图所示所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304252130605.jpeg" alt="img" style="zoom:67%;"></p>
<p>凸优化问题的例子包括支持向量机、线性回归等
线性模型，非凸优化问题的例子包括低秩模型（如矩阵分解）、深度神经网络模型等。</p>
<h3><span id="二-正则化项">二、正则化项</span></h3>
<p>使用正则化项，也就是给loss
function加上一个参数项，正则化项有<strong>L1正则化、L2正则化、ElasticNet</strong>。加入这个正则化项好处：</p>
<ul>
<li>控制参数幅度，不让模型“无法无天”。</li>
<li>限制参数搜索空间</li>
<li>解决欠拟合与过拟合的问题。</li>
</ul>
<h3><span id="三-常见的几种最优化方法">三、常见的几种最优化方法</span></h3>
<p><font color="red">下面以线性模型损失函数为例：</font></p>
<h4><span id="31-梯度下降法"><strong>3.1 梯度下降法</strong></span></h4>
<p>梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当<strong>目标函数是凸函数时，梯度下降法的解是全局解</strong>。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。
<span class="math display">\[
\begin{gathered}\frac{\partial}{\partial \theta_j}
J(\theta)=\frac{\partial}{\partial \theta_j}
\frac{1}{2}\left(h_\theta(x)-y\right)^2 \\ =2 \cdot
\frac{1}{2}\left(h_\theta(x)-y\right) \frac{\partial}{\partial
\theta_j}\left(h_\theta(x)-y\right) \\ =\left(h_\theta(x)-y\right)
x_j\end{gathered}
\]</span>
梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的搜索迭代示意图如下图所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304252127619.png" alt="img" style="zoom: 67%;"></p>
<ul>
<li><strong>批量梯度下降法（BGD）</strong></li>
</ul>
<p><strong>参数θ的值每更新一次都要遍历样本集中的所有的样本</strong>，得到新的θj，看是否满足阈值要求，若满足，则迭代结束，根据此值就可以得到；否则继续迭代。【易受极小值影响】</p>
<ul>
<li><h5><span id="随机梯度下降算法sgd单样本增量梯度下降">随机梯度下降算法（SGD）【单样本增量梯度下降】</span></h5></li>
</ul>
<p>每次更新只用到一个训练样本，若根据当前严格不能进行迭代得到一个，此时会得到一个，有新样本进来之后，在此基础上继续迭代，又得到一组新的和，以此类推。</p>
<p>缺点：靠近极小值时收敛速度减慢；直线搜索时可能会产生一些问题；可能会“之字形”地下降。</p>
<h4><span id="32-牛顿法">3.2 牛顿法</span></h4>
<p>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数 <span class="math inline">\(\mathrm{f}(\mathrm{x})\)</span>
的泰勒级数的前面几项来寻找方程 <span class="math inline">\((x)=0\)</span>
的根。牛顿法最大的特点就在于它的收敛速度很快。具体步骤：</p>
<ul>
<li>首先, 选择一个接近函数 <span class="math inline">\(f(x)\)</span>
零点的 <span class="math inline">\(x 0\)</span>, 计算相应的 <span class="math inline">\(f(x 0)\)</span> 和切线斜率 <span class="math inline">\(f^{\prime}(x 0)\)</span> (这里 <span class="math inline">\(f^{\prime}\)</span> 表示函数 <span class="math inline">\(f\)</span> 的导 数)</li>
<li>然后我们计算穿过点 <span class="math inline">\((x 0, f(x
0))\)</span> 并且斜率为 <span class="math inline">\(f^{\prime}(x
0)\)</span> 的直线和 <span class="math inline">\(x\)</span> 轴的交点的
<span class="math inline">\(x\)</span> 坐标, 也就是求如下方程的解: <span class="math inline">\(x *
f^{\prime}\left(x_0\right)+f\left(x_0\right)-x_0 *
f^{\prime}\left(x_0\right)=0\)</span></li>
<li>我们将新求得的点的 <span class="math inline">\(x\)</span> 坐标命名为
<span class="math inline">\(x 1\)</span>, 通常 <span class="math inline">\(x 1\)</span> 会比 <span class="math inline">\(x
0\)</span> 更接近方程 <span class="math inline">\(f(x)=0\)</span>
的解。因此我们现在可以利用 <span class="math inline">\(x 1\)</span>
开始下一轮迭代。</li>
</ul>
<p>由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法搜索动态示例图：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304252126567.gif" alt="img" style="zoom: 67%;"></p>
<p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。<strong>缺点：</strong></p>
<ul>
<li>牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
<ul>
<li>在高维情况下这个矩阵非常大，计算和存储都是问题。</li>
</ul></li>
<li>在小批量的情况下，牛顿法对于二阶导数的估计噪声太大。
<ul>
<li>目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。</li>
</ul></li>
</ul>
<h4><span id="33-拟牛顿法">3.3 拟牛顿法</span></h4>
<p>拟牛顿法是求解非线性优化问题最有效的方法之一，<strong>本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</strong>拟牛顿法和梯度下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于梯度下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>
<p><strong>DFP、BFGS、L-BFGS:</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261133926.png" alt="image-20220509171547743" style="zoom:50%;"></p>
<h4><span id="34-共轭梯度法">3.4 共轭梯度法</span></h4>
<p>共轭梯度法是介于梯度下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。
在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p>
<p>具体的实现步骤请参加wiki百科<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB">共轭梯度法</a>。下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304252126043.jpeg" alt="img" style="zoom:67%;"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3KD4010/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3KD4010/" class="post-title-link" itemprop="url">贝叶斯分类器（1）朴素贝叶斯</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:43:32" itemprop="dateCreated datePublished" datetime="2022-03-15T22:43:32+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 17:15:46" itemprop="dateModified" datetime="2023-04-21T17:15:46+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" itemprop="url" rel="index"><span itemprop="name">贝叶斯分类器</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>朴素贝叶斯（Naive
Bayes）是基于<strong>贝叶斯定理</strong>与<strong>特征条件假设</strong>的<strong>分类</strong>方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯是<strong>选出各个分类类别后验概率最大</strong>的作为最终分类。</p>
<ul>
<li><strong>优点</strong>：对小规模的数据表现很好，适合多分类任务，<strong>适合增量式训练</strong>。</li>
<li><strong>缺点</strong>：对输入数据的表达形式很敏感<strong>（离散、连续，值极大极小之类的）。</strong></li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/3KD4010/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/8R7RKW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/8R7RKW/" class="post-title-link" itemprop="url">贝叶斯分类器（2）贝叶斯估计</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:43:32" itemprop="dateCreated datePublished" datetime="2022-03-15T22:43:32+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 16:02:10" itemprop="dateModified" datetime="2023-04-21T16:02:10+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" itemprop="url" rel="index"><span itemprop="name">贝叶斯分类器</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-最大似然估计-最大后验估计-贝叶斯估计的对比">一、最大似然估计、最大后验估计、贝叶斯估计的对比</span></h3>
<h4><span id="11-贝叶斯公式">1.1 <strong>贝叶斯公式</strong></span></h4>
<p>这三种方法都和贝叶斯公式有关，所以我们先来了解下贝叶斯公式：</p>
<p><span class="math display">\[
p(\theta \mid X)=\frac{p(X \mid \theta) p(\theta)}{p(X)}
\]</span> 每一项的表示如下: <span class="math display">\[
\text { posterior }=\frac{\text { likehood } * \text { prior }}{\text {
evidence }}
\]</span> + posterior: 通过样本X得到参数 <span class="math inline">\(\theta\)</span> 的概率, 也就是后验概率。 +
likehood: 通过参数 <span class="math inline">\(\theta\)</span>
得到样本X的概率, 似然函数, 通常就是我们的数据集的表现。 + prior: 参数
<span class="math inline">\(\theta\)</span> 的先验概率,
一般是根据人的先验知识来得出的。</p>
<h4><span id="12-极大似然估计-mle">1.2 极大似然估计 (MLE)</span></h4>
<p>极大似然估计的核心思想是:
认为当前发生的事件是概率最大的事件。<strong>因此就可以给定的数据集,
使得该数据集发生的概率最大来求得模型中的参数</strong>。似然函数如下:
<span class="math display">\[
p(X \mid \theta)=\prod_{x 1}^{x n} p(x i \mid \theta)
\]</span> 为了便于计算, 我们对似然函数两边取对数,
生成新的对数似然函数（因为对数函数是单调增函数, 因此求似然函数最大化就可
以转换成对数似然函数最大化）： <span class="math display">\[
p(X \mid \theta)=\prod_{x 1}^{x n} p(x i \mid \theta)=\sum_{x 1}^{x n}
\log p(x i \mid \theta)
\]</span> 求对数似然函数最大化, 可以通过导数为 0
来求解。<strong><font color="red"> 极大似然估计只关注当前的样本,
也就是只关注当前发生的事情,
不考虑事情的先验情况</font></strong>。由于计算简单, 而且不需要关注先验
知识, 因此在机器学习中的应用非常广, 最常见的就是逻辑回归。</p>
<h4><span id="13-最大后验估计-map">1.3 最大后验估计 (MAP)</span></h4>
<p>和最大似然估计不同的是,
最大后验估计中引入了<strong>先验概率</strong>（先验分布属于贝叶斯学派引入的,
像L1, L2正则化就是对参数引入 了拉普拉斯先验分布和高斯先验分布）,
而且最大后验估计要求的是 <span class="math inline">\(p(\theta \mid
X)\)</span></p>
<p>最大后验估计可以写成下面的形式: <span class="math display">\[
\operatorname{argmaxp}(\theta \mid X)=\operatorname{argmax} \frac{p(X
\mid \theta) p(\theta)}{p(X)}=\operatorname{argmax}\left(\prod_{x 1}^{x
n} p(x i \mid \theta)\right) p(\theta)
\]</span> 在求最大后验概率时, 可以忽略分母 <span class="math inline">\(p(x)\)</span>, 因为该值不影响对 <span class="math inline">\(\theta\)</span> 的估计。同样为了便于计算,
对两边取对数, 后验概率最大化就变成了: <span class="math display">\[
\operatorname{argmax}\left(\sum_{x 1}^{x n} \operatorname{logp}(x i \mid
\theta)+\log p(\theta)\right)
\]</span> <strong><font color="red">
最大后验估计不只是关注当前的样本的情况，还关注已经发生过的先验知识。在朴素贝叶斯中会有最大后验概率的应用，但并没有用上最大后验估计来求参数（因为朴素贝叶斯中的θ其实就是分类的类别）。</font></strong></p>
<p><strong>最大后验估计和最大似然估计的区别：</strong>最大后验估计允许我们把先验知识加入到估计模型中，<strong>这在样本很少的时候是很有用的（因此朴素贝叶斯在较少的样本下就能有很好的表现）</strong>，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如beta分布的α，β，我们还可以调节把估计的结果“拉”向先验的幅度，α，β越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1RM9XFV/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1RM9XFV/" class="post-title-link" itemprop="url">贝叶斯分类器（3）总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:43:32" itemprop="dateCreated datePublished" datetime="2022-03-15T22:43:32+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 17:15:39" itemprop="dateModified" datetime="2023-04-21T17:15:39+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" itemprop="url" rel="index"><span itemprop="name">贝叶斯分类器</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="朴素贝叶斯-qampa">朴素贝叶斯 Q&amp;A</span></h3>
<blockquote>
<ul>
<li>朴素贝叶斯分类器原理以及公式，出现估计概率值为 0
怎么处理（拉普拉斯平滑），缺点；</li>
<li>解释贝叶斯公式和朴素贝叶斯分类。</li>
<li>贝叶斯分类，这是一类分类方法，主要代表是朴素贝叶斯，朴素贝叶斯的原理，重点在假设各个属性类条件独立。然后能根据贝叶斯公式具体推导。考察给你一个问题，如何利用朴素贝叶斯分类去分类，比如：给你一个人的特征，判断是男是女，比如身高，体重，头发长度等特征的的数据，那么你要能推到这个过程。给出最后的分类器公式。</li>
<li>那你说说贝叶斯怎么分类啊？<strong>比如说看看今天天气怎么样？</strong>我：blabla，，，利用天气的历史数据，可以知道天气类型的先验分布，以及每种类型下特征数据（比如天气数据的特征：温度啊，湿度啊）的条件分布，这样我们根据贝叶斯公式就能求得天气类型的后验分布了。。。。面试官：en（估计也比较满意吧）<strong>那你了解关于求解模型的优化方法吗？一般用什么优化方法来解？</strong></li>
<li>贝叶斯分类器的优化和特殊情况的处理</li>
</ul>
</blockquote>
<h4><span id="1-朴素贝叶斯-svm和lr的区别"><strong><font color="red">
1、朴素贝叶斯、SVM和LR的区别？</font></strong></span></h4>
<p><strong>朴素贝叶斯是生成模型</strong>，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解P(Y|X)。</p>
<p><strong>LR是判别模型</strong>，根据极大化对数似然函数直接求出条件概率P(Y|X)；朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求；<strong>朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。</strong></p>
<table>
<colgroup>
<col style="width: 8%">
<col style="width: 30%">
<col style="width: 30%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>算法</th>
<th>SVM</th>
<th>LR</th>
<th>朴素贝叶斯</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>思想</strong></td>
<td><strong>想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面</strong>。</td>
<td>使用线性回归模型的预测值逼近分类任务真实标记的对数几率。</td>
<td>基于<strong>贝叶斯定理</strong>与<strong>特征条件假设</strong>的<strong>分类</strong>方法。选出各个分类类别后验概率最大的作为最终分类。</td>
</tr>
<tr class="even">
<td><strong>输出</strong></td>
<td>判别模型、<strong>非概率方法</strong>；</td>
<td><strong>概率方法</strong>；需要对<span class="math inline">\(p(y|x)\)</span>进行假设，具有概率意义。</td>
<td>生成模型</td>
</tr>
<tr class="odd">
<td><strong>经验损失函数</strong></td>
<td><strong>合页损失函数</strong>；有一段平的零区域、使得SVM的对偶性有稀疏性。</td>
<td><strong>交叉熵损失函数</strong></td>
<td><strong>后验概率最大</strong></td>
</tr>
<tr class="even">
<td><strong>训练样本</strong></td>
<td><strong>支持向量</strong>（少数样本），SVM的参数和假设函数只和支持向量有关。</td>
<td>全样本</td>
<td>全样本</td>
</tr>
<tr class="odd">
<td><strong>优化方法</strong></td>
<td>次梯度下降和坐标梯度下降 【<strong>SMO算法</strong>】</td>
<td><strong>梯度下降</strong></td>
<td>无</td>
</tr>
<tr class="even">
<td>多分类</td>
<td><strong>多分类SVM</strong></td>
<td><strong>Softmax回归</strong></td>
<td>后验概率最大</td>
</tr>
<tr class="odd">
<td><strong>敏感程度</strong></td>
<td><strong>SVM考虑分类边界线附近的样本</strong>（决定分类超平面的样本）。在支持向量外添加或减少任何样本点对分类决策面没有任何影响；【不敏感】</td>
<td><strong>LR受所有数据点的影响</strong>。直接依赖数据分布，每个样本点都会影响决策面的结果。如果训练数据不同类别严重不平衡。【敏感】</td>
<td><strong>特征值是基于频数进行统计的。</strong>一个值的异常（变成了别的数），<strong>只是贝叶斯公式里的计算概率的分子或者分母发生微小的变化，整体结果影响不大</strong>，不敏感【概率排序】</td>
</tr>
</tbody>
</table>
<h4><span id="2-朴素贝叶斯朴素在哪里">2、<strong><font color="red">
朴素贝叶斯“朴素”在哪里？</font></strong></span></h4>
<p>简单来说：它假定<strong>所有的特征在数据集中的作用是同样重要和独立的</strong>，正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。</p>
<p>利用贝叶斯定理求解联合概率<span class="math inline">\(P(XY)\)</span>时，需要计算条件概率<span class="math inline">\(P(X|Y)\)</span>。在计算<span class="math inline">\(P(X|Y)\)</span>时，朴素贝叶斯做了一个很强的条件独立假设（当Y确定时，X的各个分量取值之间相互独立），即<span class="math inline">\(P(X1=x1,X2=x2,…Xj=xj|Y=yk) =
P(X1=x1|Y=yk)…*P(Xj=xj|Y=yk)\)</span>。
多个特征全是独立的，需要分别相乘。</p>
<h4><span id="3-在估计条件概率pxy时出现概率为0的情况怎么办">3、<strong>在估计条件概率P(X|Y)时出现概率为0的情况怎么办？</strong></span></h4>
<p><strong>拉普拉斯平滑法</strong>是朴素贝叶斯中处理零概率问题的一种修正方式。在进行分类的时候，可能会出现某个属性在训练集中没有与某个类同时出现过的情况，如果直接基于朴素贝叶斯分类器的表达式进行计算的话就会出现<strong>零概率现象</strong>。</p>
<p>为了避免其他属性所携带的信息被训练集中未出现过的属性值“抹去”，所以才使用拉普拉斯估计器进行修正。具体的方法是：<strong>在分子上加1,对于先验概率，在分母上加上训练集中label的类别数；对于特征i
在label下的条件概率，则在分母上加上第i个属性可能的取值数（特征 i
的unique()）</strong></p>
<p>先验概率的贝叶斯估计: <span class="math display">\[
P_\lambda\left(Y=c_k\right)=\frac{\sum_{i=1}^N
I\left(y_i=c_k\right)+\lambda}{N+K \lambda}
\]</span> 条件概率的贝叶斯估计：【离散型】 <span class="math display">\[
P_\lambda\left(X^{(j)}=a_{j l} \| Y=c_k\right)=\frac{\sum_{i=1}^N
I\left(x_i^{(j)}=a_{j l}, y_{i=} c_k\right)+\lambda}{\sum_{i=1}^N
I\left(y_i=c_k\right)+S_j \lambda}
\]</span></p>
<h4><span id="4-先验概率和后验概率都是">4、<strong>先验概率和后验概率都是？</strong></span></h4>
<p><strong>先验概率是指根据以往经验和分析得到的概率</strong>,如全概率公式,它往往作为"由因求果"问题中的"因"出现.<strong>后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</strong></p>
<p><strong>先验概率和后验概率是相对的。</strong>如果以后还有新的信息引入，更新了现在所谓的后验概率，得到了新的概率值，那么这个新的概率值被称为后验概率。</p>
<h4><span id="5-朴素贝叶斯算法的前提假设是什么">5、<strong>朴素贝叶斯算法的前提假设是什么？</strong></span></h4>
<ol type="1">
<li>特征之间相互独立</li>
<li>每个特征同等重要</li>
</ol>
<h4><span id="6-面试的时候怎么标准回答朴素贝叶斯呢">6、<strong>面试的时候怎么标准回答朴素贝叶斯呢？</strong></span></h4>
<p>首先朴素贝斯是一个<strong>生成模型（很重要）</strong>，其次它通过学习已知样本，计算出联合概率，再求条件概率。</p>
<h5><span id="生成模式和判别模式的区别常见"><strong>生成模式和判别模式的区别(常见)：</strong></span></h5>
<p><strong>生成模式</strong>：由数据学得<strong>联合概率分布，求出条件概率分布P(Y|X)的预测模型</strong>；<strong>比较在乎数据是怎么生成的</strong>；常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机。</p>
<p><strong>判别模式</strong>：由数据学得<strong>决策函数或条件概率分布作为预测模型</strong>，<strong>要关注在数据的差异分布上，而不是生成</strong>；常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场。</p>
<h4><span id="7-为什么属性独立性假设在实际情况中很难成立但朴素贝叶斯仍能取得较好的效果排序能力">7、<strong>为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?</strong>【排序能力】</span></h4>
<p>首先独立性假设在实际中不存在，确实会导致朴素贝叶斯不如一些其他算法，但是就算法本身而言，朴素贝叶斯也会有不错的分类效果，原因是：</p>
<ul>
<li><strong>分类问题看中的是类别的条件概率的排序</strong>，而不是具体的概率值，所以这里面对精准概率值的计算是有一定的容错的。</li>
<li>如果特征属性之间的依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。</li>
</ul>
<h4><span id="8-朴素贝叶斯中概率计算的下溢问题如何解决"><strong><font color="red">
8、朴素贝叶斯中概率计算的下溢问题如何解决？</font></strong></span></h4>
<p><strong>在朴素贝叶斯的计算过程中，需要对特定分类中各个特征出现的概率进行连乘</strong>，小数相乘，越乘越小，这样就造成下溢出。在程序中，在相应小数位置进行四舍五入，计算结果可能就变成0了。</p>
<p>为了解决这个问题，<strong>对乘积结果取自然对数</strong>。将小数的乘法操作转化为取对数后的加法操作，规避了变为0的风险同时并不影响分类结果。</p>
<h4><span id="9-朴素贝叶斯分类器对异常值和缺失值敏感吗">9、<strong>朴素贝叶斯分类器对异常值和缺失值敏感吗？</strong></span></h4>
<p>回想朴素贝叶斯的计算过程，它在推理的时候，输入的某个特征组合，<strong>他们的特征值在训练的时候在贝叶斯公式中都是基于频数进行统计的。</strong>所以一个值的异常（变成了别的数），<strong>只是贝叶斯公式里的计算概率的分子或者分母发生微小的变化，整体结果影响不大</strong>，就算微微影响最终概率值的获得，由于<strong>分类问题只关注概率的排序而不关注概率的值，所以影响不大</strong>，保留异常值还可以提高模型的泛化性能。</p>
<p>缺失值也是一样，如果一个数据实例缺失了一个属性的数值，在建模的时将被忽略，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。</p>
<h4><span id="10-朴素贝叶斯中有没有超参数可以调">10、<strong>朴素贝叶斯中有没有超参数可以调？</strong></span></h4>
<p><strong>朴素贝叶斯是没有超参数可以调的，所以它不需要调参</strong>，朴素贝叶斯是根据训练集进行分类，分类出来的结果基本上就是确定了的，拉普拉斯估计器不是朴素贝叶斯中的参数，不能通过拉普拉斯估计器来对朴素贝叶斯调参。</p>
<h4><span id="11-朴素贝叶斯有哪三个模型">11、<strong>朴素贝叶斯有哪三个模型？</strong></span></h4>
<ul>
<li><strong>多项式模型对应于离散变量</strong>，其中离散变量指的是category型变量，也就是类别变量，比如性别；连续变量一般是数字型变量，比如年龄，身高，体重。</li>
<li><strong>高斯模型 对应于连续变量</strong>（每一维服从正态分布）</li>
<li><strong>伯努利模型</strong> <strong>对应于文本分类</strong>
（特征只能是0或者1）</li>
</ul>
<h4><span id="12-朴素贝叶斯为什么适合增量计算"><strong><font color="red">
12、朴素贝叶斯为什么适合增量计算？</font></strong></span></h4>
<p>朴素贝叶斯在训练过程中实际上需要<strong>计算出各个类别的概率和各个特征的条件概率</strong>，这些概率以频数统计比值（对于多项式模型而言）的形式产生概率值，<strong>可以快速根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算。</strong></p>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li><p>https://scikit-learn.org/dev/modules/naive_bayes.html#naive-bayes</p></li>
<li><p><a target="_blank" rel="noopener" href="https://plushunter.github.io/2017/02/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">FREE
WILL 机器学习算法系列（10）：朴素贝叶斯</a></p></li>
<li><h5><span id="最大似然估计-最大后验估计-贝叶斯估计的对比">最大似然估计、最大后验估计、贝叶斯估计的对比</span></h5>
<ul>
<li><h5><span id="httpswwwcnblogscomjiangxinyangp9378535html">https://www.cnblogs.com/jiangxinyang/p/9378535.html</span></h5></li>
</ul></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2P1GDXT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2P1GDXT/" class="post-title-link" itemprop="url">支持向量机（1）硬间隔对偶性</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:38:05" itemprop="dateCreated datePublished" datetime="2022-03-15T22:38:05+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:46:27" itemprop="dateModified" datetime="2023-04-22T19:46:27+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">支持向量机</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong><font color="red"> SVM
是一个非常优雅的算法，具有完善的数学理论，虽然如今工业界用到的不多，但还是决定花点时间去写篇文章整理一下。</font></strong></p>
<p><strong>本质：SVM
想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</strong>为了对数据中的噪声有一定的容忍能力。<strong>以几何的角度，在丰富的数据理论的基础上，简化了通常的分类和回归问题。</strong></p>
<p><strong>几何意义</strong>：找到一个超平面将特征空间的正负样本分开，最大分隔（对噪音有一定的容忍能力）；</p>
<p><strong>间隔表示</strong>：划分超平面到属于不同标记的最近样本的距离之和；</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191157912.jpg" alt="【机器学习】支持向量机 SVM（非常详细）" style="zoom: 33%;"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/2P1GDXT/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/22/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><span class="page-number current">23</span><a class="page-number" href="/page/24/">24</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/24/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
