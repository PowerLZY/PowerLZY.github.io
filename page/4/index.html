<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/4/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">239</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1VGG9SJ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1VGG9SJ/" class="post-title-link" itemprop="url">深度学习-GNN（7）【Nan】HetGNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-11 17:57:32 / 修改时间：19:07:40" itemprop="dateCreated datePublished" datetime="2022-07-11T17:57:32+08:00">2022-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/GNN/" itemprop="url" rel="index"><span itemprop="name">GNN</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="hetgnn">HetGNN</span></h2><blockquote>
<p>  HetGNN —异构图处理 - wlkq的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/411528472">https://zhuanlan.zhihu.com/p/411528472</a></p>
<p>  异构图embedding学习，舍HetGNN其谁？ - 李杰的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/392367843">https://zhuanlan.zhihu.com/p/392367843</a></p>
</blockquote>
<p>图嵌入领域，同构图算法大行其道，如：DeepWalk、Node2vec、GCN、GraphSage、GAT。但实际业务场景中异构图居多，除了经典的MetaPath2vec、RGCN，貌似其他选项并不多，今天就为大家介绍一款异构图嵌入学习神器—-HetGNN。</p>
<h3><span id="hetgnn简介">HetGNN简介</span></h3><p>提出的网络名称：<strong>HetGNN（Heterogeneous Graph Neural Network），2019 SIGKDD</strong></p>
<p>核心理念：heterogeneous structural graph information + <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=heterogeneous&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;392367843&quot;}">heterogeneous</a> attributes or contents for each node</p>
<h3><span id="异构图嵌入学习">异构图嵌入学习</span></h3><p>异构图算法相较同构图算法有如下三方面挑战：</p>
<ul>
<li>异质图中的大多数节点并不会连接所有类型的其他节点。如academic graph中user节点不会直接连到venue（论文）节点上。另外说节点能够连接的邻居数也不一样。大部分GNN直接聚合邻居（一阶）节点信息，而远处传过来的节点信息会随着距离而减弱。hub节点会被弱关联的邻居节点扰乱信息，冷启动的节点会因为邻居不足而导致不能充分表示。<strong>那么问题1就是：如何对异质图上的每个节点采样到强相关的邻居节点呢？（这边我认为一般都是用了注意力机制了）</strong></li>
<li>每个节点都带有非结构化的属性特征，如text、image，常用的从concatenate或者linear transformation不能建模节点属性间的deep interaction。<strong>那么问题2就是：如何设计异质图上节点属性的encoder（编码器）来处理不同节点内容异质性问题。</strong></li>
<li>不同类型的邻居节点对生成节点embedding的贡献也不一样。例如在academic graph，author和paper节点对author的embedding的影响会强如venue，venue节点包含不同的主题，具有更一般的嵌入，而大部分gnn集中在同质图的处理上，也没有考虑这种不同类型节点的影响。 <strong>挑战3是:如何通过考虑不同节点类型的影响来聚合异构邻居的特征信息。</strong>【<strong>自注意力机制</strong>】</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1WZGAE8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1WZGAE8/" class="post-title-link" itemprop="url">高级威胁发现（6）UNICORN: Provenance-Based Detector for APTs</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-10 21:02:53" itemprop="dateCreated datePublished" datetime="2022-07-10T21:02:53+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-19 15:58:24" itemprop="dateModified" datetime="2023-04-19T15:58:24+08:00">2023-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/" itemprop="url" rel="index"><span itemprop="name">学术前沿</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="ndss20-unicorn-provenance-based-detector-for-apts">NDSS20 UNICORN: Provenance-Based Detector for APTs</span></h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg5MTM5ODU2Mg==&amp;mid=2247492967&amp;idx=1&amp;sn=60f14977758cc7d1e8504446422e5ec0&amp;chksm=cfcf55aaf8b8dcbc86935b76e7f36961202174ecba1df597fee9d44428c607e80d41add1b2f9&amp;scene=178&amp;cur_album_id=1776483007625822210#rd">[AI安全论文] 06.NDSS20 UNICORN: Provenance-Based Detector for APTs</a></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555641.png" alt="图片"></p>
<blockquote>
<p>  原文作者：Xueyuan Han, Thomas Pasquier, Adam Bates, James Mickens and Margo Seltzer<br>  原文标题：UNICORN: Runtime Provenance-Based Detector for Advanced Persistent Threats<br>  原文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.01525.pdf">https://arxiv.org/pdf/2001.01525.pdf</a><br>  发表会议：NDSS 2020参考文献：感谢两位老师  <a target="_blank" rel="noopener" href="https://blog.csdn.net/Sc0fie1d/article/details/104868847">https://blog.csdn.net/Sc0fie1d/article/details/104868847</a>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/xjxtx1985/article/details/106473928">https://blog.csdn.net/xjxtx1985/article/details/106473928</a></p>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p>本文提出的<strong>UNICORN是一种基于异常的APT检测器</strong>，可以有效利用数据<strong>Provenance进行分析</strong>。通过广泛且快速的图分析，使用<strong>graph sketching技术</strong>，UNICORN可以在长期运行的系统中分析Provenance Graph，从而识别未知慢速攻击。其中，Provenance graph提供了丰富的上下文和历史信息，实验证明了其先进性和较高准确率。</p>
<p>由于APT（Advanced Persistent Threats）攻击具有缓慢可持续的攻击模式以及频繁使用0-day漏洞的高级特性使其很难被检测到。<strong>本文利用数据来源分析（provenance）提出了一种基于异常的APT检测方法，称为UNICORN。</strong></p>
<ul>
<li>从建模到检测，UNICORN专门针对APT的独有特性（low-and-slow、0-Days）设计。</li>
<li>UNICONRN利用高效的图分析方法结合溯源图丰富的上下文语义和历史信息，在没有预先设定攻击特征情况下识别隐蔽异常行为。</li>
<li>通过图概要（graph sketching）技术，它有效概括了长时间系统运行来对抗长时间缓慢攻击。</li>
<li>UNICONRN使用一种新的建模方法来更好地捕捉长期行为规律，以提高其检测能力。</li>
</ul>
<p>最后通过大量实验评估表明，本文提出的方法优于现有最先进的APT检测系统，并且在真实APT环境中有较高的检测精度。</p>
<h3><span id="一-引言">一、引言</span></h3><p>APT攻击现在变得越来越普遍。这种攻击的时间跨度长，且与传统攻击行为有着本质的区别。APT攻击者的目的是获取特定系统的访问控制，并且能够长期潜伏而不被发现。攻击者通常使用0-day漏洞来获取受害者系统的访问控制。</p>
<p><strong>传统检测系统通常无法检测到APT攻击。</strong></p>
<ul>
<li>依赖恶意软件签名的检测器对利用新漏洞的攻击无效。</li>
<li>基于异常检测的系统通常分析一系列的系统调用或日志系统事件，其中大部分方法无法对长期行为进行建模。</li>
<li>由于基于异常检测的方法只能检测系统调用和事件的短序列很容易被绕过。</li>
</ul>
<p>综上，当前针对APT攻击的检测方法很少能成功。攻击者一旦使用0-Day漏洞，防御者便无计可施；而基于系统调用和系统事件的检测方法，由于数据过于密集，这些方法难以对长时间的行为模式进行建模。<strong>因此，数据溯源（data provenance）是一种检测APT更合适的数据。</strong></p>
<p>最近的研究成果表明数据溯源是一个很好的APT检测数据源。数据溯源将系统执行表示成一个有向无环图（DAG），该图描述了系统主体（如进程）和对象（文件或sockets）之间的信息流。即使跨了很时间，在图中也把因果相关的事件关联到一起。因此，即使遭受APT攻击的系统与正常系统比较类似，但是溯源图中丰富的上下文语义信息中也可以很好地区分正常行为与恶意行为。</p>
<p><strong>然而，基于数据溯源的实时APT检测依然具有挑战。</strong><br>随着APT攻击的渗透的进行，数据溯源图的规模会不断增大。其中必要的上下文分析需要处理大量图中的元素，而图上的分析通常复杂度比较高。当前基于数据溯源的APT检测方法根据已有的攻击知识通过简单的边匹配实现APT检测，无法处理未知的APT攻击。基于溯源的异常检测系统主要是基于图模型的邻域搜索，利用动态或静态模型识别正常行为模式。理论上关联的上下文越丰富越好，但是实际中由于图分析的复杂性较高限制了其可行性。</p>
<ul>
<li>Provenance Graph的分析是相当耗费计算资源，因为APT是可持续攻击，图的规模也会越来越大</li>
</ul>
<p><strong><font color="red"> 当前APT检测系统面临如下三种问题：</font></strong></p>
<ul>
<li>静态模型难以捕获长时间的系统行为；</li>
<li>low-and-slow APT投毒攻击：由于APT高级可持续的特性可以在系统中潜伏很长时间，相关的行为会被认为是正常行为，这样的攻击会影响检测模型；</li>
<li>在主存内进行计算的方法，应对长期运行的攻击表现不佳。</li>
</ul>
<p>基于此，本文提出了UNICORN，使用graph sketching来建立一个增量更新、固定大小的纵向图数据结构。这种纵向性质允许进行广泛的图探索，使得UNICORN可以追踪隐蔽的入侵行为。而固定大小和增量更新可以避免在内存中来表示provenance graph，因此UNICORN具有可扩展性，且计算和存储开销较低。UNICORN在训练过程中直接对系统的行为进行建模，但此后不会更新模型，从而防止模型的投毒攻击。</p>
<p><strong><font color="red"> 本文的主要贡献如下：</font></strong></p>
<ul>
<li>针对APT攻击特性提出一种<strong>基于Provenance的异常检测系统</strong>。</li>
<li><strong>引入一种新的基于概要的（sketch-based）、时间加权的（time-weighted）溯源编码</strong>，该编码非常紧凑且可处理长时间的溯源图。</li>
<li>通过模拟和真实的APT攻击来评估UNICORN，证明其能高精度检测APT活动。</li>
<li><strong><font color="red"> 实现代码开源。</font></strong></li>
</ul>
<h3><span id="二-背景">二、背景</span></h3><h4><span id="21-系统调用追踪的挑战"><strong>2.1 系统调用追踪的挑战</strong></span></h4><p><strong>系统调用抽象提供了一个简单的接口，用户级应用程序可以通过这个接口请求操作系统的服务</strong>。作为调用系统服务的机制，系统调用接口通常也是攻击者入侵的入口点。因此，系统调用跟踪一直被认为是入侵检测的实际信息源。然而：</p>
<ul>
<li>当前的攻击检测系统是对非结构化的系统调用的审记日志进行分析，但捕获的系统调用杂乱分散，传统基于异常检测的思路无法处理APT。因此需要将其关联成data provenance，基于溯源的方法是将历史上下文数据都编码到因果关系图中。</li>
<li>数据溯源方法已经被应用到攻击调查中，已经有一些方法能够根据审计数据构建系统溯源图用以实现对系统执行过程的建模。然而这些方法依然存在一些局限：(1) 这种事后构建很难保证溯源图的正确性，由于系统调用问题存大量并发，溯源图的完整性与可靠性无法保证；(2) 容易被绕过；(3) 时空复杂度较高。</li>
<li>由于一些内核线程不使用系统调用，因此<strong>基于Syscall生成的Provenance是一些分散的图，而不是一张系统运行状况的完整图</strong></li>
</ul>
<h4><span id="22-全系统追踪溯源">2.2 <strong>全系统追踪溯源</strong></span></h4><p><strong>全系统溯源运行在操作系统层面，捕获的是所有系统行为和它们之间的交互</strong>。通过捕获信息流和因果关系，即使攻击者通过操作内核对象来隐藏自己的行踪也无济于事。</p>
<p>本文使用CamFlow，采用了Linux安全模块（Linux Security Modules，LSM）框架来确保高效可靠的信息流记录。LSM可以消除race condition。</p>
<blockquote>
<p>  <strong>CamFlow：溯源搜集系统</strong>，参考官网 <a target="_blank" rel="noopener" href="https://camflow.org/。">https://camflow.org/。</a></p>
<p>  <strong>CamFlow 将系统的执行表示为有向无环图</strong>。图中的顶点表示内核对象（例如线程、文件、套接字等）的状态，关系表示这些状态之间的信息流。</p>
<p>  <img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555346.png" alt="CamFlow 图表概览" style="zoom: 33%;"></p>
<p>  在上面的示例中<code>process 1</code>克隆<code>process 2</code>。 <code>process 2</code>写到一个<code>pipe</code>。 <code>process 1</code>同读<code>pipe</code>。创建版本是为了保证非周期性并代表信息的正确排序（有关详细信息，请参阅我们的<a target="_blank" rel="noopener" href="http://camflow.org/publications/ccs-2018.pdf">CCS’18 论文</a>）。</p>
</blockquote>
<h3><span id="23-问题描述">2.3 问题描述</span></h3><p>现有基于数据溯源的APT攻击检测方法主要存在如下缺陷：</p>
<ul>
<li>预定义的边匹配规则过于敏感，很难检测到APT攻击中的0-Day漏洞；</li>
<li>溯源图的近邻约束导致其只能提供局部上下文信息（而非whole-system），然而这会影响相关异常检测精度；</li>
<li>系统行为模型难以检测APT：静态模型无法捕获长期运行的系统的行为；动态模型容易遭受中毒攻击；</li>
<li>溯源图的存储与计算都是在内存中，在执行长期检测上有局限性。</li>
</ul>
<p><strong>UNICORN可以解决如上问题，其本质是把APT检测问题看成大规模、带有属性的实时溯源图异常检测问题。在任何时间，从系统启动到其当前状态捕获的溯源图都将与已知正常行为的溯源图进行比较。如果有明显差别，那么就认为该系统正在遭受攻击。</strong></p>
<p>对于APT检测来说，理想基于溯源的IDS应该如下：</p>
<ul>
<li>充分利用溯源图的丰富上下文，以时间与空间有效的方法持续分析溯源图；</li>
<li>在不假设攻击行为的基础上，应考虑系统执行的整个持续时间；</li>
<li>只学习正常行为的变化，而不是学习攻击者指示的变化。</li>
</ul>
<h3><span id="三-威胁模型">三、威胁模型</span></h3><p>假设主机入侵检测有适当的场景：攻击者非法获得对系统的访问权限，并计划在不被检测的情况下驻留在系统中很长一段时间。<strong>攻击者可能分阶段执行攻击，在每个阶段还会使用大量的攻击技术</strong>。UNICORN的目标是通过解决主机生成的溯源来实现在所有阶段对APT攻击进行检测。本文假设，我们假设在受到攻击之前，UNICORN在正常运行期间会完全观察主机系统，并且在此初始建模期间不会发生攻击。</p>
<p>数据收集框架的完整性是UNICORN正确性的核心，因此<strong>我们假定所使用的CamFlow中，LSM完整性是可信的。同时，本文假设内核、溯源数据和分析引擎的正确性，我们重点关注UNICORN的分析能力。</strong></p>
<h3><span id="四-系统设计">四、系统设计</span></h3><p>独角兽是一个基于主机的入侵检测系统，能够同时检测在网络主机集合上的入侵。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555615.png" alt="图片" style="zoom: 67%;"></p>
<ol>
<li><strong>以一个带标签的流式溯源图作为输入</strong>。该图由CamFlow生成，每条边是带属性的。溯源系统构建一个具有偏序关系的DAG溯源图，能实现有效的流式计算和上下文分析。</li>
<li><strong>建立一个运行时的内存直方图</strong>。UNICORN有效构建一个流式直方图，该直方图表示系统执行的历史，如果有新边产生则实时更新直方图的计数结果。通过迭代的探索大规模图的近邻关系，发现了在上下文环境中系统实体的因果关系。该工作是UNICORN的第一步，具体来说，<strong>直方图中每个元素描述了图中唯一的一个子结构</strong>，同时考虑了子结构中的顶点与边上的异构标签，以及这些边的时间顺序。APT攻击缓慢的渗透攻击目标系统，希望基于的异常检测方法最终忘记这一行为，把其当成正常的系统行为，但是APT攻击并不能破坏攻击成功的相关信息流依赖关系。</li>
<li><strong>定期计算固定大小的概要图（graph sketch）</strong>。在纯流式环境，当UNICORN对整个溯源进行汇总时，唯一直方图元素的数量可能会任意增长。这种动态变化导致两个直方图之间的相似计算变得非常有挑战，从而使得基于直方图相似计算的建模以及检测算法变的不可行。<strong>UNICORN采用相似度保存的hash技术把直方图转换成概要图。概要图可以增量维护，也意味着UNICORN并不需要将整个溯源图都保存在内存中。</strong>另外，<strong>概要图保存了两个直方图之间的jaccard相似性，这在后续图聚类分析中特别有效。</strong></li>
<li><strong>将简略图聚类为模型</strong>。UNICORN可以在没有攻击知识的前提下实现APT攻击检测。与传统的聚类方法不同，UNICORN利用它的流处理能力生成一个动态演化模型。该模型通过在其运行的各个阶段对系统活动进行聚类捕获单个执行中的行为改变，但是UNICORN无法在攻击者破坏系统时动态实时修改模型。因此，它更适合APT攻击这类长期运行的攻击。</li>
</ol>
<h4><span id="41-溯源图">4.1 溯源图</span></h4><p>最近几年溯源图在攻击分析中越来越流行，并且本身固有的特别可以有效的用于APT检测。溯源图挖掘事件之间的因果关系，因果关系有助于对时间跨度较远的事件进行推理分析，因此有助于在检测APT相关攻击。</p>
<p>UNICORN根据两个系统执行的溯源图的相似性还判定两个系统的行为相似性。而且UNICORN总是考虑整个溯源来检测长期持续的攻击行为。<strong>当前已经有许多图相似度计算方法，然而这些算法大部分是NPC的，即使多项式时间复杂度的算法也无法满足整个溯源图快速增涨的需求。</strong></p>
<h4><span id="42-构建graph直方图">4.2 构建Graph直方图</span></h4><p>本文方法的目标是有效对溯源图进行比较分析，同时容忍正常执行中的微小变化。对于算法，我们有两个标准：</p>
<ul>
<li>图表示应考虑长期的因果关系；</li>
<li>必须能够在<strong>实时流图数据</strong>上实现该算法，以便能够在入侵发生时阻止入侵（不仅仅是检测到入侵）。</li>
</ul>
<p><strong><font color="red"> 本文基于一维WL同构检验，采用了线性时间的、快速的Weisfeiler-Lehman（WL）子树图核算法。该算法的使用依赖于构造的顶点直方图的能力，需要直方图能捕捉每个顶点周围的结构信息。</font></strong>根据扩充的顶点标签对顶点进行分类，这些标签完全描述了顶点的领域，并且<strong>通过迭代的标签传播来构造这些扩展的顶点标签</strong>。</p>
<p>同构性的WL检验及其子树kernel变化，以其对多种图的判别能力而闻名，超越了许多最新的图学习算法（例如，图神经网络）。对Weisfeiler-Lehman（WL）子树图核的使用取决于我们构建顶点直方图的能力，捕获围绕每个顶点的图结构。我们根据增强顶点标签对顶点进行分类，标签描述了顶点的R-hop邻居。</p>
<p>为了简单说明，假设有一个完整静态图，重标记对所有的输入标签的聚合。对每个顶点都重复执行这个过程来实现对n跳邻居的描述。一旦为图中的每个顶点都构建了扩展标签，那么就可以基于此生成一个直方图，其中每个bucket表示一个标签。两个图的相似性比较是基于以下假设：两个图如果相似那么在相似的标签上会有相似的分布。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555022.png" alt="图片" style="zoom: 67%;"></p>
<p>我们的目标是构建一个直方图，图中的每个元素对应一个唯一的顶点标签，用于捕获顶点的R-hop的in-coming邻居。</p>
<p><strong>信息流的多样性与复杂性（Streaming Variant and Complexity）</strong>。算法1只有新顶点出现或是新边出现对其邻顶点有影响时才会执行。本文方法只需要为每条新边更新其目标顶点的邻域。UNICORN采用这种偏序关系来最小化计算代价。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555986.png" alt="图片" style="zoom: 67%;"></p>
<p><strong>直方图元素的概念漂移问题</strong>。APT攻击场景需要模型必须能够处理长期运行行为分析能力，而系统行为的动态变化会导致溯源图的统计信息也随之变化，这种现象就叫概念漂移（concept drift）。</p>
<p>UNICORN通过对直方图元素计数使用指数权重衰减来逐渐消除过时的数据（逐渐忘记机制），从而解决了系统行为中的此类变化。它分配的权重与数据的年龄成反比。</p>
<script type="math/tex; mode=display">
H_h=\sum_t \mathbb{1}_{x_t=h}</script><p><strong>入侵检测场景中的适用性</strong>。上述“逐渐忘记”的方法，使得UNICORN可以着眼于当前的系统执行动态，而且那些与先前的object/activity有关系的事件不会被忘记。</p>
<h4><span id="43-生成概要图graph-sketches">4.3 生成概要图（Graph Sketches）</span></h4><p>Graph直方图是描述系统执行的简单向量空间图统计量。然而，与传统的基于直方图的相似性分析不同，UNICORN会随着新边的到来不断更新直方图。另外，UNCORN会根据图特征的分布来计算相似性，而不是利用绝对统计值。</p>
<p><strong><font color="red"> 本文采用locality sensitive hashing，也称作similarity-preserving data sketching。UNICORN的部署采用了前人的研究成果HistoSketch，该方法是一种基于一致加权采样的方法，且时间得性是常数。</font></strong></p>
<h4><span id="44-学习进化模型">4.4 学习进化模型</span></h4><p>在给定graph sketch和相似性度量的情况下，聚类是检测离群点常用的数据挖掘手段。<strong>然而传统的聚类方法无法捕获系统不断发展的行为</strong>。UNICORN利用其流处理的能力，创建了进化模型，可以捕获系统正常行为的变化。更重要的是，模型的建立是在训练阶段完成的，而不是在部署阶段，因为部署阶段训练模型可能会遭受中毒攻击。</p>
<p><strong><font color="red"> UNICORN在训练期间创建一个时序sketches，然后使用著名的K-medods算法从单个服务器对该概要序列进行聚类，使用轮廓系数（silhouette coefficient）确定最佳K值。</font></strong>每个簇表示系统执行的元状态（meta-states），如启动、初始化、稳定状态。然后UNICORN使用所有簇中sketches的时间顺序和每个簇的统计量（如直径、medoid），来生成系统进化的模型。</p>
<blockquote>
<ul>
<li><p>更新C中的每一列，即类中心 <img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]"> ,对于第j类，<strong>中心 <img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]"> 需要通过遍历所有该类中的样本，取与该类所有样本距离和最小的样本为该中心。</strong></p>
<p>K-means 模型: $\min <em>{G, C} \sum</em>{i=1}^n \sum<em>{j=1}^k g</em>{i j}\left|x_i-c_j\right|_2$<br>算法流程：</p>
</li>
</ul>
<ul>
<li>固定C,更新 G<ul>
<li>更新C中的每一列, 即类中心 $c_j$,其通过计算第j类中样本的平均值得到</li>
</ul>
</li>
</ul>
<p>K-mediods:模型： $\min <em>{G, C \subseteq X} \sum</em>{i=1}^n \sum<em>{j=1}^k g</em>{i j}\left|x_i-c_j\right|_1 \quad$ 可以是曼哈顿距离或其它距离度量;由于类中心的更新规 则，该方法较之于K-means更鲁棒。<br>  算法流程:</p>
<ul>
<li>固定C,更新 G</li>
<li>更新C中的每一列，即类中心$c<em>{j}$,对于第j类，**中心$c</em>{j}$需要通过遍历所有该类中的样本，取与该类所有样本距离和最小的样本为该中心。**</li>
</ul>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191556090.png" alt="图片" style="zoom: 67%;"></p>
<p><strong>对于每个训练实例，UNICORN创建一个模型，该模型捕获系统运行时执行状态的更新。直观地说，这类似于跟踪系统执行状态的自动机。</strong>最终的模型由训练数据中所有种源图的多个子模型组成。</p>
<h4><span id="45-异常检测">4.5 异常检测</span></h4><p>在部署期间，异常检测遵循前面章节中描述的相同流模式。UNICORN周期性地创建graph sketch，因为直方图从流式溯源图演变而来。给定一个概要图，UNICORN将该概要与建模期间学习的所有子模型进行比较，将其拟合到每个子模型中的一个聚类中。</p>
<p><strong>UNICORN假设监视从系统启动开始，并跟踪每个子模型中的系统状态转换。要在任何子模型中为有效，概要必须适合当前状态或下一个状态；否则，被视为异常。因此，我们检测到两种形式的异常行为：</strong></p>
<ul>
<li>不符合现有聚类的概要</li>
<li>聚类之间的无效转换</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1116ENV/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1116ENV/" class="post-title-link" itemprop="url">特征工程（0）【draft】数据清洗</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-08 17:48:29" itemprop="dateCreated datePublished" datetime="2022-07-08T17:48:29+08:00">2022-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:15:43" itemprop="dateModified" datetime="2023-04-22T19:15:43+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>533</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="特征工程-数据清洗">特征工程-数据清洗</span></h2><blockquote>
<p>  特征工程 - 未来达摩大师的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/476659737">https://zhuanlan.zhihu.com/p/476659737</a></p>
<p>  这9个特征工程使用技巧，解决90%机器学习问题！ - Python与数据挖掘的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/462744763">https://zhuanlan.zhihu.com/p/462744763</a></p>
<p>  有哪些精彩的特征工程案例？ - 京东科技风险算法与技术的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/400064722/answer/1308358333">https://www.zhihu.com/question/400064722/answer/1308358333</a></p>
</blockquote>
<p><img src="https://pic1.zhimg.com/v2-3baead31deae5b339481aa843a13e21c_b.jpg" alt="img"></p>
<p>数据格式内容错误数据来源有多种，有些是传感器采集，然后算法提取的特征数据；有些是采集的控制器的数据；还有一些应用场合，则是用户/访客产生的，数据肯定存在格式和内容上不一致的情况，所以在进行模型构建之前需要先进行数据的格式内容清洗操作。逻辑错误清洗主要是通过简单的逻辑推理发现数据中的问题数据，防止分析结果走偏，主要包含以下几个步骤：</p>
<p><strong><em>1.数据去重，去除或替换不合理的值；</em></strong></p>
<p><strong><em>2.去除或重构不可靠的字段值（修改矛盾的内容）；</em></strong></p>
<p><strong><em>3.去除异常点数据。</em></strong></p>
<h2><span id="采样">采样</span></h2><blockquote>
<p>  随机采样方法整理与讲解（MCMC、Gibbs Sampling等） - 向阳树的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109978580">https://zhuanlan.zhihu.com/p/109978580</a></p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/67Q9G2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/67Q9G2/" class="post-title-link" itemprop="url">风控算法（4）特征工程-时间滑窗统计特征体系</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-08 14:52:55 / 修改时间：15:00:37" itemprop="dateCreated datePublished" datetime="2022-07-08T14:52:55+08:00">2022-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/" itemprop="url" rel="index"><span itemprop="name">应用场景</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E4%B8%9A%E5%8A%A1%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">业务安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>855</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="风控特征时间滑窗统计特征体系">风控特征—时间滑窗统计特征体系</span></h2><h4><span id="风控业务背景"><strong>风控业务背景</strong></span></h4><p>俗话说，路遥知马力，日久见人心。在风控中也是如此，我们常<strong>从时间维度提取借款人在不同时间点的特征</strong>，以此来判断借款人的风险。在实践中，这类特征通常会占到80%以上。由于是<strong>通过时间切片和聚合统计函数来构造，因此一般被称为时间滑窗统计特征。</strong></p>
<h3><span id="一-观察期-观察点及表现期">一、<strong>观察期、观察点及表现期</strong></span></h3><p>理解这三者的概念是风控建模前期样本准备的基础，在此简单介绍。</p>
<ul>
<li><strong>观察点（</strong>Observation Point<strong>）</strong>：并非是一个具体的时间点，而是一个时间区间，表示的是客户申请贷款的时间。在该时间段申请的客户<strong>可能</strong>会是我们用来建模的样本 。（提示：为什么用“可能”这个描述，因为还需剔除一些强规则命中的异常样本，这部分样本将不会加入建模）</li>
<li><strong>观察期</strong>（Observation Window）：用以<strong>构造特征X</strong>的时间窗口。相对于观察点而言，是<strong>历史</strong>时间。观察期的选择依赖于用户数据的厚薄程度。通常数据越厚，可提取的信息也就越全面、可靠。</li>
<li><strong>表现期</strong>（Performance Window）：定义<strong>好坏标签Y</strong>的时间窗口。相对于观察点而言，是<strong>未来</strong>时间。由于风险需要有一定时间窗才能表现出来，因此信贷风险具有<strong>滞后性</strong>。表现期的长短可以通过Vintage分析和滚动率分析来确定，在此不做展开。<br><img src="https://pic3.zhimg.com/80/v2-c47416a557f573a72acccb00ec5a37fe_1440w.jpg" alt="img"></li>
</ul>
<p>表现期越长，信用风险暴露将越彻底，但意味着观察期离当前将越远，用以提取样本特征的历史数据将越陈旧，建模样本和未来样本的差异也越大。反之，表现期越短，风险还未暴露完全，但好处是能用到更近的样本。</p>
<h3><span id="二-rfm模型介绍">二、<strong>RFM模型介绍</strong></span></h3><p><strong>RFM模型最早是用来衡量客户价值和客户创利能力</strong>。理解RFM框架的思想是构造统计类特征的基础，其含义为：</p>
<ul>
<li><strong>R（Recency）</strong>：客户最近一次交易消费时间的间隔。R值越大，表示客户交易发生的日期越久，反之则表示客户交易发生的日期越近。</li>
<li><strong>F（Frequency）</strong>：客户在最近一段时间内交易消费的次数。F值越大，表示客户交易越频繁，反之则表示客户交易不够活跃。</li>
<li><strong>M（Monetary）</strong>：客户在最近一段时间内交易消费的金额。M值越大，表示客户价值越高，反之则表示客户价值越低。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3NAHDTK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3NAHDTK/" class="post-title-link" itemprop="url">模型训练（2）梯度消失&爆炸</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-06 20:40:21" itemprop="dateCreated datePublished" datetime="2022-07-06T20:40:21+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-14 18:05:06" itemprop="dateModified" datetime="2022-07-14T18:05:06+08:00">2022-07-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="缓解梯度消失amp爆炸">缓解梯度消失&amp;爆炸</span></h2><blockquote>
<p>  <strong>残差神经网络为什么可以缓解梯度消失？</strong> - 十三的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/452867110">https://zhuanlan.zhihu.com/p/452867110</a></p>
</blockquote>
<h3><span id="前言-从反向传播推导到梯度消失and爆炸的原因及解决方案"><font color="red"> 前言 - 从反向传播推导到梯度消失and爆炸的原因及解决方案？</font></span></h3><blockquote>
<ul>
<li><p>从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导） - 韦伟的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76772734">https://zhuanlan.zhihu.com/p/76772734</a></p>
<p><strong>本质上</strong>是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。<strong><font color="red"> 梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。</font></strong></p>
</li>
</ul>
</blockquote>
<h3><span id="一-反向传播推导到梯度消失and爆炸的原因及解决方案">一、反向传播推导到梯度消失and爆炸的原因及解决方案</span></h3><h4><span id="11-反向传播推导">1.1 ==反向传播推导：==</span></h4><p><img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img"></p>
<p>以上图为例开始推起来，先说明几点，i1，i2是输入节点，h1，h2为隐藏层节点，o1，o2为输出层节点，除了输入层，其他两层的节点结构为下图所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img"></p>
<p>举例说明，<img src="https://www.zhihu.com/equation?tex=NET_%7Bo1%7D" alt="[公式]"> 为输出层的输入，也就是隐藏层的输出经过线性变换后的值， <img src="https://www.zhihu.com/equation?tex=OUT_%7Bo1%7D" alt="[公式]"> 为经过激活函数sigmoid后的值；同理 <img src="https://www.zhihu.com/equation?tex=NET_%7Bh1%7D" alt="[公式]"> 为隐藏层的输入，也就是输入层经过线性变换后的值， <img src="https://www.zhihu.com/equation?tex=OUT_%7Bh1%7D" alt="[公式]"> 为经过激活函数sigmoid 的值。只有这两层有激活函数，输入层没有。</p>
<blockquote>
<p>  <strong>定义一下sigmoid的函数：</strong> <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29+%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D" alt="[公式]"><br>  <strong>说一下sigmoid的求导：</strong></p>
</blockquote>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csigma%5E%7B%5Cprime%7D%28z%29+%26%3D%5Cleft%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D%5Cright%29%5E%7B%5Cprime%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7B-z%7D%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B1%2Be%5E%7B-z%7D-1%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B%5Csigma%28z%29%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Csigma%28z%29%281-%5Csigma%28z%29%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"></p>
<p>定义一下损失函数，这里的损失函数是均方误差函数，即：</p>
<p><img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D%5Csum+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget+-+output%7D%29%5E%7B2%7D%5C%5C" alt="[公式]"></p>
<p>具体到上图，就是：</p>
<p><img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget1+-+out_o1%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget2+-+out_o2%7D%29%5E%7B2%7D%5C%5C" alt="[公式]"></p>
<p>到这里，所有前提就交代清楚了，前向传播就不推了，默认大家都会，下面推反向传播。</p>
<ul>
<li><strong>第一个反向传播（热身）</strong></li>
</ul>
<p>先来一个简单的热热身，求一下损失函数对W5的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D" alt="[公式]"></p>
<p><img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img"></p>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>首先根据链式求导法则写出对W5求偏导的总公式，再把图拿下来对照（如上），可以看出，需要计算三部分的求导【损失函数、激活函数、线性函数】，下面就一步一步来：</p>
<p><img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_%7B5%7D%7D+%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B2%7D%28%7Btarget_1+-+out_%7Bo1%7D%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28+%7Btarget_2+-+out_%7Bo2%7D%7D%29%5E%7B2%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bo1%7D%7D%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+w_5%7D+%3Dout_%7Bh_1%7D+%5C%5C" alt="[公式]"></p>
<p>综上三个步骤，得到总公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%28out_%7Bo1%7D+-+target_1%29+%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+out_%7Bh_1%7D++%5C%5C" alt="[公式]"></p>
<ul>
<li><strong>第二个反向传播：</strong></li>
</ul>
<p>接下来，要求损失函数对w1的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D" alt="[公式]"></p>
<p><img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img"></p>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>还是把图摆在这，方便看，先写出总公式，对w1求导有个地方要注意，w1的影响不仅来自o1还来自o2，从图上可以一目了然，所以总公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D%2B%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo2%7D%7D%7B%5Cpartial+net_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo2%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%5C%5C" alt="[公式]"></p>
<p>所以总共分为左右两个式子，分别又对应5个步骤，详细写一下左边，右边同理：</p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3Dw_5%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%3D+%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29+%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+i_1w_1%2Bi_2w_2%7D%7B%5Cpartial+w_1%7D+%3Di_1%5C%5C" alt="[公式]"></p>
<p>右边也是同理，就不详细写了，写一下总的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D++%26%3D%5Cleft%28%28out_%7Bo1%7D+-+target_1%29%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+w_5+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29+%5C%5C+%26%2B%7B%5Cleft%28%28out_%7Bo2%7D+-+target_2%29%5Ccdot+%28%5Csigma%28net_%7Bo2%7D%29%281-%5Csigma%28net_%7Bo2%7D%29%29%29%5Ccdot+w_7+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29%7D+%5C%5C+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"></p>
<p>这个公式只是对如此简单的一个网络结构的一个节点的偏导，就这么复杂。。亲自推完才深深的意识到。。。</p>
<p>为了后面描述方便，把上面的公式化简一下， <img src="https://www.zhihu.com/equation?tex=out_%7Bo1%7D+-+target_1" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D" alt="[公式]"> ，则：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+C_%7Bo1%7D+%5Ccdot+%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_5+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1+%2B+C_%7Bo2%7D+%5Ccdot+%5Csigma%28net_%7Bo2%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_7+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1%5C%5C" alt="[公式]"></p>
<h4><span id="12-梯度消失爆炸产生原因">1.2 <strong>==梯度消失，爆炸产生原因：==</strong></span></h4><p>从上式其实已经能看出来，求和操作其实不影响，主要是是看乘法操作就可以说明问题，可以看出，损失函数对w1的偏导，与 <img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]"> ，权重w，sigmoid的导数有关，明明还有输入i为什么不提？因为如果是多层神经网络的中间某层的某个节点，那么就没有输入什么事了。所以产生影响的就是刚刚提的三个因素。</p>
<p>再详细点描述，如图，多层神经网络：</p>
<p><img src="https://pic2.zhimg.com/80/v2-0f2ded75fbecc449a25bfd58b8c58d35_1440w.jpg" alt="img"></p>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25631496">PENG：神经网络训练中的梯度消失与梯度爆炸282 赞同 · 26 评论文章</a></p>
<p>假设（假设每一层只有一个神经元且对于每一层 <img src="https://www.zhihu.com/equation?tex=y_i%3D%5Csigma%5Cleft%28z_i%5Cright%29%3D%5Csigma%5Cleft%28w_ix_i%2Bb_i%5Cright%29" alt="[公式]">，其中<img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]">为sigmoid函数），如图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-ea9beb6c28c7d4e89be89dc5f4cbae2e_1440w.png" alt="img"></p>
<p>则：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+y_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+y_%7B4%7D%7D%7B%5Cpartial+z_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B4%7D%7D%7B%5Cpartial+x_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B4%7D%7D%7B%5Cpartial+z_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B3%7D%7D%7B%5Cpartial+x_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B3%7D%7D%7B%5Cpartial+z_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B2%7D%7D%7B%5Cpartial+x_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B2%7D%7D%7B%5Cpartial+z_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B1%7D%7D%7B%5Cpartial+b_%7B1%7D%7D%7D+%5C%5C+%7B%3DC_%7By4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B4%7D%5Cright%29+w_%7B4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B3%7D%5Cright%29+w_%7B3%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B2%7D%5Cright%29+w_%7B2%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B1%7D%5Cright%29%7D%5Cend%7Barray%7D+%5C%5C" alt="[公式]"></p>
<p>看一下sigmoid函数的求导之后的样子：</p>
<p><img src="https://pic4.zhimg.com/80/v2-208a4aa5dc657fe86919f3549d853793_1440w.jpg" alt="img"></p>
<p><strong>发现sigmoid函数求导后最大最大也只能是0.25。</strong></p>
<p>再来看W，一般我们初始化权重参数W时，通常都小于1，用的最多的应该是0，1正态分布吧。</p>
<font color="red">**所以 ![[公式]](https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq0.25) ，多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说几乎不更新，这就是梯度消失的根本原因。**</font>

<p>再来看看<strong>梯度爆炸</strong>的原因，也就是说如果 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cgeq1" alt="[公式]"> 时，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。sigmoid的函数是不可能大于1了，上图看的很清楚，那只能是w了，这也就是经常看到别人博客里的一句话，初始权重过大，一直不理解为啥。。现在明白了。</p>
<p>但梯度爆炸的情况一般不会发生，对于sigmoid函数来说， <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29%5E%7B%5Cprime%7D" alt="[公式]"> 的大小也与w有关，因为 <img src="https://www.zhihu.com/equation?tex=z%3Dwx%2Bb" alt="[公式]"> ，除非该层的输入值<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">在一直一个比较小的范围内。</p>
<p>其实<strong>梯度爆炸和梯度消失问题都是因为网络太深</strong>，网络权值更新不稳定造成的，本质上是因为<strong>梯度反向传播中的连乘效应</strong>。</p>
<p>==<strong>所以，总结一下，为什么会发生梯度爆炸和消失：</strong>==</p>
<blockquote>
<p>  本质上是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。</p>
</blockquote>
<h3><span id="二-梯度消失-爆炸解决方案">二、梯度消失、爆炸解决方案？</span></h3><blockquote>
<p>   <strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33006526">DoubleV：详解深度学习中的梯度消失、爆炸原因及其解决方法</a></p>
<ul>
<li>预训练加微调</li>
<li>梯度剪切、正则</li>
</ul>
</blockquote>
<h4><span id="21预训练加微调"><strong>2.1（预训练加微调）：</strong></span></h4><p>提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（<strong>fine-tunning</strong>）。</p>
<p>Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p>
<h4><span id="22梯度剪切-正则"><strong>2.2（梯度剪切、正则）：</strong></span></h4><p><strong>梯度剪切</strong>这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p>
<p><strong>正则化</strong>是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=Loss%3D%28y-W%5ETx%29%5E2%2B+%5Calpha+%7C%7CW%7C%7C%5E2%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</p>
<p>注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些</p>
<h4><span id="23改变激活函数"><strong><font color="red"> 2.3（改变激活函数）：</font></strong></span></h4><p>首先说明一点，<strong>tanh激活函数不能有效的改善这个问题</strong>，先来看tanh的形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctanh+%28x%29%3D%5Cfrac%7Be%5E%7Bx%7D-e%5E%7B-x%7D%7D%7Be%5E%7Bx%7D%2Be%5E%7B-x%7D%7D%5C%5C" alt="[公式]"></p>
<p>再来看tanh的导数图像：</p>
<p><img src="https://pic4.zhimg.com/80/v2-66a7e4fcf11a2d85c15e7bf7b88b2d1b_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>发现虽然比sigmoid的好一点，sigmoid的最大值小于0.25，tanh的最大值小于1，但仍是小于1的，所以并不能解决这个问题。</strong></p>
<p><strong>Relu</strong>:思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Coperatorname%7BRe%7D+%5Coperatorname%7Blu%7D%28%5Cmathrm%7Bx%7D%29%3D%5Cmax+%28%5Cmathrm%7Bx%7D%2C+0%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D%7B0%2C+x%3C0%7D+%5C%5C+%7Bx%2C+x%3E0%7D%5Cend%7Barray%7D%5Cright%5C%7D%5C%5C" alt="[公式]"></p>
<p><img src="https://pic2.zhimg.com/80/v2-55475ee2d90cd7257a39f62549a65769_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>从上图中，我们可以很容易看出，<strong>relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</strong></p>
<p><strong>relu</strong>的主要贡献在于：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练</li>
</ul>
<p>同时也存在一些<strong>缺点</strong>：</p>
<ul>
<li><strong>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</strong></li>
<li>输出不是以0为中心的</li>
</ul>
<p><strong>leakrelu</strong></p>
<p>leakrelu就是为了解决relu的0区间带来的影响，其数学表达为： <img src="https://www.zhihu.com/equation?tex=leakrelu%3D%5Cbegin%7Bequation%7D+f%28x%29%3D+%5Cbegin%7Bcases%7D+x%2C+%26+%7Bx%5Cgt+0%7D+%5C%5C%5C%5C+x%2Ak%2C+%26+%7Bx%5Cleq+0%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]"> 其中k是leak系数，一般选择0.1或者0.2，或者通过学习而来解决死神经元的问题。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3ab1bd8fb85542a0c85eb907b73fa327_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>leakrelu解决了0区间带来的影响，而且包含了relu的所有优点</p>
<p><strong>elu</strong></p>
<p>elu激活函数也是为了解决relu的0区间带来的影响，其数学表达为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7Bx%2C%7D+%26+%7B%5Ctext+%7B+if+%7D+x%3E0%7D+%5C%5C+%7B%5Calpha%5Cleft%28e%5E%7Bx%7D-1%5Cright%29%2C%7D+%26+%7B%5Ctext+%7B+otherwise+%7D%7D%5Cend%7Barray%7D%5Cright.%5C%5C" alt="[公式]"></p>
<p>其函数及其导数数学形式为：</p>
<p><img src="https://pic3.zhimg.com/80/v2-ec3c80e51129bd76d49cad6e52d449c2_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>但是elu相对于leakrelu来说，计算要更耗时间一些，因为有e。</p>
<h4><span id="24batchnorm梯度消失"><strong>2.4（batchnorm）：</strong>【梯度消失】</span></h4><p><strong>Batchnorm</strong>是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。</p>
<p>具体的batchnorm原理非常复杂，在这里不做详细展开，此部分大概讲一下batchnorm解决梯度的问题上。具体来说就是反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子： 正向传播中<img src="https://www.zhihu.com/equation?tex=f_3%3Df_2%28w%5ET%2Ax%2Bb%29" alt="[公式]">，那么反向传播中，<img src="https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cpartial+f_2%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+f_1%7Dw" alt="[公式]">，反向传播式子中有w的存在，所以<img src="https://www.zhihu.com/equation?tex=w" alt="[公式]">的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出做scale和shift的方法，通过一定的规范化手段，<strong>把每层神经网络任意神经元这个输入值的分布【假设原始是正态分布】强行拉回到接近均值为0方差为1的标准正太分布，即严重偏离的分布强制拉回比较标准的分布，<font color="red"> 这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生</font>，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<h4><span id="25残差结构"><strong><font color="red"> 2.5（残差结构）：</font></strong></span></h4><p><img src="https://pic4.zhimg.com/80/v2-3134d24348c47ca2001d37fef1c3f8bf_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>如图，把输入加入到某层中，这样求导时，总会有个1在，这样就不会梯度消失了。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot+%5Cfrac%7B%5Cpartial+x_%7BL%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot%5Cleft%281%2B%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D+F%5Cleft%28x_%7Bi%7D%2C+W_%7Bi%7D%5Cright%29%5Cright%29%5C%5C" alt="[公式]"></p>
<p>式子的第一个因子 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D" alt="[公式]"> 表示的损失函数到达 L 的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p>
<p><code>注：上面的推导并不是严格的证</code>，只为帮助理解</p>
<h4><span id="26lstm">==<strong>2.6（LSTM）：</strong>==</span></h4><p>在介绍这个方案之前，有必要来推导一下RNN的反向传播，<strong>因为关于梯度消失的含义它跟DNN不一样！不一样！不一样！</strong></p>
<p>先推导再来说，从这copy的：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28687529">沉默中的思索：RNN梯度消失和爆炸的原因565 赞同 </a></p>
<p>RNN结构如图：</p>
<p><img src="https://pic4.zhimg.com/80/v2-ab844e07a86f910d2852198c3117ddb7_1440w.jpg" alt="img"></p>
<p>假设我们的时间序列只有三段， <img src="https://www.zhihu.com/equation?tex=S_%7B0%7D" alt="[公式]"> 为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下：<br><img src="https://www.zhihu.com/equation?tex=S_%7B1%7D%3DW_%7Bx%7DX_%7B1%7D%2BW_%7Bs%7DS_%7B0%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B1%7D%3DW_%7Bo%7DS_%7B1%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B2%7D%3DW_%7Bx%7DX_%7B2%7D%2BW_%7Bs%7DS_%7B1%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B2%7D%3DW_%7Bo%7DS_%7B2%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B3%7D%3DW_%7Bx%7DX_%7B3%7D%2BW_%7Bs%7DS_%7B2%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B3%7D%3DW_%7Bo%7DS_%7B3%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p>假设在t=3时刻，损失函数为 <img src="https://www.zhihu.com/equation?tex=L_%7B3%7D%3D%5Cfrac%7B1%7D%7B2%7D%28Y_%7B3%7D-O_%7B3%7D%29%5E%7B2%7D" alt="[公式]"> 。</p>
<p>则对于一次训练任务的损失函数为 <img src="https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bt%3D0%7D%5E%7BT%7D%7BL_%7Bt%7D%7D" alt="[公式]"> ，即每一时刻损失值的累加。</p>
<p>使用随机梯度下降法训练RNN其实就是对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D+" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=W_%7Bo%7D" alt="[公式]"> 以及 <img src="https://www.zhihu.com/equation?tex=b_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=b_%7B2%7D" alt="[公式]"> 求偏导，并不断调整它们以使L尽可能达到最小的过程。</p>
<p>现在假设我们我们的时间序列只有三段，t1，t2，t3。</p>
<p><strong>我们只对t3时刻的 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D%E3%80%81W_%7B0%7D" alt="[公式]"> 求偏导（其他时刻类似）：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7B0%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bo%7D%7D%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D" alt="[公式]"></p>
<p><strong>可以看出对于 <img src="https://www.zhihu.com/equation?tex=W_%7B0%7D" alt="[公式]"> 求偏导并没有长期依赖，但是对于 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导，会随着时间序列产生长期依赖</strong>。因为 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]"> 随着时间序列向前传播，而 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]"> 又是 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]">的函数。</p>
<p>根据上述求偏导的过程，我们可以得出任意时刻对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Csum_%7Bk%3D0%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%7B%5Cpartial%7BS_%7Bt%7D%7D%7D%7D%28%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D%29%5Cfrac%7B%5Cpartial%7BS_%7Bk%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]"></p>
<p>任意时刻对<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]"> 求偏导的公式同上。</p>
<font color="red"> 如果加上激活函数， ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bj%7D%3Dtanh%28W_%7Bx%7DX_%7Bj%7D%2BW_%7Bs%7DS_%7Bj-1%7D%2Bb_%7B1%7D%29) ，则 ![[公式]](https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D) = ![[公式]](https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7Btanh%5E%7B%27%7D%7DW_%7Bs%7D)激活函数tanh和它的导数图像在上面已经说过了，所以原因在这就不赘述了，还是一样的，激活函数导数小于1。</font>

<blockquote>
<p>  ==<strong>现在来解释一下，为什么说RNN和DNN的梯度消失问题含义不一样？</strong>==</p>
<ol>
<li><strong>先来说DNN中的反向传播：</strong>在上文的DNN反向传播中，我推导了两个权重的梯度，第一个梯度是直接连接着输出层的梯度，求解起来并没有梯度消失或爆炸的问题，因为它没有连乘，只需要计算一步。第二个梯度出现了连乘，也就是说越靠近输入层的权重，梯度消失或爆炸的问题越严重，可能就会消失会爆炸。<strong>一句话总结一下，DNN中各个权重的梯度是独立的，该消失的就会消失，不会消失的就不会消失。</strong></li>
<li><strong>再来说RNN：</strong>RNN的特殊性在于，它的权重是共享的。抛开W_o不谈，因为它在某时刻的梯度不会出现问题（某时刻并不依赖于前面的时刻），但是W_s和W_x就不一样了，每一时刻都由前面所有时刻共同决定，是一个相加的过程，这样的话就有个问题，当距离长了，计算最前面的导数时，最前面的导数就会消失或爆炸，但当前时刻整体的梯度并不会消失，因为它是求和的过程，当下的梯度总会在，只是前面的梯度没了，但是更新时，由于权值共享，所以整体的梯度还是会更新，<strong>通常人们所说的梯度消失就是指的这个，指的是当下梯度更新时，用不到前面的信息了，因为距离长了，前面的梯度就会消失，也就是没有前面的信息了，但要知道，整体的梯度并不会消失，因为当下的梯度还在，并没有消失。</strong></li>
<li><strong>一句话概括：</strong>RNN的梯度不会消失，RNN的梯度消失指的是当下梯度用不到前面的梯度了，但DNN靠近输入的权重的梯度是真的会消失。</li>
</ol>
</blockquote>
<p>说完了RNN的反向传播及梯度消失的含义，终于该说<strong>为什么LSTM可以解决这个问题了</strong>，这里默认大家都懂LSTM的结构，对结构不做过多的描述。<strong>见第三节</strong>。【LSTM通过它的“门控装置”有效的缓解了这个问题，这也就是为什么我们现在都在使用LSTM而非普通RNN。】</p>
<h2><span id="缓解梯度消失amp爆炸-qampa">缓解梯度消失&amp;爆炸 Q&amp;A</span></h2><h3><span id="1-残差神经网络为什么可以缓解梯度消失">1、残差神经网络为什么可以缓解梯度消失？</span></h3><p><strong>残差单元可以以跳层连接的形式实现，即将单元的输入直接与单元输出加在一起，然后再激活</strong>。因此残差网络可以轻松地用主流的自动微分深度学习框架实现，直接使用BP算法更新参数损失对某低层输出的梯度，被分解为了两项。</p>
<h5><span id="1从前后向信息传播的角度来看">（1）<strong>从前后向信息传播的角度来看</strong></span></h5><p><strong>普通神经网络前向传播</strong>。前向传播将数据特征逐层抽象，最终提取出完成任务所需要的特征/表示。</p>
<p><img src="https://www.zhihu.com/equation?tex=a%5E%7Bl_2%7D+%3D+F%28a%5E%7Bl_2-1%7D%29+%3D+F%28F%28a%5E%7Bl_2-2%7D%29%29+%3D+...+%5C%5C" alt="[公式]"></p>
<p><strong>普通神经网络反向传播</strong>。梯度涉及两层参数交叉相乘，可能会在离输入近的网络中产生梯度消失的现象。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_2%7D%7D++%5Cfrac%7B%5Cpartial+a%5E%7Bl_2%7D%7D%7B%5Cpartial+a%5E%7Bl_1%7D%7D++%3D+%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_2%7D%7D++%5Cfrac%7B%5Cpartial+a%5E%7Bl_2%7D%7D%7B%5Cpartial+a%5E%7Bl_2-1%7D%7D+...%5Cfrac%7B%5Cpartial+a%5E%7Bl_1%2B1%7D%7D%7B%5Cpartial+a%5E%7Bl_1%7D%7D++%5C%5C" alt="[公式]"></p>
<p><strong>残差网络前向传播</strong>。输入信号可以从任意低层直接传播到高层。由于包含了一个天然的恒等映射，一定程度上可以解决网络退化问题。</p>
<p><img src="https://www.zhihu.com/equation?tex=a%5E%7Bl_2%7D+%3D+a%5E%7Bl_2-1%7D+%2B++F%28a%5E%7Bl_2-1%7D%29+%3D+%28a%5E%7Bl_2-2%7D+%2B++F%28a%5E%7Bl_2-2%7D%29%29+%2B+F%28a%5E%7Bl_2-1%7D%29+%3D+...+%3D+a%5E%7Bl_1%7D+%2B+%5Csum_%7Bi%3Dl_1%7D%5E%7Bl_2-1%7DF%28a%5Ei%29+%5C%5C" alt="[公式]"></p>
<p><strong>残差网络反向传播</strong>。<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_2%7D%7D" alt="[公式]">表明，反向传播时，错误信号可以不经过任何中间权重矩阵变换直接传播到低层，一定程度上可以缓解梯度弥散问题（即便中间层矩阵权重很小，梯度也基本不会消失）。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_2%7D%7D+++%281+%2B+%5Cfrac%7B%5Cpartial%5Csum_%7Bi%3Dl_1%7D%5E%7Bl_2-1%7DF%28a%5Ei%29%7D%7Ba%5E%7Bl_1%7D%7D%29+%5C%5C" alt="[公式]"></p>
<p>所以可以认为残差连接使得信息前后向传播更加顺畅。</p>
<h5><span id="2集成学习的角度">（2）<strong>集成学习的角度</strong></span></h5><p>将残差网络展开，以一个三层的ResNet为例，可得到下面的树形结构：</p>
<p><img src="https://www.zhihu.com/equation?tex=y_3+%3D+y_2+%2B+f_3%28y_2%29+%3D+%5By_1+%2B+f_2%28y_1%29%5D+%2B+f_3%28y_1+%2B+f_2%28y_1%29%29+%3D+%5By_0+%2B+f_1%28y_0%29+%2B+f_2%28y_0+%2B+f_1%28y_0%29+%29%5D+%2B+f_3%28y_0+%2B+f_1%28y_0%29+%2B+f_2%28y_0+%2B+f_1%28y_0%29+%29%29+%5C%5C" alt="[公式]"></p>
<p><img src="https://pic1.zhimg.com/80/v2-4ec65b70ecd7000389d46cad8c6c8b9d_1440w.jpg?source=d16d100b" alt="img"></p>
<p>残差网络就可以被看作是一系列路径集合组装而成的一个集成模型，其中不同的路径包含了不同的网络层子集。</p>
<h5><span id="特点">特点：</span></h5><ol>
<li>跳层连接</li>
<li>例子：DCN（Deep Cross Network）、Transformer</li>
<li>有效的缓解梯度消失问题的手段</li>
<li>输入和输出维度一致，因为残差正向传播有相加的过程<img src="https://www.zhihu.com/equation?tex=a%5E%7Bl_2%7D+%3D+a%5E%7Bl_2-1%7D+%2B++F%28a%5E%7Bl_2-1%7D%29" alt="[公式]"></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/K4HD8V/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/K4HD8V/" class="post-title-link" itemprop="url">模型训练（1）loss不下降</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-06 20:13:08 / 修改时间：20:42:56" itemprop="dateCreated datePublished" datetime="2022-07-06T20:13:08+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="神经网络模型训练集和测试集loss不下降原因汇总">神经网络模型训练集和测试集loss不下降原因汇总</span></h2><h3><span id="一-训练的时候-loss-不下降">一、<strong>训练的时候 loss 不下降</strong></span></h3><ul>
<li><strong>模型结构问题</strong>。当模型结构不好、规模小时，模型对数据的拟合能力不足。</li>
<li>训练时间问题。不同的模型有不同的计算量，当需要的计算量很大时，耗时也会很大</li>
<li><strong>权重初始化问题</strong>。常用的初始化方案有全零初始化、正态分布初始化和均匀分布初始化等，合适的初始化方案很重要，之前提到过<strong><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/OcEqqQq59a-djZqBsh_7Zw">神经网络初始化为0可能会带来的影响</a></strong></li>
<li><strong>正则化问题</strong>。L1、L2以及Dropout是为了防止过拟合的，当训练集loss下不来时，就要考虑一下是不是正则化过度，导致模型欠拟合了。正则化相关可参考<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/418228948">正则化之L1 &amp; L2</a></strong></li>
<li><strong>激活函数问题</strong>。全连接层多用ReLu，神经网络的输出层会使用sigmoid 或者 softmax。激活函数可参考<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/455086947">常用的几个激活函数</a></strong>。在使用Relu激活函数时，当每一个神经元的输入为负时，会使得该神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。</li>
<li><strong>优化器问题</strong>。优化器一般选取Adam，但是当Adam难以训练时，需要使用如SGD之类的其他优化器。常用优化器可参考<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/446357922">机器学习中常用的优化器有哪些？</a></strong></li>
<li><strong>学习率问题</strong>。学习率决定了网络的训练速度，但学习率不是越大越好，当网络趋近于收敛时应该选择较小的学习率来保证找到更好的最优点。所以，我们需要手动调整学习率，首先选择一个合适的初始学习率，当训练不动之后，稍微降低学习率。</li>
<li><strong><font color="red"> 梯度消失和爆炸。</font></strong>这时需要考虑激活函数是否合理，网络深度是否合理，可以通过调节sigmoid -&gt; relu，假如残差网络等，相关可参考<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442914336">为什么神经网络会有梯度消失和梯度爆炸问题？如何解决？</a></strong></li>
<li><strong>batch size过小</strong>，会导致模型损失波动大，难以收敛，过大时，模型前期由于梯度的平均，导致收敛速度过慢。</li>
<li>数据集问题。（1）数据集未打乱，可能会导致网络在学习过程中产生一定的偏见（2）噪声过多、标注有大量错误时，会导致神经网络难以学到有用的信息，从而出现摇摆不定的情况，<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/434532885">噪声、缺失值、异常值</a></strong>（3）数据类别不均衡使得少数类别由于信息量不足，难以学到本质特征，样本不均衡相关可以看<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/411613746">样本不均衡及其解决办法</a></strong>。</li>
<li><strong>特征问题</strong>。特征选择不合理，会使网络学习难度增加。之前有提到过特征选择的文章，<strong><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/WO_NCTt1O1tgYkZV4vnd7g">如何找到有意义的组合特征</a></strong>,<strong><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/CN7AIhoU3SKOK_Sej6i5Jw">特征选择方法</a></strong></li>
</ul>
<h3><span id="二-测试的时候-loss-不下降">二、<strong>测试的时候 loss 不下降</strong></span></h3><blockquote>
<p>  训练的时候过拟合导致效果不好 </p>
</blockquote>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/437803892">交叉检验</a></strong>，通过交叉检验得到较优的模型参数;</li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/434089082">特征选择</a></strong>，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;</li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/418228948">正则化</a></strong>，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;</li>
<li>如果有正则项则可以考虑增大正则项参数<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]">;</li>
<li>增加训练数据可以有限的避免过拟合;</li>
<li>Bagging ,将多个弱学习器Bagging 一下效果会好很多，比如随机森林等.</li>
<li>早停策略。本质上是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据。</li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410867062">DropOut策略</a></strong>。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2E809GM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2E809GM/" class="post-title-link" itemprop="url">深度学习-NLP-命名实体识别</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-03 22:24:04 / 修改时间：22:42:05" itemprop="dateCreated datePublished" datetime="2022-07-03T22:24:04+08:00">2022-07-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>91</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="命名实体识别-bilstm-crf模型">命名实体识别-BiLSTM-CRF模型</span></h2><blockquote>
<p>  最通俗易懂的BiLSTM-CRF模型中的CRF层介绍 - 孙孙的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44042528">https://zhuanlan.zhihu.com/p/44042528</a></p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/31GH3TV/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/31GH3TV/" class="post-title-link" itemprop="url">高级威胁发现（5）Log2vec: A Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats within Enterprise</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-03 17:46:43" itemprop="dateCreated datePublished" datetime="2022-07-03T17:46:43+08:00">2022-07-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-19 15:55:27" itemprop="dateModified" datetime="2023-04-19T15:55:27+08:00">2023-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/" itemprop="url" rel="index"><span itemprop="name">学术前沿</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="log2vec-a-heterogeneous-graph-embedding-based-approach-for-detecting-cyber-threats-within-enterprise"><strong>Log2vec: A Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats within Enterprise</strong></span></h2><blockquote>
<p>  【顶会论文解读】Log2vec 基于异构图Heterogeneous Graph 检测网络空间威胁 - 笑个不停的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/275952146">https://zhuanlan.zhihu.com/p/275952146</a></p>
<p>  <strong>AIops博客</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/markaustralia/category_11284226.html">https://blog.csdn.net/markaustralia/category_11284226.html</a></p>
<p>  <a target="_blank" rel="noopener" href="https://randy.blog.csdn.net/article/details/111773951?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-111773951-blog-118577347.pc_relevant_multi_platform_whitelistv1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-111773951-blog-118577347.pc_relevant_multi_platform_whitelistv1&amp;utm_relevant_index=2">基于深度学习的日志数据异常检测</a></p>
</blockquote>
<p>发表在<strong>CCS2019</strong>会议的一篇应用异质图embedding进行企业内部网络空间威胁检测的文章。</p>
<h3><span id="摘要">摘要</span></h3><p>内部人员的攻击以及APT攻击是组织常见的攻击类型，现有的检测算法多基于行为检测，大部分方法考虑log日志的序列关系以及用户的行为序列，忽略了其他的关系导致不使用于丰富多样的攻击场景。<strong>本文提出的Log2vec模型，将日志转化为异质图，将日志学习为低纬度的embedding并使用检测算法进行攻击检测（由于攻击样本较少，因此使用聚类算法进行检测）。</strong></p>
<h3><span id="一-引言">一、引言</span></h3><p>现有方法一般会转换用户的各种操作（也包括日志条目）分成序列，这些序列可以保存信息，例如 日志条目之间的顺序关系，然后使用序列处理技术，例如 深度学习从过去的事件中学习并预测下一个事件。<strong>本质上，这些日志条目级别的方法可以模拟用户的正常行为行为并将其偏离标记为异常。</strong></p>
<p>但是，这种方法忽略了其他关系。 例如，<strong>比较用户的日常行为是常规内部威胁检测的一种常用方法</strong>。 此检测基于以下前提：用户的日常行为在一段时间内（几天之间的逻辑关系）相对规则。 上述预测方法忽略了这种关系，并且会降低其性能。 此外，他们需要正常的日志条目，甚至需要大量标记数据来进行模型训练。 但是，在现实世界中，存在罕见的攻击动作，从而限制了其正确预测的能力。</p>
<p><strong>对于检测内部威胁以及APT攻击来说，我们面临三个问题：</strong></p>
<p>（1）<strong>如何同时检测上述两种攻击情形，特别是考虑到检测系统中提到的所有三种关系（日志之间序列关系 sequantial relationship among log entries、几天之内的逻辑关系logical relationship among days以及交互关系interactive relationship among hosts）；</strong></p>
<p><strong>解决办法</strong>：构建异构图来表示前面提到的三种关系；</p>
<p>（2）<strong>如何在APT场景中进行细粒度的检测，尤其是深入挖掘和分析主机内日志条目之间的关系；</strong></p>
<p><strong>解决办法</strong>：将日志条目分为五个属性。根据这些属性，我们深入考虑了主机内日志之间的关系，并设计了精细的规则来关联它们。这种设计使正常和异常日志条目可以在这种图中拥有不同的拓扑；</p>
<p>（3）<strong>如何针对训练模型进行无攻击样本的检测。【内部威胁】</strong></p>
<p><strong>解决办法</strong>：log2vec的图嵌入和检测算法将日志条目表示并分组到不同的群集中，而没有攻击样本，适用于数据不平衡的情况（针对问题3）。此外，图形嵌入本身可以自动学习每个操作的表示形式（矢量），而无需手动提取特定于领域的特征，从而独立于专家的知识。我们的改进版本可以进一步差分提取并表示来自上述异构图的操作之间的多个关系。</p>
<h3><span id="二-文章的设计">二、<strong>文章的设计</strong></span></h3><p><strong>Log2vec包含以下三个组件：</strong></p>
<ul>
<li><p><strong>图的构造。</strong> Log2vec构造一个异构图以集成日志之间的多个关系条目;</p>
</li>
<li><p><strong>图嵌入</strong>（也是图表示学习）。这是一种强大的图形处理方法，可用于了解每个操作的</p>
<p>表示（向量）基于它们在这种图中的关系。通过矢量化用户操作，可以直接比较他们的操作</p>
<p>找出异常的相似之处；</p>
</li>
<li><p><strong>检测算法，</strong>无监督，要有效将恶意操作分组为单个群集</p>
</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555029.png" alt="image-20220703183858975" style="zoom:50%;"></p>
<h3><span id="三-概述">三、概述</span></h3><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555735.png" alt="image-20220703184011385" style="zoom:50%;"></p>
<p>如图2（a）所示，<strong>日志记录了用户的操作，如登录操作，设备之间移动操作以及网络操作</strong>。图2（b）描述日志的属性，<strong>主体（user），操作类型（visit或者send），客体（网址或者是邮件）以及时间和主机（设备ID）</strong>。实际上，日志的属性能够反映用户的行为，比如，第一个登录的时间和日志结束的时间能够反映用户的工作时长。系统管理员会频繁的登录服务器和操作对系统进行维护。</p>
<p>图2（c）将a中显示的<strong>日志组成序列</strong>，然后使用如LSTM的方法学习日志序列信息进行预测。此类模型能够捕捉日志的因果信息和序列关系。但是，这样的方法忽视了其他关系，比如（a）中day3序列，有大量的设备连接和文件复制的操作，远比以前多（这意味着数据泄露）。 可以通过直接比较用户的日常行为来检测出这种差异。进行比较的前提是，在一段时间内（几天之间的逻辑关系），用户的日常行为相对规则且相似。 虽然是深度学习，例如 LSTM可以记住序列的长期依赖关系（多天），它没有明确比较用户的日常行为，也无法获得令人满意的性能。 类似地，它们不能保持图2d中主机之间的另一种关系，交互关系，并且不能在APT检测中正常工作。 另外，其中一些需要大量标记数据进行培训。 但是，在我们的检测方案中，存在罕见的攻击行为。</p>
<p>图2d展示了图2a中的登录（红色字体）的图表，<strong>该图表指示用户在主机之间的行为</strong>。 我们可以分析主机之间的这些交互关系，以发现异常登录。 例如，管理员可以定期登录到一组主机以进行系统维护，而APT实施者只能访问他可以访问的主机。 登录跟踪的功能可以捕获这种差异。例如，良性跟踪中涉及的主机数量（1或3，实线）通常不同于APT（2，虚线）。在分析了这些功能之后，可以识别出受感染的主机。 但是，这些主机包含许多良性操作，并且手动提取的特定于域的特征显然无法应用于图2c中的攻击。</p>
<p>本文构建的模型用于检测以下类型的攻击： 第一种情况是<strong>内部人员滥用职权执行恶意操作，例如访问数据库或应用程序服务器，然后破坏系统或窃取知识产权以谋取个人利益</strong>。 其次，<strong>恶意内部人员通过窥视或密钥记录器获取其他合法用户的凭据，并利用此新身份来寻找机密信息或在公司中造成混乱</strong>。 这两种情况属于内部员工的典型攻击。 第三种攻击是<strong>APT参与者破坏了系统中的主机，并从该主机中持续破坏了多个主机，以提升其特权并窃取机密文件。</strong></p>
<h3><span id="四-详细论述"><strong>四、详细论述</strong></span></h3><h4><span id="41-图构建">4.1 图构建</span></h4><p><strong><font color="red"> 本文构建的图的节点是日志，只有一种节点类型，边是通过规则来建立联系的，本文提出10条规则，代表着图有10种边类型。因此，本文构建的异构图只是边类型不同，节点类型是相同的。</font></strong></p>
<p><strong>定义：<sub,obj,a,t,h> 提取日志的五个主要属性：subject, object, operation type, time and host，称为元属性</sub,obj,a,t,h></strong></p>
<ul>
<li>sub表示用户集合；</li>
<li>obj表示客体集合（文件、移动存储设备、网站）；</li>
<li>A是操作类型集合（文件操作和网页利用）；</li>
<li>T表示时间</li>
<li>H是主机（计算机或者是服务器）。</li>
</ul>
<p><strong>sub,obj,A,H有自己的子属性</strong>。比如，用户在服务器中写入文件，sub的属性包括用户角色（系统管理员等）和所属单位；obj属性包括文件类型和大小；H属性包括是文件服务器还是邮件服务器；对于登录操作，A的属性包括身份验证。对于用户登录，可以表示为user (sub) logs in to (A) a destination host (obj) in a source one (H),</p>
<h4><span id="42-构建图的规则">4.2 <strong>构建图的规则</strong></span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555928.png" alt="image-20220703193951973" style="zoom:50%;"></p>
<p>上图，R1-3描述一天中的因果关系和序列关系；R4-6描述多天之间的逻辑关系；R7 R9描述用户登录和web浏览行为序列。R8 R10按照逻辑关系进行关联。</p>
<p><strong>log2vec考虑三种关系：</strong></p>
<ul>
<li>causal and sequential relationships within a day; 一天内的因果关系和顺序关系；</li>
<li>logical relationships among days；日之间的逻辑关系</li>
<li>logical relationships among objects；对象之间的逻辑关系。</li>
</ul>
<p>在设计有关这三种关系的规则时，我们考虑这些元属性的不同组合，以关联较少的日志条目，并将更精细的日志关系映射到图中。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555332.png" alt="image-20220703194402679" style="zoom:50%;"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/160TFHT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/160TFHT/" class="post-title-link" itemprop="url">集成学习（6）CatBoost</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-02 15:21:08" itemprop="dateCreated datePublished" datetime="2022-07-02T15:21:08+08:00">2022-07-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 21:25:03" itemprop="dateModified" datetime="2023-04-21T21:25:03+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="深入理解catboost">深入理解CatBoost</span></h2><blockquote>
<p>  深入理解CatBoost - Microstrong的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102540344">https://zhuanlan.zhihu.com/p/102540344</a></p>
</blockquote>
<p><strong>本文主要内容概览：</strong></p>
<p><img src="https://pic1.zhimg.com/v2-f6a9520c6db0ba77ad620800cb36c054_b.jpg" alt="img" style="zoom: 67%;"></p>
<h3><span id="一-catboost简介"><strong>一、CatBoost简介</strong></span></h3><p>CatBoost是俄罗斯的搜索巨头Yandex在2017年开源的机器学习库，是Boosting族算法的一种。CatBoost和XGBoost、LightGBM并称为GBDT的三大主流神器，都是在GBDT算法框架下的一种改进实现。XGBoost被广泛的应用于工业界，LightGBM有效的提升了GBDT的计算效率，而Yandex的CatBoost号称是比XGBoost和LightGBM在算法准确率等方面表现更为优秀的算法。</p>
<p>CatBoost是一种基于对称决策树（oblivious trees）为基学习器实现的参数较少、支持类别型变量和高准确性的GBDT框架，主要解决的痛点是高效合理地处理类别型特征，这一点从它的名字中可以看出来，<strong>CatBoost是由Categorical和Boosting组成。此外，CatBoost还解决了梯度偏差（Gradient Bias）以及预测偏移（Prediction shift）的问题，从而减少过拟合的发生，进而提高算法的准确性和泛化能力。</strong></p>
<p><strong>与XGBoost、LightGBM相比，CatBoost的创新点有：</strong></p>
<ul>
<li><strong>嵌入了自动将类别型特征处理为数值型特征的创新算法。首先对categorical features做一些统计，计算某个类别特征（category）出现的频率，之后加上超参数，生成新的数值型特征（numerical features）。</strong></li>
<li><strong>Catboost还使用了组合类别特征，可以利用到特征之间的联系，这极大的丰富了特征维度</strong>。</li>
<li>采用排序提升的方法对抗训练集中的噪声点，从而避免梯度估计的偏差，进而解决预测偏移的问题。</li>
<li>采用了<strong>完全对称树作为基模型</strong>。</li>
</ul>
<h3><span id="二-类别型特征">二、<strong>类别型特征</strong></span></h3><p><strong>所谓类别型特征，即这类特征不是数值型特征，而是离散的集合</strong>，比如省份名（山东、山西、河北等），城市名（北京、上海、深圳等），学历（本科、硕士、博士等）。在梯度提升算法中，最常用的是将这些类别型特征转为数值型来处理，一般类别型特征会转化为一个或多个数值型特征。</p>
<p><strong>类别型特征基数比较低（low-cardinality features）</strong>，即该特征的所有值去重后构成的集合元素个数比较少，一般利用One-hot编码方法将特征转为数值型。One-hot编码可以在数据预处理时完成，也可以在模型训练的时候完成，从训练时间的角度，后一种方法的实现更为高效，CatBoost对于基数较低的类别型特征也是采用后一种实现。</p>
<p><strong>高基数类别型特征（high cardinality features）</strong>当中，比如 <code>user ID</code>，这种编码方式会产生大量新的特征，造成维度灾难。一种折中的办法是可以将类别分组成有限个的群体再进行One-hot编码。<strong>一种常被使用的方法是根据目标变量统计（Target Statistics，以下简称TS）进行分组</strong>，目标变量统计用于估算每个类别的目标变量期望值。甚至有人直接用TS作为一个新的数值型变量来代替原来的类别型变量。<strong><font color="red"> 重要的是，可以通过对TS数值型特征的阈值设置，基于对数损失、基尼系数或者均方差，得到一个对于训练集而言将类别一分为二的所有可能划分当中最优的那个。</font></strong></p>
<p>在LightGBM当中，类别型特征用每一步梯度提升时的梯度统计（Gradient Statistics，以下简称GS）来表示。虽然为建树提供了重要的信息，但是这种方法有以下两个缺点：</p>
<ul>
<li>增加计算时间，因为需要对每一个类别型特征，在迭代的每一步，都需要对GS进行计算；</li>
<li>增加存储需求，对于一个类别型变量，需要存储每一次分离每个节点的类别；</li>
</ul>
<p><strong>为了克服这些缺点，LightGBM以损失部分信息为代价将所有的长尾类别归为一类</strong>，作者声称这样处理高基数类别型特征时比One-hot编码还是好不少。不过如果采用TS特征，那么对于每个类别只需要计算和存储一个数字。</p>
<p>因此，采用TS作为一个新的数值型特征是最有效、信息损失最小的处理类别型特征的方法。TS也被广泛应用在点击预测任务当中，这个场景当中的类别型特征有用户、地区、广告、广告发布者等。接下来我们着重讨论TS，暂时将One-hot编码和GS放一边。</p>
<h4><span id="21-目标变量统计target-statistics">2.1 <strong>目标变量统计（Target Statistics）</strong></span></h4><p><strong><font color="red"> CatBoost算法的设计初衷是为了更好的处理GBDT特征中的categorical features</font></strong>。在处理 GBDT特征中的categorical features的时候，最简单的方法是用 categorical feature 对应的标签的平均值来替换。在决策树中，标签平均值将作为节点分裂的标准。<strong>这种方法被称为 Greedy Target-based Statistics , 简称 Greedy TS</strong>，用公式来表达就是：</p>
<script type="math/tex; mode=display">
\hat{x}_k^i=\frac{\sum_{j=1}^n\left[x_{j, k}=x_{i, k}\right] \cdot Y_i}{\sum_{j=1}^n\left[x_{j, k}=x_{i, k}\right]}</script><p>这种方法有一个显而易见的缺陷，就是通常特征比标签包含更多的信息，<strong><font color="red"> 如果强行用标签的平均值来表示特征的话，当训练数据集和测试数据集数据结构和分布不一样的时候会出条件偏移问题。</font></strong></p>
<p>一个标准的改进 Greedy TS的方式是添加先验分布项，这样可以减少噪声和低频率类别型数据对于数据分布的影响：</p>
<script type="math/tex; mode=display">
\hat{x}_k^i=\frac{\sum_{j=1}^{p-1}\left[x_{\sigma_{j, k}}=x_{\sigma_{p, k}}\right] Y_{\sigma_j}+a \cdot p}{\sum_{j=1}^{p-1}\left[x_{\sigma_{j, k}}=x_{\sigma_{p, k}}\right]+a}</script><p> 其中$p$是添加的先验项， $a$通常是大于 0 的权重系数。添加先验项是一个普遍做法，针对类别数较少的特征，它可以减少噪声数据。对于回归问题，一般情况下，先验项可取数据集label的均值。对于二分类，先验项是正例的先验概率。利用多个数据集排列也是有效的，但是，如果直接计算可能导致过拟合。CatBoost利用了一个比较新颖的计算叶子节点值的方法，这种方式(oblivious trees，对称树)可以避免多个数据集排列中直接计算会出现过拟合的问题。</p>
<p>当然，在论文《CatBoost: unbiased boosting with categorical features》中，还提到了其它几种改进Greedy TS的方法，分别有：Holdout TS、Leave-one-out TS、Ordered TS。我这里就不再翻译论文中的这些方法了，感兴趣的同学可以自己翻看一下原论文。</p>
<h4><span id="22-特征组合">2.2 <strong>特征组合</strong></span></h4><p>值得注意的是几个类别型特征的任意组合都可视为新的特征。例如，在音乐推荐应用中，我们有两个类别型特征：用户ID和音乐流派。如果有些用户更喜欢摇滚乐，将用户ID和音乐流派转换为数字特征时，根据上述这些信息就会丢失。结合这两个特征就可以解决这个问题，并且可以得到一个新的强大的特征。然而，组合的数量会随着数据集中类别型特征的数量成指数增长，因此不可能在算法中考虑所有组合。为当前树构造新的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=分割点&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">分割点</a>时，CatBoost会采用贪婪的策略考虑组合。对于树的第一次分割，不考虑任何组合。对于下一个分割，CatBoost将当前树的所有组合、类别型特征与数据集中的所有类别型特征相结合，并将新的组合类别型特征动态地转换为数值型特征。CatBoost还通过以下方式生成数值型特征和类别型特征的组合：树中选定的所有分割点都被视为具有两个值的类别型特征，并像类别型特征一样被进行组合考虑。</p>
<h4><span id="23-catboost处理categorical-features总结">2.3  <strong>CatBoost处理Categorical features总结</strong></span></h4><ul>
<li><strong>首先计算一些数据的statistics。计算某个category出现的频率，加上超参数，生成新的numerical features</strong>。这一策略要求同一标签数据不能排列在一起（即先全是0之后全是1这种方式），训练之前需要打乱数据集。</li>
<li>使用数据的不同排列（实际上是4个）。在每一轮建立树之前，先扔一轮骰子，决定使用哪个排列来生成树。</li>
<li>考虑使用categorical features的不同组合。例如颜色和种类组合起来，可以构成类似于blue dog这样的特征。当需要组合的categorical features变多时，CatBoost只考虑一部分<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=combinations&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">combinations</a>。在选择第一个节点时，只考虑选择一个特征，例如A。在生成第二个节点时，考虑A和任意一个categorical feature的组合，选择其中最好的。就这样使用贪心算法生成combinations。</li>
<li><strong>除非向gender这种维数很小的情况，不建议自己生成One-hot编码向量，最好交给算法来处理。</strong></li>
</ul>
<h3><span id="三-catboostqampa">三、CatboostQ&amp;A</span></h3><h4><span id="31-catboost与xgboost-lightgbm的联系与区别">3.1 <strong>CatBoost与XGBoost、LightGBM的联系与区别？</strong></span></h4><p>（1）2014年3月XGBoost算法首次被<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=陈天奇&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">陈天奇</a>提出，但是直到2016年才逐渐著名。2017年1月微软发布LightGBM第一个稳定版本。2017年4月Yandex开源CatBoost。自从XGBoost被提出之后，很多文章都在对其进行各种改进，CatBoost和LightGBM就是其中的两种。</p>
<p>（2）<strong>CatBoost处理类别型特征十分灵活，可直接传入类别型特征的列标识，模型会自动将其使用One-hot编码，还可通过设置 one_hot_max_size参数来限制One-hot特征向量的长度</strong>。如果不传入类别型特征的列标识，那么CatBoost会把所有列视为数值特征。对于One-hot编码超过设定的one_hot_max_size值的特征来说，CatBoost将会使用一种高效的encoding方法，与mean encoding类似，但是会降低过拟合。处理过程如下：</p>
<ul>
<li>将输入样本集随机排序，并生成多组随机排列的情况；</li>
<li>将浮点型或属性值标记转化为整数；</li>
<li>将所有的类别型特征值结果都根据以下公式，转化为数值结果；</li>
</ul>
<script type="math/tex; mode=display">
avg_target =\frac{\text { countInClass }+ \text { prior }}{\text { totalCount }+1}</script><p>其中 countInClass 表示在当前类别型特征值中有多少样本的标记值是1；prior 是分子的初始值，根据初始参数确定。totalCount 是在所有样本中（包含当前样本）和当前样本具有相同的类别型特征值的样本数量。</p>
<p>LighGBM 和 CatBoost 类似，也可以通过使用特征名称的输入来处理类别型特征数据，它没有对数据进行独热编码，因此速度比独热编码快得多。LighGBM 使用了一个特殊的算法来确定属性特征的分割值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = lgb.Dataset(data, label=label, feature_name=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>], categorical_feature=[<span class="string">&#x27;c3&#x27;</span>])</span><br><span class="line"><span class="comment"># 注意，在建立适用于 LighGBM 的数据集之前，需要将类别型特征变量转化为整型变量，此算法不允许将字符串数据传给类别型变量参数。</span></span><br></pre></td></tr></table></figure>
<p>（3）XGBoost 和 CatBoost、 LighGBM 算法不同，XGBoost 本身无法处理类别型特征，而是像随机森林一样，只接受数值数据。因此在将类别型特征数据传入 XGBoost 之前，必须通过各种编码方式：例如序号编码、独热编码和二进制编码等对数据进行处理。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/34HK4X9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/34HK4X9/" class="post-title-link" itemprop="url">风控算法（5）数据挖掘-手机App数据挖掘实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-01 19:35:48" itemprop="dateCreated datePublished" datetime="2022-07-01T19:35:48+08:00">2022-07-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-09 17:01:04" itemprop="dateModified" datetime="2022-07-09T17:01:04+08:00">2022-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/" itemprop="url" rel="index"><span itemprop="name">应用场景</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E4%B8%9A%E5%8A%A1%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">业务安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="风控数据手机app数据挖掘实践思路">风控数据—手机App数据挖掘实践思路</span></h2><h3><span id="引言"><strong>引言</strong></span></h3><p>作为移动互联网时代的主要载体，智能手机逐渐成为人们日常生活中不可或缺的一部分，改变着人们的生活习惯。比如，可以用“饿了么”点外卖，“支付宝”可以用来种树，“抖音”可以用来上厕所……强大的App给我们的生活带来了巨大的便利。</p>
<p><img src="https://pic3.zhimg.com/80/v2-87f00e7e18438ad4bf65c09feb5f38e6_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>正因为如此，App与用户之间存在着密不可分的联系，用户在频繁使用这些App过程中也积累了大量的个人历史数据。<strong><font color="red"> 这些App数据能帮助我们更好地去理解用户，推测用户的性别、职业、收入、兴趣、偏好等属性</font></strong>，也就是所谓的<strong>KYC（Know your customer）</strong>。</p>
<p>在风控中，App数据也有其重要价值，常用于反欺诈、风控建模特征工程等。本文将分享App数据的一些挖掘思路，以及实践建议。</p>
<p><strong>首先，让我们思考下几个问题：</strong></p>
<ul>
<li>如何获取数据？</li>
<li>数据长啥样？</li>
<li>数据如何和业务相结合去理解？</li>
<li>可以采用什么算法实现高效提取信息？</li>
<li>如何利用这块数据服务业务？</li>
</ul>
<h3><span id="一-app数据长啥样">一、 App数据长啥样？</span></h3><p>根据资料显示，当前手机App数据主要包括：<strong>App安装包名称、App中文名、App安装列表、App安装序列。</strong></p>
<p><strong>为便于区分，常把App中文名记为app_name，App包名称（package）记为pkg_name。其中pkg_name是App的唯一ID</strong>，app_name则因为下载渠道、版本更新、数据采集等因素影响导致不唯一。例如，”企业微信”的pkg_name为“com.tencent.wework”，而app_name可能会有“企业微信”、“微信企业版”、“微信（企业版）”等多个值。</p>
<p>至于如何获取手机App package？可以参考这里：<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/gufenchen/article/details/91410667">实现获取appPackage和appActivity的方法</a></p>
<p><img src="https://pic3.zhimg.com/80/v2-e7c62877c3193cec175852170ee3eae2_1440w.jpg" alt="img"></p>
<ol>
<li><strong>App安装集合（App List</strong>）：指手机上安装的所有App的集合，一般用逗号隔开，如：“com.alibaba.android.rimet,com.tencent.mm,com.citiccard.mobilebank,com.icbc,com.hongxin1.rm”。可以认为是一个集合，因此是<strong>无序</strong>的。</li>
<li><strong>App安装序列（App Seq）</strong>：指手机上包含安装时间的App序列。如：[“ 信”,”1558014854044”,”<a href="https://link.zhihu.com/?target=http%3A//com.tencent.mm/">com.tencent.mm</a>“,,”7.0.4”]，分别代表App中文名、App安装时间戳、App包名称和版本号。由于可根据安装时间戳得到安装顺序，因此是<strong>有序</strong>的。</li>
</ol>
<p>我们拿“微信”的package在腾讯应用宝中检索，那么就可以找到以下<strong>App描述数据</strong>：</p>
<ol>
<li><strong>分类标签</strong>：标签精确表达了App的核心功能。但可能是开发者在发布App时从可选项中主观选择了一个标签，也有可能腾讯会在后期维护标签。标签不一定准确，但可作为一个重要的参考维度。</li>
<li><strong>下载量</strong>：可作为判断App是否小众的一个参考维度。然而，能在应用宝上架的App一般是合规的；对于一些质量较差无法上架的app就无法获取到下载量。</li>
<li><strong>应用描述</strong>：开发者对App的主要功能给出的描述性文本，可提取关键词、主题等内容。但如果只是根据关键词匹配，很容易出错。比如微信中藏有游戏中心入口，文本中出现“游戏”关键词，但这并不是一个游戏类App。</li>
</ol>
<h3><span id="二-app如何有效分类">二、App如何有效分类？</span></h3>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
