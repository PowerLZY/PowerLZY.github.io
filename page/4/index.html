<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/4/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">255</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">52</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1799EFH/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1799EFH/" class="post-title-link" itemprop="url">机器学习（8）【Nan】CRF</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-18 21:00:40 / 修改时间：22:17:32" itemprop="dateCreated datePublished" datetime="2022-07-18T21:00:40+08:00">2022-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">概率图模型</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1QB4KAB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1QB4KAB/" class="post-title-link" itemprop="url">工业落地-蚂蚁安全-柳星《FXY：Security-Scenes-Feature-Engineering-Toolkit》</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-15 13:40:50 / 修改时间：13:55:29" itemprop="dateCreated datePublished" datetime="2022-07-15T13:40:50+08:00">2022-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/" itemprop="url" rel="index"><span itemprop="name">工业落地</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/%E9%AB%98%E7%BA%A7%E5%A8%81%E8%83%81%E5%8F%91%E7%8E%B0/" itemprop="url" rel="index"><span itemprop="name">高级威胁发现</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="fxysecurity-scenes-feature-engineering-toolkit">FXY：<em>Security-Scenes-Feature-Engineering-Toolkit</em></span></h2>
<blockquote>
<p>https://github.com/404notf0und/FXY/blob/master/docs/%E9%9C%80%E6%B1%82%E5%92%8C%E8%AE%BE%E8%AE%A1.md</p>
</blockquote>
<h3><span id="介绍">介绍</span></h3>
<p>FXY是一款特征工程框架，用于安全场景中数据预处理、数据预分析、数据特征化和向量化等任务。FXY这个名字一方面代表这款工具的目的是从原始安全数据中获取Feature
X和Feature
Y用于对接人工智能算法，另一方面寓意着人工智能的本质，函数Y=F(X)。FXY的特性是支持多种安全场景多种安全数据的预处理和特征化，内置多种NLP通用特征提取方法，内置脚本扩展支持二次开发。</p>
<h3><span id="需求">需求</span></h3>
<p>无论机器学习、深度学习还是强化学习应用在哪个领域，<strong>其处理流程主要有五个环节：问题-&gt;数据-&gt;特征化-&gt;算法-&gt;结果</strong>，数据的数字化，狭义的来说是数据的特征化，在整个流程中起到了承上启下的关键作用，承上，<strong>特征化的好坏直接反映了对问题本质的理解深入与否</strong>，启下，作为算法的输入，一定程度上决定了最终结果的天花板。这是FXY定位于安全场景下特征工程环节的一点原因。另一点原因是考虑到算法环节的不确定性因素和确定性因素，不确定性因素导致难以形成统一的范式，确定性因素导致问题已被解决。就算法的应用来说，机器学习算法、深度学习算法和超参数众多，在同一特征化方法下，难以客观比较不同算法的性能，并且找到泛化性强的SOTA算法。<strong>就算法本身来说，现有的框架tensorflow、keras等对算法的封装已经很完美了，重复造轮子意义不大。如果给算法环节盖上安全场景的帽子，问题依然如此，这是FXY不选择定位于安全场景下算法环节的原因。</strong></p>
<h3><span id="架构设计">架构设计</span></h3>
<p><img src="https://i.imgur.com/d2Rq9hc.png"></p>
<p>因为机器学习解决安全问题的流程固定为安全问题-&gt;数据-&gt;数字化-&gt;算法-&gt;结果，<strong>具体到FXY的架构设计，从下到上依次是安全场景层-&gt;数据的数据层-&gt;数据清洗层-&gt;特征层-&gt;算法层-&gt;API层</strong>，对应的FXY各模块层次结构依次为内置函数模块-&gt;数据预处理模块-&gt;特征工程模块-&gt;tensorflow/keras-&gt;控制器模块。</p>
<p>扩展可扩展的，因为安全场景较多且杂，完全不可能用一种或少数几种特征方法解决所有问题，想到的一种解决方式是针对安全问题做特征方法的插件化扩展，把每个安全问题对应每个CMS，每个feature
engineering方法对应每个POC，那么就可以像写CMS
POC一样专注于安全场景的底层数据feature engineering。</p>
<h3><span id="集成">集成</span></h3>
<p>笔者Github上AI-for-Security-Learning仓库专注于知识，而此FXY仓库专注于工具，现依赖前者仓库，笔者开始二刷，站在前人的肩膀上，不断集成优质方法到FXY框架中，此框架不做未知的创新。现已集成4种安全场景，4种特征工程方法，四种安全场景分别是<strong>LSTM识别恶意HTTP请求@cdxy，AI-Driven-WAF@exp-db，Phishing
URL
Classification@surajr，使用深度学习检测XSS@Webber，基于深度学习的恶意样本行为检测@ApplePig<span class="citation" data-cites="360云影实验室">@360云影实验室</span></strong>，四种特征工程方法分别是<strong>钓鱼url的统计特征，恶意url和恶意软件api的词典索引特征，恶意url的TF-IDF特征，xss的word2vec词嵌入向量</strong>。</p>
<p>在二刷并集成的过程中，需要彻底读懂原作者的文章思路和代码，然后改写到FXY限定的框架中，学到了很多。同时输出一份二刷笔记，里面不但包括已集成代码到框架中的原文理解，还包括一些暂时无法集成的文章的理解，文档化记录了原作者用到的安全场景、解决的思路、数据的构成、数据预处理方法、特征的方法、使用的模型、有无监督分类，二刷笔记持续更新。</p>
<h3><span id="潜在问题">潜在问题</span></h3>
<p><strong>FXY框架专注于安全问题、数据和特征化三个环节</strong>:这其中数据环节存在数据源难获取的问题，有些文章中的数据属于公司级数据不会开源，较难获取，这导致只能使用开源数据集或自己采集数据集复现原作者的实验，集成并测试框架，虽说不会影响FXY框架的预处理、预分析和特征化等主要功能，但这会导致数据环节数据本身的价值变小，一定程度上减小了FXY框架的价值，因为可能大多数人遇到的问题不是没有方法，而是没有数据，数据本身的价值和数据分析的价值都很高，前者价值甚至大于后者。而CMS的POC框架可以靠各种搜索引擎和爬虫来获取数据源，输送给POC脚本，就不会存在此问题。</p>
<p>这促使我们是不是可以通过爬虫爬取更多异源开源数据，用开源弥补闭源，或是本地搭建环境采集数据，缓解数据源缺失的问题，从而使FXY框架的价值不只在于数据分析，更在于数据集本身，采集数据集是个脏活累活，在规划中。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/H1YMKT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/H1YMKT/" class="post-title-link" itemprop="url">工业落地-阿里云恶意软件检测平台</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-15 13:22:12 / 修改时间：13:26:00" itemprop="dateCreated datePublished" datetime="2022-07-15T13:22:12+08:00">2022-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/" itemprop="url" rel="index"><span itemprop="name">工业落地</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/%E6%81%B6%E6%84%8F%E8%BD%AF%E4%BB%B6%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">恶意软件检测</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>984</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-阿里云恶意文件检测平台">一、阿里云恶意文件检测平台</span></h2>
<p>Linux沙箱 ｜
阿里云恶意文件检测平台开放Linux二进制文件检测：https://www.anquanke.com/post/id/276349#10006-weixin-1-52626-6b3bffd01fdde4900130bc5a2751b6d1</p>
<h3><span id="11-简介">1.1 简介</span></h3>
<p>在病毒检测方向，一直以来常见的两种手段就是<strong>静态特征检测</strong>与<strong>动态行为检测</strong>。两者各有优势与不足，<strong>静态特征检测方案实施成本更低，检出结果也更精准，但是其泛化能力不足</strong>，针对具有高级对抗能力的恶意文件显得力不从心，并且天然地处于被动的地位，人力运营成本会更高。</p>
<p>动态行为检测实施成本相对较高，需要有足够的资源，而且检出的结果在一定程度上不如静态检测精准，但是它最大的优势是可以从<strong>恶意行为、技术手段</strong>的角度识别恶意文件，具有极大的泛化能力。对于需要检测大量恶意文件的安全厂商来说，人工运营所有样本并提取静态特征是不现实的，而沙箱的作用也就显现出来了：在海量的文件中，识别出最值得关注的恶意文件。</p>
<h3><span id="12-沙箱优势">1.2 沙箱优势</span></h3>
<h4><span id="高性能的环境仿真">高性能的环境仿真</span></h4>
<p><strong>云沙箱依托于阿里云神龙架构，在具备高性能的仿真的同时，还支持资源池化和自动化运维的能力</strong>。利用自定义的虚拟化技术和定制的沙箱OS内核，对恶意样本使用的反虚拟化的技术具备天然的对抗能力，配合上专门打造的二进制检测探针，可以在安全、高效、仿真的隔离环境中对二进制进行深度的行为分析。</p>
<h4><span id="全面的动态行为分析">全面的动态行为分析</span></h4>
<p><strong>基于虚拟化构建的沙箱深度分析技术，对进程、文件、网络、敏感系统调用、rootkit、漏洞利用等进行全面监控</strong>，配合智能模型规则检测引擎，快速分析出样本潜在的恶意行为。</p>
<h4><span id="海量的数据积累">海量的数据积累</span></h4>
<p>阿里云沙箱服务于阿里云安全云上恶意文件检测，积累了海量样本数据，提炼出大量有独检优势的行为检测规则。</p>
<h3><span id="33-检测优势">3.3 检测优势</span></h3>
<h4><span id="算法模型覆盖未知威胁">算法模型——覆盖未知威胁</span></h4>
<p>基于阿里云平台海量样本数据和强劲计算能力，<strong>采用“机器智能(神经网络)”与“专家智能(行为标签、ATT&amp;CK)”结合的智能安全思想</strong>，挖掘海量样本数据中可疑内容信息和行为标签威胁值，构建智能的威胁检测模型发现新威胁。</p>
<p>将专家知识与海量数据结合智能化构建以<strong>ATT&amp;CK为核心的多模态特征表示</strong>，对样本行为从技术战术度量、敏感信息表征、意图逻辑推理等角度进行多维度刻画、分析，同时依赖机器智能的学习泛化能力、检测模型能覆盖更多的未知，拓展威胁发现边界。</p>
<figure>
<img src="https://p2.ssl.qhimg.com/t019938cd9a1afcd563.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1VGG9SJ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1VGG9SJ/" class="post-title-link" itemprop="url">深度学习-GNN（7）【Nan】HetGNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-11 17:57:32 / 修改时间：19:07:40" itemprop="dateCreated datePublished" datetime="2022-07-11T17:57:32+08:00">2022-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/GNN/" itemprop="url" rel="index"><span itemprop="name">GNN</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="hetgnn">HetGNN</span></h2>
<blockquote>
<p>HetGNN --异构图处理 - wlkq的文章 - 知乎
https://zhuanlan.zhihu.com/p/411528472</p>
<p>异构图embedding学习，舍HetGNN其谁？ - 李杰的文章 - 知乎
https://zhuanlan.zhihu.com/p/392367843</p>
</blockquote>
<p>图嵌入领域，同构图算法大行其道，如：DeepWalk、Node2vec、GCN、GraphSage、GAT。但实际业务场景中异构图居多，除了经典的MetaPath2vec、RGCN，貌似其他选项并不多，今天就为大家介绍一款异构图嵌入学习神器---HetGNN。</p>
<h3><span id="hetgnn简介">HetGNN简介</span></h3>
<p>提出的网络名称：<strong>HetGNN（Heterogeneous Graph Neural
Network），2019 SIGKDD</strong></p>
<p>核心理念：heterogeneous structural graph information + <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=heterogeneous&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22392367843%22%7D">heterogeneous</a>
attributes or contents for each node</p>
<h3><span id="异构图嵌入学习">异构图嵌入学习</span></h3>
<p>异构图算法相较同构图算法有如下三方面挑战：</p>
<ul>
<li>异质图中的大多数节点并不会连接所有类型的其他节点。如academic
graph中user节点不会直接连到venue（论文）节点上。另外说节点能够连接的邻居数也不一样。大部分GNN直接聚合邻居（一阶）节点信息，而远处传过来的节点信息会随着距离而减弱。hub节点会被弱关联的邻居节点扰乱信息，冷启动的节点会因为邻居不足而导致不能充分表示。<strong>那么问题1就是：如何对异质图上的每个节点采样到强相关的邻居节点呢？（这边我认为一般都是用了注意力机制了）</strong></li>
<li>每个节点都带有非结构化的属性特征，如text、image，常用的从concatenate或者linear
transformation不能建模节点属性间的deep
interaction。<strong>那么问题2就是：如何设计异质图上节点属性的encoder（编码器）来处理不同节点内容异质性问题。</strong></li>
<li>不同类型的邻居节点对生成节点embedding的贡献也不一样。例如在academic
graph，author和paper节点对author的embedding的影响会强如venue，venue节点包含不同的主题，具有更一般的嵌入，而大部分gnn集中在同质图的处理上，也没有考虑这种不同类型节点的影响。
<strong>挑战3是:如何通过考虑不同节点类型的影响来聚合异构邻居的特征信息。</strong>【<strong>自注意力机制</strong>】</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1WZGAE8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1WZGAE8/" class="post-title-link" itemprop="url">高级威胁发现（6）UNICORN: Provenance-Based Detector for APTs</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-10 21:02:53" itemprop="dateCreated datePublished" datetime="2022-07-10T21:02:53+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-19 15:58:24" itemprop="dateModified" datetime="2023-04-19T15:58:24+08:00">2023-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/" itemprop="url" rel="index"><span itemprop="name">学术前沿</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="ndss20unicorn-provenance-based-detector-for-apts">NDSS20
UNICORN: Provenance-Based Detector for APTs</span></h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg5MTM5ODU2Mg==&amp;mid=2247492967&amp;idx=1&amp;sn=60f14977758cc7d1e8504446422e5ec0&amp;chksm=cfcf55aaf8b8dcbc86935b76e7f36961202174ecba1df597fee9d44428c607e80d41add1b2f9&amp;scene=178&amp;cur_album_id=1776483007625822210#rd">[AI安全论文]
06.NDSS20 UNICORN: Provenance-Based Detector for APTs</a></p>
</blockquote>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555641.png" alt="图片">
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<blockquote>
<p>原文作者：Xueyuan Han, Thomas Pasquier, Adam Bates, James Mickens and
Margo Seltzer 原文标题：UNICORN: Runtime Provenance-Based Detector for
Advanced Persistent Threats
原文链接：https://arxiv.org/pdf/2001.01525.pdf 发表会议：NDSS
2020参考文献：感谢两位老师
https://blog.csdn.net/Sc0fie1d/article/details/104868847
https://blog.csdn.net/xjxtx1985/article/details/106473928</p>
</blockquote>
<h3><span id="摘要">摘要</span></h3>
<p>本文提出的<strong>UNICORN是一种基于异常的APT检测器</strong>，可以有效利用数据<strong>Provenance进行分析</strong>。通过广泛且快速的图分析，使用<strong>graph
sketching技术</strong>，UNICORN可以在长期运行的系统中分析Provenance
Graph，从而识别未知慢速攻击。其中，Provenance
graph提供了丰富的上下文和历史信息，实验证明了其先进性和较高准确率。</p>
<p>由于APT（Advanced Persistent
Threats）攻击具有缓慢可持续的攻击模式以及频繁使用0-day漏洞的高级特性使其很难被检测到。<strong>本文利用数据来源分析（provenance）提出了一种基于异常的APT检测方法，称为UNICORN。</strong></p>
<ul>
<li>从建模到检测，UNICORN专门针对APT的独有特性（low-and-slow、0-Days）设计。</li>
<li>UNICONRN利用高效的图分析方法结合溯源图丰富的上下文语义和历史信息，在没有预先设定攻击特征情况下识别隐蔽异常行为。</li>
<li>通过图概要（graph
sketching）技术，它有效概括了长时间系统运行来对抗长时间缓慢攻击。</li>
<li>UNICONRN使用一种新的建模方法来更好地捕捉长期行为规律，以提高其检测能力。</li>
</ul>
<p>最后通过大量实验评估表明，本文提出的方法优于现有最先进的APT检测系统，并且在真实APT环境中有较高的检测精度。</p>
<h3><span id="一-引言">一、引言</span></h3>
<p>APT攻击现在变得越来越普遍。这种攻击的时间跨度长，且与传统攻击行为有着本质的区别。APT攻击者的目的是获取特定系统的访问控制，并且能够长期潜伏而不被发现。攻击者通常使用0-day漏洞来获取受害者系统的访问控制。</p>
<p><strong>传统检测系统通常无法检测到APT攻击。</strong></p>
<ul>
<li>依赖恶意软件签名的检测器对利用新漏洞的攻击无效。</li>
<li>基于异常检测的系统通常分析一系列的系统调用或日志系统事件，其中大部分方法无法对长期行为进行建模。</li>
<li>由于基于异常检测的方法只能检测系统调用和事件的短序列很容易被绕过。</li>
</ul>
<p>综上，当前针对APT攻击的检测方法很少能成功。攻击者一旦使用0-Day漏洞，防御者便无计可施；而基于系统调用和系统事件的检测方法，由于数据过于密集，这些方法难以对长时间的行为模式进行建模。<strong>因此，数据溯源（data
provenance）是一种检测APT更合适的数据。</strong></p>
<p>最近的研究成果表明数据溯源是一个很好的APT检测数据源。数据溯源将系统执行表示成一个有向无环图（DAG），该图描述了系统主体（如进程）和对象（文件或sockets）之间的信息流。即使跨了很时间，在图中也把因果相关的事件关联到一起。因此，即使遭受APT攻击的系统与正常系统比较类似，但是溯源图中丰富的上下文语义信息中也可以很好地区分正常行为与恶意行为。</p>
<p><strong>然而，基于数据溯源的实时APT检测依然具有挑战。</strong>
随着APT攻击的渗透的进行，数据溯源图的规模会不断增大。其中必要的上下文分析需要处理大量图中的元素，而图上的分析通常复杂度比较高。当前基于数据溯源的APT检测方法根据已有的攻击知识通过简单的边匹配实现APT检测，无法处理未知的APT攻击。基于溯源的异常检测系统主要是基于图模型的邻域搜索，利用动态或静态模型识别正常行为模式。理论上关联的上下文越丰富越好，但是实际中由于图分析的复杂性较高限制了其可行性。</p>
<ul>
<li>Provenance
Graph的分析是相当耗费计算资源，因为APT是可持续攻击，图的规模也会越来越大</li>
</ul>
<p><strong><font color="red">
当前APT检测系统面临如下三种问题：</font></strong></p>
<ul>
<li>静态模型难以捕获长时间的系统行为；</li>
<li>low-and-slow
APT投毒攻击：由于APT高级可持续的特性可以在系统中潜伏很长时间，相关的行为会被认为是正常行为，这样的攻击会影响检测模型；</li>
<li>在主存内进行计算的方法，应对长期运行的攻击表现不佳。</li>
</ul>
<p>基于此，本文提出了UNICORN，使用graph
sketching来建立一个增量更新、固定大小的纵向图数据结构。这种纵向性质允许进行广泛的图探索，使得UNICORN可以追踪隐蔽的入侵行为。而固定大小和增量更新可以避免在内存中来表示provenance
graph，因此UNICORN具有可扩展性，且计算和存储开销较低。UNICORN在训练过程中直接对系统的行为进行建模，但此后不会更新模型，从而防止模型的投毒攻击。</p>
<p><strong><font color="red"> 本文的主要贡献如下：</font></strong></p>
<ul>
<li>针对APT攻击特性提出一种<strong>基于Provenance的异常检测系统</strong>。</li>
<li><strong>引入一种新的基于概要的（sketch-based）、时间加权的（time-weighted）溯源编码</strong>，该编码非常紧凑且可处理长时间的溯源图。</li>
<li>通过模拟和真实的APT攻击来评估UNICORN，证明其能高精度检测APT活动。</li>
<li><strong><font color="red"> 实现代码开源。</font></strong></li>
</ul>
<h3><span id="二-背景">二、背景</span></h3>
<h4><span id="21-系统调用追踪的挑战"><strong>2.1 系统调用追踪的挑战</strong></span></h4>
<p><strong>系统调用抽象提供了一个简单的接口，用户级应用程序可以通过这个接口请求操作系统的服务</strong>。作为调用系统服务的机制，系统调用接口通常也是攻击者入侵的入口点。因此，系统调用跟踪一直被认为是入侵检测的实际信息源。然而：</p>
<ul>
<li>当前的攻击检测系统是对非结构化的系统调用的审记日志进行分析，但捕获的系统调用杂乱分散，传统基于异常检测的思路无法处理APT。因此需要将其关联成data
provenance，基于溯源的方法是将历史上下文数据都编码到因果关系图中。</li>
<li>数据溯源方法已经被应用到攻击调查中，已经有一些方法能够根据审计数据构建系统溯源图用以实现对系统执行过程的建模。然而这些方法依然存在一些局限：(1)
这种事后构建很难保证溯源图的正确性，由于系统调用问题存大量并发，溯源图的完整性与可靠性无法保证；(2)
容易被绕过；(3) 时空复杂度较高。</li>
<li>由于一些内核线程不使用系统调用，因此<strong>基于Syscall生成的Provenance是一些分散的图，而不是一张系统运行状况的完整图</strong></li>
</ul>
<h4><span id="22-全系统追踪溯源">2.2 <strong>全系统追踪溯源</strong></span></h4>
<p><strong>全系统溯源运行在操作系统层面，捕获的是所有系统行为和它们之间的交互</strong>。通过捕获信息流和因果关系，即使攻击者通过操作内核对象来隐藏自己的行踪也无济于事。</p>
<p>本文使用CamFlow，采用了Linux安全模块（Linux Security
Modules，LSM）框架来确保高效可靠的信息流记录。LSM可以消除race
condition。</p>
<blockquote>
<p><strong>CamFlow：溯源搜集系统</strong>，参考官网
https://camflow.org/。</p>
<p><strong>CamFlow
将系统的执行表示为有向无环图</strong>。图中的顶点表示内核对象（例如线程、文件、套接字等）的状态，关系表示这些状态之间的信息流。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555346.png" alt="CamFlow 图表概览" style="zoom: 33%;"></p>
<p>在上面的示例中<code>process 1</code>克隆<code>process 2</code>。
<code>process 2</code>写到一个<code>pipe</code>。
<code>process 1</code>同读<code>pipe</code>。创建版本是为了保证非周期性并代表信息的正确排序（有关详细信息，请参阅我们的<a target="_blank" rel="noopener" href="http://camflow.org/publications/ccs-2018.pdf">CCS'18
论文</a>）。</p>
</blockquote>
<h3><span id="23-问题描述">2.3 问题描述</span></h3>
<p>现有基于数据溯源的APT攻击检测方法主要存在如下缺陷：</p>
<ul>
<li>预定义的边匹配规则过于敏感，很难检测到APT攻击中的0-Day漏洞；</li>
<li>溯源图的近邻约束导致其只能提供局部上下文信息（而非whole-system），然而这会影响相关异常检测精度；</li>
<li>系统行为模型难以检测APT：静态模型无法捕获长期运行的系统的行为；动态模型容易遭受中毒攻击；</li>
<li>溯源图的存储与计算都是在内存中，在执行长期检测上有局限性。</li>
</ul>
<p><strong>UNICORN可以解决如上问题，其本质是把APT检测问题看成大规模、带有属性的实时溯源图异常检测问题。在任何时间，从系统启动到其当前状态捕获的溯源图都将与已知正常行为的溯源图进行比较。如果有明显差别，那么就认为该系统正在遭受攻击。</strong></p>
<p>对于APT检测来说，理想基于溯源的IDS应该如下：</p>
<ul>
<li>充分利用溯源图的丰富上下文，以时间与空间有效的方法持续分析溯源图；</li>
<li>在不假设攻击行为的基础上，应考虑系统执行的整个持续时间；</li>
<li>只学习正常行为的变化，而不是学习攻击者指示的变化。</li>
</ul>
<h3><span id="三-威胁模型">三、威胁模型</span></h3>
<p>假设主机入侵检测有适当的场景：攻击者非法获得对系统的访问权限，并计划在不被检测的情况下驻留在系统中很长一段时间。<strong>攻击者可能分阶段执行攻击，在每个阶段还会使用大量的攻击技术</strong>。UNICORN的目标是通过解决主机生成的溯源来实现在所有阶段对APT攻击进行检测。本文假设，我们假设在受到攻击之前，UNICORN在正常运行期间会完全观察主机系统，并且在此初始建模期间不会发生攻击。</p>
<p>数据收集框架的完整性是UNICORN正确性的核心，因此<strong>我们假定所使用的CamFlow中，LSM完整性是可信的。同时，本文假设内核、溯源数据和分析引擎的正确性，我们重点关注UNICORN的分析能力。</strong></p>
<h3><span id="四-系统设计">四、系统设计</span></h3>
<p>独角兽是一个基于主机的入侵检测系统，能够同时检测在网络主机集合上的入侵。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555615.png" alt="图片" style="zoom: 67%;"></p>
<ol type="1">
<li><strong>以一个带标签的流式溯源图作为输入</strong>。该图由CamFlow生成，每条边是带属性的。溯源系统构建一个具有偏序关系的DAG溯源图，能实现有效的流式计算和上下文分析。</li>
<li><strong>建立一个运行时的内存直方图</strong>。UNICORN有效构建一个流式直方图，该直方图表示系统执行的历史，如果有新边产生则实时更新直方图的计数结果。通过迭代的探索大规模图的近邻关系，发现了在上下文环境中系统实体的因果关系。该工作是UNICORN的第一步，具体来说，<strong>直方图中每个元素描述了图中唯一的一个子结构</strong>，同时考虑了子结构中的顶点与边上的异构标签，以及这些边的时间顺序。APT攻击缓慢的渗透攻击目标系统，希望基于的异常检测方法最终忘记这一行为，把其当成正常的系统行为，但是APT攻击并不能破坏攻击成功的相关信息流依赖关系。</li>
<li><strong>定期计算固定大小的概要图（graph
sketch）</strong>。在纯流式环境，当UNICORN对整个溯源进行汇总时，唯一直方图元素的数量可能会任意增长。这种动态变化导致两个直方图之间的相似计算变得非常有挑战，从而使得基于直方图相似计算的建模以及检测算法变的不可行。<strong>UNICORN采用相似度保存的hash技术把直方图转换成概要图。概要图可以增量维护，也意味着UNICORN并不需要将整个溯源图都保存在内存中。</strong>另外，<strong>概要图保存了两个直方图之间的jaccard相似性，这在后续图聚类分析中特别有效。</strong></li>
<li><strong>将简略图聚类为模型</strong>。UNICORN可以在没有攻击知识的前提下实现APT攻击检测。与传统的聚类方法不同，UNICORN利用它的流处理能力生成一个动态演化模型。该模型通过在其运行的各个阶段对系统活动进行聚类捕获单个执行中的行为改变，但是UNICORN无法在攻击者破坏系统时动态实时修改模型。因此，它更适合APT攻击这类长期运行的攻击。</li>
</ol>
<h4><span id="41-溯源图">4.1 溯源图</span></h4>
<p>最近几年溯源图在攻击分析中越来越流行，并且本身固有的特别可以有效的用于APT检测。溯源图挖掘事件之间的因果关系，因果关系有助于对时间跨度较远的事件进行推理分析，因此有助于在检测APT相关攻击。</p>
<p>UNICORN根据两个系统执行的溯源图的相似性还判定两个系统的行为相似性。而且UNICORN总是考虑整个溯源来检测长期持续的攻击行为。<strong>当前已经有许多图相似度计算方法，然而这些算法大部分是NPC的，即使多项式时间复杂度的算法也无法满足整个溯源图快速增涨的需求。</strong></p>
<h4><span id="42-构建graph直方图">4.2 构建Graph直方图</span></h4>
<p>本文方法的目标是有效对溯源图进行比较分析，同时容忍正常执行中的微小变化。对于算法，我们有两个标准：</p>
<ul>
<li>图表示应考虑长期的因果关系；</li>
<li>必须能够在<strong>实时流图数据</strong>上实现该算法，以便能够在入侵发生时阻止入侵（不仅仅是检测到入侵）。</li>
</ul>
<p><strong><font color="red">
本文基于一维WL同构检验，采用了线性时间的、快速的Weisfeiler-Lehman（WL）子树图核算法。该算法的使用依赖于构造的顶点直方图的能力，需要直方图能捕捉每个顶点周围的结构信息。</font></strong>根据扩充的顶点标签对顶点进行分类，这些标签完全描述了顶点的领域，并且<strong>通过迭代的标签传播来构造这些扩展的顶点标签</strong>。</p>
<p>同构性的WL检验及其子树kernel变化，以其对多种图的判别能力而闻名，超越了许多最新的图学习算法（例如，图神经网络）。对Weisfeiler-Lehman（WL）子树图核的使用取决于我们构建顶点直方图的能力，捕获围绕每个顶点的图结构。我们根据增强顶点标签对顶点进行分类，标签描述了顶点的R-hop邻居。</p>
<p>为了简单说明，假设有一个完整静态图，重标记对所有的输入标签的聚合。对每个顶点都重复执行这个过程来实现对n跳邻居的描述。一旦为图中的每个顶点都构建了扩展标签，那么就可以基于此生成一个直方图，其中每个bucket表示一个标签。两个图的相似性比较是基于以下假设：两个图如果相似那么在相似的标签上会有相似的分布。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555022.png" alt="图片" style="zoom: 67%;"></p>
<p>我们的目标是构建一个直方图，图中的每个元素对应一个唯一的顶点标签，用于捕获顶点的R-hop的in-coming邻居。</p>
<p><strong>信息流的多样性与复杂性（Streaming Variant and
Complexity）</strong>。算法1只有新顶点出现或是新边出现对其邻顶点有影响时才会执行。本文方法只需要为每条新边更新其目标顶点的邻域。UNICORN采用这种偏序关系来最小化计算代价。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555986.png" alt="图片" style="zoom: 67%;"></p>
<p><strong>直方图元素的概念漂移问题</strong>。APT攻击场景需要模型必须能够处理长期运行行为分析能力，而系统行为的动态变化会导致溯源图的统计信息也随之变化，这种现象就叫概念漂移（concept
drift）。</p>
<p>UNICORN通过对直方图元素计数使用指数权重衰减来逐渐消除过时的数据（逐渐忘记机制），从而解决了系统行为中的此类变化。它分配的权重与数据的年龄成反比。</p>
<p><span class="math display">\[
H_h=\sum_t \mathbb{1}_{x_t=h}
\]</span></p>
<p><strong>入侵检测场景中的适用性</strong>。上述“逐渐忘记”的方法，使得UNICORN可以着眼于当前的系统执行动态，而且那些与先前的object/activity有关系的事件不会被忘记。</p>
<h4><span id="43-生成概要图graph-sketches">4.3 生成概要图（Graph Sketches）</span></h4>
<p>Graph直方图是描述系统执行的简单向量空间图统计量。然而，与传统的基于直方图的相似性分析不同，UNICORN会随着新边的到来不断更新直方图。另外，UNCORN会根据图特征的分布来计算相似性，而不是利用绝对统计值。</p>
<p><strong><font color="red"> 本文采用locality sensitive
hashing，也称作similarity-preserving data
sketching。UNICORN的部署采用了前人的研究成果HistoSketch，该方法是一种基于一致加权采样的方法，且时间得性是常数。</font></strong></p>
<h4><span id="44-学习进化模型">4.4 学习进化模型</span></h4>
<p>在给定graph
sketch和相似性度量的情况下，聚类是检测离群点常用的数据挖掘手段。<strong>然而传统的聚类方法无法捕获系统不断发展的行为</strong>。UNICORN利用其流处理的能力，创建了进化模型，可以捕获系统正常行为的变化。更重要的是，模型的建立是在训练阶段完成的，而不是在部署阶段，因为部署阶段训练模型可能会遭受中毒攻击。</p>
<p><strong><font color="red">
UNICORN在训练期间创建一个时序sketches，然后使用著名的K-medods算法从单个服务器对该概要序列进行聚类，使用轮廓系数（silhouette
coefficient）确定最佳K值。</font></strong>每个簇表示系统执行的元状态（meta-states），如启动、初始化、稳定状态。然后UNICORN使用所有簇中sketches的时间顺序和每个簇的统计量（如直径、medoid），来生成系统进化的模型。</p>
<blockquote>
<ul>
<li>更新C中的每一列，即类中心 <img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]">
,对于第j类，<strong>中心 <img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]">
需要通过遍历所有该类中的样本，取与该类所有样本距离和最小的样本为该中心。</strong></li>
</ul>
<p>K-means 模型: <span class="math inline">\(\min _{G, C} \sum_{i=1}^n
\sum_{j=1}^k g_{i j}\left\|x_i-c_j\right\|_2\)</span> 算法流程：</p>
<ul>
<li>固定C,更新 G</li>
<li>更新C中的每一列, 即类中心 <span class="math inline">\(c_j\)</span>,其通过计算第j类中样本的平均值得到</li>
</ul>
<p>K-mediods:模型： <span class="math inline">\(\min _{G, C \subseteq X}
\sum_{i=1}^n \sum_{j=1}^k g_{i j}\left\|x_i-c_j\right\|_1 \quad\)</span>
可以是曼哈顿距离或其它距离度量;由于类中心的更新规
则，该方法较之于K-means更鲁棒。 算法流程:</p>
<ul>
<li>固定C,更新 G</li>
<li>更新C中的每一列，即类中心<span class="math inline">\(c_{j}\)</span>,对于第j类，<strong>中心<span class="math inline">\(c_{j}\)</span>需要通过遍历所有该类中的样本，取与该类所有样本距离和最小的样本为该中心。</strong></li>
</ul>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191556090.png" alt="图片" style="zoom: 67%;"></p>
<p><strong>对于每个训练实例，UNICORN创建一个模型，该模型捕获系统运行时执行状态的更新。直观地说，这类似于跟踪系统执行状态的自动机。</strong>最终的模型由训练数据中所有种源图的多个子模型组成。</p>
<h4><span id="45-异常检测">4.5 异常检测</span></h4>
<p>在部署期间，异常检测遵循前面章节中描述的相同流模式。UNICORN周期性地创建graph
sketch，因为直方图从流式溯源图演变而来。给定一个概要图，UNICORN将该概要与建模期间学习的所有子模型进行比较，将其拟合到每个子模型中的一个聚类中。</p>
<p><strong>UNICORN假设监视从系统启动开始，并跟踪每个子模型中的系统状态转换。要在任何子模型中为有效，概要必须适合当前状态或下一个状态；否则，被视为异常。因此，我们检测到两种形式的异常行为：</strong></p>
<ul>
<li>不符合现有聚类的概要</li>
<li>聚类之间的无效转换</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/P9Y8FT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/P9Y8FT/" class="post-title-link" itemprop="url">特征工程（1）【draft】数据清洗</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-08 17:48:29" itemprop="dateCreated datePublished" datetime="2022-07-08T17:48:29+08:00">2022-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:18:45" itemprop="dateModified" datetime="2023-04-26T14:18:45+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>543</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-特征工程-数据清洗">一、特征工程-数据清洗</span></h3>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304261223770.jpg" alt="img" style="zoom: 67%;"></p>
<p>数据格式内容错误数据来源有多种，有些是传感器采集，然后算法提取的特征数据；有些是采集的控制器的数据；还有一些应用场合，则是用户/访客产生的，数据肯定存在格式和内容上不一致的情况，所以在进行模型构建之前需要先进行数据的格式内容清洗操作。逻辑错误清洗主要是通过简单的逻辑推理发现数据中的问题数据，防止分析结果走偏，主要包含以下几个步骤：</p>
<p><strong><em>1.数据去重，去除或替换不合理的值；</em></strong></p>
<p><strong><em>2.去除或重构不可靠的字段值（修改矛盾的内容）；</em></strong></p>
<p><strong><em>3.去除异常点数据。</em></strong></p>
<h3><span id="二-采样">二、采样</span></h3>
<p>随机采样方法整理与讲解（MCMC、Gibbs Sampling等） - 向阳树的文章 -
知乎 https://zhuanlan.zhihu.com/p/109978580</p>
<h3><span id="三-参考文献">三、参考文献</span></h3>
<ul>
<li><p>特征工程 - 未来达摩大师的文章 - 知乎
https://zhuanlan.zhihu.com/p/476659737</p></li>
<li><p>这9个特征工程使用技巧，解决90%机器学习问题！ -
Python与数据挖掘的文章 - 知乎
https://zhuanlan.zhihu.com/p/462744763</p></li>
<li><p>有哪些精彩的特征工程案例？ - 京东科技风险算法与技术的回答 - 知乎
https://www.zhihu.com/question/400064722/answer/1308358333</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/67Q9G2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/67Q9G2/" class="post-title-link" itemprop="url">风控算法（4）特征工程-时间滑窗统计特征体系</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-08 14:52:55 / 修改时间：15:00:37" itemprop="dateCreated datePublished" datetime="2022-07-08T14:52:55+08:00">2022-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/" itemprop="url" rel="index"><span itemprop="name">应用场景</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/%E4%B8%9A%E5%8A%A1%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">业务安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>855</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="风控特征时间滑窗统计特征体系">风控特征—时间滑窗统计特征体系</span></h2>
<h4><span id="风控业务背景"><strong>风控业务背景</strong></span></h4>
<p>俗话说，路遥知马力，日久见人心。在风控中也是如此，我们常<strong>从时间维度提取借款人在不同时间点的特征</strong>，以此来判断借款人的风险。在实践中，这类特征通常会占到80%以上。由于是<strong>通过时间切片和聚合统计函数来构造，因此一般被称为时间滑窗统计特征。</strong></p>
<h3><span id="一-观察期-观察点及表现期">一、<strong>观察期、观察点及表现期</strong></span></h3>
<p>理解这三者的概念是风控建模前期样本准备的基础，在此简单介绍。</p>
<ul>
<li><strong>观察点（</strong>Observation
Point<strong>）</strong>：并非是一个具体的时间点，而是一个时间区间，表示的是客户申请贷款的时间。在该时间段申请的客户<strong>可能</strong>会是我们用来建模的样本
。（提示：为什么用“可能”这个描述，因为还需剔除一些强规则命中的异常样本，这部分样本将不会加入建模）</li>
<li><strong>观察期</strong>（Observation
Window）：用以<strong>构造特征X</strong>的时间窗口。相对于观察点而言，是<strong>历史</strong>时间。观察期的选择依赖于用户数据的厚薄程度。通常数据越厚，可提取的信息也就越全面、可靠。</li>
<li><strong>表现期</strong>（Performance
Window）：定义<strong>好坏标签Y</strong>的时间窗口。相对于观察点而言，是<strong>未来</strong>时间。由于风险需要有一定时间窗才能表现出来，因此信贷风险具有<strong>滞后性</strong>。表现期的长短可以通过Vintage分析和滚动率分析来确定，在此不做展开。
<img src="https://pic3.zhimg.com/80/v2-c47416a557f573a72acccb00ec5a37fe_1440w.jpg" alt="img"></li>
</ul>
<p>表现期越长，信用风险暴露将越彻底，但意味着观察期离当前将越远，用以提取样本特征的历史数据将越陈旧，建模样本和未来样本的差异也越大。反之，表现期越短，风险还未暴露完全，但好处是能用到更近的样本。</p>
<h3><span id="二-rfm模型介绍">二、<strong>RFM模型介绍</strong></span></h3>
<p><strong>RFM模型最早是用来衡量客户价值和客户创利能力</strong>。理解RFM框架的思想是构造统计类特征的基础，其含义为：</p>
<ul>
<li><strong>R（Recency）</strong>：客户最近一次交易消费时间的间隔。R值越大，表示客户交易发生的日期越久，反之则表示客户交易发生的日期越近。</li>
<li><strong>F（Frequency）</strong>：客户在最近一段时间内交易消费的次数。F值越大，表示客户交易越频繁，反之则表示客户交易不够活跃。</li>
<li><strong>M（Monetary）</strong>：客户在最近一段时间内交易消费的金额。M值越大，表示客户价值越高，反之则表示客户价值越低。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3NAHDTK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3NAHDTK/" class="post-title-link" itemprop="url">模型训练（2）梯度消失&爆炸</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-06 20:40:21" itemprop="dateCreated datePublished" datetime="2022-07-06T20:40:21+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-14 18:05:06" itemprop="dateModified" datetime="2022-07-14T18:05:06+08:00">2022-07-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="缓解梯度消失amp爆炸">缓解梯度消失&amp;爆炸</span></h2>
<blockquote>
<p><strong>残差神经网络为什么可以缓解梯度消失？</strong> - 十三的文章 -
知乎 https://zhuanlan.zhihu.com/p/452867110</p>
</blockquote>
<h3><span id="前言-从反向传播推导到梯度消失and爆炸的原因及解决方案"><font color="red">
前言 - 从反向传播推导到梯度消失and爆炸的原因及解决方案？</font></span></h3>
<blockquote>
<ul>
<li>从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导）
- 韦伟的文章 - 知乎 https://zhuanlan.zhihu.com/p/76772734</li>
</ul>
<p><strong>本质上</strong>是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。<strong><font color="red">
梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。</font></strong></p>
</blockquote>
<h3><span id="一-反向传播推导到梯度消失and爆炸的原因及解决方案">一、反向传播推导到梯度消失and爆炸的原因及解决方案</span></h3>
<h4><span id="11-反向传播推导">1.1 ==反向传播推导：==</span></h4>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以上图为例开始推起来，先说明几点，i1，i2是输入节点，h1，h2为隐藏层节点，o1，o2为输出层节点，除了输入层，其他两层的节点结构为下图所示：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>举例说明，<img src="https://www.zhihu.com/equation?tex=NET_%7Bo1%7D" alt="[公式]"> 为输出层的输入，也就是隐藏层的输出经过线性变换后的值，
<img src="https://www.zhihu.com/equation?tex=OUT_%7Bo1%7D" alt="[公式]"> 为经过激活函数sigmoid后的值；同理 <img src="https://www.zhihu.com/equation?tex=NET_%7Bh1%7D" alt="[公式]">
为隐藏层的输入，也就是输入层经过线性变换后的值， <img src="https://www.zhihu.com/equation?tex=OUT_%7Bh1%7D" alt="[公式]">
为经过激活函数sigmoid 的值。只有这两层有激活函数，输入层没有。</p>
<blockquote>
<p><strong>定义一下sigmoid的函数：</strong> <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29+%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D" alt="[公式]"> <strong>说一下sigmoid的求导：</strong></p>
</blockquote>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csigma%5E%7B%5Cprime%7D%28z%29+%26%3D%5Cleft%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D%5Cright%29%5E%7B%5Cprime%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7B-z%7D%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B1%2Be%5E%7B-z%7D-1%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B%5Csigma%28z%29%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Csigma%28z%29%281-%5Csigma%28z%29%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>定义一下损失函数，这里的损失函数是均方误差函数，即：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D%5Csum+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget+-+output%7D%29%5E%7B2%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>具体到上图，就是：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget1+-+out_o1%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget2+-+out_o2%7D%29%5E%7B2%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>到这里，所有前提就交代清楚了，前向传播就不推了，默认大家都会，下面推反向传播。</p>
<ul>
<li><strong>第一个反向传播（热身）</strong></li>
</ul>
<p>先来一个简单的热热身，求一下损失函数对W5的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D" alt="[公式]"></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>首先根据链式求导法则写出对W5求偏导的总公式，再把图拿下来对照（如上），可以看出，需要计算三部分的求导【损失函数、激活函数、线性函数】，下面就一步一步来：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_%7B5%7D%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B2%7D%28%7Btarget_1+-+out_%7Bo1%7D%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28+%7Btarget_2+-+out_%7Bo2%7D%7D%29%5E%7B2%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bo1%7D%7D%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+w_5%7D+%3Dout_%7Bh_1%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>综上三个步骤，得到总公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%28out_%7Bo1%7D+-+target_1%29+%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+out_%7Bh_1%7D++%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<ul>
<li><strong>第二个反向传播：</strong></li>
</ul>
<p>接下来，要求损失函数对w1的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D" alt="[公式]"></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>还是把图摆在这，方便看，先写出总公式，对w1求导有个地方要注意，w1的影响不仅来自o1还来自o2，从图上可以一目了然，所以总公式为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D%2B%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo2%7D%7D%7B%5Cpartial+net_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo2%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>所以总共分为左右两个式子，分别又对应5个步骤，详细写一下左边，右边同理：</p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]"></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3Dw_5%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%3D+%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+i_1w_1%2Bi_2w_2%7D%7B%5Cpartial+w_1%7D+%3Di_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>右边也是同理，就不详细写了，写一下总的公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D++%26%3D%5Cleft%28%28out_%7Bo1%7D+-+target_1%29%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+w_5+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29+%5C%5C+%26%2B%7B%5Cleft%28%28out_%7Bo2%7D+-+target_2%29%5Ccdot+%28%5Csigma%28net_%7Bo2%7D%29%281-%5Csigma%28net_%7Bo2%7D%29%29%29%5Ccdot+w_7+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29%7D+%5C%5C+%5Cend%7Baligned%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>这个公式只是对如此简单的一个网络结构的一个节点的偏导，就这么复杂。。亲自推完才深深的意识到。。。</p>
<p>为了后面描述方便，把上面的公式化简一下， <img src="https://www.zhihu.com/equation?tex=out_%7Bo1%7D+-+target_1" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]"> ，
<img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D" alt="[公式]"> ，则：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+C_%7Bo1%7D+%5Ccdot+%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_5+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1+%2B+C_%7Bo2%7D+%5Ccdot+%5Csigma%28net_%7Bo2%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_7+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="12梯度消失爆炸产生原因">1.2
<strong>==梯度消失，爆炸产生原因：==</strong></span></h4>
<p>从上式其实已经能看出来，求和操作其实不影响，主要是是看乘法操作就可以说明问题，可以看出，损失函数对w1的偏导，与
<img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]">
，权重w，sigmoid的导数有关，明明还有输入i为什么不提？因为如果是多层神经网络的中间某层的某个节点，那么就没有输入什么事了。所以产生影响的就是刚刚提的三个因素。</p>
<p>再详细点描述，如图，多层神经网络：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-0f2ded75fbecc449a25bfd58b8c58d35_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25631496">PENG：神经网络训练中的梯度消失与梯度爆炸282
赞同 · 26 评论文章</a></p>
<p>假设（假设每一层只有一个神经元且对于每一层 <img src="https://www.zhihu.com/equation?tex=y_i%3D%5Csigma%5Cleft%28z_i%5Cright%29%3D%5Csigma%5Cleft%28w_ix_i%2Bb_i%5Cright%29" alt="[公式]">，其中<img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]">为sigmoid函数），如图：</p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-ea9beb6c28c7d4e89be89dc5f4cbae2e_1440w.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>则：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+y_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+y_%7B4%7D%7D%7B%5Cpartial+z_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B4%7D%7D%7B%5Cpartial+x_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B4%7D%7D%7B%5Cpartial+z_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B3%7D%7D%7B%5Cpartial+x_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B3%7D%7D%7B%5Cpartial+z_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B2%7D%7D%7B%5Cpartial+x_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B2%7D%7D%7B%5Cpartial+z_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B1%7D%7D%7B%5Cpartial+b_%7B1%7D%7D%7D+%5C%5C+%7B%3DC_%7By4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B4%7D%5Cright%29+w_%7B4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B3%7D%5Cright%29+w_%7B3%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B2%7D%5Cright%29+w_%7B2%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B1%7D%5Cright%29%7D%5Cend%7Barray%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>看一下sigmoid函数的求导之后的样子：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-208a4aa5dc657fe86919f3549d853793_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>发现sigmoid函数求导后最大最大也只能是0.25。</strong></p>
<p>再来看W，一般我们初始化权重参数W时，通常都小于1，用的最多的应该是0，1正态分布吧。</p>
<p><font color="red"><strong>所以 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq0.25" alt="[公式]">
，多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说几乎不更新，这就是梯度消失的根本原因。</strong></font></p>
<p>再来看看<strong>梯度爆炸</strong>的原因，也就是说如果 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cgeq1" alt="[公式]">
时，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。sigmoid的函数是不可能大于1了，上图看的很清楚，那只能是w了，这也就是经常看到别人博客里的一句话，初始权重过大，一直不理解为啥。。现在明白了。</p>
<p>但梯度爆炸的情况一般不会发生，对于sigmoid函数来说， <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29%5E%7B%5Cprime%7D" alt="[公式]"> 的大小也与w有关，因为 <img src="https://www.zhihu.com/equation?tex=z%3Dwx%2Bb" alt="[公式]">
，除非该层的输入值<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">在一直一个比较小的范围内。</p>
<p>其实<strong>梯度爆炸和梯度消失问题都是因为网络太深</strong>，网络权值更新不稳定造成的，本质上是因为<strong>梯度反向传播中的连乘效应</strong>。</p>
<p>==<strong>所以，总结一下，为什么会发生梯度爆炸和消失：</strong>==</p>
<blockquote>
<p>本质上是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。</p>
</blockquote>
<h3><span id="二-梯度消失-爆炸解决方案">二、梯度消失、爆炸解决方案？</span></h3>
<blockquote>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33006526">DoubleV：详解深度学习中的梯度消失、爆炸原因及其解决方法</a></p>
<ul>
<li>预训练加微调</li>
<li>梯度剪切、正则</li>
</ul>
</blockquote>
<h4><span id="21预训练加微调"><strong>2.1（预训练加微调）：</strong></span></h4>
<p>提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（<strong>fine-tunning</strong>）。</p>
<p>Hinton在训练深度信念网络（Deep Belief
Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p>
<h4><span id="22梯度剪切-正则"><strong>2.2（梯度剪切、正则）：</strong></span></h4>
<p><strong>梯度剪切</strong>这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p>
<p><strong>正则化</strong>是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss%3D%28y-W%5ETx%29%5E2%2B+%5Calpha+%7C%7CW%7C%7C%5E2%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]">
是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</p>
<p>注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些</p>
<h4><span id="23改变激活函数"><strong><font color="red">
2.3（改变激活函数）：</font></strong></span></h4>
<p>首先说明一点，<strong>tanh激活函数不能有效的改善这个问题</strong>，先来看tanh的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Ctanh+%28x%29%3D%5Cfrac%7Be%5E%7Bx%7D-e%5E%7B-x%7D%7D%7Be%5E%7Bx%7D%2Be%5E%7B-x%7D%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>再来看tanh的导数图像：</p>
<p><img src="https://pic4.zhimg.com/80/v2-66a7e4fcf11a2d85c15e7bf7b88b2d1b_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>发现虽然比sigmoid的好一点，sigmoid的最大值小于0.25，tanh的最大值小于1，但仍是小于1的，所以并不能解决这个问题。</strong></p>
<p><strong>Relu</strong>:思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Coperatorname%7BRe%7D+%5Coperatorname%7Blu%7D%28%5Cmathrm%7Bx%7D%29%3D%5Cmax+%28%5Cmathrm%7Bx%7D%2C+0%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D%7B0%2C+x%3C0%7D+%5C%5C+%7Bx%2C+x%3E0%7D%5Cend%7Barray%7D%5Cright%5C%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-55475ee2d90cd7257a39f62549a65769_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>从上图中，我们可以很容易看出，<strong>relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</strong></p>
<p><strong>relu</strong>的主要贡献在于：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练</li>
</ul>
<p>同时也存在一些<strong>缺点</strong>：</p>
<ul>
<li><strong>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</strong></li>
<li>输出不是以0为中心的</li>
</ul>
<p><strong>leakrelu</strong></p>
<p>leakrelu就是为了解决relu的0区间带来的影响，其数学表达为： <img src="https://www.zhihu.com/equation?tex=leakrelu%3D%5Cbegin%7Bequation%7D+f%28x%29%3D+%5Cbegin%7Bcases%7D+x%2C+%26+%7Bx%5Cgt+0%7D+%5C%5C%5C%5C+x%2Ak%2C+%26+%7Bx%5Cleq+0%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]">
其中k是leak系数，一般选择0.1或者0.2，或者通过学习而来解决死神经元的问题。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3ab1bd8fb85542a0c85eb907b73fa327_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>leakrelu解决了0区间带来的影响，而且包含了relu的所有优点</p>
<p><strong>elu</strong></p>
<p>elu激活函数也是为了解决relu的0区间带来的影响，其数学表达为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7Bx%2C%7D+%26+%7B%5Ctext+%7B+if+%7D+x%3E0%7D+%5C%5C+%7B%5Calpha%5Cleft%28e%5E%7Bx%7D-1%5Cright%29%2C%7D+%26+%7B%5Ctext+%7B+otherwise+%7D%7D%5Cend%7Barray%7D%5Cright.%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其函数及其导数数学形式为：</p>
<p><img src="https://pic3.zhimg.com/80/v2-ec3c80e51129bd76d49cad6e52d449c2_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>但是elu相对于leakrelu来说，计算要更耗时间一些，因为有e。</p>
<h4><span id="24batchnorm梯度消失"><strong>2.4（batchnorm）：</strong>【梯度消失】</span></h4>
<p><strong>Batchnorm</strong>是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch
normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。</p>
<p>具体的batchnorm原理非常复杂，在这里不做详细展开，此部分大概讲一下batchnorm解决梯度的问题上。具体来说就是反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：
正向传播中<img src="https://www.zhihu.com/equation?tex=f_3%3Df_2%28w%5ET%2Ax%2Bb%29" alt="[公式]">，那么反向传播中，<img src="https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cpartial+f_2%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+f_1%7Dw" alt="[公式]">，反向传播式子中有w的存在，所以<img src="https://www.zhihu.com/equation?tex=w" alt="[公式]">的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出做scale和shift的方法，通过一定的规范化手段，<strong>把每层神经网络任意神经元这个输入值的分布【假设原始是正态分布】强行拉回到接近均值为0方差为1的标准正太分布，即严重偏离的分布强制拉回比较标准的分布，<font color="red">
这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生</font>，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<h4><span id="25残差结构"><strong><font color="red">
2.5（残差结构）：</font></strong></span></h4>
<p><img src="https://pic4.zhimg.com/80/v2-3134d24348c47ca2001d37fef1c3f8bf_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>如图，把输入加入到某层中，这样求导时，总会有个1在，这样就不会梯度消失了。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot+%5Cfrac%7B%5Cpartial+x_%7BL%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot%5Cleft%281%2B%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D+F%5Cleft%28x_%7Bi%7D%2C+W_%7Bi%7D%5Cright%29%5Cright%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>式子的第一个因子 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D" alt="[公式]"> 表示的损失函数到达 L
的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p>
<p><code>注：上面的推导并不是严格的证</code>，只为帮助理解</p>
<h4><span id="26lstm">==<strong>2.6（LSTM）：</strong>==</span></h4>
<p>在介绍这个方案之前，有必要来推导一下RNN的反向传播，<strong>因为关于梯度消失的含义它跟DNN不一样！不一样！不一样！</strong></p>
<p>先推导再来说，从这copy的：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28687529">沉默中的思索：RNN梯度消失和爆炸的原因565
赞同</a></p>
<p>RNN结构如图：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-ab844e07a86f910d2852198c3117ddb7_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>假设我们的时间序列只有三段， <img src="https://www.zhihu.com/equation?tex=S_%7B0%7D" alt="[公式]">
为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下： <img src="https://www.zhihu.com/equation?tex=S_%7B1%7D%3DW_%7Bx%7DX_%7B1%7D%2BW_%7Bs%7DS_%7B0%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B1%7D%3DW_%7Bo%7DS_%7B1%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B2%7D%3DW_%7Bx%7DX_%7B2%7D%2BW_%7Bs%7DS_%7B1%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B2%7D%3DW_%7Bo%7DS_%7B2%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B3%7D%3DW_%7Bx%7DX_%7B3%7D%2BW_%7Bs%7DS_%7B2%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B3%7D%3DW_%7Bo%7DS_%7B3%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p>假设在t=3时刻，损失函数为 <img src="https://www.zhihu.com/equation?tex=L_%7B3%7D%3D%5Cfrac%7B1%7D%7B2%7D%28Y_%7B3%7D-O_%7B3%7D%29%5E%7B2%7D" alt="[公式]"> 。</p>
<p>则对于一次训练任务的损失函数为 <img src="https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bt%3D0%7D%5E%7BT%7D%7BL_%7Bt%7D%7D" alt="[公式]"> ，即每一时刻损失值的累加。</p>
<p>使用随机梯度下降法训练RNN其实就是对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D+" alt="[公式]"> 、
<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]">
、 <img src="https://www.zhihu.com/equation?tex=W_%7Bo%7D" alt="[公式]"> 以及 <img src="https://www.zhihu.com/equation?tex=b_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=b_%7B2%7D" alt="[公式]">
求偏导，并不断调整它们以使L尽可能达到最小的过程。</p>
<p>现在假设我们我们的时间序列只有三段，t1，t2，t3。</p>
<p><strong>我们只对t3时刻的 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D%E3%80%81W_%7B0%7D" alt="[公式]"> 求偏导（其他时刻类似）：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7B0%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bo%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>可以看出对于 <img src="https://www.zhihu.com/equation?tex=W_%7B0%7D" alt="[公式]">
求偏导并没有长期依赖，但是对于 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导，会随着时间序列产生长期依赖</strong>。因为 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]">
随着时间序列向前传播，而 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]"> 又是
<img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]">的函数。</p>
<p>根据上述求偏导的过程，我们可以得出任意时刻对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导的公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Csum_%7Bk%3D0%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%7B%5Cpartial%7BS_%7Bt%7D%7D%7D%7D%28%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D%29%5Cfrac%7B%5Cpartial%7BS_%7Bk%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>任意时刻对<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]"> 求偏导的公式同上。</p>
<p><font color="red"> 如果加上激活函数， <img src="https://www.zhihu.com/equation?tex=S_%7Bj%7D%3Dtanh%28W_%7Bx%7DX_%7Bj%7D%2BW_%7Bs%7DS_%7Bj-1%7D%2Bb_%7B1%7D%29" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D" alt="[公式]"> = <img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7Btanh%5E%7B%27%7D%7DW_%7Bs%7D" alt="[公式]">激活函数tanh和它的导数图像在上面已经说过了，所以原因在这就不赘述了，还是一样的，激活函数导数小于1。</font></p>
<blockquote>
<p>==<strong>现在来解释一下，为什么说RNN和DNN的梯度消失问题含义不一样？</strong>==</p>
<ol type="1">
<li><strong>先来说DNN中的反向传播：</strong>在上文的DNN反向传播中，我推导了两个权重的梯度，第一个梯度是直接连接着输出层的梯度，求解起来并没有梯度消失或爆炸的问题，因为它没有连乘，只需要计算一步。第二个梯度出现了连乘，也就是说越靠近输入层的权重，梯度消失或爆炸的问题越严重，可能就会消失会爆炸。<strong>一句话总结一下，DNN中各个权重的梯度是独立的，该消失的就会消失，不会消失的就不会消失。</strong></li>
<li><strong>再来说RNN：</strong>RNN的特殊性在于，它的权重是共享的。抛开W_o不谈，因为它在某时刻的梯度不会出现问题（某时刻并不依赖于前面的时刻），但是W_s和W_x就不一样了，每一时刻都由前面所有时刻共同决定，是一个相加的过程，这样的话就有个问题，当距离长了，计算最前面的导数时，最前面的导数就会消失或爆炸，但当前时刻整体的梯度并不会消失，因为它是求和的过程，当下的梯度总会在，只是前面的梯度没了，但是更新时，由于权值共享，所以整体的梯度还是会更新，<strong>通常人们所说的梯度消失就是指的这个，指的是当下梯度更新时，用不到前面的信息了，因为距离长了，前面的梯度就会消失，也就是没有前面的信息了，但要知道，整体的梯度并不会消失，因为当下的梯度还在，并没有消失。</strong></li>
<li><strong>一句话概括：</strong>RNN的梯度不会消失，RNN的梯度消失指的是当下梯度用不到前面的梯度了，但DNN靠近输入的权重的梯度是真的会消失。</li>
</ol>
</blockquote>
<p>说完了RNN的反向传播及梯度消失的含义，终于该说<strong>为什么LSTM可以解决这个问题了</strong>，这里默认大家都懂LSTM的结构，对结构不做过多的描述。<strong>见第三节</strong>。【LSTM通过它的“门控装置”有效的缓解了这个问题，这也就是为什么我们现在都在使用LSTM而非普通RNN。】</p>
<h2><span id="缓解梯度消失amp爆炸-qampa">缓解梯度消失&amp;爆炸 Q&amp;A</span></h2>
<h3><span id="1-残差神经网络为什么可以缓解梯度消失">1、残差神经网络为什么可以缓解梯度消失？</span></h3>
<p><strong>残差单元可以以跳层连接的形式实现，即将单元的输入直接与单元输出加在一起，然后再激活</strong>。因此残差网络可以轻松地用主流的自动微分深度学习框架实现，直接使用BP算法更新参数损失对某低层输出的梯度，被分解为了两项。</p>
<h5><span id="1从前后向信息传播的角度来看">（1）<strong>从前后向信息传播的角度来看</strong></span></h5>
<p><strong>普通神经网络前向传播</strong>。前向传播将数据特征逐层抽象，最终提取出完成任务所需要的特征/表示。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=a%5E%7Bl_2%7D+%3D+F%28a%5E%7Bl_2-1%7D%29+%3D+F%28F%28a%5E%7Bl_2-2%7D%29%29+%3D+...+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>普通神经网络反向传播</strong>。梯度涉及两层参数交叉相乘，可能会在离输入近的网络中产生梯度消失的现象。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_2%7D%7D++%5Cfrac%7B%5Cpartial+a%5E%7Bl_2%7D%7D%7B%5Cpartial+a%5E%7Bl_1%7D%7D++%3D+%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_2%7D%7D++%5Cfrac%7B%5Cpartial+a%5E%7Bl_2%7D%7D%7B%5Cpartial+a%5E%7Bl_2-1%7D%7D+...%5Cfrac%7B%5Cpartial+a%5E%7Bl_1%2B1%7D%7D%7B%5Cpartial+a%5E%7Bl_1%7D%7D++%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>残差网络前向传播</strong>。输入信号可以从任意低层直接传播到高层。由于包含了一个天然的恒等映射，一定程度上可以解决网络退化问题。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=a%5E%7Bl_2%7D+%3D+a%5E%7Bl_2-1%7D+%2B++F%28a%5E%7Bl_2-1%7D%29+%3D+%28a%5E%7Bl_2-2%7D+%2B++F%28a%5E%7Bl_2-2%7D%29%29+%2B+F%28a%5E%7Bl_2-1%7D%29+%3D+...+%3D+a%5E%7Bl_1%7D+%2B+%5Csum_%7Bi%3Dl_1%7D%5E%7Bl_2-1%7DF%28a%5Ei%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>残差网络反向传播</strong>。<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_2%7D%7D" alt="[公式]">表明，反向传播时，错误信号可以不经过任何中间权重矩阵变换直接传播到低层，一定程度上可以缓解梯度弥散问题（即便中间层矩阵权重很小，梯度也基本不会消失）。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cepsilon%7D%7B%5Cpartial+a%5E%7Bl_2%7D%7D+++%281+%2B+%5Cfrac%7B%5Cpartial%5Csum_%7Bi%3Dl_1%7D%5E%7Bl_2-1%7DF%28a%5Ei%29%7D%7Ba%5E%7Bl_1%7D%7D%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>所以可以认为残差连接使得信息前后向传播更加顺畅。</p>
<h5><span id="2集成学习的角度">（2）<strong>集成学习的角度</strong></span></h5>
<p>将残差网络展开，以一个三层的ResNet为例，可得到下面的树形结构：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=y_3+%3D+y_2+%2B+f_3%28y_2%29+%3D+%5By_1+%2B+f_2%28y_1%29%5D+%2B+f_3%28y_1+%2B+f_2%28y_1%29%29+%3D+%5By_0+%2B+f_1%28y_0%29+%2B+f_2%28y_0+%2B+f_1%28y_0%29+%29%5D+%2B+f_3%28y_0+%2B+f_1%28y_0%29+%2B+f_2%28y_0+%2B+f_1%28y_0%29+%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://pic1.zhimg.com/80/v2-4ec65b70ecd7000389d46cad8c6c8b9d_1440w.jpg?source=d16d100b" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>残差网络就可以被看作是一系列路径集合组装而成的一个集成模型，其中不同的路径包含了不同的网络层子集。</p>
<h5><span id="特点">特点：</span></h5>
<ol type="1">
<li>跳层连接</li>
<li>例子：DCN（Deep Cross Network）、Transformer</li>
<li>有效的缓解梯度消失问题的手段</li>
<li>输入和输出维度一致，因为残差正向传播有相加的过程<img src="https://www.zhihu.com/equation?tex=a%5E%7Bl_2%7D+%3D+a%5E%7Bl_2-1%7D+%2B++F%28a%5E%7Bl_2-1%7D%29" alt="[公式]"></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/K4HD8V/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/K4HD8V/" class="post-title-link" itemprop="url">模型训练（1）loss不下降</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-06 20:13:08 / 修改时间：20:42:56" itemprop="dateCreated datePublished" datetime="2022-07-06T20:13:08+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="神经网络模型训练集和测试集loss不下降原因汇总">神经网络模型训练集和测试集loss不下降原因汇总</span></h2>
<h3><span id="一-训练的时候-loss不下降">一、<strong>训练的时候 loss
不下降</strong></span></h3>
<ul>
<li><strong>模型结构问题</strong>。当模型结构不好、规模小时，模型对数据的拟合能力不足。</li>
<li>训练时间问题。不同的模型有不同的计算量，当需要的计算量很大时，耗时也会很大</li>
<li><strong>权重初始化问题</strong>。常用的初始化方案有全零初始化、正态分布初始化和均匀分布初始化等，合适的初始化方案很重要，之前提到过<strong><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/OcEqqQq59a-djZqBsh_7Zw">神经网络初始化为0可能会带来的影响</a></strong></li>
<li><strong>正则化问题</strong>。L1、L2以及Dropout是为了防止过拟合的，当训练集loss下不来时，就要考虑一下是不是正则化过度，导致模型欠拟合了。正则化相关可参考<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/418228948">正则化之L1 &amp;
L2</a></strong></li>
<li><strong>激活函数问题</strong>。全连接层多用ReLu，神经网络的输出层会使用sigmoid
或者 softmax。激活函数可参考<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/455086947">常用的几个激活函数</a></strong>。在使用Relu激活函数时，当每一个神经元的输入为负时，会使得该神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。</li>
<li><strong>优化器问题</strong>。优化器一般选取Adam，但是当Adam难以训练时，需要使用如SGD之类的其他优化器。常用优化器可参考<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/446357922">机器学习中常用的优化器有哪些？</a></strong></li>
<li><strong>学习率问题</strong>。学习率决定了网络的训练速度，但学习率不是越大越好，当网络趋近于收敛时应该选择较小的学习率来保证找到更好的最优点。所以，我们需要手动调整学习率，首先选择一个合适的初始学习率，当训练不动之后，稍微降低学习率。</li>
<li><strong><font color="red">
梯度消失和爆炸。</font></strong>这时需要考虑激活函数是否合理，网络深度是否合理，可以通过调节sigmoid
-&gt; relu，假如残差网络等，相关可参考<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442914336">为什么神经网络会有梯度消失和梯度爆炸问题？如何解决？</a></strong></li>
<li><strong>batch
size过小</strong>，会导致模型损失波动大，难以收敛，过大时，模型前期由于梯度的平均，导致收敛速度过慢。</li>
<li>数据集问题。（1）数据集未打乱，可能会导致网络在学习过程中产生一定的偏见（2）噪声过多、标注有大量错误时，会导致神经网络难以学到有用的信息，从而出现摇摆不定的情况，<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/434532885">噪声、缺失值、异常值</a></strong>（3）数据类别不均衡使得少数类别由于信息量不足，难以学到本质特征，样本不均衡相关可以看<strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/411613746">样本不均衡及其解决办法</a></strong>。</li>
<li><strong>特征问题</strong>。特征选择不合理，会使网络学习难度增加。之前有提到过特征选择的文章，<strong><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/WO_NCTt1O1tgYkZV4vnd7g">如何找到有意义的组合特征</a></strong>,<strong><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/CN7AIhoU3SKOK_Sej6i5Jw">特征选择方法</a></strong></li>
</ul>
<h3><span id="二-测试的时候-loss不下降">二、<strong>测试的时候 loss
不下降</strong></span></h3>
<blockquote>
<p>训练的时候过拟合导致效果不好</p>
</blockquote>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/437803892">交叉检验</a></strong>，通过交叉检验得到较优的模型参数;</li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/434089082">特征选择</a></strong>，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;</li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/418228948">正则化</a></strong>，常用的有
L1、L2 正则。而且 L1正则还可以自动进行特征选择;</li>
<li>如果有正则项则可以考虑增大正则项参数<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]">;</li>
<li>增加训练数据可以有限的避免过拟合;</li>
<li>Bagging ,将多个弱学习器Bagging
一下效果会好很多，比如随机森林等.</li>
<li>早停策略。本质上是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据。</li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410867062">DropOut策略</a></strong>。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2E809GM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2E809GM/" class="post-title-link" itemprop="url">深度学习-NLP-命名实体识别</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-07-03 22:24:04 / 修改时间：22:42:05" itemprop="dateCreated datePublished" datetime="2022-07-03T22:24:04+08:00">2022-07-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>91</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="命名实体识别-bilstm-crf模型">命名实体识别-BiLSTM-CRF模型</span></h2>
<blockquote>
<p>最通俗易懂的BiLSTM-CRF模型中的CRF层介绍 - 孙孙的文章 - 知乎
https://zhuanlan.zhihu.com/p/44042528</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
