<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/9/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/9/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/9/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">263</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/28RKHA6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/28RKHA6/" class="post-title-link" itemprop="url">深度学习-NLP（6）【Nan】FastText</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-13 16:58:03" itemprop="dateCreated datePublished" datetime="2022-06-13T16:58:03+08:00">2022-06-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 22:02:29" itemprop="dateModified" datetime="2023-04-21T22:02:29+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="fasttext">FastText</span></h2>
<blockquote>
<ul>
<li>FastText代码详解(一) - BlockheadLS的文章 - 知乎
https://zhuanlan.zhihu.com/p/52154254</li>
<li>【DL&amp;NLP】fasttext不仅仅只做文本分类 - 叮当猫的文章 - 知乎
https://zhuanlan.zhihu.com/p/442768234</li>
<li><strong>fastText原理及实践 - 陈运文的文章</strong> - 知乎
https://zhuanlan.zhihu.com/p/32965521</li>
</ul>
</blockquote>
<h3><span id="abstract">Abstract</span></h3>
<p>“本文探索了一种简单有效的文本分类基准（方法）。我们的实验表明，我们的快速文本分类器fastText在准确性方面与深度学习分类器平分秋色，其训练和评估速度（相比深度学习模型更是）要快许多个数量级。我们可以使用标准的多核CPU在不到10分钟的时间内用fastText训练超过10亿个单词，并在一分钟之内将50万个句子在31万2千个类中做分类。”</p>
<p>作者中又出现了托老师，不知道是不是受他影响，这篇文章在表述上也很有word2vec的味道，更不用说模型本身。fastText和word2vec的卖点都是简单高效（快）。</p>
<h3><span id="一-什么是fasttext">一、什么是fastText？</span></h3>
<p>先说结论，fastText在不同语境中至少有两个含义：</p>
<ol type="1">
<li>在文章Bag of Tricks for Efficient Text Classification[<a href="#ref_1">1]</a>中，fastText是作者提出的文本分类器的名字。<strong>与sub-word无关！也不是新的词嵌入训练模型！是<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=word2vec&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22138019676%22%7D">word2vec</a>中CBOW模型的简单变种。</strong></li>
<li>作为Facebook开源包，<a href="https://link.zhihu.com/?target=https%3A//fasttext.cc/">fastText</a>是用来训练词嵌入或句嵌入的，其不仅包括1中论文的代码实现，还包括Enriching
Word Vectors with Subword Information[<a href="#ref_2">2]</a>及FastText.zip: Compressing text classification
models[<a href="#ref_3">3]</a>两文的代码实现。</li>
</ol>
<p>本来觉得这些含义区别不重要，直到连我自己都被弄迷糊了。在写这篇解读前，我心中的fastText一直是第三种含义：<strong>用sub-word信息加强词嵌入训练，解决OOV（Out-Of-Vocabulary）表征的方法</strong>。结果带着这个预先的理解读Bag
of Tricks for Efficient Text Classification，越读越迷惑。</p>
<p>为理清思路，fastText（一）中我们就先讲讲Bag of Tricks for Efficient
Text Classification中的fastText，fastText（二）则围绕Enriching Word
Vectors with Subword Information。</p>
<h3><span id="二-一句话介绍fasttext">二、一句话介绍fastText</span></h3>
<p>word2vec的CBOW模型中将中心词替换为类别标签就得到了fastText。</p>
<p>具体到一些小区别：</p>
<ul>
<li>CBOW中词袋的大小由window_size决定，而fastText中就是整个要分类的文本。</li>
<li>CBOW实际运行中用Hierarchical <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=softmax&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22138019676%22%7D">softmax</a>，fastText用softmax或Hierarchical
softmax，具体试类的数量决定。</li>
</ul>
<p>这就是一个标配版且可以实际应用的fastText了，我要再强调三点它和CBOW无区别的地方，因为在别的讲该论文的文章中看到了一些错误的理解：</p>
<ul>
<li>CBOW和fastText都是用平均值来预测的。（CBOW不是求和，是求平均）</li>
<li>N-gram对于CBOW和fastText都是锦上添花的元素，不是标配。</li>
<li><strong><font color="red">
词向量初始化都是随机的</font></strong>，fastText并没有在word2vec预训练词嵌入的基础上再训练。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/RGS2PC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/RGS2PC/" class="post-title-link" itemprop="url">恶意加密流量（3）西湖论剑AI大数据安全分析赛</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-12 20:23:33" itemprop="dateCreated datePublished" datetime="2022-06-12T20:23:33+08:00">2022-06-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 20:38:38" itemprop="dateModified" datetime="2023-04-18T20:38:38+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B/" itemprop="url" rel="index"><span itemprop="name">算法比赛</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">加密流量检测</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>99</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="西湖论剑ai大数据安全分析赛加密恶意流量检测">西湖论剑AI大数据安全分析赛加密恶意流量检测</span></h2>
<blockquote>
<p>数据集：csv</p>
<p>加密恶意流量检测初赛第一名，决赛第二名方案：https://github.com/undefinedXD/WestLakeSwordComp</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/DTTFVX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/DTTFVX/" class="post-title-link" itemprop="url">大数据处理（1）高维向量相似度匹配</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-11 19:01:05" itemprop="dateCreated datePublished" datetime="2022-06-11T19:01:05+08:00">2022-06-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-19 13:52:45" itemprop="dateModified" datetime="2023-03-19T13:52:45+08:00">2023-03-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">【draft】工程</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%B7%A5%E7%A8%8B/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">大数据处理</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>845</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="文本相似度匹配">文本相似度匹配</span></h2>
<blockquote>
<p>from 《Scaling Up All Pairs Similarity Search》</p>
<p>海量文本的成对相似度的高性能计算（待续） - 马东什么的文章 - 知乎
https://zhuanlan.zhihu.com/p/457947482</p>
</blockquote>
<h4><span id="摘要">摘要：</span></h4>
<p>对于高维空间中的大量稀疏向量数据，我们研究了寻找相似性分数(由余弦距离等函数确定)高于给定阈值的所有向量对的问题。我们提出了一个简单的算法<strong>，基于新的索引和优化策略，解决了这个问题，而不依赖于近似方法或广泛的参数调整</strong>。我们展示了该方法在广泛的相似阈值设置中有效地处理各种数据集，与以前的最先进的方法相比有很大的加速。</p>
<p>(海量成对文本相似度问题对于反欺诈而言非常重要，因为无论是电商中重要的地址信息，还是设备或用户的离散features之间的相似度计算和构图，都依赖于高性能的文本相似度计算方法，在实践中，我们不可能直接写双循环去做计算，即使是离线也往往需要耗费大量的时间)</p>
<h3><span id="一-介绍">一、介绍</span></h3>
<p>许多现实世界的应用程序需要解决一个相似度搜索问题，在这个问题中，人们对所有相似度高于指定阈值的对象对都感兴趣。</p>
<ul>
<li>web搜索的查询细化:搜索引擎通常会建议其他查询公式（例如你在百度中输入百，会给你推荐百度）。生成此类查询建议的一种方法是根据查询[19]的搜索结果的相似性来查找所有的相似查询对。由于目标是只提供高质量的建议，所以我们只需要找到相似度高于阈值的查询对（topk问题）。</li>
<li>协同过滤:协同过滤算法通过确定哪些用户有相似的品味来进行推荐。因此，算法需要计算相似度高于某个阈值的相似用户对。</li>
<li>接近重复的文档检测和消除:特别是在文档索引领域，检测和清除等价的文档是重要的。在许多情况下，由于简单的相等性检验不再满足要求，微小修改的存在使这种检测变得困难。通过具有很高的相似度阈值的相似度搜索，可以实现近重复检测（这方面的工作之前看过simhash，google做海量网页去重的方法）。</li>
<li>团伙检测:最近的工作已经应用算法在一个应用程序中寻找所有相似的用户，以识别点击欺诈者[13]团伙。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/X5SQ7R/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/X5SQ7R/" class="post-title-link" itemprop="url">python-装饰器</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-11 15:51:56" itemprop="dateCreated datePublished" datetime="2022-06-11T15:51:56+08:00">2022-06-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-01 16:51:06" itemprop="dateModified" datetime="2022-07-01T16:51:06+08:00">2022-07-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">【draft】工程</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%B7%A5%E7%A8%8B/%E6%B5%81%E7%A8%8B%E7%9A%84Python/" itemprop="url" rel="index"><span itemprop="name">流程的Python</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="python-函数装饰器和闭包">Python 函数装饰器和闭包</span></h2>
<blockquote>
<p>函数装饰器用于在源码中“标记”函数，以某种方式增强函数的行为。这是一项强大的功能，但是若想掌握，必须理解闭包。</p>
</blockquote>
<p>修饰器和闭包经常在一起讨论,
因为<strong>修饰器就是闭包的一种形式</strong>.
闭包还是<strong>回调式异步编程</strong>和<strong>函数式编程风格</strong>的基础.装饰器是语法糖,
它其实是将函数作为参数让其他函数处理. 装饰器有两大特征:</p>
<ul>
<li><strong>把被装饰的函数替换成其他函数</strong></li>
<li><font color="red">
<strong>装饰器在加载模块时立即执行</strong></font></li>
</ul>
<h3><span id="一-装饰器基础知识">一、装饰器基础知识</span></h3>
<p><strong>装饰器</strong>是<strong>可调用的对象</strong>,
其<strong>参数是另一个函数</strong>(被装饰的函数).
装饰器可能会处理被装饰的函数, 然后把它返回,
或者将其替换成<strong>另一个函数或可调用对象</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@decorate</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">target</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;running target()&#x27;</span>)复制代码</span><br></pre></td></tr></table></figure>
<p>这种写法与下面写法完全等价:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">target</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;running target()&#x27;</span>)</span><br><span class="line">target = decorate(target)</span><br></pre></td></tr></table></figure>
<p>要理解立即执行看下等价的代码就知道了,
<code>target = decorate(target)</code> 这句调用了函数.
一般情况下装饰函数都会将某个函数作为返回值.</p>
<h3><span id="二-变量作用域规则">二、变量作用域规则</span></h3>
<p>要理解装饰器中变量的作用域, 应该要理解闭包,
我觉得书里将闭包和作用域的顺序换一下比较好. <strong>在python中,
一个变量的查找顺序是 <code>LEGB</code> (L：Local 局部环境，E：Enclosing
闭包，G：Global 全局，B：Built-in 内置).</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">base = <span class="number">20</span> <span class="comment"># 3</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_compare</span>():</span><br><span class="line">    base = <span class="number">10</span> <span class="comment"># 2</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">real_compare</span>(<span class="params">value</span>):</span><br><span class="line">    		base = <span class="number">5</span> <span class="comment"># 1</span></span><br><span class="line">        <span class="keyword">return</span> value &gt; base</span><br><span class="line">    <span class="keyword">return</span> real_compare</span><br><span class="line">    </span><br><span class="line">compare_10 = get_compare()</span><br><span class="line"><span class="built_in">print</span>(compare_10(<span class="number">5</span>))复制代码</span><br></pre></td></tr></table></figure>
<p>在闭包的函数 <code>real_compare</code> 中, 使用的变量
<code>base</code> 其实是 <code>base = 10</code> 的.
因为base这个变量在闭包中就能命中, 而不需要去 <code>global</code>
中获取.</p>
<h3><span id="三-闭包">三、闭包</span></h3>
<p>闭包其实挺好理解的, 当匿名函数出现的时候, 才使得这部分难以掌握.
简单简短的解释闭包就是:</p>
<p><strong>名字空间与函数捆绑后的结果被称为一个闭包(closure).</strong></p>
<p>这个名字空间就是 <code>LEGB</code> 中的 <code>E</code> .
所以闭包不仅仅是将函数作为返回值.
而是将名字空间和函数捆绑后作为返回值的. 多少人忘了理解这个
<code>"捆绑"</code> , 不知道变量最终取的哪和哪啊. 哎.</p>
<h4><span id="标准库中的装饰器">标准库中的装饰器</span></h4>
<p>python内置了三个用于装饰方法的函数: <code>property</code> 、
<code>classmethod</code> 和 <code>staticmethod</code> .
这些是用来丰富类的.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">age</span>():</span><br><span class="line">        <span class="keyword">return</span> <span class="number">12</span></span><br></pre></td></tr></table></figure>
<h3><span id="四-应用">四、应用</span></h3>
<p>非常适合有切面需求的场景，比如<strong>权限校验，日志记录和性能测试</strong>等等。比如你想要执行某个函数前记录日志或者记录时间来统计性能，又不想改动这个函数，就可以通过装饰器来实现。</p>
<h5><span id="不用装饰器我们会这样来实现在函数执行前插入日志">不用装饰器，我们会这样来实现在函数执行前插入日志：</span></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;i am foo&#x27;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;foo is running&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;i am foo&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>虽然这样写是满足了需求，但是改动了原有的代码，如果有其他的函数也需要插入日志的话，就需要改写所有的函数，不能复用代码。<strong>可以这么写</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_logg</span>(<span class="params">func</span>):</span><br><span class="line">    logging.warn(<span class="string">&quot;%s is running&quot;</span> % func.__name__)</span><br><span class="line">    func()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bar</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;i am bar&#x27;</span>)</span><br><span class="line">use_log(bar)    <span class="comment">#将函数作为参数传入</span></span><br></pre></td></tr></table></figure>
<p>这样写的确可以复用插入的日志，缺点就是显示的封装原来的函数，我们希望透明的做这件事。<strong>用装饰器来写</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_log</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args,**kwargs</span>):</span><br><span class="line">        logging.warn(<span class="string">&#x27;%s is running&#x27;</span> % func.__name___)</span><br><span class="line">        <span class="keyword">return</span> func(*args,**kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bar</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;I am bar&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">bar = use_log(bar)</span><br><span class="line">bar()</span><br></pre></td></tr></table></figure>
<p><code>use_log()</code> 就是装饰器，它把真正我们想要执行的函数
<code>bar()</code>
封装在里面，返回一个封装了加入代码的新函数，看起来就像是
<code>bar()</code>
被装饰了一样。这个例子中的切面就是函数进入的时候，在这个时候，我们插入了一句记录日志的代码。这样写还是不够透明，通过@语法糖来起到
<code>bar = use_log(bar)</code> 的作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">bar = use_log(bar)<span class="keyword">def</span> <span class="title function_">use_log</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args,**kwargs</span>):</span><br><span class="line">        logging.warn(<span class="string">&#x27;%s is running&#x27;</span> % func.__name___)</span><br><span class="line">        <span class="keyword">return</span> func(*args,**kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@use_log</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bar</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;I am bar&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">@use_log</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">haha</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;I am haha&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">bar()</span><br><span class="line">haha()</span><br></pre></td></tr></table></figure>
<h5><span id="装饰器也是可以带参数的这位装饰器提供了更大的灵活性">装饰器也是可以带参数的，这位装饰器提供了更大的灵活性。</span></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_log</span>(<span class="params">level</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>): </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            <span class="keyword">if</span> level == <span class="string">&quot;warn&quot;</span>:</span><br><span class="line">                logging.warn(<span class="string">&quot;%s is running&quot;</span> % func.__name__)</span><br><span class="line">            <span class="keyword">return</span> func(*args)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@use_log(<span class="params">level=<span class="string">&quot;warn&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params">name=<span class="string">&#x27;foo&#x27;</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;i am %s&quot;</span> % name)</span><br><span class="line">foo()</span><br></pre></td></tr></table></figure>
<p><font color="red"><strong>实际上是对装饰器的一个函数封装，并返回一个装饰器。</strong></font><strong>这里涉及到作用域的概念，之前有一篇博客提到过。可以把它看成一个带参数的闭包</strong>。当使用
<code>@use_log(level='warn')</code> 时，会将 <code>level</code>
的值传给装饰器的环境中。它的效果相当于
<code>use_log(level='warn')(foo)</code> ，也就是一个三层的调用。</p>
<p>这里有一个美中不足，<code>decorator</code>
不会改变装饰的函数的功能，但会悄悄的改变一个 <code>__name__</code>
的属性(还有其他一些元信息)，因为 <code>__name__</code>
是跟着函数命名走的。<font color="red"> <strong>可以用
<code>@functools.wraps(func)</code> 来让装饰器仍然使用 <code>func</code>
的名字。</strong></font><strong>functools.wraps
也是一个装饰器，它将原函数的元信息拷贝到装饰器环境中，从而不会被所替换的新函数覆盖掉。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;call %s():&#x27;</span> % func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kw)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2QJPFS5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2QJPFS5/" class="post-title-link" itemprop="url">工业落地-蚂蚁安全-柳星《我对安全与NLP的实践和思考》</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-11 15:08:50" itemprop="dateCreated datePublished" datetime="2022-06-11T15:08:50+08:00">2022-06-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 15:14:41" itemprop="dateModified" datetime="2023-04-18T15:14:41+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/" itemprop="url" rel="index"><span itemprop="name">工业落地</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%B7%A5%E4%B8%9A%E8%90%BD%E5%9C%B0/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/%E9%AB%98%E7%BA%A7%E5%A8%81%E8%83%81%E5%8F%91%E7%8E%B0/" itemprop="url" rel="index"><span itemprop="name">高级威胁发现</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="柳星-我对安全与nlp的实践和思考">柳星-我对安全与NLP的实践和思考</span></h1>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/404notf0und">404notf0und</a>/<strong><a target="_blank" rel="noopener" href="https://github.com/404notf0und/FXY">FXY</a></strong></p>
</blockquote>
<h3><span id="一-个人思考">一、个人思考</span></h3>
<p><strong><font color="red">
通过对安全与NLP的实践和思考，有以下三点产出：</font></strong></p>
<ul>
<li><strong>首先，产出一种通用解决方案和轮子，一把梭实现对各种安全场景的安全检测。</strong>通用解决方案给出一类安全问题的解决思路，打造轮子来具体解决这一类问题，而不是使用单个技术点去解决单个问题。<strong>具体来说，将安全与NLP结合，在各种安全场景中，将其安全数据统一视作文本数据，从NLP视角，统一进行文本预处理、特征化、预训练和模型训练。</strong>例如，在Webshell检测中，Webshell文件内容，在恶意软件检测中，API序列，都可以视作长文本数据，使用NLP技术进行分词、向量化、预训练等操作，同理，在Web安全中，SQl、XSS等URL类安全数据，在DNS安全中，DGA域名、DNS隧道等域名安全数据，同样可以视作短文本数据。因此，只要安全场景中安全数据可以看作文本数据，<a target="_blank" rel="noopener" href="https://github.com/404notf0und/FXY">FXY：<em>Security-Scenes-Feature-Engineering-Toolkit</em></a>
中，内置多种通用特征化方法和多种通用深度学习模型，以支持多种安全场景的特征化和模型训练，达到流水线式作业。</li>
<li><strong>其次，是对应用能力和底层能力的思考</strong>。之前写过一篇文章《<a target="_blank" rel="noopener" href="https://4o4notfound.org/index.php/archives/188/">应用型安全算法工程师的自我修养</a>》，在我当时预期想法中，我理解的应用型，重点在于解决实际安全问题，不必苛求于对使用技术本身的理解深度，可以不具备研究型、轮子型的底层能力。映射到我自身，我做安全和算法，最初想法很好，安全和算法两者我都要做好，这里做好，仅仅指用好。之后，面试时暴露了问题，主管给出的建议是两者都要做好。这里做好，不单单指用好，还要知其所以然。举个例子，就是不仅要调包调参玩的6，还要掌握算法的底层原理，这就是底层能力。当时，懂，也不懂，似懂非懂，因为，说，永远是别人的，悟，才是自己的。在实现通用解决方案和轮子的过程中，遇到关于word2vec底层的非预期问题，才深刻体会到，底层能力对应用能力的重要性。过程中遇到的预期和非预期问题，下文会详述。<strong>现在我理解的应用型，重点还是在解决安全问题，以及对安全问题本身的理解，但应用型还需具备研究型、轮子型等上下游岗位的底层能力。</strong>安全算法是这样，其他细分安全领域也是一样，都需要底层能力，以发展技术深度。</li>
<li>最后，带来思考和认识的提升。<strong>从基于机器学习的XX检测，基于深度学习的XX检测，等各种单点检测，到基于NLP的通用安全检测，是一个由点到面的认知提升</strong>。从安全和算法都要做好，到安全和算法都要做好，其中蕴含着认知的提升。从之前写过一篇安全与NLP的文章《<a target="_blank" rel="noopener" href="https://www.4o4notfound.org/index.php/archives/190/">当安全遇上NLP</a>》，到现在这篇文章。对一件事物的认识，在不同阶段应该是不一样的，甚至可能完全推翻自己之前的认识。我们能做的，是保持思考，重新认识过去的经历，提升对事物的认知和认知能力。这个提升认知的过程，类似boosting的残差逼近和强化学习的奖惩，是一个基于不知道不知道-&gt;知道不知道&gt;知道知道-&gt;不知道知道的螺旋式迭代上升过程。</li>
</ul>
<h3><span id="二-预期问题">二、预期问题</span></h3>
<h4><span id="21-分词粒度">2.1 分词粒度</span></h4>
<p><strong>首先是分词粒度，粒度这里主要考虑字符粒度和词粒度。在不同的安全场景中，安全数据不同，采用的分词粒度也可能不同：</strong></p>
<ul>
<li><strong><font color="red">恶意样本检测的动态API行为序列数据，需要进行单词粒度的划分。</font></strong></li>
<li><strong>域名安全检测中的域名数据，最好采用字符粒度划分。</strong></li>
<li><strong>URL安全检测中的URL数据，使用字符和单词粒度划分都可以。</strong></li>
<li><strong>XSS检测文中，是根据具体的XSS攻击模式，写成正则分词函数，对XSS数据进行划分，这是一种基于攻击模式的词粒度分词模式，但这种分词模式很难扩展到其他安全场景中。</strong></li>
</ul>
<p><strong>FXY特征化类wordindex和word2vec中参数char_level实现了该功能</strong>。在其他安全场景中，可以根据此思路，写自定义的<strong>基于攻击模式的分词</strong>，但适用范围有限。我这里提供了两种通用词粒度分词模式，第一种是忽略特殊符号的简洁版分词模式，第二种是考虑全量特殊符号的完整版分词模式，这两种分词模式可以适用于各种安全场景中。FXY特征化类word2vec中参数punctuation的值‘concise’，‘all’和‘define’实现了两种通用分词和自定义安全分词功能。下文的实验部分，会测试不同安全场景中，使用字符粒度和词粒度，使用不同词粒度分词模式训练模型的性能对比。</p>
<h4><span id="22-语料库">2.2 语料库</span></h4>
<p><strong>关于预训练前字典的建立（语料库）</strong>。特征化类word2vec的预训练需求直接引发了字典建立的相关问题。在word2vec预训练前，需要考虑预训练数据的产生。基于深度学习的XSS检测文中，是通过<strong>建立一个基于黑样本数据的指定大小的字典，不在字典内的数据全部泛化为一个特定词，将泛化后的数据作为预训练的数据</strong>。这里我们将此思路扩充，增加使用全量数据建立任意大小的字典。具体到word2vec类中，参数one_class的True
or
False决定了预训练的数据来源是单类黑样本还是全量黑白样本，参数vocabulary_size的值决定了字典大小，如果为None，就不截断，为全量字典数据。下文的实验部分会测试是<strong>单类黑样本预训练</strong>word2vec好，还是<strong>全量数据预训练</strong>更占优势，是<strong>字典截断</strong>好，还是用全量字典来预训练好。</p>
<h4><span id="23-序列">2.3 序列</span></h4>
<p><font color="red">
<strong>关于序列的问题，具体地说，是长文本数据特征化需求</strong>。</font><strong>webshell检测等安全场景，引发了序列截断和填充的问题。</strong>短文本数据的特征化，可以保留所有原始信息。而在某些安全场景中的长文本数据，特征化比较棘手，保留全部原始信息不太现实，需要对其进行截断，截断的方式主要有<strong>字典截断、序列软截断、序列硬截断</strong>。</p>
<ul>
<li><strong>序列软截断</strong>是指对不在某个范围内（参数num_words控制范围大小）的数据，直接去除或填充为某值，长文本选择直接去除，缩短整体序列的长度，尽可能保留后续更多的原始信息。如果长本文数据非常非常长，那么就算有字典截断和序列软截断，截断后的序列也可能非常长，超出了模型和算力的承受范围;</li>
<li><strong>序列硬截断</strong>（参数max_length控制）可以发挥实际作用，直接整整齐齐截断和填充序列，保留指定长度的序列数据。这里需要注意的是，为了兼容后文将说到的“预训练+微调”训练模式中的<strong>预训练矩阵</strong>，序列填充值默认为0。</li>
</ul>
<h4><span id="24-词向量">2.4 词向量</span></h4>
<p>词向量的问题，具体说，是词嵌入向量问题。词嵌入向量的产生有三种方式：</p>
<ul>
<li>词序列索引+有嵌入层的深度学习模型</li>
<li>word2vec预训练产生词嵌入向量+无嵌入层的深度学习模型</li>
<li><strong>word2vec预训练产生预训练矩阵+初始化参数为预训练矩阵的嵌入层的深度学习模型。</strong></li>
</ul>
<p>这里我把这三种方式简单叫做微调、预训练、预训练+微调，从特征工程角度，这三种方式是产生词嵌入向量的方法，从模型角度，也可以看作是模型训练的三种方法。第一种微调的方式实现起来比较简单，直接使用keras的文本处理类Tokenizer就可以分词，转换为词序列，得到词序列索引，输入到深度学习模型中即可。第二种预训练的方式，调个gensim库中word2vec类预训练，对于不在预训练字典中的数据，其词嵌入向量直接填充为0，第三种预训练+微调的方式，稍微复杂一点，简单来说就是前两种方式的组合，用第二种方式得到预训练矩阵，作为嵌入层的初始化权重矩阵参数，用第一种方式得到词序列索引，作为嵌入层的原始输入。下文的实验部分会测试并对比按这三种方式训练模型的性能，<strong>先说结论：预训练+微调&gt;预训练&gt;微调</strong>。</p>
<h3><span id="三-非预期问题">三、非预期问题</span></h3>
<h4><span id="31已知的库和函数不能满足我们的需求">3.1
已知的库和函数不能满足我们的需求</span></h4>
<p>使用keras的文本处理类Tokenizer预处理文本数据，得到词序列索引，完全没有问题。<strong>但类Tokenizer毕竟是文本数据处理类，没有考虑到安全领域的需求。</strong>
+ <strong><font color="red">
类Tokenizer的单词分词默认会过滤所有的特殊符号，仅保留单词，而特殊符号在安全数据中是至关重要的，很多payload的构成都有着大量特殊符号，忽略特殊符号会流失部分原始信息。</font></strong>
+
<strong>首先阅读了keras的文本处理源码和序列处理源码</strong>，不仅搞懂了其结构和各函数的底层实现方式，还学到了一些trick和优质代码的特性。搞懂了其结构和各函数的底层实现方式，还学到了一些trick和优质代码的特性。下图为Tokenizer类的结构。借鉴并改写Tokenizer类，加入了多种分词模式，我们实现了wordindex类。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181514240.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4><span id="32-对word2vec的理解不到位">3.2 对word2vec的理解不到位</span></h4>
<p>第二个非预期问题是，对word2vec的理解不到位，尤其是其底层原理和代码实现，导致会有一些疑惑，无法得到验证，这是潜在的问题。虽然可以直接调用gensim库中的word2vec类暂时解决问题，但我还是决定把word2vec深究深究，一方面可以答疑解惑，另一方面，就算不能调用别人的库，自己也可以造轮子自给自足。限于篇幅问题，不多讲word2vec的详细原理，原理是我们私下里花时间可以搞清楚的，不算是干货，对原理有兴趣的话，这里给大家推荐几篇优质文章，在github仓库<a target="_blank" rel="noopener" href="https://github.com/404notf0und/Always-Learning">Always-Learning</a>中。</p>
<p><strong>word2vec本质上是一个神经网络模型，具体来说此神经网络模型是一个输入层-嵌入层-输出层的三层结构，我们用到的词嵌入向量只是神经网络模型的副产物，是模型嵌入层的权重矩阵。</strong>以word2vec实现方式之一的skip-gram方法为例，此方法本质是通过中心词预测周围词。如果有一段话，要对这段话训练一个word2vec模型，那么很明显需要输入数据，还要是打标的数据。以这段话中的某个单词为中心词为例，在一定滑动窗口内的其他单词都默认和此单词相关，此单词和周围其他单词，一对多产生多个组合，默认是相关的，因此label为1，即是输入数据的y为1，而这些单词组合的one-hot编码是输入数据的x。<strong>那么很明显label全为1，全为positive
sample，需要负采样来中和。这里的负采样不是简单地从滑动窗口外采样，而是按照词频的概率，取概率最小的一批样本来做负样本（这个概念下面马上要用到），因为和中心词毫不相关，自然label为0。</strong></p>
<p><strong><font color="red">
tensorflow中的nce_loss函数实现了负采样。</font></strong></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2MYAND2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2MYAND2/" class="post-title-link" itemprop="url">恶意软件检测（8）【draft】Heterogeneous Temporal Graph Transformer: An Intelligent System for Evolving Android Malware Detection</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-10 22:49:26" itemprop="dateCreated datePublished" datetime="2022-06-10T22:49:26+08:00">2022-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 12:21:02" itemprop="dateModified" datetime="2023-04-18T12:21:02+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">【draft】应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/" itemprop="url" rel="index"><span itemprop="name">学术前沿</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/%E6%81%B6%E6%84%8F%E8%BD%AF%E4%BB%B6%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">恶意软件检测</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3KV4V6N/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3KV4V6N/" class="post-title-link" itemprop="url">深度学习-NLP（4）BERT*-P2</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-10 22:13:40" itemprop="dateCreated datePublished" datetime="2022-06-10T22:13:40+08:00">2022-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-08-30 00:04:36" itemprop="dateModified" datetime="2022-08-30T00:04:36+08:00">2022-08-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="fun-facts-about-bert">Fun Facts about BERT</span></h1>
<h4><span id="why-does-bert-work">Why does BERT work?</span></h4>
<p>"为什么BERT有用？"最常见的解释是，当输入一串文本时，每个文本都有一个对应的向量。对于这个向量，我们称之为<strong>embedding</strong>。</p>
<p><img src="image-20220613180116363.png" alt="image-20220613180116363" style="zoom:50%;"></p>
<p>它的特别之处在于，这些向量代表了<strong>输入词</strong>的<strong>含义</strong>。例如，模型输入
"台湾大学"（国立台湾大学），输出4个向量。这4个向量分别代表
"台"、"湾"、"大 "和
"学"。更具体地说，如果你把这些词所对应的向量画出来，或者计算它们之间的<strong>距离</strong>。</p>
<p><img src="image-20220613180203638.png" alt="image-20220613180203638" style="zoom:50%;"></p>
<p>你会发现，<strong>意思比较相似的词</strong>，它们的<strong>向量比较接近</strong>。例如，水果和草都是植物，它们的向量比较接近。但这是一个假的例子，我以后会给你看一个真正的例子。"鸟
"和 "鱼 "是动物，所以它们可能更接近。</p>
<p>你可能会问，中文有歧义，其实不仅是中文，很多语言都有歧义，<strong>BERT可以考虑上下文</strong>，所以，同一个词，比如说
"苹果"，它的上下文和另一个 "苹果 "不同，它们的向量也不会相同。</p>
<p>水果 "苹果 "和手机 "苹果 "都是
"苹果"，但根据上下文，它们的<strong>含义是不同</strong>的。所以，它的<strong>向量和相应的embedding会有很大不同</strong>。水果
"苹果 "可能更接近于 "草"，手机 "苹果 "可能更接近于 "电"。</p>
<p><strong>现在我们看一个真实的例子</strong>。假设我们现在考虑 "苹果
"这个词，我们会收集很多有 "苹果 "这个词的句子，比如
"喝苹果汁"、"苹果Macbook "等等。然后，我们把这些句子放入BERT中。</p>
<p><img src="image-20220613180822924.png" alt="image-20220613180822924" style="zoom:50%;"></p>
<p>接下来，我们将计算 "苹果 "一词的相应embedding。输入
"喝苹果汁"，得到一个 "苹果
"的向量。为什么不一样呢？在Encoder中存在Self-Attention，所以根据 "苹果
"一词的不同语境，得到的向量会有所不同。接下来，我们计算这些结果之间的==cosine
similarity==，即计算它们的相似度。结果是这样的，这里有10个句子：</p>
<p><img src="image-20220613180921490.png" alt="image-20220613180921490" style="zoom:50%;"></p>
<ul>
<li><p>前5个句子中的 "苹果
"代表<strong>可食用</strong>的苹果。例如，第一句是
"我今天买了苹果吃"，第二句是 "进口富士苹果平均每公斤多少钱"，第三句是
"苹果茶很难喝"，第四句是 "智利苹果的季节来了"，第五句是
"关于进口苹果的事情"，这五个句子都有 "苹果 "一词，</p></li>
<li><p>后面五个句子也有 "苹果
"一词，但提到的是<strong>苹果公司</strong>的苹果。例如，"苹果即将在下个月发布新款iPhone"，"苹果获得新专利"，"我今天买了一部苹果手机"，"苹果股价下跌"，"苹果押注指纹识别技术"，共有十个
"苹果"</p></li>
</ul>
<p>计算每一对之间的相似度，得到一个10×10的矩阵。<strong>相似度越高，这个颜色就越浅</strong>。所以，自己和自己之间的相似度一定是最大的，自己和别人之间的相似度一定是更小的。但前五个
"苹果 "和后五个 "苹果 "之间的相似度相对较低。<font color="red">
<strong>BERT知道，前五个 "苹果
"是指可食用的苹果，所以它们比较接近。最后五个 "苹果
"指的是苹果公司，所以它们比较接近。所以BERT知道，上下两堆 "苹果
"的含义不同</strong>。</font></p>
<p>BERT的这些向量是输出向量，每个向量代表该词的含义。BERT在填空的过程中已经学会了每个汉字的意思。",也许它真的理解了中文，对它来说，汉字不再是毫无关联的，既然它理解了中文，它就可以在接下来的任务中做得更好。</p>
<p>那么接下来你可能会问，"为什么BERT有如此神奇的能力？",为什么......,为什么它能输出代表输入词含义的向量？
这里，约翰-鲁伯特-弗斯，一位60年代的语言学家，提出了一个假说。他说，要知道一个词的意思，我们需要看它的
"<strong>Company</strong>"，也就是经常和它<strong>一起出现的词汇</strong>，也就是它的<strong>上下文</strong>。</p>
<p><img src="image-20220613181131296.png" alt="image-20220613181131296" style="zoom:50%;"></p>
<p>一个词的意思，取决于它的上下文</p>
<ul>
<li><p>所以以苹果（apple）中的果字为例。如果它经常与 "吃"、"树
"等一起出现，那么它可能指的是可食用的苹果。</p></li>
<li><p>如果它经常与电子、专利、股票价格等一起出现，那么它可能指的是苹果公司。</p></li>
</ul>
<p>当我们训练BERT时，我们给它w1、w2、w3和w4，我们覆盖w2，并告诉它预测w2，而它就是从上下文中提取信息来预测w2。所以这个向量是其上下文信息的精华，可以用来预测w2是什么。</p>
<p><font color="red"> <strong>这样的想法在BERT之前已经存在了。在word
embedding中，有一种技术叫做CBOW</strong>。</font></p>
<p><img src="image-20220613181612756.png" alt="image-20220613181612756" style="zoom:50%;"></p>
<p>CBOW所做的，与BERT完全一样。做一个空白，并要求它预测空白处的内容。这个CBOW，这个word
embedding技术，可以给每个词汇一个向量，代表这个词汇的意义。</p>
<p>CBOW是一个非常简单的模型，它使用两个变换，是一个<strong>非常简单的模型</strong>，有人会问，"为什么它只使用两个变换？"，"它可以更复杂吗？"，CBOW的作者，Thomas
Mikolov，曾经来到台湾。当时我在上课的时候，经常有人问我，为什么CBOW只用线性，为什么不用深度学习，我问过Thomas
Mikolov这个问题，他说可以用深度学习，但是之所以选择线性模型，一个简单的模型，最大的担心，其实是<strong>算力问题</strong>。当时的计算能力和现在不在一个数量级上，可能是2016年的时候，几年前的技术也不在一个数量级上，当时要训练一个非常大的模型还是比较困难的，所以他选择了一个比较简单的模型。</p>
<p><font color="red">
<strong>今天，当你使用BERT的时候，就相当于一个深度版本的CBOW</strong></font>。你可以做更复杂的事情，而且<strong>BERT还可以根据不同的语境，从同一个词汇产生不同的embedding</strong>。因为它是一个考虑到语境的高级版本的词embedding，BERT也被称为Contextualized
embedding，这些由<strong>BERT提取的向量或embedding被称为Contextualized
embedding</strong>，希望大家能接受这个答案。</p>
<h4><span id="但是这个答案它真的是真的吗">但是，这个答案，它真的是真的吗？</span></h4>
<p>这是你在文献中听到最多的答案。当你和别人讨论BERT时，这是大多数人都会告诉你的理由。它真的是真的吗？这里有一个难以理解的，由我们实验室的一个学生做的实验。实验是这样的：<strong>我们应用为文本训练的BERT对蛋白质、DNA链和音乐进行分类</strong>。</p>
<p><img src="image-20220613182040464.png" alt="image-20220613182040464" style="zoom:50%;"></p>
<p>让我们以DNA链的分类为例。DNA是一系列的脱氧核团核酸，有四种，分别用A、T、C和G表示，所以一条DNA链是这样的。你可能会问，"EI
IE和N代表什么？"不要在意细节，我也不知道，总之，这是一个分类问题。只要用训练数据和标记数据来训练它，就可以了。神奇的部分来了，DNA可以用ATCG来表示，现在，我们要用BERT来对DNA进行分类：</p>
<p><img src="image-20220613182514103.png" alt="image-20220613182514103" style="zoom:50%;"></p>
<p>例如，"A "是 "we"，"T "是 "you"，"C "是 "he"，"G "是
"she"。对应的词并不重要，你可以随机生成。"A "可以对应任何词汇，"T"、"C
"和 "G "也可以，这并不重要，对结果影响很小。只是这串文字无法理解。</p>
<p>例如，"AGAC "变成了 "we she we
he"，不知道它在说什么。然后，把它扔进一个一般的BERT，用CLS标记，一个输出向量，一个Linear
transform，对它进行分类。只是分类到了DNA类,我不知道他们是什么意思。和以前一样，Linear
transform使用随机初始化，而BERT是通过预训练模型初始化的。但用于初始化的模型，是学习填空的模型。它已经学会了英语填空。你可能会认为,这个实验完全是无稽之谈。如果我们把一个DNA序列预处理成一个无意义的序列,那么BERT的目的是什么?
大家都知道,BERT可以分析一个有效句子的语义,你怎么能给它一个无法理解的句子呢?
做这个实验的意义是什么?</p>
<p>蛋白质有三种分类，那么蛋白质是由氨基酸组成的，有十种氨基酸，只要给每个氨基酸一个随机的词汇，那么DNA是一组ATCG，音乐也是一组音符，给它每个音符一个词汇，然后，把它作为一个文章分类问题来做。</p>
<p><img src="image-20220613182551067.png" alt="image-20220613182551067" style="zoom:50%;"></p>
<p>你会发现，如果你不使用BERT，你得到的结果是蓝色部分，如果你使用BERT，你得到的结果是红色部分，这实际上更好，你们大多数人现在一定很困惑。这个实验只能用神奇来形容，没有人知道它为什么有效，而且目前还没有很好的解释，我之所以要谈这个实验，是想告诉你们，要了解BERT的力量，还有很多工作要做。</p>
<p>我并不是要否认BERT能够分析句子的含义这一事实。从embedding中，我们清楚地观察到，BERT知道每个词的含义，它能找出含义相似的词和不相似的词。但正如我想指出的那样，即使你给它一个无意义的句子，它仍然可以很好地对句子进行分类。</p>
<p>所以，<strong>也许它的力量并不完全来自于对实际文章的理解</strong>。也许还有其他原因。例如，也许，BERT只是一套更好的初始参数。也许这与语义不一定有关。也许这套初始参数，只是在训练大型模型时更好。</p>
<p>是这样吗？这个问题<strong>需要进一步研究</strong>来回答。我之所以要讲这个实验，是想让大家知道，我们目前使用的模型往往是非常新的，需要进一步的研究，以便我们了解它的能力。如果你想了解更多关于BERT的知识，你可以参考这些链接。你的作业不需要它，,这学期剩下的时间也不需要。我只想告诉你，BERT还有很多其他的变种。</p>
<h2><span id="multi-lingual-bert">Multi-lingual BERT</span></h2>
<p>接下来，我要讲的是，一种叫做<strong>Multi-lingual
BERT的BERT（多语言）</strong>。Multi-lingual BERT有什么神奇之处？</p>
<p><img src="image-20220613182656588.png" alt="image-20220613182656588" style="zoom:50%;"></p>
<p>它是由很多语言来训练的，比如中文、英文、德文、法文等等，用填空题来训练BERT，这就是Multi-lingual
BERT的训练方式。</p>
<h3><span id="zero-shot-readingcomprehension">Zero-shot Reading
Comprehension</span></h3>
<p>google训练了一个Multi-lingual
BERT，它能够做这104种语言的填空题。神奇的地方来了，如果你用<strong>英文</strong>问答<strong>数据</strong>训练它，它就会自动学习如何做<strong>中文问答</strong>：</p>
<p><img src="image-20220613182729448.png" alt="image-20220613182729448" style="zoom:50%;"></p>
<p>我不知道你是否完全理解我的意思，所以这里有一个真实的实验例子。</p>
<p><img src="image-20220613182748495.png" alt="image-20220613182748495" style="zoom:50%;"></p>
<p>这是一些训练数据。他们用SQuAD进行fine-tune。这是一个英文Q&amp;A数据集。中文数据集是由台达电发布的，叫DRCD。这个数据集也是我们在作业中要用到的数据集。</p>
<p>在BERT提出之前，效果并不好。在BERT之前，最强的模型是QANet。它的正确率只有......，嗯，我是说F1得分，而不是准确率，但你可以暂时把它看成是准确率或正确率。</p>
<p>如果我们允许用中文填空题进行预训练，然后用中文Q&amp;A数据进行微调，那么它在中文Q&amp;A测试集上的正确率达到89%。因此，其表现是相当令人印象深刻的。</p>
<p>神奇的是，如果我们把一个Multi-lingual的BERT，用英文Q&amp;A数据进行微调，它仍然可以回答中文Q&amp;A问题，并且有78%的正确率，这几乎与QANet的准确性相同。它从未接受过中文和英文之间的翻译训练，也从未阅读过中文Q&amp;A的数据收集。,它在没有任何准备的情况下参加了这个中文Q&amp;A测试，尽管它从未见过中文测试，但不知为何，它能回答这些问题。</p>
<h3><span id="cross-lingual-alignment">Cross-lingual Alignment?</span></h3>
<p>你们中的一些人可能会说："它在预训练中读过104种语言，104种语言中的一种是中文，是吗？
如果是，这并不奇怪。"但是在预训练中，学习的目标是填空。它只能用中文填空。有了这些知识，再加上做英文问答的能力，不知不觉中，它就自动学会了做中文问答。</p>
<p><img src="image-20220613183052341.png" alt="image-20220613183052341" style="zoom:50%;"></p>
<p>听起来很神奇，那么BERT是怎么做到的呢？一个简单的解释是：也许对于多语言的BERT来说，<strong>不同的语言并没有那么大的差异</strong>。无论你用中文还是英文显示，对于具有相同含义的单词，它们的embedding都很接近。</p>
<p>汉语中的 "跳 "与英语中的 "jump "接近，汉语中的 "鱼 "与英语中的 "fish
"接近，汉语中的 "游 "与英语中的 "swim
"接近，也许在学习过程中它已经自动学会了。它是可以被验证的。我们实际上做了一些验证。<font color="red">验证的标准被称为<strong>Mean
Reciprocal
Rank</strong>，缩写为MRR。我们在这里不做详细说明。你只需要知道，<strong>MRR的值越高，不同embedding之间的Alignment就越好</strong>。
</font></p>
<p>更好的Alignment意味着，具有相同含义但来自不同语言的词将被转化为更接近的向量。如果MRR高，那么具有相同含义但来自不同语言的词的向量就更接近。</p>
<p><img src="image-20220613183548674.png" alt="image-20220613183548674" style="zoom:50%;"></p>
<p><strong>这条深蓝色的线是谷歌发布的104种语言的Multi-lingual
BERT的MRR，它的值非常高，这说明不同语言之间没有太大的差别。Multi-lingual
BERT只看意思，不同语言对它没有太大的差别。</strong></p>
<p>橙色这条是我们试图自己训练Multi-lingual
BERT。我们使用的<strong>数据较少</strong>，每种语言只使用了20万个句子。数据较少。我们自我训练的模型结果并不好。我们不知道为什么我们的Multi-lingual
BERT不能将不同的语言统一起来。似乎它不能学习那些在不同语言中具有相同含义的符号，它们应该具有相同的含义。这个问题困扰了我们很长时间。</p>
<p>为什么我们要做这个实验？为什么我们要自己训练Multi-lingual
BERT？因为我们想了解，是什么让Multi-lingual
BERT。我们想设置不同的参数，不同的向量，看看哪个向量会影响Multi-lingual
BERT。</p>
<p>但是我们发现，对于我们的Multi-lingual
BERT来说，无论你如何调整参数，它就是不能达到Multi-lingual的效果，它就是不能达到Alignment的效果。我们把数据量<strong>增加了五倍</strong>，看看能不能达到Alignment的效果。在做这个实验之前，大家都有点抵触，大家都觉得有点害怕，因为训练时间要比原来的长五倍。<strong>训练了两天后，什么也没发生，损失甚至不能减少，就在我们要放弃的时候，损失突然下降了</strong>。</p>
<p><img src="image-20220613183941470.png" alt="image-20220613183941470" style="zoom:50%;"></p>
<p>用了8个V100来训练，我们的实验室也没有8个V100，是在NCHC（国家高性能计算中心）的机器上运行的，训练了两天后，损失没有下降，似乎失败了。当我们要放弃的时候，损失下降了。</p>
<p>这是某个学生在Facebook上发的帖子，我在这里引用它来告诉你，我当时心里的感叹。整个实验，必须运行一个多星期，才能把它学好，每一种语言1000K的数据。</p>
<p><img src="image-20220613184000971.png" alt="image-20220613184000971" style="zoom:50%;"></p>
<p>所以看起来，<strong>数据量是一个非常关键的因素</strong>，关系到能否成功地将不同的语言排列在一起。所以有时候，神奇的是，很多问题或很多现象，只有在有足够的数据量时才会显现出来。它可以在A语言的QA上进行训练，然后直接转移到B语言上，从来没有人说过这一点。这是过去几年才出现的，一个可能的原因是，过去没有足够的数据，现在有足够的数据，现在有大量的计算资源，所以这个现象现在有可能被观察到。</p>
<p>最后一个神奇的实验，我觉得这件事很奇怪：</p>
<h4><span id="why">Why？</span></h4>
<p><img src="image-20220613184030869.png" alt="image-20220613184030869" style="zoom:50%;"></p>
<p>你说BERT可以把不同语言中含义相同的符号放在一起，使它们的向量接近。但是，当训练多语言的BERT时，如果给它英语，它可以用英语填空，如果给它中文，它可以用中文填空，它不会混在一起。那么，如果不同语言之间没有区别，怎么可能只用英语标记来填英语句子呢？为什么它不会用中文符号填空呢？它就是不填，这说明它知道语言的信息也是不同的，那些不同语言的符号毕竟还是不同的，它并没有完全抹去语言信息，所以我想出了一个研究课题，我们来找找，语言信息在哪里。</p>
<p><font color="red">
后来我们发现，语言信息并没有隐藏得很深。一个学生发现，我们把所有<strong>英语单词</strong>的embedding，放到多语言的BERT中，<strong>取embedding的平均值</strong>，我们对<strong>中文单词</strong>也做<strong>同样的事情</strong>。在这里，我们给Multi-lingual
BERT一个英语句子，并得到它的embedding。我们在embedding中<strong>加上</strong>这个<strong>蓝色的向量</strong>，这就是<strong>英语和汉语之间的差距</strong>。</font></p>
<p><img src="image-20220613184257490.png" alt="image-20220613184257490" style="zoom:50%;"></p>
<p>这些向量，从Multi-lingual
BERT的角度来看，变成了汉语。有了这个神奇的东西，你可以做一个奇妙的无监督翻译。例如，你给BERT看这个中文句子。</p>
<p><img src="image-20220613184444235.png" alt="image-20220613184444235" style="zoom:50%;"></p>
<p>这个中文句子是，"能帮助我的小女孩在小镇的另一边，，没人能够帮助我"，现在我们把这个句子扔到Multi-lingual
BERT中。然后我们取出Multi-lingual
BERT中的一个层，它不需要是最后一层，可以是任何一层。我们拿出某一层，给它一个embedding，加上这个蓝色的向量。对它来说，这个句子马上就从中文变成了英文。</p>
<p>在向BERT输入英文后，通过在中间加一个<strong>蓝色的向量来转换隐藏</strong>层，转眼间，中文就出来了。"没有人可以帮助我"，变成了
"是（是）没有人（没有人）可以帮助我（我）"，"我 "变成了 "我"，"没有人
"变成了
"没有人"，所以它在某种程度上可以做无监督的标记级翻译，尽管它并不完美，神奇的是，Multi-lingual的BERT仍然保留了语义信息。</p>
<h2><span id="bert-qampa">BERT Q&amp;A</span></h2>
<h3><span id="11-bert如何解决长文本问题-"><strong>1.1 Bert
如何解决长文本问题？</strong> -</span></h3>
<p>何枝的回答 - 知乎
https://www.zhihu.com/question/327450789/answer/2455518614</p>
<p>当我们遇到一个文本分类问题，大多数人首先会想到用BERT系列的模型做尝试。对于短文本而言（小于等于510个token）是work的，但如果遇到输入文本大于510个token时，此时就无法直接调用开源的pretrained-model来做fine-tuning了，本篇文章将通过Pooling的方法来尝试解决长文本下的分类问题。</p>
<p>要想将一个大于510个token的文本输入，一般有以下几种方法：</p>
<ul>
<li><strong>Clipping（<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=截断法&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2455518614%7D">截断法</a>）</strong>：对长输入进行截断，挑选其中「重要」的token输入模型。一种最常见的办法是挑选文章的前
Top N 个 token，和文末的 Top K 个 token，保证 N + K &lt;=
510，这种方法基于的前提假设是「文章的首尾信息最能代表篇章的全局概要」；此外，还有一种
two stage 的方法：先将文章过一遍 summarize 的模型，再将 summarize
模型的输出喂给分类模型。但无论哪种截断方式，都必将会丢失一部分的文本信息，可能会导致分类错误。</li>
<li><strong>Pooling（池化法）</strong>：截断法最大的问题在于需要丢掉一部分文本信息，如果我们能够保留文本中的所有信息，想办法让模型能够接收文本中的全部信息，这样就能避免文本丢失带来的影响。</li>
<li><strong>RNN（<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=循环法&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2455518614%7D">循环法</a>）</strong>：BERT之所以会有最大长度的限制，是因为其在进行MLM预训练的时候就规定了最大的输入长度，而对于类RNN的网络来讲则不会有句子长度的限制（有多少个token就过多少次NN就行了）。但RNN相较于
Transformer 来讲最大问题就在于效果不好，如何将 RNN 的思想用在
Transformer 中就是一个比较有意思的话题了。推荐可以看看<a href="%5BERNIE/README_zh.md%20at%20repro%20·%20PaddlePaddle/ERNIE%5D(https://link.zhihu.com/?target=https%3A//github.com/PaddlePaddle/ERNIE/blob/repro/ernie-doc/README_zh.md)">ERNIE-DOC</a>，官网上是这么描述的，感兴趣的同学可以研究研究</li>
</ul>
<blockquote>
<p>#### Pooling思想</p>
<p>#### 1.1 句子分片</p>
<p>由于 BERT 最多只能接受 510 个token
的输入，因此我们需要将长文本切割成若干段。</p>
<p>假设我们有 2 句 1000 个 token 的句子，那么我就需要先将这 2 个句子切成
4 段（第 1 个句子的 2 段 + 第 2 个句子的 2 段），并放到一个 batch
的输入中喂给模型。</p>
<p><img src="https://pic2.zhimg.com/50/v2-f6b167e4ef46242086e5946ed6256462_720w.jpg?source=1940ef5c" alt="img" style="zoom: 50%;"></p>
<p>文本分段，BERT输入数据维度（4, 510）</p>
<p>#### 1.2 Pooling</p>
<p><strong>当切完片后的数据喂给 BERT 后，我们取 BERT 模型的 [CLS]
的输出，此时输出维度应该为：(4, 768) 。</strong></p>
<p><strong>随即，我们需要将这 4 个 output
按照所属句子分组，由下图所示，前 2
个向量属于一个句子，因此我们将它们归为一组，此时的维度变化：(4, 768)
-&gt; (2, 2, 768)。</strong></p>
<p>接着，我们对同一组的向量进行 Pooling 操作，使其下采样为 1
维的向量，即（1, 768）。</p>
<p>Pooling 的方式有两种：Max-Pooling 和
Avg-Pooling，我们会在后面的实验中比较两种不同 Pooling 的效果。</p>
<p>这里推荐大家使用Max-Pooling比较好，因为 Avg-Pooling
很有可能把特征值给拉平，选择保留显著特征（Max-Pooling）效果会更好一些。</p>
<p><img src="https://pica.zhimg.com/80/v2-2b4ffc235b1135bedc1bc5caa700502b_1440w.jpg?source=1940ef5c" alt="img" style="zoom: 33%;"></p>
</blockquote>
<h4><span id></span></h4>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1ZBPFBF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1ZBPFBF/" class="post-title-link" itemprop="url">深度学习-NLP（4）BERT*-P1</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-10 22:13:40" itemprop="dateCreated datePublished" datetime="2022-06-10T22:13:40+08:00">2022-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 14:56:50" itemprop="dateModified" datetime="2022-06-28T14:56:50+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="self-supervisedlearningbert-p1">Self - supervised
Learning（BERT-P1）</span></h2>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613162648282.png" alt="image-20220613162648282" style="zoom:50%;"></p>
<p>每个人都应该熟悉监督学习，当我们做监督学习时，我们只有一个模型，这个模型的输入是x，输出是y。假设你今天想做情感分析，你就是让机器阅读一篇文章，而机器需要对这篇文章进行分类，是正面的还是负面的，你必须先找到大量的文章，你需要对所有的文章进行label。我们需要<strong>有标签和文章数据来训练监督模型</strong>。</p>
<p>"Self-supervised
"是用另一种方式来监督，没有标签。假设我们只有一堆没有label的文章，但我们试图找到一种方法把它<strong>分成两部分</strong>。我们让其中一部分作为模型的输入数据，另一部分作为标签。</p>
<h3><span id="masking-input">Masking Input</span></h3>
<p>Self-supervised
Learning是什么意思呢，我们直接拿BERT模型来说。<font color="red">
<strong>BERT是一个transformer的Encoder</strong>，我们已经讲过<strong>transformer</strong>了，我们也花了很多时间来介绍<strong>Encoder</strong>和<strong>Decoder</strong>，transformer中的Encoder它实际上是BERT的架构，它和transformer的Encoder完全一样，里面有很多<strong>Self-Attention</strong>和<strong>Residual
connection</strong>，还有Normalization等等，那么，这就是<strong>BERT</strong>。</font></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613164111945.png" alt="image-20220613164111945" style="zoom:50%;"></p>
<p>如果你已经忘记了Encoder里有哪些部件，你需要记住的关键点是，<font color="red"><strong>BERT可以输入一行向量，然后输出另一行向量，输出的长度与输入的长度相同</strong>
</font>。
BERT一般用于自然语言处理，用于文本场景，所以一般来说，它的输入是一串文本，也是一串数据。</p>
<p>当我们真正谈论Self-Attention的时候，我们也说<strong>不仅文本是一种序列，而且语音也可以看作是一种序列，甚至图像也可以看作是一堆向量</strong>。BERT同样的想法是，不仅用于NLP，或者用于文本，它也可以用于语音和视频。</p>
<p>接下来我们需要做的是，随机<strong>盖住</strong>一些输入的文字，<strong>被mask的部分是随机决定的</strong>，例如，我们输入100个token，什么是token？<strong>在中文文本中，我们通常把一个汉字看作是一个token</strong>，当我们输入一个句子时，其中的一些词会被随机mask。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613164315477.png" alt="image-20220613164315477" style="zoom:50%;"></p>
<h4><span id="mask的具体实现有两种方法">mask的具体实现有<strong>两种方法</strong>：</span></h4>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613164327649.png" alt="image-20220613164327649" style="zoom:50%;"></p>
<ul>
<li>第一种方法是，用一个<strong>特殊的符号替换句子中的一个词</strong>，我们用
"MASK
"标记来表示这个特殊符号，你可以把它看作一个新字，这个字完全是一个新词，它不在你的字典里，这意味着mask了原文。</li>
<li>另外一种方法，<strong>随机</strong>把某一个字<strong>换成另一个字</strong>。中文的
"湾"字被放在这里，然后你可以选择另一个中文字来替换它，它可以变成 "一
"字，变成 "天 "字，变成 "大 "字，或者变成 "小
"字，我们只是用随机选择的某个字来替换它</li>
</ul>
<p>所以有两种方法来做mask，一种是添加一个特殊的标记
"MASK"，另一种是用一个字来替换某个字。</p>
<p>两种方法都可以使用。<strong>使用哪种方法也是随机决定的</strong>。因此，当BERT进行训练时，向BERT输入一个句子，<strong>先随机决定哪一部分的汉字将被mask</strong>。mask后，一样是输入一个序列，我们把BERT的相应输出看作是另一个序列，接下来，我们在输入序列中寻找mask部分的相应输出，然后，这个向量将通过一个==Linear
transform==。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613164413322.png" alt="image-20220613164413322" style="zoom:50%;"></p>
<p>所谓的Linear
transform是指，输入向量将与一个<strong>矩阵相乘</strong>，然后做softmax，输出一个分布。这与我们在Seq2Seq模型中提到的使用transformer进行翻译时的输出分布相同。输出是一个很长的向量，包含我们想要处理的每个汉字，每一个字都对应到一个分数。</p>
<p>在训练过程中。我们知道被mask的字符是什么，而BERT不知道，我们可以用一个one-hot
vector来表示这个字符，并使输出和one-hot vector之间的交叉熵损失最小。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613164503646.png" alt="image-20220613164503646" style="zoom:50%;"></p>
<p>或者说得简单一点，我们实际上是在解决一个<strong>分类问题</strong>。现在，BERT要做的是，<strong>预测什么被盖住</strong>。被掩盖的字符，属于
"湾"类。在训练中，我们<strong>在BERT之后添加一个线性模型</strong>，并将它们<strong>一起训练</strong>。所以，BERT里面是一个transformer的Encoder，它有一堆参数。这两个需要共同训练，并试图<strong>预测被覆盖的字符是什么</strong>，这叫做mask。</p>
<h3><span id="next-sentence-prediction">Next Sentence Prediction</span></h3>
<p>事实上，当我们训练BERT时，除了mask之外，我们还会使用另一种方法，这种额外的方法叫做==Next
Sentence Prediction== 。</p>
<p>它的意思是，我们从数据库中拿出两个句子，这是我们通过在互联网上抓取和搜索文件得到的大量句子集合，我们在这<strong>两个句子之间</strong>添加一个<strong>特殊标记</strong>。这样，BERT就可以知道，这两个句子是不同的句子，因为这两个句子之间有一个分隔符。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613165146926.png" alt="image-20220613165146926" style="zoom:50%;"></p>
<p><font color="red">
<strong>我们还将在句子的开头添加一个特殊标记，这里我们用CLS来表示这个特殊标记。</strong></font></p>
<p>现在，我们有一个很长的序列，包括<strong>两个句子</strong>，由SEP标记和前面的CLS标记分开。如果我们把它传给BERT，它应该输出一个序列，因为输入也是一个序列，这毕竟是Encoder的目的。我们将<strong>只看CLS的输出</strong>，我们将把它乘以一个Linear
transform。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613165244059.png" alt="image-20220613165244059" style="zoom:50%;"></p>
<p>现在它必须做一个<strong>二分类问题</strong>，有两个可能的输出：是或不是。这个方法被称为Next
Sentence Prediction ，所以我们需要预测，第二句是否是第一句的后续句。</p>
<p>然而，后来的研究发现，对于BERT要做的任务来说，<strong>Next Sentence
Prediction 并没有真正的帮助</strong>。例如，有一篇论文叫 "Robustly
Optimized BERT
Approach"，简称RoBERTa。在这篇论文中，它明确指出，实施Next Sentence
Prediction ，几乎没有任何帮助。然后，这个概念不知不觉地成为主流。</p>
<p>在这之后，另一篇论文说下一句话预测没有用，所以在它之后的许多论文也开始说它没有用。例如，SCAN-BERT和XLNet都说Next
Sentence Prediction 方法是无用的。它可能是无用的原因之一是，<strong>Next
Sentence Prediction 太简单了</strong>，是一项容易的任务。</p>
<p>这个任务的典型方法是，首先随机选择一个句子，然后从数据库中或随机选择要与前一个句子相连的句子。通常，当我们随机选择一个句子时，它看起来与前一个句子有很大不同。对于BERT来说，预测两个句子是否相连并不是太难。因此，在训练BERT完成Next
Sentence Prediction
的任务时，<strong>没有学到什么太有用的东西</strong>。</p>
<p>还有一种类似于Next Sentence Prediction
的方法，它在纸面上看起来更有用，它被称为==<strong>Sentence order
prediction</strong>==，简称<strong>SOP</strong>。</p>
<p>这个方法的主要思想是，我们最初挑选的两个句子可能是相连的。可能有两种可能性：要么句子1在句子2后面相连，要么句子2在句子1后面相连。有两种可能性，我们问BERT是哪一种。</p>
<p>也许因为这个任务更难，它似乎更有效。它被用在一个叫ALBERT的模型中，这是BERT的高级版本。由于ALBERT这个名字与爱因斯坦相似，我在幻灯片中放了一张爱因斯坦的图片。</p>
<h4><span id="bert学了什么">BERT学了什么？</span></h4>
<p>当我们训练时，我们要求BERT学习两个任务。</p>
<ul>
<li><p>一个是掩盖一些字符，具体来说是汉字，然后要求它填补缺失的字符。</p></li>
<li><p>另一个任务表明它能够预测两个句子是否有顺序关系。</p></li>
</ul>
<p><strong><font color="red">
所以总的来说，BERT它学会了如何填空。BERT的神奇之处在于，在你训练了一个填空的模型之后，它还可以用于其他任务。这些任务不一定与填空有关</font></strong>，也可能是完全不同的任务，但BERT仍然可以用于这些任务，这些任务是BERT实际使用的任务，它们被称为==<strong>Downstream
Tasks</strong>==(下游任务)，以后我们将谈论一些Downstream Tasks
的例子。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613165843213.png" alt="image-20220613165843213" style="zoom:50%;"></p>
<p>所谓的 "Downstream Tasks
"是指，你真正关心的任务。但是，当我们想让BERT学习做这些任务时，我们仍然<strong>需要一些标记的信息</strong>。</p>
<p>总之，BERT只是学习填空，但是，以后可以用来做各种你感兴趣的Downstream
Tasks
。它就像胚胎中的干细胞,它有各种无限的潜力，虽然它还没有使用它的力量,它只能填空,但以后它有能力解决各种任务。我们只需要给它一点数据来激发它，然后它就能做到。</p>
<h4><span id="bert怎么测试性能">BERT怎么测试性能？</span></h4>
<p><font color="red"><strong>BERT分化成各种任务的功能细胞，被称为==Fine-tune==(微调)</strong>
</font>。所以，我们经常听到有人说，他对BERT进行了微调，也就是说他手上有一个BERT，他对这个BERT进行了微调，使它能够完成某种任务，与微调相反，在微调之前产生这个BERT的过程称为==预训练==。<strong>所以，生成BERT的过程就是Self-supervised学习。但是，你也可以称之为预训练</strong>。</p>
<p>好的，在我们谈论如何微调BERT之前，我们应该先看看它的能力。今天，为了测试Self-supervised学习的能力，通常，你会在<strong>多个任务上测试</strong>它。因为我们刚才说，BERT就像一个胚胎干细胞，它要分化成各种任务的功能细胞，我们通常不会只在一个任务上测试它的能力，你会让这个BERT分化成各种任务的功能细胞，看看它在每个任务上的准确性，然后我们取其平均值，得到一个总分。这种不同任务的集合，，我们可以称之为任务集。任务集中最著名的基准被称为==GLUE==，它是<strong>General
Language Understanding Evaluation</strong>的缩写。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613170138102.png" alt="image-20220613170138102" style="zoom: 50%;"></p>
<p>在GLUE中，总共有9个任务。一般来说，你想知道像BERT这样的模型是否被训练得很好。所以，你实际上会得到9个模型，用于9个单独的任务。你看看这<strong>9个任务的平均准确率</strong>，然后，你得到一个值。这个值代表这个Self-supervised模型的性能。让我们看看BERT在GLUE上的性能。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613170213354.png" alt="image-20220613170213354" style="zoom:50%;"></p>
<p>有了BERT，GLUE得分，也就是9个任务的平均得分，确实逐年增加。在这张图中，,横轴表示不同的模型，这里列出了，你可以发现，除了ELMO和GPT，其他的还有很多BERT，各种BERT。</p>
<p><strong>黑色的线</strong>，表示<strong>人类的工作</strong>，也就是人类在这个任务上的准确度，那么，我们把这个当作1，这里每一个点代表一个任务，那么，你为什么要和人类的准确度进行比较呢？</p>
<p>人类的准确度是1，如果他们比人类好，这些点的值就会大于1，如果他们比人类差，这些点的值就会小于1，这是因为这些任务，其评价指标可能不是准确度。每个任务使用的评价指标是不同的，它可能不是准确度。如果我们只是比较它们的值，可能是没有意义的。所以，这里我们看的是人类之间的差异。所以，你会发现，在原来的9个任务中，只有1个任务，机器可以比人类做得更好。随着越来越多的技术被提出，越来越多的,还有3个任务可以比人类做得更好。对于那些远不如人类的任务，,它们也在逐渐追赶。</p>
<p><strong>蓝色曲线</strong>表示机器<strong>GLUE得分的平均值</strong>。还发现最近的一些强势模型，例如XLNET，甚至超过了人类。当然，这只是这些数据集的结果，并不意味着机器真的在总体上超过了人类。它<strong>在这些数据集上超过了人类</strong>。这意味着这些数据集并不能代表实际的表现，而且难度也不够大。所以，在GLUE之后，有人做了Super
GLUE。他们找到了更难的自然语言处理任务，让机器来解决。好了！展示这幅图的意义主要是告诉大家，有了BERT这样的技术，机器在自然语言处理方面的能力确实又向前迈进了一步。</p>
<h2><span id="how-to-use-bert">How to use BERT</span></h2>
<h3><span id="case-1-sentiment-analysis">Case 1： Sentiment analysis</span></h3>
<p>第一个案例是这样的，我们假设我们的Downstream Tasks
是输入一个序列，然后输出一个class，这是一个分类问题。比如说
<strong>Sentiment analysis
情感分析</strong>，就是给机器一个句子，让它判断这个句子是正面的还是负面的。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613170630474.png" alt="image-20220613170630474" style="zoom: 50%;"></p>
<h4><span id="对于bert来说它是如何解决情感分析的问题的">对于BERT来说，它是如何解决情感分析的问题的？</span></h4>
<p>你只要给它一个句子，也就是你想用它来判断情绪的句子，然后把<strong>CLS标记放在这个句子的前面</strong>，我刚才提到了CLS标记。<font color="red">
我们把CLS标记放在前面，扔到BERT中,这<strong>4个输入实际上对应着4个输出</strong>。然后，我们<strong>只看CLS的部分。</strong>CLS在这里输出一个向量，我们对它进行<strong>Linear
transform</strong>，也就是将它乘以一个Linear
transform的矩阵，这里省略了Softmax。</font></p>
<p>然而，在实践中，你必须为你的Downstream Tasks
提供<strong>标记数据</strong>，换句话说，<strong>BERT没有办法从头开始解决情感分析问题，你仍然需要向BERT提供一些标记数据，你需要向它提供大量的句子，以及它们的正负标签，来训练这个BERT模型。</strong></p>
<p>在训练的时候，<strong>Linear
transform</strong>和<strong>BERT模型</strong>都是利用Gradient
descent来更新参数的。</p>
<ul>
<li><font color="red"> Linear
transform的参数是<strong>随机初始化</strong>的</font></li>
<li><font color="red">
而BERT的参数是由<strong>学会填空的BERT初始化</strong>的。</font></li>
</ul>
<p>每次我们训练模型的时候，我们都要初始化参数，我们利用梯度下降来更新这些参数，然后尝试minimize
loss，例如，我们正在做情感分类，但是，我们现在有BERT。我们不必随机初始化所有的参数。,我们唯一随机初始化的部分是Linear这里。BERT的骨干是一个巨大的transformer的Encoder。这个网络的参数不是随机初始化的。把学过填空的BERT参数，放到这个地方的BERT中作为参数初始化。</p>
<h4><span id="我们为什么要这样做呢为什么要用学过填空的bert再放到这里呢">我们为什么要这样做呢？为什么要用学过填空的BERT，再放到这里呢？</span></h4>
<p><font color="red">
<strong>最直观和最简单的原因是，它比随机初始化新参数的网络表现更好。当你把学会填空的BERT放在这里时，它将获得比随机初始化BERT更好的性能。</strong></font></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613171706649.png" alt="image-20220613171706649" style="zoom:50%;"></p>
<p>在这里有篇文章中有一个例子。横轴是训练周期，纵轴是训练损失，到目前为止，大家对这种图一定很熟悉，随着训练的进行，损失当然会越来越低，这个图最有趣的地方是，有各种任务。我们不会解释这些任务的细节，我只想说明有各种任务。</p>
<ul>
<li>"fine-tune"是指模型被用于预训练，这是网络的BERT部分。该部分的参数是由学习到的BERT的参数来初始化的，以填补空白。</li>
<li>scratch表示整个模型，包括BERT和Encoder部分都是随机初始化的。</li>
</ul>
<p>首先，在训练网络时，scratch与用学习填空的BERT初始化的网络相比，<strong>损失下降得比较慢</strong>，最后，用随机初始化参数的网络的损失仍然高于用学习填空的BERT初始化的参数。</p>
<ul>
<li>当你进行Self-supervised学习时，你使用了大量的<strong>无标记数据</strong>。</li>
<li>另外，Downstream Tasks 需要少量的<strong>标记数据</strong>。</li>
</ul>
<p>所谓的 "半监督
"是指，你有大量的无标签数据和少量的有标签数据，这种情况被称为
"半监督"，所以使用BERT的整个过程是连续应用Pre-Train和Fine-Tune，它可以被视为一种半监督方法。</p>
<h3><span id="case-2-pos-tagging">Case 2 ：POS tagging</span></h3>
<p>第二个案例是，输入一个序列，然后输出另一个序列，而输入和输出的长度是一样的。我们在讲Self-Attention的时候，也举了类似的例子。
例如，==POS tagging==。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613172015343.png" alt="image-20220613172015343" style="zoom:50%;"></p>
<p>POS
tagging的意思是词性标记。你给机器一个句子，它必须告诉你这个句子中每个词的词性，即使这个词是相同的，也可能有不同的词性。</p>
<p>你只需向BERT输入一个句子。之后，对于这个句子中的每一个标记，它是一个中文单词，有一个代表这个单词的相应向量。然后，这些向量会依次通过Linear
transform和Softmax层。最后，网络会预测给定单词所属的类别，例如，它的词性。</p>
<p>当然，类别取决于你的任务，如果你的任务不同，相应的类别也会不同。接下来你要做的事情和案例1完全一样。换句话说，你需要有一些标记的数据。这仍然是一个典型的分类问题。唯一不同的是，BERT部分，即网络的Encoder部分，其参数不是随机初始化的。在<strong>预训练过程中，它已经找到了不错的参数</strong>。</p>
<p>当然，我们在这里展示的例子属于自然语言处理。但是，你可以把这些例子改成<strong>其他任务</strong>，例如，你可以把它们改成语音任务，或者改成计算机视觉任务。我在Self-supervised
Learning一节中提到，语音、文本和图像都可以表示为一排向量。虽然下面的例子是文字，但这项技术<strong>不仅限于处理文字</strong>，它还可以用于其他任务，如计算机视觉。</p>
<h3><span id="case-3natural-languageinference">Case 3：Natural Language
Inference</span></h3>
<p>在案例3中，模型输入两个句子，输出一个类别。好了，第三个案例以两个句子为输入，输出一个类别，什么样的任务采取这样的输入和输出？
最常见的是Natural Language Inference ，它的缩写是NLI</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613172722820.png" alt="image-20220613172722820" style="zoom:50%;"></p>
<p>机器要做的是判断，是否有可能<strong>从前提中推断出假设</strong>。这个前提与这个假设相矛盾吗？或者说它们不是相矛盾的句子？</p>
<p>在这个例子中，我们的前提是，一个人骑着马，然后他跳过一架破飞机，这听起来很奇怪。但这个句子实际上是这样的。这是一个基准语料库中的例子。</p>
<p>这里的<strong>假设</strong>是，这个人在一个餐馆。所以<strong>推论</strong>说这是一个<strong>矛盾</strong>。</p>
<p>所以机器要做的是，把两个句子作为输入，并输出这两个句子之间的关系。这种任务很常见。它可以用在哪里呢？例如，舆情分析。给定一篇文章，下面有一个评论，这个消息是同意这篇文章，还是反对这篇文章？该模型想要预测的是每条评论的位置。事实上，有很多应用程序接收两个句子，并输出一个类别。</p>
<p>BERT是如何解决这个问题的？你只要给它两个句子，我们在这两个句子之间放一个<strong>特殊的标记</strong>，并在最开始放CLS标记。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613172928476.png" alt="image-20220613172928476" style="zoom:50%;"></p>
<p>这个序列是BERT的输入。但我们只把CLS标记作为Linear
transform的输入。它决定这两个输入句子的类别。对于NLI，你必须问，这两个句子是否是矛盾的。它是用一些<strong>预先训练好的权重来初始化的</strong>。</p>
<h3><span id="case4extraction-based-question-answering-qa">Case
4：Extraction-based Question Answering (QA)</span></h3>
<p>如果你不理解前面的案例，就忘掉它们。这第四个案例，就是我们在作业7中要做的。作业7是一个问题回答系统。也就是说，在机器读完一篇文章后，你问它一个问题，它将给你一个答案。</p>
<p>但是，这里的问题和答案稍有限制。这是Extraction-based的QA。也就是说，我们假设<strong>答案必须出现在文章</strong>中。答案必须是文章中的一个片段。</p>
<p>在这个任务中，一个输入序列包含<strong>一篇文章</strong>和<strong>一个问题</strong>，文章和问题都是一个<strong>序列</strong>。对于中文来说，每个d代表一个汉字，每个q代表一个汉字。你把d和q放入QA模型中，我们希望它输出<strong>两个正整数s和e</strong>。根据这两个正整数，我们可以直接从文章中<strong>截取一段</strong>，它就是答案。这个片段就是正确的答案。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613173207333.png" alt="image-20220613173207333" style="zoom:50%;"></p>
<p>这听起来很疯狂，但是，这是现在使用的一个相当标准的方法。六年前，当我第一次听说这个机制可以解决QA任务时，我简直不敢相信。但是，无论如何，这是今天一个非常普遍的方法。</p>
<p>好吧，如果你仍然不明白我在说什么，更具体地说，这里有一个问题和一篇文章，正确答案是
"gravity"。机器如何输出正确答案？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613173414502.png" alt="image-20220613173414502" style="zoom:50%;"></p>
<p>你的保证模型应该输出，s等于17，e等于17，来表示gravity。因为它是整篇文章中的第17个词，所以s等于17，e等于17，意味着输出第17个词作为答案。或者举另一个例子，答案是，"within
a
cloud"，这是文章中的第77至79个词。你的模型要做的是，输出77和79这两个正整数，那么文章中从第77个词到第79个词的分割应该是最终的答案。这就是作业7要你做的。当然，我们不是从头开始训练QA模型，为了训练这个QA模型，我们<strong>使用BERT预训练的模型</strong>。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613173621416.png" alt="image-20220613173621416" style="zoom:50%;"></p>
<p>这个解决方案是这样的。对于BERT来说，你必须向它展示一个问题，一篇文章，以及在问题和文章之间的一个特殊标记，然后我们在开头放一个CLS标记。在这个任务中，你唯一需要<strong>从头训练</strong>的只有<strong>两个向量</strong>。"从头训练
"是指<strong>随机初始化</strong>。这里我们用橙色向量和蓝色向量来表示，这两个向量的长度与BERT的输出相同。假设BERT的输出是768维的向量，这两个向量也是768维的向量。那么，如何使用这两个向量？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613173644376.png" alt="image-20220613173644376" style="zoom:50%;"></p>
<p>首先,计算这个<strong>橙色向量</strong>和那些与文件相对应的<strong>输出向量的内积</strong>,由于有3个代表文章的标记,它将输出三个向量,计算这三个向量与橙色向量的内积,你将得到三个值,然后将它们通过softmax函数,你将得到另外三个值。</p>
<p>这个内积<strong>和attention很相似</strong>，你可以把橙色部分看成是query，黄色部分看成是key，这是一个attention，那么我们应该尝试找到分数最大的位置，就是这里，橙色向量和d2的内积，如果这是最大值，s应该等于2，你输出的起始位置应该是2蓝色部分做的是完全一样的事情。</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220613173749525.png" alt="image-20220613173749525">
<figcaption aria-hidden="true">image-20220613173749525</figcaption>
</figure>
<p>蓝色部分代表答案的终点，我们计算这个蓝色向量与文章对应的黄色向量的内积，然后，我们在这里也使用softmax，最后，找到最大值，如果第三个值是最大的，e应该是3，正确答案是d2和d3。</p>
<p>因为答案必须在文章中，如果<strong>答案不在文章中，你就不能使用这个技巧</strong>。这就是一个QA模型需要做的。注意，这两个向量是随机初始化的,而BERT是通过它预先训练的权重初始化的。</p>
<h2><span id="qampa">Q&amp;A</span></h2>
<p><strong>Q：</strong>==<font color="red"><strong>BERT的输入长度有限制吗？</strong>
</font>==</p>
<p><strong>A：</strong>理论上，没有。在现实中，是的，在理论上，因为BERT模型，是一个transformer的Encoder，所以它可以输入很长的序列，只要你必须能够做Self-Attention，但Self-Attention的计算复杂性是非常高的。所以你会发现，在实践中，<strong>BERT实际上不能输入太长的序列，你最多可以输入512长度的序列，如果你输入一个512长度的序列，Self-Attention在中间就要产生512乘以512大小的Attention
Metric</strong>，那么你可能会被计算所淹没。所以实际上它的长度不是无限的。在助教的程序中，已经为大家处理了这个问题。我们限制了BERT的输入长度，而且用一篇文章来训练需要很长的时间。然后每次，我们只取其中的一段进行训练。我们不会将整篇文章输入BERT。因为你想要的距离太长了，你的训练会有问题。</p>
<p><strong>Q：</strong> "它与填空题有什么关系？</p>
<p><strong>A：</strong>",哇，这个问题很好。,你会认为这个填空题只是一个填空题。但我要在这里做一个Q&amp;A。,这两件事之间有什么关系呢？这里先卖个关子，待会会试着回答你。</p>
<h2><span id="training-bert-is-challenging">Training BERT is challenging!</span></h2>
<p>BERT是这样一个著名的模型，它可以做任何事情，那么你可能会认为BERT，在预训练中，它只是填空题，但是，<strong>你自己真的不能把它训练起来</strong>。</p>
<p>首先，谷歌最早的BERT,它使用的<strong>数据规模已经很大</strong>了,它的数据中包含了30亿个词汇,30亿个词汇有多少？,是《哈利波特全集》的3000倍。,《哈利波特全集》大约是100万个词汇。,那么谷歌在训练BERT时，最早的BERT，它使用的数据量是《哈利波特全集》的3000倍。</p>
<p>所以你处理起来会比较痛苦,<strong>更痛苦的是训练过程</strong>,为什么我知道训练过程是痛苦的呢,因为我们实验室有一个学生,他其实是助教之一,他自己试着训练一个BERT,他觉得他不能重现谷歌的结果,好,那么在这个图中,纵轴代表GLUE分数,我们刚才讲到GLUE,对吧?有9个任务，平均有9个任务，,平均分数就叫GLUE分数,好的，那么蓝线就是，谷歌原来的BERT的GLUE分数。</p>
<p>那么我们的目标其实不是实现BERT，我们的目标是实现ALBERT。ALBERT是一个高级版本，是橙色的线，蓝线是我们自己训练的ALBERT，但是我们实际训练的不是最大版本，BERT有一个base版本和一个large版本。对于大版本，我们很难自己训练它，所以我们尝试用最小的版本来训练，看它是否与谷歌的结果相同。</p>
<p>你可能会说这30亿个数据，30亿个词似乎有很多数据。实际上，因为它是无标签数据，所以你只是从互联网上整理了一堆文本，有相同的信息量。所以你要<strong>爬上这个级别的信息并不难，难的是训练过程</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613174300000.png" alt="image-20220613174300000" style="zoom:50%;"></p>
<p>好的，这个横轴是训练过程，参数更新多少次，大约一百万次的更新，需要多长时间，用TPU运行8天，所以你的TPU要运行8天，如果你在Colab上做，这个至少要运行200天，你甚至可能到明年才能得到结果。</p>
<p>所以，你真的很难自己训练这种BERT模型。幸运的是，作业只是对它进行微调。你可以在Colab上进行微调，在Colab上微调BERT只需要半小时到一小时。但是，如果你想从头开始训练它，也就是说，训练它做填空题，这将需要大量的时间，而且，你不能在Colab上自己完成它。</p>
<h2><span id="bert-embryology-胚胎學">BERT Embryology (胚胎學)</span></h2>
<p>谷歌已经训练了BERT，而且这些Pre-Train模型是公开的，我们自己训练一个，结果和谷歌的BERT差不多，这有什么意义呢？其实是想建立==BERT胚胎学==。"BERT胚胎学是什么意思？"</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613174341275.png" alt="image-20220613174341275" style="zoom:50%;">我们知道在BERT的训练过程中需要非常大的计算资源，所以我们想知道有没有可能，<strong>节省这些计算资源</strong>？有没有可能让它训练得更快？,要知道如何让它训练得更快，也许我们可以从观察它的训练过程开始。</p>
<p>过去没有人观察过BERT的训练过程。因为在谷歌的论文中，他们只是告诉你，我有这个BERT。然后它在各种任务中做得很好。</p>
<p>BERT在学习填空的过程中，学到了什么？"它在这个过程中何时学会填动词？什么时候学会填名词？
什么时候学会填代词？ 没有人研究过这个问题。</p>
<p>所以我们自己训练BERT后，可以观察到BERT什么时候学会填什么词汇，它是如何提高填空能力的？
好了，细节不是这门课的重点，所以我不在这里讲了。我把论文的链接https://arxiv.org/abs/2010.02480放在这里，供大家参考。不过可以提前爆冷一下就是：事实和你直观想象的不一样。</p>
<h2><span id="pre-training-a-seq2seq-model">Pre-training a seq2seq model</span></h2>
<p>我们补充一点，上述的任务都不包括，Seq2Seq模型，如果我们要解决，Seq2Seq模型呢？BERT只是一个预训练Encoder，有没有办法预训练Seq2Seq模型的Decoder？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613174910438.png" alt="image-20220613174910438" style="zoom:50%;"></p>
<p>有，你就说我有一个Seq2Seq模型，有一个transformer，还有一个Encoder和Decoder。输入是一串句子，输出是一串句子，中间用Cross
Attention连接起来，然后你故意在Encoder的输入上做一些<strong>干扰来破坏它</strong>，我以后会具体告诉你我说的
"破坏 "是什么意思</p>
<p>Encoder看到的是被破坏的结果，那么Decoder应该输出句子被破坏前的结果，训练这个模型实际上是预训练一个Seq2Seq模型。</p>
<h4><span id="有一篇论文叫mass">有一篇论文叫<strong>MASS</strong>：</span></h4>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613174937690.png" alt="image-20220613174937690" style="zoom:50%;"></p>
<p>在MASS中，它说破坏的方法是，就像BERT做的那样，只要遮住一些地方就可以了，然后有各种方法来破坏它，比如，删除一些词，打乱词的顺序，旋转词的顺序。或者插入一个MASK，再去掉一些词。总之，有各种方法。在破坏了输入的句子之后，它可以通过Seq2Seq模型来恢复它。</p>
<p>你可能会问,有那么多的<strong>mask方法</strong>,哪种方法更好呢?也许你想自己做一些实验来试试,让我告诉你,你不需要做,谷歌为你做的,有一篇论文叫T5。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220613175206776.png" alt="image-20220613175206776" style="zoom:50%;"></p>
<p><strong>T5的全称是Transfer Text-To-Text
Transformer，有五个T，所以叫T5。在这个T5里面，它只是做了各种尝试，它做了你能想象的所有组合。</strong>这篇论文有67页，你可以回去读一下，看看结论。</p>
<p>T5是在一个语料库上训练的，叫 "Colossal Clean Crawled
Corpus"，对于这个数据集，Colossal就是巨无霸，就是非常巨大的意思，它叫C4，你用C4来训练T5，大家都是命名高手。这个命名非常强大，这个C4有多大？</p>
<p>C4是一个公共数据集，你可以下载它，它是公共的，但是它的原始文件大小是7TB，你可以下载它，但是你不知道把它保存在哪里，加载之后，你可以通过脚本做预处理，由谷歌提供。这个脚本有一个文件，我看到它在网站上发布了，语料库网站上的文件说，用一个GPU做预处理需要355天，你可以下载它，但你在预处理时有问题。所以，你可以发现，在深度学习中，数据量和模型都很惊人。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2NQRYHA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2NQRYHA/" class="post-title-link" itemprop="url">深度学习（9）Transformer*-p1</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-10 21:00:41" itemprop="dateCreated datePublished" datetime="2022-06-10T21:00:41+08:00">2022-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-18 19:22:24" itemprop="dateModified" datetime="2023-03-18T19:22:24+08:00">2023-03-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Seq2Seq/" itemprop="url" rel="index"><span itemprop="name">Seq2Seq</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-李宏毅-transformer_encoder"><strong><font color="red">
一、李宏毅 - Transformer_Encoder </font></strong></span></h2>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614150150803.png" alt="image-20220614150150803" style="zoom:50%;"></p>
<p>变形金刚的英文就是Transformer,那Transformer也跟我们之后会,提到的BERT有非常强烈的关系,所以这边有一个BERT探出头来,代表说Transformer跟BERT,是很有关系的。</p>
<h3><span id="11-sequence-to-sequenceseq2seq">1.1 Sequence-to-sequence
(Seq2seq)</span></h3>
<p>Transformer就是一个,==Sequence-to-sequence==的model,他的缩写,我们会写做==Seq2seq==,那Sequence-to-sequence的model,又是什么呢？</p>
<p>举例来说,Seq2seq一个很好的应用就是 <strong>语音辨识</strong>：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614150604358.png" alt="image-20220614150604358" style="zoom:50%;"></p>
<p>在做语音辨识的时候,输入是声音讯号<strong>,声音讯号其实就是一串的vector</strong>,输出是语音辨识的结果,也就是输出的这段声音讯号,所对应的文字。我们这边用圈圈来代表文字,每一个圈圈就代表,比如说中文里面的一个方块子,今天<strong>输入跟输出的长度,当然是有一些关系,但是却没有绝对的关系</strong>，输入的声音讯号,他的长度是大T,我们并没有办法知道说,根据大T输出的这个长度N一定是多少。<strong>输出的长度由机器自己决定</strong>,由机器自己去听这段声音讯号的内容,自己决定他应该要输出几个文字,他输出的语音辨识结果,输出的句子里面应该包含几个字,由机器自己来决定,这个是语音辨识。</p>
<h3><span id="12-question-answering-qa">1.2 Question Answering (QA)</span></h3>
<p>那事实上Seq2Seq model,在NLP的领域,在natural language
processing的领域的使用,是比你想像的更为广泛,其实很多<strong>natural
language processing的任务,都可以想成是==question
answering,QA==的任务。</strong>Question
Answering,就是给机器读一段文字,然后你问机器一个问题,希望他可以给你一个正确的答案。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614151056373.png" alt="image-20220614151056373" style="zoom:50%;"></p>
<ul>
<li>假设你今天想做的是翻译,那机器读的文章就是一个英文句子,<strong>问题</strong>就是这个句子的德文翻译是什么,然后输出的<strong>答案</strong>就是德文</li>
<li>或者是你想要叫机器自动作摘要,摘要就是给机器读一篇长的文章,叫他把长的文章的重点节录出来,那你就是给机器一段文字,<strong>问题</strong>是这段文字的摘要是什么,然后期待他<strong>答案</strong>可以输出一个摘要</li>
<li>或者是你想要叫机器做Sentiment analysis,Sentiment
analysis就是机器要自动判断一个句子,是正面的还是负面的；假设你有做了一个产品,然后上线以后,你想要知道网友的评价,但是你又不可能一直找人家ptt上面,把每一篇文章都读过,所以就做一个Sentiment
analysis
model,看到有一篇文章里面,有提到你的产品,然后就把这篇文章丢到,你的model里面,去判断这篇文章,是正面还是负面。你就给机器要判断正面还负面的文章,<strong>问题</strong>就是这个句子,是正面还是负面的,然后希望机器可以告诉你<strong>答案</strong></li>
</ul>
<p>必须要强调一下,对多数NLP的任务,或对多数的语音相关的任务而言,往往为这些任务<strong>客制化模型,你会得到更好的结果</strong>。但是各个任务客制化的模型,就不是我们这一门课的重点了,如果你对人类语言处理,包括语音
包括自然语言处理,这些相关的任务有兴趣的话呢,可以参考一下以下课程网页的<a href="Source%20webpage:%20https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.html">连结</a>,就是去年上的深度学习,与人类语言处理,这门课的内容里面就会教你,各式各样的任务最好的模型,应该是什么。</p>
<blockquote>
<p>举例来说在做语音辨识,我们刚才讲的是一个Seq2Seq
model,输入一段声音讯号,直接输出文字,今天啊 Google的
pixel4,Google官方告诉你说,Google pixel4也是用,N to N的Neural
network,pixel4里面就是,有一个Neural
network,输入声音讯号,输出就直接是文字。</p>
<p>但他其实用的不是Seq2Seq model,他用的是一个叫做,RNN transducer的
model,像这些模型他就是为了,语音的某些特性所设计,这样其实可以表现得更好,至于每一个任务,有什么样客制化的模型,这个就是另外一门课的主题,就不是我们今天想要探讨的重点。</p>
</blockquote>
<h3><span id="13-seq2seq-for-syntacticparsing">1.3 Seq2seq for Syntactic
Parsing</span></h3>
<p>在语音还有自然语言处理上的应用,其实有很多应用,你<strong>不觉得他是一个Seq2Seq
model的问题,但你都可以硬用Seq2Seq model的问题硬解他</strong>。</p>
<p>举例来说<strong>文法剖析</strong>,给机器一段文字,比如Deep learning is
very powerful</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614151449212.png" alt="image-20220614151449212" style="zoom:50%;"></p>
<p>机器要做的事情是产生,一个<strong>文法的剖析树</strong>
告诉我们,deep加learning合起来,是一个名词片语,very加powerful合起来,是一个形容词片语,形容词片语加is以后会变成,一个动词片语,动词片语加名词片语合起来,是一个句子。</p>
<p>那今天文法剖析要做的事情,就是产生这样子的一个Syntactic
tree,所以在文法剖析的任务里面,假设你想要deep
learning解的话,输入是一段文字,他是一个Sequence,但输出看起来不像是一个Sequence,输出是一个树状的结构,但<strong>事实上一个树状的结构,可以硬是把他看作是一个Sequence</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614151522382.png" alt="image-20220614151522382" style="zoom:50%;"></p>
<p>这个树状结构可以对应到一个,这样子的Sequence,从这个Sequence里面,你也可以看出</p>
<ul>
<li>这个树状的结构有一个S，有一个左括号,有一个右括号</li>
<li>S里面有一个noun phrase,有一个左括号跟右括号</li>
<li>NP里面有一个左括号跟右括号,NP里面有is</li>
<li>然后有这个形容词片语,他有一个左括号右括号</li>
</ul>
<p>这一个<strong>Sequence就代表了这一个tree
的structure</strong>,你先把tree
的structure,转成一个Sequence以后,你就可以用Seq2Seq
model硬解他。train一个Seq2Seq
model,读这个句子,然后直接输入这一串文字,再把这串文字转成一个树状的结构,你就可以硬是用Seq2Seq
model,来做文法剖析这件事,这个概念听起来非常的狂,但这是真的可以做得到的。</p>
<h3><span id="14multi-label-classification"><strong><font color="red"> 1.4
multi-label classification</font></strong></span></h3>
<p>还有一些任务可以用seq2seq's model,举例来说
==multi-label的classification==。==multi-class==的classification,跟==multi-label==的classification,听起来名字很像,但他们其实是不一样的事情,multi-class的classification意思是说,我们有不只一个class机器要做的事情,是从数个class里面,选择某一个class出来。</p>
<p>但是multi-label的classification,意思是说<strong>同一个东西,它可以属于多个class</strong>,举例来说
你在做文章分类的时候。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614151845511.png" alt="image-20220614151845511" style="zoom:50%;"></p>
<p>可能这篇文章 属于class 1跟3,这篇文章属于class 3 9
17等等,你可能会说,这种multi-label
classification的问题,能不能<strong>直接把它当作一个multi-class
classification的问题来解</strong></p>
<p>举例来说,我把这些文章丢到一个classifier里面：</p>
<ul>
<li><strong>本来classifier只会输出一个答案,输出分数最高的那个答案</strong></li>
<li><strong>我现在就输出分数最高的前三名,看看能不能解,multi-label的classification的问题</strong></li>
</ul>
<p>但<strong>这种方法可能是行不通的</strong>,因为每一篇文章对应的class的数目,根本不一样
有些东西 有些文章,对应的class的数目,是两个 有的是一个 有的是三个。所以
如果你说 我直接取一个threshold,我直接取分数最高的前三名,class file
output分数最高的前三名,来当作我的输出 显然,不一定能够得到好的结果
那怎么办呢？</p>
<p>这边可以用seq2seq硬做,<strong>输入一篇文章</strong>
<strong>输出就是class</strong> 就结束了,机器自己决定
它要输出几个class。我们说seq2seq
model,就是由机器自己决定输出几个东西,输出的output
sequence的长度是多少,既然你没有办法决定class的数目,那就让机器帮你决定,每篇文章
要属于多少个class。</p>
<h3><span id="15-encoder-decoder">1.5 Encoder-Decoder</span></h3>
<p><strong><font color="red">
我们现在就是要来学,怎么做seq2seq这件事,一般的seq2seq's
model,它里面会分成两块一块是Encoder,另外一块是Decoder。</font></strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614152636868.png" alt="image-20220614152636868" style="zoom:50%;"></p>
<p>你input一个sequence有Encoder,负责处理这个sequence,再把处理好的结果丢给Decoder,由Decoder决定,它要输出什么样的sequence,等一下
我们都还会再细讲,Encoder跟 Decoder内部的架构。seq2seq
model的起源,其实非常的早 在14年的9月,就有一篇seq2seq's
model,用在翻译的文章 被放到Arxiv上。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614152900181.png" alt="image-20220614152900181" style="zoom:50%;"></p>
<p><strong>可以想像当时的seq2seq's
model,看起来还是比较年轻的,今天讲到seq2seq's
model的时候,大家第一个会浮现在脑中的,可能都是我们今天的主角,也就是transformer</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614152931023.png" alt="image-20220614152931023" style="zoom: 50%;"></p>
<p>它有一个Encoder架构,有一个Decoder架构,它里面有很多花花绿绿的block,等一下就会讲一下,这里面每一个花花绿绿的block,分别在做的事情是什么。</p>
<h3><span id="151-encoder">1.5.1 Encoder</span></h3>
<p><strong><font color="red"> seq2seq model
==Encoder==要做的事情,就是给一排向量，输出另外一排向量</font></strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614153244524.png" alt="image-20220614153244524" style="zoom:50%;"></p>
<p>给一排向量、输出一排向量这件事情,很多模型都可以做到,可能第一个想到的是,我们刚刚讲完的self-attention,其实不只self-attention,RNN
CNN 其实也都能够做到,input一排向量, output另外一个同样长度的向量。</p>
<p>在transformer里面,transformer的Encoder,用的就是self-attention,
这边看起来有点复杂,我们用另外一张图,来仔细地解释一下,这个Encoder的架构,等一下再来跟原始的transformer的,论文里面的图进行比对。</p>
<p>现在的Encoder里面,会<strong>分成很多很多的block</strong>：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614153801973.png" alt="image-20220614153801973" style="zoom:50%;"></p>
<p>每一个block都是输入一排向量,输出一排向量,你输入一排向量
第一个block,第一个block输出另外一排向量,再输给另外一个block,到最后一个block,会输出最终的vector
sequence,<strong>每一个block 其实,并不是neural
network的一层</strong>。</p>
<p><strong>每一个block里面做的事情,是好几个layer在做的事情</strong>,在transformer的Encoder里面,每一个block做的事情,大概是这样子的：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614153842957.png" alt="image-20220614153842957" style="zoom:50%;"></p>
<ul>
<li><strong><font color="red">先做一个self-attention,input一排vector以后,做self-attention,考虑整个sequence的资讯，Output另外一排vector。</font></strong></li>
<li><strong><font color="red"> 接下来这一排vector,会再丢到fully
connected的feed forward
network里面,再output另外一排vector,这一排vector就是block的输出。</font></strong></li>
</ul>
<h4><span id="multi-self-attention-residual-connection">Multi-self-attention +
residual connection</span></h4>
<p>事实上在原来的<strong>transformer里面,它做的事情是更复杂的</strong>。在之前self-attention的时候,我们说
输入一排vector,就输出一排vector,这边的每一个vector,它是<strong>考虑了所有的input以后</strong>,所得到的结果：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614154211285.png" alt="image-20220614154211285" style="zoom:50%;"></p>
<p>在transformer里面,它加入了一个设计,我们<strong>不只是输出这个vector</strong>,我们还要<strong>把这个vector加上它的input</strong>,它要把input拉过来
直接加给输出,得到新的output。也就是说,这边假设这个vector叫做<span class="math inline">\(a\)</span>,这个vector叫做<span class="math inline">\(b\)</span> 你要把<span class="math inline">\(a+b\)</span>当作是新的输出。</p>
<p><strong><font color="red"> 这样子的network架构,叫做==residual
connection==,那其实这种residual connection</font></strong>,在deep
learning的领域用的是非常的广泛,之后如果我们有时间的话,再来详细介绍,为什么要用residual
connection。</p>
<p>那你现在就先知道说,有一种network设计的架构,叫做<strong>residual
connection,它会把input直接跟output加起来,得到新的vector。</strong></p>
<h4><span id="norm">Norm</span></h4>
<p>得到<strong>residual</strong>的结果以后,再把它做一件事情叫做<strong>normalization</strong>,这边用的不是batch
normalization,这边用的叫做==<strong>layer normalization</strong>==。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614155147989.png" alt="image-20220614155147989" style="zoom:50%;"></p>
<p>layer normalization做的事情,比bacth
normalization更简单一点。输入一个向量，输出另外一个向量,不需要考虑batch,它会<strong>把输入的这个向量,计算它的mean跟standard
deviation</strong>。</p>
<p>但是要注意一下,<strong>==batch
normalization==是对不同example,不同feature的同一个dimension,去计算mean跟standard
deviation</strong>。但<strong>==layer
normalization==,它是对同一个feature,同一个example里面,不同的dimension,去计算mean跟standard
deviation</strong></p>
<p>计算出mean,跟standard deviation以后,就可以做一个normalize,我们把input
这个vector里面每一个,dimension减掉mean,再除以standard
deviation以后得到x',就是layer normalization的输出。 <span class="math display">\[
x&#39;_i=\frac{x_i-m}{\sigma}
\]</span> <strong>得到layer normalization的输出以后,它的这个输出，才是FC
network的输入。</strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614155555973.png" alt="image-20220614155555973" style="zoom:50%;"></p>
<p>而<strong>FC network这边,也有residual的架构</strong>,所以 我们会把FC
network的input,跟它的output加起来做一下residual,得到新的输出。这个FC
network做完residual以后,还不是结束
你要把residual的结果,<strong>再做一次layer
normalization</strong>,得到的输出,才是residual
network里面,一个block的输出。</p>
<p><img src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210429212721750.png?raw=true" alt="image-20210429212721750" style="zoom:50%;"></p>
<ul>
<li>首先有self-attention,其实在input的地方,还有加上<strong>positional
encoding</strong>,我们之前已经有讲过,如果你只光用self-attention,你没有未知的资讯,所以你需要加上positional的information,然后在这个图上,有特别画出positional的information。</li>
<li><strong>Multi-Head
Attention</strong>,这个就是self-attention的block,这边有特别强调说,它是Multi-Head的self-attention。</li>
<li><strong>Add&amp;norm</strong>,就是residual加layer
normalization,我们刚才有说self-attention,有加上residual的connection,加下来还要过<strong>layer
normalization</strong>,这边这个图上的Add&amp;norm,就是residual加layer
norm的意思。</li>
<li>接下来,要过<strong>feed forward network</strong>，fc的feed forward
network以后再做一次<strong>Add&amp;norm</strong>,再做一次residual加layer
norm,才是一个block的输出。</li>
</ul>
<p>然后这个block会重复n次,这个复杂的block,其实在之后会讲到的,一个非常重要的模型BERT里面,会再用到
BERT,它其实就是transformer的encoder。</p>
<h2><span id="to-learn-more">To Learn more</span></h2>
<p>讲到这边 你心里一定充满了问号,就是为什么
transformer的encoder,要这样设计 不这样设计行不行?</p>
<p>行
不一定要这样设计,这个encoder的network架构,现在设计的方式,本文是按照原始的论文讲给你听的,但<strong>原始论文的设计
不代表它是最好的,最optimal的设计</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614155914421.png" alt="image-20220614155914421" style="zoom:50%;"></p>
<ul>
<li>有一篇文章叫,<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">on layer
normalization in the transformer
architecture</a>，它问的问题就是<strong>为什么,layer
normalization是放在那个地方呢,</strong>为什么我们是先做,residual再做layer
normalization,能不能够把layer
normalization,放到每一个block的input,也就是说 你做residual以后,再做layer
normalization,再加进去
你可以看到说左边这个图,是原始的transformer,右边这个图是稍微把block,更换一下顺序以后的transformer,更换一下顺序以后
结果是会比较好的,这就代表说,原始的transformer
的架构,并不是一个最optimal的设计,你永远可以思考看看,有没有更好的设计方式</li>
<li><strong><font color="red"> 再来还有一个问题就是,为什么是layer norm
为什么是别的,不是别的,为什么不做batch
normalization</font></strong>,也许这篇paper可以回答你的问题,这篇paper是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.07845">Power Norm：,Rethinking Batch
Normalization In Transformers</a>,它首先告诉你说 为什么,batch
normalization不如,layer normalization,在Transformers里面为什么,batch
normalization不如,layer
normalization,接下来在说,它提出来一个<strong>power
normalization</strong>,一听就是很power的意思,都可以比layer
normalization,还要performance差不多或甚至好一点。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/S7AAWG/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/S7AAWG/" class="post-title-link" itemprop="url">深度学习（9）Transformer*-p2</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-10 21:00:41" itemprop="dateCreated datePublished" datetime="2022-06-10T21:00:41+08:00">2022-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-15 18:12:45" itemprop="dateModified" datetime="2022-06-15T18:12:45+08:00">2022-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Seq2Seq/" itemprop="url" rel="index"><span itemprop="name">Seq2Seq</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>21 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="二-李宏毅-transformer_decoder-p2">二、李宏毅 -
Transformer_decoder P2</span></h2>
<h3><span id="21-decoder-autoregressiveat">2.1 Decoder – Autoregressive
(AT)</span></h3>
<p>Decoder其实有两种,接下来会花比较多时间介绍,比较常见的
==<strong>Autoregressive Decoder（自回归解码器）</strong>==,这个
Autoregressive 的 Decoder,是怎么运作的。</p>
<p>用<strong>语音辨识</strong>,来当作例子来说明,或用在作业里面的<strong>机器翻译</strong>,其实是一模一样的,你只是把输入输出,改成不同的东西而已。<strong>语音辨识</strong>就是<strong>输入一段声音,输出一串文字</strong>,你会把一段声音输入给
Encoder,比如说你对机器说,机器学习,机器收到一段声音讯号,声音讯号 进入
Encoder以后,输出会是什么,输出会变成一排 Vector。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615160428118.png" alt="image-20220615160428118" style="zoom:50%;"></p>
<p><strong>Encoder</strong> 做的事情,就是<strong>输入一个 Vector
Sequence</strong>,<strong>输出另外一个 Vector
Sequence</strong>。接下来,就轮到 Decoder 运作了,<strong>Decoder
要做的事情就是产生输出</strong>,也就是<strong>产生语音辨识的结果</strong>,</p>
<h4><span id="decoder怎么产生这个语音辨识的结果">Decoder
怎么产生这个语音辨识的结果？</span></h4>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615160726062.png" alt="image-20220615160726062" style="zoom:50%;"></p>
<p>Decoder 做的事情,就是<strong>把 Encoder
的输出先读进去</strong>,至于怎么读进去,这个我们等一下再讲
我们先,你先假设 Somehow 就是有某种方法,把 Encoder 的输出读到 Decoder
里面,这步我们等一下再处理。</p>
<h4><span id="decoder-怎么产生一段文字">Decoder 怎么产生一段文字？</span></h4>
<p><strong>首先,你要先给它一个特殊的符号,这个特殊的符号,代表开始,在助教的投影片里面,是写
Begin Of Sentence,缩写是 BOS</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615160908630.png" alt="image-20220615160908630" style="zoom:50%;"></p>
<p>BOS就是 Begin 的意思,这个是一个 Special 的 Token,你就是在你的个
Lexicon 里面,你就在你可能,本来 Decoder
可能产生的文字里面,多加一个特殊的字,这个字就代表了
BEGIN,代表了开始这个事情。</p>
<p>在这个机器学习里面,假设你要处理 NLP 的问题,<strong>每一个
Token,你都可以把它用一个 One-Hot 的 Vector 来表示</strong>,One-Hot
Vector 就其中一维是 1,其他都是 0,所以 <strong>BEGIN 也是用 One-Hot
Vector 来表示</strong>,其中一维是 1,其他是 0。<strong><font color="red">
接下来Decoder 会吐出一个向量,这个 Vector 的长度很长,跟你的 Vocabulary 的
Size 是一样的。</font></strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615161211480.png" alt="image-20220615161211480" style="zoom:50%;"></p>
<blockquote>
<p>#### Vocabulary Size 则是什么意思?</p>
<p>你就先想好说,你的 Decoder
<strong>输出的单位</strong>是什么,假设我们今天做的是中文的语音辨识,我们
Decoder 输出的是中文,你这边的 Vocabulary 的 Size
,可能就是中文的方块字的数目。用 Subword
当作英文的单位,就有一些方法,可以把英文的字首字根切出来,拿字首字根当作单位,如果中文的话,我觉得就比较单纯,通常今天你可能就用中文的这个方块字,来当作单位。</p>
<p>每一个中文的字,都会对应到一个数值,因为在<strong>产生这个向量之前,你通常会先跑一个
Softmax</strong>,就跟做分类一样,所以这一个向量里面的分数,它是一个
Distribution,也就是,它这个向量里面的值,它全部加起来,总和 会是 1</p>
</blockquote>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615162226148.png" alt="image-20220615162226148" style="zoom:50%;"></p>
<p><strong>分数最高的一个中文字,它就是最终的输出</strong>。在这个例子里面,机的分数最高,所以机,就当做是这个
Decoder 第一个输出。然后接下来,你<strong>把“机”当做是 Decoder 新的
Input</strong>,原来 Decoder 的 Input,只有 BEGIN
这个特别的符号,现在它除了 BEGIN 以外,它还有“机”作为它的 Input。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615162308793.png" alt="image-20220615162308793" style="zoom:50%;"></p>
<p>所以 Decoder <strong>现在它有两个输入</strong></p>
<ul>
<li>一个是 <strong>BEGIN</strong> 这个符号</li>
<li>一个是<strong>“机”</strong></li>
</ul>
<p>根据这两个输入,它输出一个蓝色的向量,根据这个蓝色的向量里面,给每一个中文的字的分数,我们会决定第二个输出，哪一个字的分数最高,它就是输出,假设"器"的分数最高,<strong>"器"就是输出</strong>。</p>
<p>然后现在 Decoder</p>
<ul>
<li>看到了 BEGIN</li>
<li>看到了"机"</li>
<li>看到了"器"</li>
</ul>
<p>它接下来,还要再决定接下来要输出什么,它可能,就输出"学",这一个过程就反覆的持续下去</p>
<p>所以现在 Decode</p>
<ul>
<li><p>看到了 BEGIN</p></li>
<li><p>看到了"机"</p></li>
<li><p>看到了"器"</p></li>
<li><p>还有"学"</p></li>
</ul>
<p><strong>Encoder 这边其实也有输入</strong>,等一下再讲 Encoder
的输入,Decoder 是怎么处理的,所以 Decoder 看到 Encoder
这边的输入,看到"机" 看到"器"
看到"学",决定接下来输出一个向量,这个向量里面,"习"这个中文字的分数最高的,所以它就输出"习"。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615162732493.png" alt="image-20220615162732493" style="zoom:50%;"></p>
<p><strong>然后这个 Process
,就反覆持续下去</strong>,这边有一个关键的地方,我们特别用红色的虚线把它标出来。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615162754592.png" alt="image-20220615162754592" style="zoom:50%;"></p>
<p><strong><font color="red"> Decoder
看到的输入,其实是它在前一个时间点自己的输出,Decoder
会把自己的输出,当做接下来的输入。</font></strong></p>
<blockquote>
<p>如果Decoder 看到<strong>错误的输入</strong>,让 Decoder
看到自己产生出来的错误的输入,再被 Decoder 自己吃进去,会不会造成 ==Error
Propagation== 的问题.</p>
<p>Error Propagation 的问题就是,<strong>一步错
步步错</strong>这样,就是在这个地方,如果不小心把机器的“器”,不小心写成天气的"气",会不会接下来就整个句子都坏掉了,都没有办法再产生正确的词汇了?</p>
<p>有可能,这个等一下,我们最后会稍微讲一下,这个问题要怎么处理,我们现在,先无视这个问题,继续走下去</p>
</blockquote>
<p>我们来看一下这个 <strong>Decoder内部的结构</strong>长什么样子?</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615162953318.png" alt="image-20220615162953318" style="zoom:50%;"></p>
<p>那我们这边,<strong>把 Encoder 的部分先暂时省略掉</strong>,那在
Transformer 里面,Decoder 的结构,长得是这个样子的,看起来有点复杂,比
Encoder 还稍微复杂一点,那我们现在先把 Encoder 跟 Decoder 放在一起。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615163023898.png" alt="image-20220615163023898" style="zoom:50%;"></p>
<p>稍微比较一下它们之间的差异,那你会发现说,如果我们把 Decoder
中间这一块,<strong>中间这一块把它盖起来,其实 Encoder 跟
Decoder,并没有那么大的差别</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615163100379.png" alt="image-20220615163100379" style="zoom:50%;"></p>
<p>你看 Encoder 这边,<strong>Multi-Head Attention</strong>,然后
<strong>Add &amp; Norm</strong>,<strong>Feed Forward,Add &amp;
Norm</strong>,重复 N 次,Decoder 其实也是一样。</p>
<p>当我们把中间这一块遮起来以后,我们等一下再讲,遮起来这一块里面做了什么事,但当我们把中间这块遮起来以后,
那 Decoder 也是,有一个 Multi-Head Attention,Add &amp; Norm,然后 Feed
Forward,然后 Add &amp; Norm,所以 Encoder 跟
Decoder,其实并没有非常大的差别,除了中间这一块不一样的地方,那只是最后,我们可能会再做一个
Softmax,使得它的输出变成一个机率,那这边有一个<strong>稍微不一样的地方</strong>是,在
Decoder 这边,Multi-Head Attention 这一个 Block 上面,还<strong>加了一个
==Masked==</strong>,</p>
<p>这个 Masked 的意思是这样子的,这是我们原来的 Self-Attention ，Input
一排 Vector,Output 另外一排 Vector,这一排 Vector
<strong>每一个输出</strong>,都要看过完整的 Input 以后,才做决定,所以输出
<span class="math inline">\(b^1\)</span> 的时候,其实是根据 <span class="math inline">\(a^1\)</span> 到 <span class="math inline">\(a^4\)</span> 所有的资讯,去输出 <span class="math inline">\(b^1\)</span>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615163239723.png" alt="image-20220615163239723" style="zoom:50%;"></p>
<p><strong><font color="red"> 当我们把 Self-Attention,转成 Masked
Attention 的时候,它的不同点是,现在我们不能再看右边的部分,也就是产生
<span class="math inline">\(b^1\)</span> 的时候,我们只能考虑 <span class="math inline">\(a^1\)</span> 的资讯,你不能够再考虑 <span class="math inline">\(a^2\)</span> <span class="math inline">\(a^3\)</span> <span class="math inline">\(a^4\)</span></font></strong>。产生 <span class="math inline">\(b^2\)</span> 的时候,你只能考虑 <span class="math inline">\(a^1\)</span> <span class="math inline">\(a^2\)</span> 的资讯,不能再考虑 <span class="math inline">\(a^3\)</span> <span class="math inline">\(a^4\)</span> 的资讯。产生 <span class="math inline">\(b^3\)</span> 的时候,你就不能考虑 <span class="math inline">\(a^4\)</span> 的资讯。产生 <span class="math inline">\(b^4\)</span> 的时候,你可以用整个 Input Sequence
的资讯,这个就是 Masked 的 Self-Attention。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615163708618.png" alt="image-20220615163708618" style="zoom:50%;"></p>
<p>讲得更具体一点,你做的事情是,当我们要产生 <span class="math inline">\(b^2\)</span> 的时候,我们只拿第二个位置的 Query
<span class="math inline">\(b^2\)</span>,去跟第一个位置的
Key,和第二个位置的 Key,去计算
Attention,第三个位置跟第四个位置,就不管它,不去计算 Attention。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615163452245.png" alt="image-20220615163452245" style="zoom:50%;"></p>
<p>我们这样子不去管这个 <span class="math inline">\(a^2\)</span>
右边的地方,只考虑 <span class="math inline">\(a^1\)</span> 跟 <span class="math inline">\(a^2\)</span>,只考虑 <span class="math inline">\(q^1\)</span> <span class="math inline">\(q^2\)</span>,只考虑 <span class="math inline">\(k^1\)</span> <span class="math inline">\(k^2\)</span>,<span class="math inline">\(q^2\)</span> 只跟 <span class="math inline">\(k^1\)</span> 跟 <span class="math inline">\(k^2\)</span> 去计算 Attention,然后最后只计算 <span class="math inline">\(b^1\)</span> 跟 <span class="math inline">\(b^2\)</span> 的 Weighted Sum。然后当我们输出这个
<span class="math inline">\(b^2\)</span> 的时候,<span class="math inline">\(b^2\)</span> 就只考虑了 <span class="math inline">\(a^1\)</span> 跟 <span class="math inline">\(a^2\)</span>,就没有考虑到 <span class="math inline">\(a^3\)</span> 跟 <span class="math inline">\(a^4\)</span>。</p>
<h4><span id="那为什么会这样为什么需要加-masked"><strong><font color="red">
那为什么会这样,为什么需要加 Masked ？</font></strong></span></h4>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615163642567.png" alt="image-20220615163642567" style="zoom:50%;"></p>
<p>这件事情其实非常地直觉:我们一开始 Decoder
的运作方式,它是<strong>一个一个输出</strong>,所以是先有 <span class="math inline">\(a^1\)</span> 再有 <span class="math inline">\(a^2\)</span>,再有 <span class="math inline">\(a^3\)</span> 再有 <span class="math inline">\(a^4\)</span>。<strong>这跟原来的 Self-Attention
不一样</strong>,原来的 Self-Attention,<span class="math inline">\(a^1\)</span> 跟 <span class="math inline">\(a^4\)</span> 是一次整个输进去你的 Model
里面的,在我们讲 Encoder 的时候,Encoder 是一次把 <span class="math inline">\(a^1\)</span> 跟 <span class="math inline">\(a^4\)</span>,都整个都读进去。但是对 Decoder
而言,先有 <span class="math inline">\(a^1\)</span> 才有 <span class="math inline">\(a^2\)</span>,才有 <span class="math inline">\(a^3\)</span> 才有 <span class="math inline">\(a^4\)</span>,所以实际上,当你有 <span class="math inline">\(a^2\)</span>,你要计算 <span class="math inline">\(b^2\)</span> 的时候,你是没有 <span class="math inline">\(a^3\)</span> 跟 <span class="math inline">\(a^4\)</span> 的,所以你根本就没有办法把 <span class="math inline">\(a^3\)</span> <span class="math inline">\(a^4\)</span> 考虑进来。</p>
<p>所以这就是为什么,在那个 Decoder 的那个图上面,Transformer 原始的 Paper
特别跟你强调说,<strong>那不是一个一般的 Attention,</strong>
<strong><font color="red"> 这是一个 Masked 的
Self-Attention,意思只是想要告诉你说,Decoder 它的
Tokent,它输出的东西是一个一个产生的,所以它只能考虑它左边的东西,它没有办法考虑它右边的东西。</font></strong></p>
<p>讲了 Decoder
的运作方式,但是这边,还有一个非常关键的问题,<strong>Decoder
必须自己决定,输出的 Sequence 的长度</strong></p>
<p>可是到底输出的 Sequence 的长度应该是多少,我们不知道。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615164352622.png" alt="image-20220615164352622" style="zoom:50%;"></p>
<p>你没有办法轻易的从输入的 Sequence 的长度,就知道输出的 Sequence
的长度是多少,并不是说,输入是 4 个向量,输出一定就是 4
个向量。这边在这个例子里面,输入跟输出的长度是一样的,但是你知道实际上在你真正的应用里面,并不是这样,输入跟输出长度的关係,是非常复杂的,我们其实是期待机器可以自己学到,今天给它一个
Input Sequence 的时候,Output 的 Sequence 应该要多长。</p>
<p>但在我们目前的这整个
Decoder的这个运作的机制里面,<strong>机器不知道它什么时候应该停下来</strong>,它产生完习以后,它还可以继续重复一模一样的
Process,就把习,当做输入,然后也许 Decoder
,就会接一个惯,然后接下来,就一直持续下去,<strong>永远都不会停下来</strong>。</p>
<p><strong><font color="red"> 我们要让 Decoder
做的事情,也是一样,要让它可以输出一个断,所以你要特别准备一个特别的符号,这个符号,就叫做断,我们这边,用
END 来表示这个特殊的符号。</font></strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615164518484.png" alt="image-20220615164518484" style="zoom:50%;"></p>
<p>所以除了所有中文的方块字,还有 BEGIN
以外,你还要<strong>准备一个特殊的符号,叫做"断"</strong>,那其实在助教的程式里面,它是把
BEGIN 跟 END,就是开始跟这个断,用同一个符号来表示。</p>
<p>反正这个,这个 BEGIN
只会在输入的时候出现,断只会在输出的时候出现,所以在助教的程式里面,如果你仔细研究一下的话,会发现说
END 跟
BEGIN,用的其实是同一个符号,但你用不同的符号,也是完全可以的,也完全没有问题</p>
<p>所以我们现在,当把"习"当作输入以后,就 Decoder 看到 Encoder 输出的这个
Embedding,看到了 "BEGIN",然后"机" "器" "学" "习"以后,看到这些资讯以后
它要知道说,这个语音辨识的结果已经结束了,不需要再产生更多的词汇了。</p>
<p>它产生出来的向量END,就是断的那个符号,它的机率必须要是最大的,然后你就输出断这个符号,那整个运作的过程,整个
Decoder 产生 Sequence 的过程,就结束了这个就是 ==Autoregressive
Decoder==,它运作的方式。</p>
<h3><span id="22-decoder-non-autoregressivenat">2.2 Decoder – Non-autoregressive
(NAT)</span></h3>
<p>Non-Autoregressive ,通常缩写成 NAT,所以有时候 Autoregressive 的
Model,也缩写成 AT,Non-Autoregressive 的 Model
是怎么运作的。<strong>先输入 BEGIN</strong>,<strong>然后</strong>出现
w1,然后<strong>再</strong>把 w1 当做输入,<strong>再</strong>输出
w2,<strong>直到输出 END 为止</strong></p>
<p>那 ==NAT== 是这样,它<strong>不是依次产生</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615164801974.png" alt="image-20220615164801974" style="zoom:50%;"></p>
<p>就假设我们现在产生是中文的句子,它不是依次产生一个字,它是<strong>一次把整个句子都产生出来</strong>。NAT
的 Decoder<strong>可能吃的是一整排的 BEGIN 的
Token</strong>,你就把一堆一排 BEGIN 的 Token 都丢给它,让它一次产生一排
Token 就结束了</p>
<p>举例来说,如果你丢给它 4 个 BEGIN 的 Token,它就产生 4
个中文的字,变成一个句子,就结束了，所以它只要一个步骤,就可以完成句子的生成，这边你可能会问一个问题：刚才不是说不知道输出的长度应该是多少吗,那我们这边<strong>怎么知道
BEGIN 要放多少个</strong>,当做 NAT Decoder 的收入？</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615172838842.png" alt="image-20220615172838842" style="zoom:50%;"></p>
<p>没错
这件事没有办法很自然的知道,没有办法很直接的知道,所以有几个,所以有几个做法</p>
<ul>
<li>一个做法是,你<strong>另外learn一个 Classifier</strong>,这个
Classifier ,它吃 Encoder 的 Input,然后输出是一个数字,这个数字代表
Decoder 应该要输出的长度,这是一种可能的做法</li>
<li>另一种可能做法就是,你就不管三七二十一,<strong>给它一堆 BEGIN 的
Token</strong>,你就假设说,你现在输出的句子的长度,绝对不会超过 300
个字,你就假设一个句子长度的上限,然后 BEGIN ,你就给它 300 个
BEGIN,然后就会输出 300 个字嘛,然后,你再看看<strong>什么地方输出
END</strong>,输出 END 右边的,就当做它没有输出,就结束了,这是另外一种处理
NAT 的这个 Decoder,它应该输出的长度的方法</li>
</ul>
<p>NAT 的
Decoder,最近它之所以是一个热门研究主题,就是它虽然表面上看起来有种种的厉害之处,尤其是<strong>平行化是它最大的优势</strong>,但是
<strong>NAT 的 Decoder ,它的 Performance,往往都不如 AT 的
Decoder</strong>。</p>
<h3><span id="23-encoder-decoder">2.3 Encoder-Decoder</span></h3>
<p>接下来就要讲<strong>Encoder 跟
Decoder它们中间是怎么传递资讯</strong>的了,也就是我们要讲,刚才我们刻意把它遮起来的那一块。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615173000109.png" alt="image-20220615173000109"></p>
<p><strong><font color="red"> 这块叫做 ==Cross Attention==,它是连接
Encoder 跟 Decoder 之间的桥樑,那这一块里面啊,会发现有两个输入来自于
Encoder,Encoder 提供两个箭头,然后 Decoder
提供了一个箭头,所以从左边这两个箭头,Decoder 可以读到 Encoder
的输出。</font></strong></p>
<p>那这个模组实际上是怎么运作的呢,那我们就实际把它运作的过程跟大家展示一下，这个是你的
Encoder：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615173054575.png" alt="image-20220615173054575" style="zoom:50%;"></p>
<p>输入一排向量,输出一排向量,我们叫它 <span class="math inline">\(a^1
a^2 a^3\)</span>。</p>
<p>接下来 轮到你的 Decoder,你的 Decoder 呢,会先吃 BEGIN 当做,BEGIN 这个
Special 的 Token,那 BEGIN 这个 Special 的 Token 读进来以后,你可能会经过
Self-Attention,这个 Self-Attention 是有做 Mask 的,然后得到一个向量,就是
<strong>Self-Attention 就算是有做
Mask,还是一样输入多少长度的向量,输出就是多少向量</strong>。</p>
<p>所以输入一个向量
输出一个向量,然后接下来把这个向量呢,乘上一个矩阵做一个
Transform,得到一个 Query 叫做 q：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615173148542.png" alt="image-20220615173148542" style="zoom:50%;"></p>
<p>然后这边的 <span class="math inline">\(a^1 a^2 a^3\)</span>
呢,也都产生 Key,Key1 Key2 Key3,那把这个 q 跟 <span class="math inline">\(k^1 k^2 k^3\)</span>,去计算 Attention 的分数,得到
<span class="math inline">\(α_1 α_2 α_3\)</span>,当然你可能一样会做
Softmax,把它稍微做一下 Normalization,所以我这边加一个 ',代表它可能是做过
Normalization。接下来再把 <span class="math inline">\(α_1 α_2
α_3\)</span>,就乘上 <span class="math inline">\(v^1 v^2
v^3\)</span>,再把它 Weighted Sum 加起来会得到 v。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615173220252.png" alt="image-20220615173220252" style="zoom:50%;"></p>
<p>那这一个 V,就是接下来会丢到 Fully-Connected 的,Network
做接下来的处理,那这个步骤就是 q 来自于 Decoder,k 跟 v 来自于
Encoder,这个步骤就叫做 Cross Attention。 <strong><font color="red">
Decoder 就是凭藉著产生一个 q,去 Encoder 这边抽取资讯出来,当做接下来的
Decoder 的,Fully-Connected 的 Network 的 Input。</font></strong></p>
<p>当然这个,就现在假设产生第二个,第一个这个中文的字产生一个“机”,接下来的运作也是一模一样的。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615173332499.png" alt="image-20220615173332499" style="zoom:50%;"></p>
<p>输入 BEGIN 输入机,产生一个向量,这个向量一样乘上一个 Linear 的
Transform,得到 q',得到一个 Query,这个 Query 一样跟 <span class="math inline">\(k^1 k^2 k^3\)</span>,去计算 Attention
的分数,一样跟 <span class="math inline">\(v^1 v^2 v^3\)</span> 做
Weighted Sum 做加权,然后加起来得到 v',交给接下来 Fully-Connected Network
做处理，所以这就是Cross Attention 的运作的过程。</p>
<p>也许有人会有<strong>疑问</strong>：那这个 Encoder 有很多层啊,Decoder
也有很多层啊,从刚才的讲解里面好像听起来,这个 Decoder
不管哪一层,都是<strong>拿 Encoder
的最后一层的输出</strong>这样对吗？</p>
<p>对,<strong>在原始 Paper
里面的实做是这样子</strong>,那一定要这样吗？</p>
<p><strong>不一定要这样</strong>,你永远可以自己兜一些新的想法,所以我这边就是引用一篇论文告诉你说,也有人尝试不同的
Cross Attension 的方式。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615173812815.png" alt="image-20220615173812815" style="zoom:50%;"></p>
<p>Encoder 这边有很多层,Decoder 这边有很多层,为什么 Decoder
这边每一层都一定要看,Encoder
的最后一层输出呢,能不能够有各式各样不同的连接方式,这完全可以当做一个研究的问题来
Study。</p>
<h3><span id="24-training">2.4 Training</span></h3>
<p>已经清楚说 Input 一个
Sequence,是怎么得到最终的输出,那接下来就进入训练的部分。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615173844330.png" alt="image-20220615173844330" style="zoom:50%;"></p>
<p>刚才讲的都还只是,假设你模型训练好以后它是怎么运作的,它是怎么做
Testing 的,它是怎么做 Inference 的,Inference 就是 Testing
，那是怎么做训练的呢？</p>
<p>接下来就要讲怎么做训练,那如果是做语音辨识,那你要有<strong>训练资料</strong>,你要收集一大堆的声音讯号,每一句声音讯号都要有工读生来听打一下,打出说它的这个对应的词汇是什么？</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615173903283.png" alt="image-20220615173903283"></p>
<p>工读生听这段是机器学习,他就把机器学习四个字打出来,所以就知道说你的这个
Transformer,应该要学到
听到这段声音讯号,它的输出就是机器学习这四个中文字。</p>
<p>那怎么让机器学到这件事呢?
我们已经知道说输入这段声音讯号,第一个应该要输出的中文字是“机”,所以今天当我们把
BEGIN,丢给这个 Encoder 的时候,它第一个输出应该要跟“机”越接近越好。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615174107627.png" alt="image-20220615174107627" style="zoom:50%;"></p>
<p><strong>“机”这个字会被表示成一个 One-Hot 的 Vector</strong>,在这个
Vector 里面,只有机对应的那个维度是 1,其他都是 0,这是正确答案,那我们的
Decoder,它的输出是一个
Distribution,是一个机率的分布,我们会希望这一个机率的分布,跟这个 One-Hot
的 Vector 越接近越好。所以你会去计算这个 Ground Truth,跟这个
Distribution 它们之间的 Cross Entropy,然后我们希望这个 ==Cross Entropy==
的值,越小越好。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615174938615.png" alt="image-20220615174938615" style="zoom:50%;"></p>
<p>它就<strong>跟分类很像</strong>,刚才助教在讲解作业的时候也有提到这件事情,你可以想成每一次我们在产生,每一次
Decoder
在产生一个中文字的时候,其实就是做了一次分类的问题,中文字假设有四千个,那就是<strong>做有四千个类别的分类的问题</strong></p>
<p>所以实际上训练的时候这个样子,我们已经知道输出应该是“机器学习”这四个字,就告诉你的
Decoder ,现在你第一次的输出 第二次的输出,第三次的输出
第四次输出,应该分别就是“机” “器” “学”跟“习”,这四个中文字的 One-Hot
Vector,我们<strong>希望我们的输出,跟这四个字的 One-Hot Vector
越接近越好</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615175011374.png" alt="image-20220615175011374" style="zoom:50%;"></p>
<p><strong><font color="red"> 在训练的时候,每一个输出都会有一个 Cross
Entropy,每一个输出跟 One-Hot Vector,跟它对应的正确答案都有一个 Cross
Entropy,我们要希望所有的 Cross Entropy
的总和最小越小越好。</font></strong>所以这边做了四次分类的问题,我们希望这些分类的问题,它总合起来的
Cross Entropy 越小越好,<strong>还有 END 这个符号</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615175531572.png" alt="image-20220615175531572" style="zoom:50%;"></p>
<p>那这个就是 Decoder 的训练，<strong>把 Ground Truth ,正确答案给它,希望
Decoder 的输出跟正确答案越接近越好</strong>。</p>
<p>那这边有一件值得我们注意的事情,在<strong>训练</strong>的时候我们会给
Decoder 看<strong>正确答案</strong>,也就是我们会告诉它说</p>
<ul>
<li>在已经有 "BEGIN",在有"机"的情况下你就要输出"器"</li>
<li>有 "BEGIN" 有"机" 有"器"的情况下输出"学"</li>
<li>有 "BEGIN" 有"机" 有"器" 有"学"的情况下输出"习"</li>
<li>有 "BEGIN" 有"机" 有"器" 有"学" 有"习"的情况下,你就要输出"断"</li>
</ul>
<p>在 Decoder
训练的时候,我们会在输入的时候给它<strong>正确的答案</strong>,那这件事情叫做
==Teacher Forcing==</p>
<p>那这个时候你马上就会有一个问题了？</p>
<ul>
<li>训练的时候,Decoder 有偷看到正确答案了</li>
<li>但是测试的时候,显然没有正确答案可以给 Decoder 看</li>
</ul>
<p>刚才也有强调说在真正使用这个模型,在 Inference 的时候,Decoder
看到的是自己的输入,这<strong>中间显然有一个
==Mismatch==</strong>,那等一下我们会有一页投影片的说明,有什么样可能的解决方式。</p>
<h3><span id="25-tips">2.5 Tips</span></h3>
<p>那接下来,不侷限于 Transformer ,讲一些训练这种 Sequence To Sequence
Model 的Tips</p>
<h4><span id="copy-mechanism">Copy Mechanism</span></h4>
<p>在我们刚才的讨论里面,我们都要求 Decoder
自己产生输出,但是对很多任务而言,也许 <strong>Decoder
没有必要自己创造输出</strong>出来,它需要做的事情,也许是<strong>从输入的东西里面复製</strong>一些东西出来。</p>
<p>像这种复製的行为在哪些任务会用得上呢,一个例子是做聊天机器人。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615175700028.png" alt="image-20220615175700028" style="zoom:50%;"></p>
<ul>
<li><p>人对机器说:你好 我是库洛洛,</p></li>
<li><p>机器应该回答说:库洛洛你好 很高兴认识你</p></li>
</ul>
<p>对机器来说,它其实<strong>没有必要创造</strong>库洛洛这个词汇,这对机器来说一定会是一个非常怪异的词汇,所以它可能很难,在训练资料里面可能一次也没有出现过,所以它不太可能正确地产生这段词汇出来。</p>
<p>但是假设今天机器它在学的时候,它学到的是看到输入的时候说我是某某某,就直接把某某某,不管这边是什么复製出来说某某某你好。</p>
<p>那这样子机器的<strong>训练显然会比较容易</strong>,它显然比较有可能得到正确的结果,所以复製对于对话来说,可能是一个需要的技术
需要的能力。</p>
<h4><span id="guided-attention">Guided Attention</span></h4>
<p>机器就是一个黑盒子,有时候它里面学到什么东西,你实在是搞不清楚,那有时候它会犯<strong>非常低级的错误</strong>。但是<strong>对语音辨识
语音合成,Guiding Attention,可能就是一个比较重要的技术</strong>。</p>
<p><strong><font color="red"> Guiding Attention
要做的事情就是,要求机器它在做 Attention
的时候,是有固定的方式的,举例来说,对语音合成或者是语音辨识来说,我们想像中的
Attention,应该就是由左向右。</font></strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615175853638.png" alt="image-20220615175853638" style="zoom:50%;"></p>
<p>在这个例子里面,我们用红色的这个曲线,来代表 Attention
的分数,这个越高就代表 Attention 的值越大</p>
<p>我们以语音合成为例,那你的输入就是一串文字,那你在合成声音的时候,显然是<strong>由左念到右</strong>,所以机器应该是,先看最左边输入的词汇产生声音,再看中间的词汇产生声音,再看右边的词汇产生声音</p>
<p>如果你今天在做语音合成的时候,你发现机器的
Attention,是<strong>颠三倒四的</strong>,它先看最后面,接下来再看前面,那再胡乱看整个句子,那显然有些是做错了,显然有些是,Something
is wrong,有些是做错了,</p>
<p>所以 Guiding Attention 要做的事情就是,强迫 Attention
有一个固定的样貌,那如果你对这个问题,本身就已经有理解知道说,语音合成 TTS
这样的问题,你的 Attention 的分数,Attention
的位置都应该由左向右,那不如就直接把这个限制,放进你的 Training
里面,要求机器学到 Attention,就应该要由左向右。</p>
<h4><span id="optimizing-evaluationmetrics">Optimizing Evaluation
Metrics?</span></h4>
<p><strong><font color="red"> 在作业里面,我们评估的标准用的是,BLEU
Score,BLEU Score 是你的
Decoder,先产生一个完整的句子以后,再去跟正确的答案一整句做比较,我们是拿两个句子之间做比较,才算出
BLEU Score。</font></strong></p>
<p>但我们在训练的时候显然不是这样,<strong>训练</strong>的时候,<strong>每一个词汇是分开考虑的</strong>,训练的时候,我们
Minimize 的是 Cross Entropy,Minimize Cross Entropy,真的可以 Maximize
BLEU Score 吗？</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615180128331.png" alt="image-20220615180128331" style="zoom:67%;"></p>
<p>不一定,因为这两个根本就是,它们可能有一点点的关联,但它们又没有那么直接相关,它们根本就是两个不同的数值,所以我们
Minimize Cross Entropy,不见得可以让 BLEU Score 比较大。</p>
<p><strong>所以你发现说在助教的程式里面,助教在做 Validation
的时候,并不是拿 Cross Entropy 来挑最好的 Model,而是挑 BLEU Score
最高的那一个 Model,所以我们训练的时候,是看 Cross
Entropy,但是我们实际上你作业真正评估的时候,看的是 BLEU Score,所以你
Validation Set,其实应该考虑用 BLEU Score</strong>。</p>
<p>那接下来有人就会想说,那我们能不能<strong>在 Training 的时候,就考虑
BLEU Score 呢</strong>,我们能不能够训练的时候就说,我的 Loss 就是,BLEU
Score 乘一个负号,那我们要 Minimize 那个 Loss,假设你的 Loss 是,BLEU
Score乘一个负号,它也等于就是 Maximize BLEU
Score。但是<strong>这件事实际上没有那么容易</strong>,你当然可以把 BLEU
Score,当做你训练的时候,你要最大化的一个目标,但是 BLEU Score
本身很复杂,它是不能微分的。</p>
<p>这边之所以採用 Cross
Entropy,而且是每一个中文的字分开来算,就是因为这样我们才有办法处理,如果你是要计算,两个句子之间的
BLEU Score,这一个
Loss,根本就没有办法做微分,那怎么办呢？这边就教大家一个口诀,遇到你在
Optimization 无法解决的问题,==用 RL 硬 Train 一发==就对了这样,遇到你无法
Optimize 的 Loss Function,把它当做是 RL 的 Reward,把你的 Decoder 当做是
Agent,它当作是 RL,Reinforcement Learning 的问题硬做。</p>
<h4><span id="scheduled-sampling计划采样">Scheduled Sampling
（计划采样）</span></h4>
<p>那我们要讲到,我们刚才反覆提到的问题了,就是<strong>训练跟测试居然是不一致</strong>的</p>
<p><strong><font color="red"> 测试的时候,Decoder
看到的是自己的输出,所以测试的时候,Decoder
会看到一些错误的东西,但是在训练的时候,Decoder
看到的是完全正确的,那这个不一致的现象叫做,==Exposure
Bias==。</font></strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615181122702.png" alt="image-20220615181122702" style="zoom: 67%;"></p>
<p>假设 Decoder
在训练的时候,永远只看过正确的东西,那在测试的时候,你只要有一个错,那就会<strong>一步错
步步错</strong>,因为对 Decoder
来说,它从来没有看过错的东西,它看到错的东西会非常的惊奇,然后接下来它产生的结果可能都会错掉。</p>
<h4><span id="所以要怎么解决这个问题呢">所以要怎么解决这个问题呢？</span></h4>
<p>有一个可以的思考的方向是,<strong>给 Decoder
的输入加一些错误的东西</strong>,就这么直觉,你不要给 Decoder
都是正确的答案,偶尔给它一些错的东西,它反而会学得更好,这一招叫做,==Scheduled
Sampling==,它不是那个 Schedule Learning Rate,刚才助教有讲 Schedule
Learning Rate,那是另外一件事,不相干的事情,这个是 Scheduled
Sampling。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615181204628.png" alt="image-20220615181204628" style="zoom: 67%;"></p>
<p>Scheduled Sampling 其实很早就有了,这个是 15 年的 Paper,很早就有
Scheduled Sampling,在还没有 Transformer,只有 LSTM 的时候,就已经有
Scheduled Sampling,但是 Scheduled Sampling
这一招,它其实会伤害到,Transformer
的平行化的能力,那细节可以再自己去了解一下,所以对 Transformer 来说,它的
Scheduled Sampling,另有招数跟传统的招数,跟原来最早提在,这个
LSTM上被提出来的招数,也不太一样,那我把一些 Reference
的,列在这边给大家参考。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220615181229880.png" alt="image-20220615181229880" style="zoom:67%;"></p>
<p>Transformer 和种种的训练技巧,这个我们已经讲完了 Encoder,讲完了
Decoder,也讲完了它们中间的关係,也讲了怎么训练,也讲了种种的 Tip。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
