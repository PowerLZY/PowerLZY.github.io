<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/9/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/9/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/9/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">236</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2NQRYHA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2NQRYHA/" class="post-title-link" itemprop="url">深度学习（9）Transformer*-p1</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-10 21:00:41" itemprop="dateCreated datePublished" datetime="2022-06-10T21:00:41+08:00">2022-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-18 19:22:24" itemprop="dateModified" datetime="2023-03-18T19:22:24+08:00">2023-03-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Seq2Seq/" itemprop="url" rel="index"><span itemprop="name">Seq2Seq</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-李宏毅-transformer_encoder"><strong><font color="red">
一、李宏毅 - Transformer_Encoder </font></strong></span></h2>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614150150803.png" alt="image-20220614150150803" style="zoom:50%;"></p>
<p>变形金刚的英文就是Transformer,那Transformer也跟我们之后会,提到的BERT有非常强烈的关系,所以这边有一个BERT探出头来,代表说Transformer跟BERT,是很有关系的。</p>
<h3><span id="11-sequence-to-sequenceseq2seq">1.1 Sequence-to-sequence
(Seq2seq)</span></h3>
<p>Transformer就是一个,==Sequence-to-sequence==的model,他的缩写,我们会写做==Seq2seq==,那Sequence-to-sequence的model,又是什么呢？</p>
<p>举例来说,Seq2seq一个很好的应用就是 <strong>语音辨识</strong>：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614150604358.png" alt="image-20220614150604358" style="zoom:50%;"></p>
<p>在做语音辨识的时候,输入是声音讯号<strong>,声音讯号其实就是一串的vector</strong>,输出是语音辨识的结果,也就是输出的这段声音讯号,所对应的文字。我们这边用圈圈来代表文字,每一个圈圈就代表,比如说中文里面的一个方块子,今天<strong>输入跟输出的长度,当然是有一些关系,但是却没有绝对的关系</strong>，输入的声音讯号,他的长度是大T,我们并没有办法知道说,根据大T输出的这个长度N一定是多少。<strong>输出的长度由机器自己决定</strong>,由机器自己去听这段声音讯号的内容,自己决定他应该要输出几个文字,他输出的语音辨识结果,输出的句子里面应该包含几个字,由机器自己来决定,这个是语音辨识。</p>
<h3><span id="12-question-answering-qa">1.2 Question Answering (QA)</span></h3>
<p>那事实上Seq2Seq model,在NLP的领域,在natural language
processing的领域的使用,是比你想像的更为广泛,其实很多<strong>natural
language processing的任务,都可以想成是==question
answering,QA==的任务。</strong>Question
Answering,就是给机器读一段文字,然后你问机器一个问题,希望他可以给你一个正确的答案。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614151056373.png" alt="image-20220614151056373" style="zoom:50%;"></p>
<ul>
<li>假设你今天想做的是翻译,那机器读的文章就是一个英文句子,<strong>问题</strong>就是这个句子的德文翻译是什么,然后输出的<strong>答案</strong>就是德文</li>
<li>或者是你想要叫机器自动作摘要,摘要就是给机器读一篇长的文章,叫他把长的文章的重点节录出来,那你就是给机器一段文字,<strong>问题</strong>是这段文字的摘要是什么,然后期待他<strong>答案</strong>可以输出一个摘要</li>
<li>或者是你想要叫机器做Sentiment analysis,Sentiment
analysis就是机器要自动判断一个句子,是正面的还是负面的；假设你有做了一个产品,然后上线以后,你想要知道网友的评价,但是你又不可能一直找人家ptt上面,把每一篇文章都读过,所以就做一个Sentiment
analysis
model,看到有一篇文章里面,有提到你的产品,然后就把这篇文章丢到,你的model里面,去判断这篇文章,是正面还是负面。你就给机器要判断正面还负面的文章,<strong>问题</strong>就是这个句子,是正面还是负面的,然后希望机器可以告诉你<strong>答案</strong></li>
</ul>
<p>必须要强调一下,对多数NLP的任务,或对多数的语音相关的任务而言,往往为这些任务<strong>客制化模型,你会得到更好的结果</strong>。但是各个任务客制化的模型,就不是我们这一门课的重点了,如果你对人类语言处理,包括语音
包括自然语言处理,这些相关的任务有兴趣的话呢,可以参考一下以下课程网页的<a href="Source%20webpage:%20https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.html">连结</a>,就是去年上的深度学习,与人类语言处理,这门课的内容里面就会教你,各式各样的任务最好的模型,应该是什么。</p>
<blockquote>
<p>举例来说在做语音辨识,我们刚才讲的是一个Seq2Seq
model,输入一段声音讯号,直接输出文字,今天啊 Google的
pixel4,Google官方告诉你说,Google pixel4也是用,N to N的Neural
network,pixel4里面就是,有一个Neural
network,输入声音讯号,输出就直接是文字。</p>
<p>但他其实用的不是Seq2Seq model,他用的是一个叫做,RNN transducer的
model,像这些模型他就是为了,语音的某些特性所设计,这样其实可以表现得更好,至于每一个任务,有什么样客制化的模型,这个就是另外一门课的主题,就不是我们今天想要探讨的重点。</p>
</blockquote>
<h3><span id="13-seq2seq-for-syntacticparsing">1.3 Seq2seq for Syntactic
Parsing</span></h3>
<p>在语音还有自然语言处理上的应用,其实有很多应用,你<strong>不觉得他是一个Seq2Seq
model的问题,但你都可以硬用Seq2Seq model的问题硬解他</strong>。</p>
<p>举例来说<strong>文法剖析</strong>,给机器一段文字,比如Deep learning is
very powerful</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614151449212.png" alt="image-20220614151449212" style="zoom:50%;"></p>
<p>机器要做的事情是产生,一个<strong>文法的剖析树</strong>
告诉我们,deep加learning合起来,是一个名词片语,very加powerful合起来,是一个形容词片语,形容词片语加is以后会变成,一个动词片语,动词片语加名词片语合起来,是一个句子。</p>
<p>那今天文法剖析要做的事情,就是产生这样子的一个Syntactic
tree,所以在文法剖析的任务里面,假设你想要deep
learning解的话,输入是一段文字,他是一个Sequence,但输出看起来不像是一个Sequence,输出是一个树状的结构,但<strong>事实上一个树状的结构,可以硬是把他看作是一个Sequence</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614151522382.png" alt="image-20220614151522382" style="zoom:50%;"></p>
<p>这个树状结构可以对应到一个,这样子的Sequence,从这个Sequence里面,你也可以看出</p>
<ul>
<li>这个树状的结构有一个S，有一个左括号,有一个右括号</li>
<li>S里面有一个noun phrase,有一个左括号跟右括号</li>
<li>NP里面有一个左括号跟右括号,NP里面有is</li>
<li>然后有这个形容词片语,他有一个左括号右括号</li>
</ul>
<p>这一个<strong>Sequence就代表了这一个tree
的structure</strong>,你先把tree
的structure,转成一个Sequence以后,你就可以用Seq2Seq
model硬解他。train一个Seq2Seq
model,读这个句子,然后直接输入这一串文字,再把这串文字转成一个树状的结构,你就可以硬是用Seq2Seq
model,来做文法剖析这件事,这个概念听起来非常的狂,但这是真的可以做得到的。</p>
<h3><span id="14multi-label-classification"><strong><font color="red"> 1.4
multi-label classification</font></strong></span></h3>
<p>还有一些任务可以用seq2seq's model,举例来说
==multi-label的classification==。==multi-class==的classification,跟==multi-label==的classification,听起来名字很像,但他们其实是不一样的事情,multi-class的classification意思是说,我们有不只一个class机器要做的事情,是从数个class里面,选择某一个class出来。</p>
<p>但是multi-label的classification,意思是说<strong>同一个东西,它可以属于多个class</strong>,举例来说
你在做文章分类的时候。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614151845511.png" alt="image-20220614151845511" style="zoom:50%;"></p>
<p>可能这篇文章 属于class 1跟3,这篇文章属于class 3 9
17等等,你可能会说,这种multi-label
classification的问题,能不能<strong>直接把它当作一个multi-class
classification的问题来解</strong></p>
<p>举例来说,我把这些文章丢到一个classifier里面：</p>
<ul>
<li><strong>本来classifier只会输出一个答案,输出分数最高的那个答案</strong></li>
<li><strong>我现在就输出分数最高的前三名,看看能不能解,multi-label的classification的问题</strong></li>
</ul>
<p>但<strong>这种方法可能是行不通的</strong>,因为每一篇文章对应的class的数目,根本不一样
有些东西 有些文章,对应的class的数目,是两个 有的是一个 有的是三个。所以
如果你说 我直接取一个threshold,我直接取分数最高的前三名,class file
output分数最高的前三名,来当作我的输出 显然,不一定能够得到好的结果
那怎么办呢？</p>
<p>这边可以用seq2seq硬做,<strong>输入一篇文章</strong>
<strong>输出就是class</strong> 就结束了,机器自己决定
它要输出几个class。我们说seq2seq
model,就是由机器自己决定输出几个东西,输出的output
sequence的长度是多少,既然你没有办法决定class的数目,那就让机器帮你决定,每篇文章
要属于多少个class。</p>
<h3><span id="15-encoder-decoder">1.5 Encoder-Decoder</span></h3>
<p><strong><font color="red">
我们现在就是要来学,怎么做seq2seq这件事,一般的seq2seq's
model,它里面会分成两块一块是Encoder,另外一块是Decoder。</font></strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614152636868.png" alt="image-20220614152636868" style="zoom:50%;"></p>
<p>你input一个sequence有Encoder,负责处理这个sequence,再把处理好的结果丢给Decoder,由Decoder决定,它要输出什么样的sequence,等一下
我们都还会再细讲,Encoder跟 Decoder内部的架构。seq2seq
model的起源,其实非常的早 在14年的9月,就有一篇seq2seq's
model,用在翻译的文章 被放到Arxiv上。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614152900181.png" alt="image-20220614152900181" style="zoom:50%;"></p>
<p><strong>可以想像当时的seq2seq's
model,看起来还是比较年轻的,今天讲到seq2seq's
model的时候,大家第一个会浮现在脑中的,可能都是我们今天的主角,也就是transformer</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614152931023.png" alt="image-20220614152931023" style="zoom: 50%;"></p>
<p>它有一个Encoder架构,有一个Decoder架构,它里面有很多花花绿绿的block,等一下就会讲一下,这里面每一个花花绿绿的block,分别在做的事情是什么。</p>
<h3><span id="151-encoder">1.5.1 Encoder</span></h3>
<p><strong><font color="red"> seq2seq model
==Encoder==要做的事情,就是给一排向量，输出另外一排向量</font></strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614153244524.png" alt="image-20220614153244524" style="zoom:50%;"></p>
<p>给一排向量、输出一排向量这件事情,很多模型都可以做到,可能第一个想到的是,我们刚刚讲完的self-attention,其实不只self-attention,RNN
CNN 其实也都能够做到,input一排向量, output另外一个同样长度的向量。</p>
<p>在transformer里面,transformer的Encoder,用的就是self-attention,
这边看起来有点复杂,我们用另外一张图,来仔细地解释一下,这个Encoder的架构,等一下再来跟原始的transformer的,论文里面的图进行比对。</p>
<p>现在的Encoder里面,会<strong>分成很多很多的block</strong>：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614153801973.png" alt="image-20220614153801973" style="zoom:50%;"></p>
<p>每一个block都是输入一排向量,输出一排向量,你输入一排向量
第一个block,第一个block输出另外一排向量,再输给另外一个block,到最后一个block,会输出最终的vector
sequence,<strong>每一个block 其实,并不是neural
network的一层</strong>。</p>
<p><strong>每一个block里面做的事情,是好几个layer在做的事情</strong>,在transformer的Encoder里面,每一个block做的事情,大概是这样子的：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614153842957.png" alt="image-20220614153842957" style="zoom:50%;"></p>
<ul>
<li><strong><font color="red">先做一个self-attention,input一排vector以后,做self-attention,考虑整个sequence的资讯，Output另外一排vector。</font></strong></li>
<li><strong><font color="red"> 接下来这一排vector,会再丢到fully
connected的feed forward
network里面,再output另外一排vector,这一排vector就是block的输出。</font></strong></li>
</ul>
<h4><span id="multi-self-attention-residual-connection">Multi-self-attention +
residual connection</span></h4>
<p>事实上在原来的<strong>transformer里面,它做的事情是更复杂的</strong>。在之前self-attention的时候,我们说
输入一排vector,就输出一排vector,这边的每一个vector,它是<strong>考虑了所有的input以后</strong>,所得到的结果：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614154211285.png" alt="image-20220614154211285" style="zoom:50%;"></p>
<p>在transformer里面,它加入了一个设计,我们<strong>不只是输出这个vector</strong>,我们还要<strong>把这个vector加上它的input</strong>,它要把input拉过来
直接加给输出,得到新的output。也就是说,这边假设这个vector叫做<span class="math inline">\(a\)</span>,这个vector叫做<span class="math inline">\(b\)</span> 你要把<span class="math inline">\(a+b\)</span>当作是新的输出。</p>
<p><strong><font color="red"> 这样子的network架构,叫做==residual
connection==,那其实这种residual connection</font></strong>,在deep
learning的领域用的是非常的广泛,之后如果我们有时间的话,再来详细介绍,为什么要用residual
connection。</p>
<p>那你现在就先知道说,有一种network设计的架构,叫做<strong>residual
connection,它会把input直接跟output加起来,得到新的vector。</strong></p>
<h4><span id="norm">Norm</span></h4>
<p>得到<strong>residual</strong>的结果以后,再把它做一件事情叫做<strong>normalization</strong>,这边用的不是batch
normalization,这边用的叫做==<strong>layer normalization</strong>==。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614155147989.png" alt="image-20220614155147989" style="zoom:50%;"></p>
<p>layer normalization做的事情,比bacth
normalization更简单一点。输入一个向量，输出另外一个向量,不需要考虑batch,它会<strong>把输入的这个向量,计算它的mean跟standard
deviation</strong>。</p>
<p>但是要注意一下,<strong>==batch
normalization==是对不同example,不同feature的同一个dimension,去计算mean跟standard
deviation</strong>。但<strong>==layer
normalization==,它是对同一个feature,同一个example里面,不同的dimension,去计算mean跟standard
deviation</strong></p>
<p>计算出mean,跟standard deviation以后,就可以做一个normalize,我们把input
这个vector里面每一个,dimension减掉mean,再除以standard
deviation以后得到x',就是layer normalization的输出。 <span class="math display">\[
x&#39;_i=\frac{x_i-m}{\sigma}
\]</span> <strong>得到layer normalization的输出以后,它的这个输出，才是FC
network的输入。</strong></p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614155555973.png" alt="image-20220614155555973" style="zoom:50%;"></p>
<p>而<strong>FC network这边,也有residual的架构</strong>,所以 我们会把FC
network的input,跟它的output加起来做一下residual,得到新的输出。这个FC
network做完residual以后,还不是结束
你要把residual的结果,<strong>再做一次layer
normalization</strong>,得到的输出,才是residual
network里面,一个block的输出。</p>
<p><img src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210429212721750.png?raw=true" alt="image-20210429212721750" style="zoom:50%;"></p>
<ul>
<li>首先有self-attention,其实在input的地方,还有加上<strong>positional
encoding</strong>,我们之前已经有讲过,如果你只光用self-attention,你没有未知的资讯,所以你需要加上positional的information,然后在这个图上,有特别画出positional的information。</li>
<li><strong>Multi-Head
Attention</strong>,这个就是self-attention的block,这边有特别强调说,它是Multi-Head的self-attention。</li>
<li><strong>Add&amp;norm</strong>,就是residual加layer
normalization,我们刚才有说self-attention,有加上residual的connection,加下来还要过<strong>layer
normalization</strong>,这边这个图上的Add&amp;norm,就是residual加layer
norm的意思。</li>
<li>接下来,要过<strong>feed forward network</strong>，fc的feed forward
network以后再做一次<strong>Add&amp;norm</strong>,再做一次residual加layer
norm,才是一个block的输出。</li>
</ul>
<p>然后这个block会重复n次,这个复杂的block,其实在之后会讲到的,一个非常重要的模型BERT里面,会再用到
BERT,它其实就是transformer的encoder。</p>
<h2><span id="to-learn-more">To Learn more</span></h2>
<p>讲到这边 你心里一定充满了问号,就是为什么
transformer的encoder,要这样设计 不这样设计行不行?</p>
<p>行
不一定要这样设计,这个encoder的network架构,现在设计的方式,本文是按照原始的论文讲给你听的,但<strong>原始论文的设计
不代表它是最好的,最optimal的设计</strong>。</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220614155914421.png" alt="image-20220614155914421" style="zoom:50%;"></p>
<ul>
<li>有一篇文章叫,<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">on layer
normalization in the transformer
architecture</a>，它问的问题就是<strong>为什么,layer
normalization是放在那个地方呢,</strong>为什么我们是先做,residual再做layer
normalization,能不能够把layer
normalization,放到每一个block的input,也就是说 你做residual以后,再做layer
normalization,再加进去
你可以看到说左边这个图,是原始的transformer,右边这个图是稍微把block,更换一下顺序以后的transformer,更换一下顺序以后
结果是会比较好的,这就代表说,原始的transformer
的架构,并不是一个最optimal的设计,你永远可以思考看看,有没有更好的设计方式</li>
<li><strong><font color="red"> 再来还有一个问题就是,为什么是layer norm
为什么是别的,不是别的,为什么不做batch
normalization</font></strong>,也许这篇paper可以回答你的问题,这篇paper是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.07845">Power Norm：,Rethinking Batch
Normalization In Transformers</a>,它首先告诉你说 为什么,batch
normalization不如,layer normalization,在Transformers里面为什么,batch
normalization不如,layer
normalization,接下来在说,它提出来一个<strong>power
normalization</strong>,一听就是很power的意思,都可以比layer
normalization,还要performance差不多或甚至好一点。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/PSEWDM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/PSEWDM/" class="post-title-link" itemprop="url">深度学习（9）Transformer-code</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-10 21:00:41" itemprop="dateCreated datePublished" datetime="2022-06-10T21:00:41+08:00">2022-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 21:12:11" itemprop="dateModified" datetime="2022-07-13T21:12:11+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Seq2Seq/" itemprop="url" rel="index"><span itemprop="name">Seq2Seq</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>19k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>35 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="transformer">Transformer</span></h1>
<blockquote>
<p>Transformer：（Self-attention）自注意力机制的序列到序列的模型</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/415318478">Transformer
代码完全解读!</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/471328838/answer/1996725528">如何从浅入深理解transformer？</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/428626879/answer/1556915218">Transformer和GNN有什么联系吗？</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221"><strong>详解<em>Transformer</em>
（Attention Is All You Need）</strong></a></li>
<li><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/438634058"><em>Transformer</em>代码+面试细节</a></strong></li>
</ul>
</blockquote>
<h3><span id="一-模型结构概述">一、模型结构概述</span></h3>
<p>如下是Transformer的两个结构示意图：</p>
<p><img src="https://pic2.zhimg.com/80/v2-7d8daf8e5dbba5ed3f26f3e03f61d395_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>上图是从一篇英文博客中截取的Transformer的结构简图，下图是原论文中给出的结构简图，更细粒度一些，可以结合着来看。</p>
<p><img src="https://pic1.zhimg.com/80/v2-f70598121a15f713eeafa4bc696d528c_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>模型大致分为<code>Encoder</code>(编码器)和<code>Decoder</code>(解码器)两个部分，分别对应上图中的左右两部分。</p>
<p><strong>编码器</strong>由N个相同的层堆叠在一起(我们后面的实验取N=6)，每一层又有两个子层：</p>
<ul>
<li>第一个子层是一个<code>Multi-Head Attention</code>(<strong>==多头的自注意机制==</strong>)
<ul>
<li>Self-attention多个头类似于cnn中多个卷积核的作用，使用多头注意力，能够从不同角度提取信息，提高信息提取的全面性。</li>
</ul></li>
<li>第二个子层是一个简单的<code>Feed Forward</code>(全连接前馈网络)</li>
<li>两个子层都添加了一个<strong>残差连接</strong>+==layer
normalization==的操作。</li>
</ul>
<p><strong>解码器</strong>同样是堆叠了N个相同的层，不过和编码器中每层的结构稍有不同。</p>
<ul>
<li>第一个子层是一个<code>Multi-Head Attention</code>(<strong>==多头的自注意机制==</strong>)</li>
<li>第二个子层是一个简单的<code>Feed Forward</code>(全连接前馈网络)</li>
<li><strong>==Masked Multi-Head Attention==</strong></li>
<li>每个子层同样也用了<strong>==residual==</strong>以及layer
normalization。</li>
</ul>
<p>模型的输入由<code>Input Embedding</code>和<code>Positional Encoding</code>(位置编码)两部分组合而成。</p>
<p>模型的输出由Decoder的输出简单的经过softmax得到。</p>
<h3><span id="二-模型输入">二、<strong>模型输入</strong></span></h3>
<p>首先我们来看模型的输入是什么样的，先明确模型输入，后面的模块理解才会更直观。输入部分包含两个模块，<code>Embedding</code>和<code>Positional Encoding</code>。</p>
<h4><span id="21-embedding层"><strong>2.1 Embedding层</strong></span></h4>
<p><strong>Embedding层的作用是将某种格式的输入数据，例如文本，转变为模型可以处理的向量表示，来描述原始数据所包含的信息</strong>。<code>Embedding</code>层输出的可以理解为当前时间步的特征，如果是文本任务，这里就可以是<code>Word Embedding</code>，如果是其他任务，就可以是任何合理方法所提取的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        类的初始化函数</span></span><br><span class="line"><span class="string">        d_model：指词嵌入的维度</span></span><br><span class="line"><span class="string">        vocab:指词表的大小</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment">#之后就是调用nn中的预定义层Embedding，获得一个词嵌入对象self.lut</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="comment">#最后就是将d_model传入类中</span></span><br><span class="line">        self.d_model =d_model</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Embedding层的前向传播逻辑</span></span><br><span class="line"><span class="string">        参数x：这里代表输入给模型的单词文本通过词表映射后的one-hot向量</span></span><br><span class="line"><span class="string">        将x传给self.lut并与根号下self.d_model相乘作为结果返回</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        embedds = self.lut(x)</span><br><span class="line">        <span class="keyword">return</span> embedds * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<h4><span id="22-位置编码">2.2 <strong>位置编码</strong></span></h4>
<p><strong><code>Positional Encodding</code>位置编码的作用是为模型提供当前时间步的前后出现顺序的信息</strong>。因为Transformer不像RNN那样的循环结构有前后不同时间步输入间天然的先后顺序，所有的时间步是同时输入，并行推理的，因此在时间步的特征中融合进位置编码的信息是合理的。位置编码可以有很多选择，可以是固定的，也可以设置成可学习的参数。这里，我们使用固定的位置编码。<strong>具体地，使用不同频率的sin和cos函数来进行位置编码</strong>，如下所示：
<span class="math display">\[
\begin{gathered}
P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text {model
}}}\right) \\
P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\text {model
}}}\right)
\end{gathered}
\]</span>
其中pos代表时间步的下标索引，向量也就是第pos个时间步的位置编码，编码长度同<code>Embedding</code>层，这里我们设置的是512。上面有两个公式，代表着位置编码向量中的元素，奇数位置和偶数位置使用两个不同的公式。思考：<strong>为什么上面的公式可以作为位置编码？</strong>我的理解：在上面公式的定义下，<strong><font color="red">
时间步p和时间步p+k的位置编码的内积，即是与p无关，只与k有关的定值（不妨自行证明下试试）。也就是说，任意两个相距k个时间步的位置编码向量的内积都是相同的，这就相当于蕴含了两个时间步之间相对位置关系的信息。</font></strong>此外，每个时间步的位置编码又是唯一的，这两个很好的性质使得上面的公式作为位置编码是有理论保障的。下面是位置编码模块的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        位置编码器类的初始化函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        共有三个参数，分别是</span></span><br><span class="line"><span class="string">        d_model：词嵌入维度</span></span><br><span class="line"><span class="string">        dropout: dropout触发比率</span></span><br><span class="line"><span class="string">        max_len：每个句子的最大长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings</span></span><br><span class="line">        <span class="comment"># 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。</span></span><br><span class="line">        <span class="comment"># 这样计算是为了避免中间的数值计算结果超出float的范围，</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>因此，可以认为，最终模型的输入是若干个时间步对应的embedding，每一个时间步对应一个embedding，可以理解为是当前时间步的一个综合的特征信息，即包含了本身的语义信息，又包含了当前时间步在整个句子中的位置信息。</p>
<h4><span id="23encoder和decoder都包含输入模块"><strong>2.3
Encoder和Decoder都包含输入模块</strong></span></h4>
<p>此外有一个点刚刚接触Transformer的同学可能不太理解，<strong>编码器和解码器两个部分都包含输入，且两部分的输入的结构是相同的，只是推理时的用法不同，编码器只推理一次，而解码器是类似RNN那样循环推理，不断生成预测结果的。</strong></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-ab0188042d72481d479f8951dc0d702c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>怎么理解？假设我们现在做的是一个法语-英语的机器翻译任务，想把<code>Je suis étudiant</code>翻译为<code>I am a student</code>。那么我们输入给编码器的就是时间步数为3的embedding数组，编码器只进行一次并行推理，即获得了对于输入的法语句子所提取的若干特征信息。而对于解码器，是循环推理，逐个单词生成结果的。最开始，由于什么都还没预测，我们会将编码器提取的特征，以及一个句子起始符传给解码器，解码器预期会输出一个单词<code>I</code>。然后有了预测的第一个单词，我们就将<code>I</code>输入给解码器，会再预测出下一个单词<code>am</code>，再然后我们将<code>I am</code>作为输入喂给解码器，以此类推直到预测出句子终止符完成预测。</p>
<h3><span id="三-encoder">三、<strong>Encoder</strong></span></h3>
<h4><span id="31-编码器"><strong>3.1 编码器</strong></span></h4>
<p><strong><font color="red">
编码器作用是用于对输入进行特征提取，为解码环节提供有效的语义信息整体来看编码器由N个编码器层简单堆叠而成</font></strong>，因此实现非常简单，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个clones函数，来更方便的将某个结构复制若干份</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Encoder</span></span><br><span class="line"><span class="string">    The encoder is composed of a stack of N=6 identical layers.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># 调用时会将编码器层传进来，我们简单克隆N分，叠加在一起，组成完整的Encoder</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p><strong>上面的代码中有一个小细节，就是编码器的输入除了x，也就是embedding以外，还有一个<code>mask</code>，为了介绍连续性</strong>，这里先忽略，后面会讲解。下面我们来看看单个的编码器层都包含什么，如何实现。</p>
<h4><span id="32-编码器层"><strong>3.2 编码器层</strong></span></h4>
<p>每个编码器层由两个子层连接结构组成：<strong>第一个子层包括一个多头自注意力层和规范化层以及一个残差连接</strong>；<strong>第二个子层包括一个前馈全连接层和规范化层以及一个残差连接</strong>；如下图所示：</p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-4a57b7e6f8a4a7260c4e841f393f873a_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>可以看到，两个子层的结构其实是一致的，只是中间核心层的实现不同</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-ee127bacaf444e5c3612ca819b53bb8c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://pic1.zhimg.com/80/v2-4f3a1f34553d5d568c99e8d2ace9e6c0_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们先定义一个SubLayerConnection类来描述这种结构关系:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    实现子层连接结构的类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 原paper的方案</span></span><br><span class="line">        <span class="comment">#sublayer_out = sublayer(x)</span></span><br><span class="line">        <span class="comment">#x_norm = self.norm(x + self.dropout(sublayer_out))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 稍加调整的版本</span></span><br><span class="line">        sublayer_out = sublayer(x)</span><br><span class="line">        sublayer_out = self.dropout(sublayer_out)</span><br><span class="line">        x_norm = x + self.norm(sublayer_out)</span><br><span class="line">        <span class="keyword">return</span> x_norm</span><br></pre></td></tr></table></figure>
<p>注：上面的实现中，我对残差的链接方案进行了小小的调整，和原论文有所不同。<strong>把x从norm中拿出来，保证永远有一条“高速公路”，这样理论上会收敛的快一些，但我无法确保这样做一定是对的，请一定注意</strong>。定义好了SubLayerConnection，我们就可以实现EncoderLayer的结构了.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;EncoderLayer is made up of two sublayer: self-attn and feed forward&quot;</span>                                                                                                         </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size   <span class="comment"># embedding&#x27;s dimention of model, 默认512</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># attention sub layer</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="comment"># feed forward sub layer</span></span><br><span class="line">        z = self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<p>继续往下拆解，我们需要了解 attention层 和
feed_forward层的结构以及如何实现。</p>
<h4><span id="33-注意力机制-self-attention">3.3 注意力机制 Self-Attention</span></h4>
<p>人类在观察事物时，无法同时仔细观察眼前的一切，只能聚焦到某一个局部。通常我们大脑在简单了解眼前的场景后，能够很快把注意力聚焦到最有价值的局部来仔细观察，从而作出有效判断。或许是基于这样的启发，大家想到了在算法中利用注意力机制。注意力计算：它需要三个指定的输入Q（query），K（key），V（value），然后通过下面公式得到注意力的计算结果。</p>
<blockquote>
<figure>
<img src="https://www.zhihu.com/equation?tex=A%28Q%2CK%2CV%29%3D%5Cmathrm%7BSoftmax%7D%28QK%5ET%29V+%5C%5C+" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<ul>
<li><figure>
<img src="https://www.zhihu.com/equation?tex=Q%2CK%2CV%3An%5Ctimes+d" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure></li>
<li>相似度计算 <img src="https://www.zhihu.com/equation?tex=QK%5ET" alt="[公式]"> ： <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]"> 与
<img src="https://www.zhihu.com/equation?tex=d%5Ctimes+n" alt="[公式]"> 运算，得到 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="[公式]">
矩阵，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29" alt="[公式]"></li>
<li>softmax计算：对每行做softmax，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%29" alt="[公式]"> ，则n行的复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2%29" alt="[公式]"></li>
<li>加权和： <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]">
运算，得到 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]"> 矩阵，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29" alt="[公式]"></li>
</ul>
<p>故最后self-attention的时间复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29" alt="[公式]">; 对于受限的self-attention，每个元素仅能和周围 <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]">
个元素进行交互，即和 <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]"> 个 <img src="https://www.zhihu.com/equation?tex=d" alt="[公式]"> 维向量做内积运算，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28rd%29" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 个元素的总时间复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%28rnd%29%7D" alt="[公式]"></p>
</blockquote>
<p><strong>计算流程图如下：</strong></p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-be94b689af1b76a1f64a2581709d67cd_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>可以这么简单的理解，<strong>当前时间步的注意力计算结果，是一个组系数
*
每个时间步的特征向量value的累加，而这个系数，通过当前时间步的query和其他时间步对应的key做内积得到，这个过程相当于用自己的query对别的时间步的key做查询，判断相似度，决定以多大的比例将对应时间步的信息继承过来</strong>。下面是注意力模块的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    <span class="comment">#首先取query的最后一维的大小，对应词嵌入维度</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#按照注意力公式，将query与key的转置相乘，这里面key是将最后两个维度进行转置，再除以缩放系数得到注意力得分张量scores</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="comment">#接着判断是否使用掩码张量</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment">#使用tensor的masked_fill方法，将掩码张量和scores张量每个位置一一比较，如果掩码张量则对应的scores张量用-1e9这个置来替换</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    <span class="comment">#对scores的最后一维进行softmax操作，使用F.softmax方法，这样获得最终的注意力张量</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#之后判断是否使用dropout进行随机置0</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="comment">#最后，根据公式将p_attn与value张量相乘获得最终的query注意力表示，同时返回注意力张量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<h4><span id="34-多头注意力机制"><strong>3.4 多头注意力机制</strong></span></h4>
<p><strong>刚刚介绍了attention机制，在搭建EncoderLayer时候所使用的Attention模块，实际使用的是多头注意力，可以简单理解为多个注意力模块组合在一起。</strong></p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-e0f18101e6c6c621c87bcb880eb3c795_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><font color="red">
多头注意力机制的作用：这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元表达，实验表明可以从而提升模型效果。</font></strong></p>
<blockquote>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BMultiHead%7D%28Q%2CK%2CV%29%3D%5Cmathrm%7BConcat%28head_1%2C...%2Chead_h%29%7DW%5EO+%5C%5C+%5Cmathrm%7Bwhere%5Cquad+head_i%7D%3DA%28QW_i%5EQ%2CKW_i%5EK%2CVW_i%5EV%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>对于multi-head attention，假设有 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 个head，这里
<img src="https://www.zhihu.com/equation?tex=h" alt="[公式]">
是一个常数，对于每个head，首先需要把三个矩阵分别映射到 <img src="https://www.zhihu.com/equation?tex=d_q%2Cd_k%2Cd_v" alt="[公式]">
维度。这里考虑一种简化情况： <img src="https://www.zhihu.com/equation?tex=d_q%3Dd_k%3Dd_v%3D%5Cfrac%7Bd%7D%7Bh%7D" alt="[公式]"> 。(对于dot-attention计算方式， <img src="https://www.zhihu.com/equation?tex=d_k" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=d_v" alt="[公式]">
可以不同)。</p>
<ul>
<li>输入线性映射的复杂度： <img src="https://www.zhihu.com/equation?tex=n+%5Ctimes+d" alt="[公式]"> 与
<img src="https://www.zhihu.com/equation?tex=d%5Ctimes+%5Cfrac%7Bd%7D%7Bh%7D" alt="[公式]"> 运算，忽略常系数，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28nd%5E2%29" alt="[公式]"> 。</li>
<li>Attention操作复杂度：主要在相似度计算及加权和的开销上， <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+%5Cfrac%7Bd%7D%7Bh%7D" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%7D%7Bh%7D%5Ctimes+%7Bn%7D" alt="[公式]"> 运算，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%29" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7B%7D" alt="[公式]"></li>
<li>输出线性映射的复杂度：concat操作拼起来形成 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]">
的矩阵，然后经过输出线性映射，保证输入输出相同，所以是 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+d" alt="[公式]"> 与
<img src="https://www.zhihu.com/equation?tex=d%5Ctimes+d" alt="[公式]"> 计算，复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28nd%5E2%29" alt="[公式]"></li>
</ul>
<p>故最后的复杂度为： <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2d%2Bnd%5E2%29" alt="[公式]"></p>
</blockquote>
<p>举个更形象的例子，<strong>bank是银行的意思，如果只有一个注意力模块，那么它大概率会学习去关注类似money、loan贷款这样的词。如果我们使用多个多头机制，那么不同的头就会去关注不同的语义，比如bank还有一种含义是河岸，那么可能有一个头就会去关注类似river这样的词汇，这时多头注意力的价值就体现出来了</strong>。下面是多头注意力机制的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment">#在类的初始化时，会传入三个参数，h代表头数，d_model代表词嵌入的维度，dropout代表进行dropout操作时置0比率，默认是0.1</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="comment">#在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，这是因为我们之后要给每个头分配等量的词特征，也就是embedding_dim/head个</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment">#得到每个头获得的分割词向量维度d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        <span class="comment">#传入头数h</span></span><br><span class="line">        self.h = h</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#创建linear层，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用，为什么是四个呢，这是因为在多头注意力中，Q,K,V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        <span class="comment">#self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None</span></span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment">#前向逻辑函数，它输入参数有四个，前三个就是注意力机制需要的Q,K,V，最后一个是注意力机制中可能需要的mask掩码张量，默认是None</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            <span class="comment">#使用unsqueeze扩展维度，代表多头中的第n头</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        <span class="comment"># 首先利用zip将输入QKV与三个线性层组到一起，然后利用for循环，将输入QKV分别传到线性层中，做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结构进行维度重塑，多加了一个维度h代表头，这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，计算机会根据这种变换自动计算这里的值，然后对第二维和第三维进行转置操作，为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，从attention函数中可以看到，利用的是原始输入的倒数第一和第二维，这样我们就得到了每个头的输入</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        <span class="comment"># 得到每个头的输入后，接下来就是将他们传入到attention中，这里直接调用我们之前实现的attention函数，同时也将mask和dropout传入其中</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        <span class="comment"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法。这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，所以，下一步就是使用view重塑形状，变成和输入形状相同。  </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment">#最后使用线性层列表中的最后一个线性变换得到最终的多头注意力结构的输出</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<h4><span id="35-前馈全连接层"><strong>3.5 前馈全连接层</strong></span></h4>
<p><strong>EncoderLayer中另一个核心的子层是 Feed Forward
Layer</strong>，我们这就介绍一下。在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-776124756aeaa1aab51f630819d372b7_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><font color="red"> Feed Forward Layer
其实就是简单的由两个前向全连接层组成，核心在于，Attention模块每个时间步的输出都整合了所有时间步的信息，==而Feed
Forward
Layer每个时间步只是对自己的特征的一个进一步整合，与其他时间步无关。==</font></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment">#初始化函数有三个输入参数分别是d_model，d_ff，和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，因为我们希望输入通过前馈全连接层后输入和输出的维度不变，第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出，最后一个是dropout置0比率。</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#输入参数为x，代表来自上一层的输出，首先经过第一个线性层，然后使用F中的relu函数进行激活，之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<p>到这里Encoder中包含的主要结构就都介绍了，上面的代码中涉及了两个小细节还没有介绍，<strong>layer
normalization 和 mask</strong>，下面来简单讲解一下。</p>
<h4><span id="36-规范化层"><strong>3.6. 规范化层</strong></span></h4>
<p><strong>规范化层的作用：它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后输出可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常慢</strong>。因此都会在一定层后接规范化层进行数值的规范化，使其特征数值在合理范围内。Transformer中使用的normalization手段是layer
norm，实现代码很简单，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, feature_size, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="comment">#初始化函数有两个参数，一个是features,表示词嵌入的维度,另一个是eps它是一个足够小的数，在规范化公式的分母中出现,防止分母为0，默认是1e-6。</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        <span class="comment">#根据features的形状初始化两个参数张量a2，和b2，第一初始化为1张量，也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，这两个张量就是规范化层的参数。因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，使其即能满足规范化要求，又能不改变针对目标的表征，最后使用nn.parameter封装，代表他们是模型的参数</span></span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(feature_size))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(feature_size))</span><br><span class="line">        <span class="comment">#把eps传到类中</span></span><br><span class="line">        self.eps = eps</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment">#输入参数x代表来自上一层的输出，在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致，接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果。</span></span><br><span class="line">    <span class="comment">#最后对结果乘以我们的缩放参数，即a2,*号代表同型点乘，即对应位置进行乘法操作，加上位移参b2，返回即可</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<h4><span id="37-掩码及其作用"><strong>3.7 掩码及其作用</strong></span></h4>
<p><strong>掩码：掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有0和1；代表位置被遮掩或者不被遮掩。</strong>掩码的作用：<strong><font color="red">
在transformer中，掩码主要的作用有两个，一个是屏蔽掉无效的padding区域，一个是屏蔽掉来自“未来”的信息。</font></strong></p>
<p><strong>Encoder中的掩码主要是起到第一个作用，Decoder中的掩码则同时发挥着两种作用</strong>。屏蔽掉无效的padding区域：我们训练需要组batch进行，就以机器翻译任务为例，一个batch中不同样本的输入长度很可能是不一样的，此时我们要设置一个最大句子长度，然后对空白区域进行padding填充，而填充的区域无论在Encoder还是Decoder的计算中都是没有意义的，因此需要用mask进行标识，屏蔽掉对应区域的响应。屏蔽掉来自未来的信息：我们已经学习了attention的计算流程，它是会综合所有时间步的计算的，那么在解码的时候，就有可能获取到未来的信息，这是不行的。因此，这种情况也需要我们使用mask进行屏蔽。现在还没介绍到Decoder，如果没完全理解，可以之后再回过头来思考下。mask的构造代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="comment">#生成向后遮掩的掩码张量，参数size是掩码张量最后两个维度的大小，它最后两维形成一个方阵</span></span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    <span class="comment">#然后使用np.ones方法向这个形状中添加1元素，形成上三角阵</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="comment">#最后将numpy类型转化为torch中的tensor，内部做一个1- 的操作。这个其实是做了一个三角阵的反转，subsequent_mask中的每个元素都会被1减。</span></span><br><span class="line">    <span class="comment">#如果是0，subsequent_mask中的该位置由0变成1</span></span><br><span class="line">    <span class="comment">#如果是1，subsequect_mask中的该位置由1变成0</span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>以上便是编码器部分的全部内容，有了这部分内容的铺垫，解码器的介绍就会轻松一些。</p>
<h3><span id="四-decoder"><strong>四、 Decoder</strong></span></h3>
<h4><span id="41-解码器整体结构"><strong>4.1 解码器整体结构</strong></span></h4>
<p>解码器的作用：根据编码器的结果以及上一次预测的结果，输出序列的下一个结果。整体结构上，解码器也是由N个相同层堆叠而成。构造代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用类Decoder来实现解码器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="comment">#初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        <span class="comment">#首先使用clones方法克隆了N个layer，然后实例化一个规范化层，因为数据走过了所有的解码器层后最后要做规范化处理。</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="comment">#forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，source_mask，target_mask代表源数据和目标数据的掩码张量，然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，得出最后的结果，再进行一次规范化返回即可。</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h4><span id="42-解码器层"><strong>4.2 解码器层</strong></span></h4>
<p><strong>每个解码器层由三个子层连接结构组成，第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接，第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接，第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接。</strong></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-fda2b501fe89662dd5f76326b102c650_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>解码器层中的各个子模块，如，多头注意力机制，规范化层，前馈全连接都与编码器中的实现相同。</p>
<p>有一个细节需要注意，第一个子层的多头注意力和编码器中完全一致，<strong><font color="red">
第二个子层，它的多头注意力模块中，query来自上一个子层，key 和 value
来自编码器的输出。</font></strong>可以这样理解，就是第二层负责，利用解码器已经预测出的信息作为query，去编码器提取的各种特征中，查找相关信息并融合到当前特征中，来完成预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用DecoderLayer的类实现解码器层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="comment">#初始化函数的参数有5个，分别是size，代表词嵌入的维度大小，同时也代表解码器的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，第三个是src_attn,多头注意力对象，这里Q!=K=V，第四个是前馈全连接层对象，最后就是dropout置0比率</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        <span class="comment">#按照结构图使用clones函数克隆三个子层连接对象</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="comment">#forward函数中的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量memory，以及源数据掩码张量和目标数据掩码张量，将memory表示成m之后方便使用。</span></span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        <span class="comment">#将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，最后一个参数时目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据。</span></span><br><span class="line">        <span class="comment">#比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用。</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        <span class="comment">#接着进入第二个子层，这个子层中常规的注意力机制，q是输入x;k,v是编码层输出memory，同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄露，而是遮蔽掉对结果没有意义的padding。</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="comment">#最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果，这就是我们的解码器结构</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3><span id="五-模型输出"><strong>五、模型输出</strong></span></h3>
<p>输出部分就很简单了，每个时间步都过一个 线性层 + softmax层</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-27f90c6393de75bc8d237fca3e4758b8_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>线性层的作用：通过对上一步的线性变化得到指定维度的输出，也就是转换维度的作用。转换后的维度对应着输出类别的个数，如果是翻译任务，那就对应的是文字字典的大小。</strong></p>
<h3><span id="六-模型构建">六、<strong>模型构建</strong></span></h3>
<p>下面是Transformer总体架构图，回顾一下，再看这张图，是不是每个模块的作用都有了基本的认知。</p>
<p><img src="https://pic1.zhimg.com/80/v2-f70598121a15f713eeafa4bc696d528c_1440w.jpg" alt="img" style="zoom: 50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model Architecture</span></span><br><span class="line"><span class="comment">#使用EncoderDecoder类来实现编码器-解码器结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. </span></span><br><span class="line"><span class="string">    Base for this and many other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span><br><span class="line">        <span class="comment">#初始化函数中有5个参数，分别是编码器对象，解码器对象,源数据嵌入函数，目标数据嵌入函数，以及输出部分的类别生成器对象.</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed    <span class="comment"># input embedding module(input embedding + positional encode)</span></span><br><span class="line">        self.tgt_embed = tgt_embed    <span class="comment"># ouput embedding module</span></span><br><span class="line">        self.generator = generator    <span class="comment"># output generation module</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="comment">#在forward函数中，有四个参数，source代表源数据，target代表目标数据,source_mask和target_mask代表对应的掩码张量,在函数中，将source source_mask传入编码函数，得到结果后与source_mask target 和target_mask一同传给解码函数</span></span><br><span class="line">        memory = self.encode(src, src_mask)</span><br><span class="line">        res = self.decode(memory, src_mask, tgt, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="comment">#编码函数，以source和source_mask为参数,使用src_embed对source做处理，然后和source_mask一起传给self.encoder</span></span><br><span class="line">        src_embedds = self.src_embed(src)</span><br><span class="line">        <span class="keyword">return</span> self.encoder(src_embedds, src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="comment">#解码函数，以memory即编码器的输出，source_mask target target_mask为参数,使用tgt_embed对target做处理，然后和source_mask,target_mask,memory一起传给self.decoder</span></span><br><span class="line">        target_embedds = self.tgt_embed(tgt)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(target_embedds, memory, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Full Model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构建模型</span></span><br><span class="line"><span class="string">    params:</span></span><br><span class="line"><span class="string">        src_vocab:</span></span><br><span class="line"><span class="string">        tgt_vocab:</span></span><br><span class="line"><span class="string">        N: 编码器和解码器堆叠基础模块的个数</span></span><br><span class="line"><span class="string">        d_model: 模型中embedding的size，默认512</span></span><br><span class="line"><span class="string">        d_ff: FeedForward Layer层中embedding的size，默认2048</span></span><br><span class="line"><span class="string">        h: MultiHeadAttention中多头的个数，必须被d_model整除</span></span><br><span class="line"><span class="string">        dropout:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1><span id="transformer-qampa">Transformer Q&amp;A</span></h1>
<blockquote>
<p>3，Transformer的Feed Forward层在训练的时候到底在训练什么？ -
zzzzzzz的回答 - 知乎
https://www.zhihu.com/question/499274875/answer/2250085650</p>
</blockquote>
<h4><span id="feed-forward-networkffn的作用"><strong>Feed forward network
(FFN)的作用？</strong></span></h4>
<p>Transformer在抛弃了 LSTM 结构后，FFN
中的激活函数成为了一个主要的提供<strong>非线性变换</strong>的单元。</p>
<h4><span id="gelu原理相比relu的优点"><strong>GELU原理？相比RELU的优点？</strong></span></h4>
<p>ReLU会<strong>确定性</strong>的将输入乘上一个0或者1(当x&lt;0时乘上0，否则乘上1)，Dropout则是随机乘上0。而GELU虽然也是将输入乘上0或1，但是输入到底是乘以0还是1，是在<strong>取决于输入自身</strong>的情况下<strong>随机</strong>选择的。</p>
<p>什么意思呢？具体来说：</p>
<p>我们将神经元的输入 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 乘上一个服从伯努利分布的 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]">
。而该伯努利分布又是依赖于 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=m+%5Csim+Bernoulli%28%5CPhi%28x%29%29+%2C+~where+~%5CPhi%28x%29+%3D+P%28X+%3C%3D+x%29" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中， <img src="https://www.zhihu.com/equation?tex=X+%5Csim+N%280%2C+1%29" alt="[公式]">，那么 <img src="https://www.zhihu.com/equation?tex=%5CPhi%28x%29" alt="[公式]">
就是标准正态分布的累积分布函数。这么做的原因是因为神经元的输入 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">
往往遵循正态分布，尤其是深度网络中普遍存在Batch
Normalization的情况下。当<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">减小时，<img src="https://www.zhihu.com/equation?tex=%5CPhi%28x%29" alt="[公式]">的值也会减小，此时<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">被“丢弃”的可能性更高。所以说这是<strong>随机依赖于输入</strong>的方式。</p>
<p>现在，给出GELU函数的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=GELU%28x%29+%3D+%5CPhi%28x%29+%2A+I%28x%29+%2B+%281+-+%5CPhi%28x%29%29+%2A+0x+%3D+x%5CPhi%28x%29" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5CPhi%28x%29" alt="[公式]">
是上文提到的标准正态分布的累积分布函数。因为这个函数没有解析解，所以要用近似函数来表示。</p>
<h5><span id="图像">图像：</span></h5>
<p><img src="https://pic3.zhimg.com/v2-0fde0599700a8045a7c1b7d006de33fa_b.jpg" alt="img" style="zoom:50%;"></p>
<h5><span id="导数形式">导数形式：</span></h5>
<p><img src="https://pic4.zhimg.com/v2-c55ded292a81733eb0944abdc4332d43_b.jpg" alt="img" style="zoom:50%;"></p>
<p>GELU和RELU一样，可以解决梯度消失，所以，GELU的优点就是在ReLU上增加随机因素，x越小越容易被mask掉。</p>
<h4><span id="为什么用layernorm不用batchnorm">==<strong>为什么用layernorm不用batchnorm？</strong>==</span></h4>
<p>对于RNN来说，sequence的长度是不一致的，所以用很多padding来表示无意义的信息。如果BN会导致有意义的embedding损失信息。所以，BN一般用于CNN，而LN用于RNN。</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=layernorm&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22438634058%22%7D">layernorm</a>是在hidden
size的维度进行的，跟batch和seq_len无关。每个hidden
state都计算自己的均值和方差，这是因为不同hidden
state的量纲不一样。beta和gamma的维度都是(hidden_size,)，经过白化的hidden
state * beta + gamma得到最后的结果。</p>
<p>LN在BERT中主要起到白化的作用，增强模型稳定性（如果删除则无法收敛）</p>
<h4><span id="multi-head-self-attention">==Multi-head Self-Attention==</span></h4>
<p>如果是<strong>单头</strong>注意力，就是每个位置的embedding对应 <img src="https://www.zhihu.com/equation?tex=Q%2CK%2CV" alt="[公式]">
三个向量，这三个向量分别是embedding点乘 <img src="https://www.zhihu.com/equation?tex=W_Q%2CW_K%2CW_V" alt="[公式]">
矩阵得来的。每个位置的Q向量去乘上所有位置的K向量，其结果经过softmax变成attention
score，以此作为权重对所有V向量做<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加权求和&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22438634058%22%7D">加权求和</a>即可。</p>
<p>用公式表示为：<img src="https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29+%3D+softmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=d_k" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=Q%2CK" alt="[公式]"> 向量的hidden size。除以 <img src="https://www.zhihu.com/equation?tex=d_k" alt="[公式]"> 叫做scaled
dot product.</p>
<ul>
<li><h5><span id="多头注意力是怎样的呢"><strong>多头</strong>注意力是怎样的呢？</span></h5></li>
</ul>
<p>Transformer中先通过切头（<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=spilt&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22438634058%22%7D">spilt</a>）再分别进行Scaled
Dot-Product Attention。</p>
<p><strong>step1</strong>：一个768维的hidden向量，被映射成Q，K，V。
然后三个向量分别切分成12(head_num)个小的64维的向量，每一组小向量之间做attention。不妨假设batch_size为32，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=seqlen&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22438634058%22%7D">seqlen</a>为512，隐层维度为768，12个head。</p>
<blockquote>
<p>hidden(32 x 512 x 768) -&gt; Q(32 x 512 x 768) -&gt; 32 x 12 x 512 x
64 hidden(32 x 512 x 768) -&gt; K(32 x 512 x 768) -&gt; 32 x 12 x 512 x
64 hidden(32 x 512 x 768) -&gt; V(32 x 512 x 768) -&gt; 32 x 12 x 512 x
64</p>
</blockquote>
<p><strong>step2</strong>：然后Q和K之间做attention，得到一个32 x 12 x
512 x 512的权重矩阵（时间复杂度O( <img src="https://www.zhihu.com/equation?tex=n%5E2d" alt="[公式]">
))，然后根据这个权重矩阵加权V中切分好的向量，得到一个32 x 12 x 512 x 64
的向量，拉平输出为768向量。</p>
<blockquote>
<p>32 x 12 x 512 x 64(query_hidden) * 32 x 12 x 64 x 512(key_hidden)
-&gt; 32 x 12 x 512 x 512 32 x 12 x 64 x 512(value_hidden) * 32 x 12 x
512 x 512 (权重矩阵) -&gt; 32 x 12 x 512 x 64</p>
</blockquote>
<p>然后再还原成 -&gt; 32 x 512 x 768
。简言之是12个头，每个头都是一个64维度，分别去与其他的所有位置的hidden
embedding做attention然后再合并还原。</p>
<ul>
<li><h5><span id="多头机制为什么有效">多头机制为什么有效？</span></h5></li>
</ul>
<p>类似于CNN中通过多通道机制进行特征选择。Transformer中使用切头(split)的方法，是为了在不增加复杂度（
<img src="https://www.zhihu.com/equation?tex=O%28n%5E2d%29" alt="[公式]"> )的前提下享受类似CNN中“不同卷积核”的优势。</p>
<h4><span id="为什么要做scaled-dotproduct">==为什么要做scaled dot
product?==</span></h4>
<p>当输入信息的维度 d 比较高，会导致 softmax
函数接近饱和区，梯度会比较小。因此，缩放点积模型可以较好地解决这一问题。</p>
<h4><span id="为什么用双线性点积模型即qk两个向量">==为什么用双线性点积模型（即Q，K两个向量）？==</span></h4>
<p>双线性点积模型使用Q，K两个向量，而不是只用一个Q向量，这样引入非对称性，更具健壮性（Attention对角元素值不一定是最大的，也就是说当前位置对自身的注意力得分不一定最高）。</p>
<h4><span id="transformer的非线性来自于哪里">==Transformer的非线性来自于哪里？==</span></h4>
<ul>
<li>FFN的gelu激活函数</li>
<li>self-attention：注意self-attention是非线性的（因为有相乘和softmax）</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2D5Z22P/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2D5Z22P/" class="post-title-link" itemprop="url">模型训练（3）Adaptive Learning Rate</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-09 11:34:59" itemprop="dateCreated datePublished" datetime="2022-06-09T11:34:59+08:00">2022-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 23:18:44" itemprop="dateModified" datetime="2022-07-13T23:18:44+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>24 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="tips-for-trainingadaptive-learning-rate">Tips for training:
Adaptive Learning Rate</span></h1>
<p>critical
point其实不一定是,你在训练一个Network的时候,会遇到的最大的障碍,今天要告诉大家的是一个叫做Adaptive
Learning Rate的技术,我们要给每一个参数不同的learning rate</p>
<h2><span id="一-training-stuck-smallgradient">一、Training stuck ≠ Small
Gradient</span></h2>
<h3><span id="peoplebelieve-training-stuck-because-the-parameters-are-around-a-criticalpoint">People
believe training stuck because the parameters are around a critical
point …</span></h3>
<p><strong>人们认为，由于参数处于临界点附近，培训陷入困境</strong>;為什麼我说这个critical
point不一定是我们训练过程中,最大的阻碍呢？</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616182046388.png" alt="image-20220616182046388">
<figcaption aria-hidden="true">image-20220616182046388</figcaption>
</figure>
<p>往往同学们,在训练一个network的时候,你会把它的loss记录下来,所以你会看到,你的loss原来很大,随著你参数不断的update,横轴代表参数update的次数,随著你参数不断的update,这个loss会越来越小,最后就卡住了,你的loss不再下降。</p>
<p>那多数这个时候,大家就会猜说,那是不是走到了critical
point,因為gradient等於零的关係,所以我们没有办法再更新参数,但是真的是这样吗？</p>
<p>当我们说 走到critical
point的时候,意味著gradient非常的小,但是你有确认过,当<strong>你的loss不再下降的时候,gradient真的很小吗？</strong>其实多数的同学可能,都没有确认过这件事,而事实上在这个例子裡面,在今天我show的这个例子裡面,当我们的loss不再下降的时候,gradient并没有真的变得很小</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616182117000.png" alt="image-20220616182117000"></p>
<p>gradient是一个向量，下面是gradient的norm,即gradient这个向量的长度,随著参数更新的时候的变化,你会发现说<strong>虽然loss不再下降,但是这个gradient的norm,gradient的大小并没有真的变得很小</strong></p>
<p>这样子的结果其实也不难猜想,也许你遇到的是这样子的状况</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616182130922.png" alt="image-20220616182130922">
<figcaption aria-hidden="true">image-20220616182130922</figcaption>
</figure>
<p>这个是我们的error surface,然后你现在的gradient,在error
surface山谷的两个谷壁间,<strong>不断的来回的震荡</strong></p>
<p>这个时候你的loss不会再下降,所以你会觉得它真的卡到了critical
point,卡到了saddle point,卡到了local
minima吗？不是的,<strong>它的gradient仍然很大,只是loss不见得再减小了</strong></p>
<p>所以你要注意,当你今天训练一个network,train到后来发现,loss不再下降的时候,你不要随便说,我卡在local
minima,我卡在saddle
point,<strong>有时候根本两个都不是,你只是单纯的loss没有办法再下降</strong></p>
<p>就是為什麼你在在<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW02/HW02-2.ipynb">作业2-2</a>,会有一个作业叫大家,算一下gradient的norm,然后算一下说,你现在是卡在saddle
point,还是critical
point,因為多数的时候,当你说你训练卡住了,很少有人会去分析卡住的原因,為了强化你的印象,我们有一个作业,让你来分析一下,卡住的原因是什麼,</p>
<h3><span id="11-wait-a-minute">1.1 Wait a minute</span></h3>
<p>有的同学就会有一个问题,如果我们在训练的时候,其实很少卡到saddle
point,或者是local minima,那这一个图是怎麼做出来的呢?</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616182149914.png" alt="image-20220616182149914">
<figcaption aria-hidden="true">image-20220616182149914</figcaption>
</figure>
<p>我们上次有画过这个图是说我们现在训练一个Network,训练到现在参数<strong>在critical
point附近,然后我们再来根据eigen value的正负号,来判断说这个critical
point,比较像是saddle point,还是local minima</strong></p>
<p>那如果实际上在训练的时候,要走到saddle point,或者是local
minima,是一件困难的事情,那这个图到底是怎麼画出来的。那这边告诉大家一个秘密,这个图你要训练出这样子的结果,你要训练到你的参数很接近critical
point,用一般的gradient descend,其实是做不到的,用一般的gradient descend
train,你往往会得到的结果是,你在这个gradient还很大的时候,你的loss就已经掉了下去,这个是需要特别方法train的。</p>
<p>所以做完这个实验以后,我更感觉你要走到一个critical
point,其实是困难的一件事,多数时候training,在还没有走到critical
point的时候,就已经停止了,那这并不代表说,critical
point不是一个问题,我只是想要告诉你说,我们真正目前,<strong>当你用gradient
descend,来做optimization的时候,你真正应该要怪罪的对象,往往不是critical
point,而是其他的原因。</strong></p>
<h3><span id="12training-can-be-difficult-even-without-critical-points">1.2
Training can be difficult even without critical points</span></h3>
<p>如果今天critical
point不是问题的话,為什麼我们的training会卡住呢,我这边举一个非常简单的例子,我这边有一个,非常简单的error
surface</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616182401982.png" alt="image-20220616182401982">
<figcaption aria-hidden="true">image-20220616182401982</figcaption>
</figure>
<p>我们只有两个参数,这两个参数值不一样的时候,Loss的值不一样,我们就画出了一个error
surface,这个<strong>error
surface的最低点</strong>在==黄色X==这个地方,事实上,这个error
surface是convex的形状(可以理解为凸的或者凹的，convex
optimization常翻译为“凸优化”)</p>
<p>如果你不知道convex是什麼,没有关係,总之它是一个,它的这个等高线是椭圆形的,只是它在横轴的地方,它的gradient非常的小,它的坡度的变化非常的小,非常的平滑,所以这个椭圆的长轴非常的长,短轴相对之下比较短,在纵轴的地方gradient的变化很大,error
surface的坡度非常的陡峭</p>
<p>那现在我们要从<strong>黑点</strong>这个地方,这个地方当作<strong>初始的点</strong>,然后来做gradient
descend，你可能觉得说,这个convex的error surface,做gradient
descend,有什麼难的吗？不就是一路滑下来,然后可能再走过去吗,应该是非常容易。你实际上自己试一下,你会发现说,就连这种convex的error
surface,形状这麼简单的error surface,你用gradient
descend,都不见得能把它做好,举例来说这个是我实际上,自己试了一下的结果</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616183353870.png" alt="image-20220616183353870">
<figcaption aria-hidden="true">image-20220616183353870</figcaption>
</figure>
<p>我learning
rate设10⁻²的时候,我的这个参数在峡谷的两端,我的参数在山壁的两端不断的震盪,我的loss掉不下去,但是gradient其实仍然是很大的。那你可能说,就是因為你<strong>learning
rate设太大了</strong>阿,learning
rate决定了我们update参数的时候步伐有多大,learning
rate显然步伐太大,你没有办法慢慢地滑到山谷裡面只要把learning
rate设小一点,不就可以解决这个问题了吗？</p>
<p>事实不然,因為我试著去,调整了这个learning
rate,就会发现你光是要train这种convex的optimization的问题,你就觉得很痛苦,我就调这个learning
rate,从10⁻²,一直调到10⁻⁷,调到10⁻⁷以后,终於不再震盪了</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616183400167.png" alt="image-20220616183400167">
<figcaption aria-hidden="true">image-20220616183400167</figcaption>
</figure>
<p>终於从这个地方滑滑滑,滑到山谷底终於左转,但是你发现说,这个训练永远走不到终点,因為我的<strong>learning
rate已经太小了</strong>,竖直往上这一段这个很斜的地方,因為这个坡度很陡,gradient的值很大,所以还能够前进一点,左拐以后这个地方坡度已经非常的平滑了,这麼小的learning
rate,根本没有办法再让我们的训练前进。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616183405984.png" alt="image-20220616183405984" style="zoom:50%;"></p>
<p>事实上在左拐这个地方,看到这边一大堆黑点,这边<strong>有十万个点</strong>,这个是张辽八百冲十万的那个十万,但是我都没有办法靠近,这个local
minima的地方,所以显然<strong>就算是一个convex的error
surface,你用gradient descend也很难train</strong></p>
<p>这个convex的optimization的问题,确实有别的方法可以解,但是你想想看,如果今天是更复杂的error
surface,你真的要train一个deep network的时候,gradient
descend是你,唯一可以仰赖的工具,但是gradient
descend这个工具,连这麼简单的error surface都做不好,==一室之不治
何以天下国家為==,这麼简单的问题都做不好,那如果难的问题,它又怎麼有可能做好呢</p>
<p>所以我们需要更好的gradient
descend的版本,在<strong>==之前我们的gradient
descend裡面,所有的参数都是设同样的learning rate,这显然是不够的,learning
rate它应该要為,每一个参数客製化==</strong>,所以接下来我们就是要讲,客製化的learning
rate,怎麼做到这件事情</p>
<h3><span id="二-differentparameters-needs-different-learning-rate">二、Different
parameters needs different learning rate</span></h3>
<p>那我们要怎麼客製化learning
rate呢,我们不同的参数到底,需要什麼样的learning
rate呢？从刚才的例子裡面,其实我们可以看到一个大原则,<strong>如果在某一个方向上,我们的gradient的值很小,非常的平坦,那我们会希望learning
rate调大一点,如果在某一个方向上非常的陡峭,坡度很大,那我们其实期待,learning
rate可以设得小一点</strong>。</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616183442460.png" alt="image-20220616183442460">
<figcaption aria-hidden="true">image-20220616183442460</figcaption>
</figure>
<h5><span id="那这个learningrate要如何自动的根据这个gradient的大小做调整呢">那这个learning
rate要如何自动的,根据这个gradient的大小做调整呢？</span></h5>
<p><strong>我们要改一下,gradient
descend原来的式子,我们只放某一个参数update的式子,我们之前在讲gradient
descend,</strong>我们往往是讲,所有参数update的式子,那这边為了等一下简化这个问题,我们只看一个参数,但是你完全可以把这个方法,推广到所有参数的状况
<span class="math display">\[
{θ{_i}{^{t+1}}} ← {θ{_i}{^{t}}}-{\eta}{g{_i}{^{t}}}
\]</span> 我们只看一个参数,这个参数叫做<span class="math inline">\({θ{_i}{^{t}}}\)</span>,这个<span class="math inline">\({θ{_i}{^{t}}}\)</span>在第t个iteration的值,减掉在第t个iteration这个参数i算出来的gradient
<span class="math inline">\({g{_i}{^{t}}}\)</span> <span class="math display">\[
{g{_i}{^{t}}}=\frac{\partial{L}}{\partial{θ_i}}|_{θ=θ^t}
\]</span> 这个<span class="math inline">\({g{_i}{^{t}}}\)</span>代表在第t个iteration,也就是θ等於θᵗ的时候,参数θᵢ对loss的微分,我们把这个θᵢᵗ减掉learning
rate,乘上gᵢᵗ会更新learning rate到θᵢᵗ⁺¹,<strong>这是我们原来的gradient
descend</strong>,<strong>我们的learning rate是固定的</strong></p>
<p>现在我们要有一个<strong>随著参数客製化的learning
rate</strong>,我们把原来learning rate <span class="math inline">\(η\)</span>这一项呢,改写成<span class="math inline">\(\frac{η}{σᵢᵗ}\)</span> <span class="math display">\[
{θ{_i}{^{t+1}}} ← {θ{_i}{^{t}}}-{\frac{η}{σᵢᵗ}}{g{_i}{^{t}}}
\]</span> <strong><font color="red"> 这个<span class="math inline">\(σᵢᵗ\)</span>你发现它有一个上标t,有一个下标i,这代表说这个σ这个参数,首先它是depend
on i的,不同的参数我们要给它不同的σ,同时它也是iteration
dependent的,不同的iteration我们也会有不同的σ。</font></strong></p>
<p>所以当我们把我们的learning rate,从η改成<span class="math inline">\(\frac{η}{σᵢᵗ}\)</span>的时候,我们就有一个,parameter
dependent的learning rate,接下来我们是要看说,这个parameter
dependent的learning rate有什麼常见的计算方式。</p>
<h3><span id="21-root-mean-square-均根方">2.1 Root mean square 均根方</span></h3>
<p>那这个σ有什麼样的方式,可以把它计算出来呢,一个常见的类型是算,gradient的<strong>Root
Mean Square（均方根）</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616183815859.png" alt="image-20220616183815859" style="zoom:50%;"></p>
<p>现在参数要update的式子,我们从θᵢ⁰初始化参数减掉gᵢ⁰,乘上learning rate
η除以σᵢ⁰,就得到θᵢ¹, <span class="math display">\[
{θ{_i}{^{1}}} ← {θ{_i}{^{0}}}-{\frac{η}{σᵢ^0}}{g{_i}{^{0}}}
\]</span></p>
<ul>
<li><p>这个<strong>σᵢ⁰</strong>在<strong>第一次update参数</strong>的时候,这个σᵢ⁰是(gᵢ⁰)²开根号
<span class="math display">\[
  {σᵢ^0}=\sqrt{({g{_i}{^{0}}})^2}=|{g{_i}{^{0}}}|
  \]</span>
这个gᵢ⁰就是我们的gradient,就是gradient的平方开根号,其实就是gᵢ⁰的绝对值,所以你把gᵢ⁰的绝对值代到<span class="math inline">\({θ{_i}{^{1}}} ←
{θ{_i}{^{0}}}-{\frac{η}{σᵢ^0}}{g{_i}{^{0}}}\)</span>,这个式子中gᵢ⁰跟这个根号底下的gᵢ⁰,它们的大小是一样的,所以式子中这一项只会有一个,要嘛是正一
要嘛是负一,就代表说我们第一次在update参数,从θᵢ⁰update到θᵢ¹的时候,要嘛是加上η
要嘛是减掉η,跟这个gradient的大小没有关係,是看你η设多少,这个是第一步的状况</p></li>
<li><p>重点是接下来怎麼处理,那θᵢ¹它要一样,减掉gradient gᵢ¹乘上η除以σᵢ¹,
<span class="math display">\[
  {θ{_i}{^{1}}}-{\frac{η}{σᵢ^1}}{g{_i}{^{1}}}
  \]</span> 现在在第二次update参数的时候,是要除以σᵢ¹
,这个σᵢ¹就是我们过去,<strong>所有计算出来的gradient,它的平方的平均再开根号</strong>
<span class="math display">\[
  {σᵢ^1}=\sqrt{\frac{1}{2}[{(g{_i}{^{0}}})^2+{(g{_i}{^{1}}})^2]}
  \]</span>
我们到目前為止,在第一次update参数的时候,我们算出了gᵢ⁰,在第二次update参数的时候,我们算出了gᵢ¹,所以这个σᵢ¹就是(gᵢ⁰)²,加上(gᵢ¹)²除以½再开根号,这个就是Root
Mean Square,我们算出这个σᵢ¹以后,我们的learning
rate就是η除以σᵢ¹,然后把θᵢ¹减掉,η除以σᵢ¹乘以gᵢ¹ 得到θᵢ² <span class="math display">\[
  {θ{_i}{^{2}}} ← {θ{_i}{^{1}}}-{\frac{η}{σᵢ^1}}{g{_i}{^{1}}}
  \]</span></p></li>
<li><p>同样的操作就反覆继续下去,在θᵢ²的地方,你要减掉η除以σᵢ²乘以gᵢ²,
<span class="math display">\[
  {θ{_i}{^{2}}}-{\frac{η}{σᵢ^2}}{g{_i}{^{2}}}
  \]</span>
那这个σ是什麼呢,这个σᵢ²就是过去,所有算出来的gradient,它的平方和的平均再开根号
<span class="math display">\[
  {σᵢ^2}=\sqrt{\frac{1}{3}[{(g{_i}{^{0}}})^2+{(g{_i}{^{1}}})^2+{(g{_i}{^{2}}})^2]}
  \]</span> 所以你把gᵢ⁰取平方,gᵢ¹取平方
gᵢ²取平方,的平均再开根号,得到σᵢ²放在这个地方,然后update参数 <span class="math display">\[
  {θ{_i}{^{3}}} ← {θ{_i}{^{2}}}-{\frac{η}{σᵢ^2}}{g{_i}{^{2}}}
  \]</span></p></li>
<li><p>所以这个process这个过程,就反覆继续下去,到第t次update参数的时候,其实这个是第t
+ 1次,第t +
1次update参数的时候,你的这个σᵢᵗ它就是过去所有的gradient,gᵢᵗ从第一步到目前為止,所有算出来的gᵢᵗ的平方和,再平均
再开根号得到σᵢᵗ, <span class="math display">\[
  {σᵢ^t}=\sqrt{\frac{1}{t+1}\sum_{i=0}^{t}{(g{_i}{^{t}}})^2}
  \]</span> 然后在把它除learning rate,然后用这一项当作是,新的learning
rate来update你的参数, <span class="math display">\[
  {θ{_i}{^{t+1}}} ← {θ{_i}{^{t}}}-{\frac{η}{σᵢ^t}}{g{_i}{^{t}}}
  \]</span></p></li>
</ul>
<h3><span id="1adagrad">（1）Adagrad</span></h3>
<p>那这一招被用在一个叫做==Adagrad==的方法裡面,<strong>為什麼这一招可以做到我们刚才讲的,坡度比较大的时候,learning
rate就减小,坡度比较小的时候,learning rate就放大呢?</strong></p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616190842918.png" alt="image-20220616190842918">
<figcaption aria-hidden="true">image-20220616190842918</figcaption>
</figure>
<p>你可以想像说,现在我们有两个参数:<strong>一个叫θᵢ¹ 一个叫θᵢ² θᵢ¹坡度小
θᵢ²坡度大</strong></p>
<ul>
<li>θᵢ¹因為它坡度小,所以你在θᵢ¹这个参数上面,算出来的gradient值都比较小</li>
<li>因為gradient算出来的值比较小,然后这个σ是gradient的平方和取平均再开根号</li>
</ul>
<p><span class="math display">\[
{σᵢ^t}=\sqrt{\frac{1}{t+1}\sum_{i=0}^{t}{(g{_i}{^{t}}})^2}
\]</span></p>
<ul>
<li>所以算出来的σ就小,σ小 learning rate就大 <span class="math display">\[
  {\frac{η}{σᵢ^t}}
  \]</span></li>
</ul>
<p>反过来说θᵢ²,θᵢ²是一个比较陡峭的参数,在θᵢ²这个方向上loss的变化比较大,所以算出来的gradient都比较大,,你的σ就比较大,你在update的时候
你的step,你的参数update的量就比较小</p>
<p>所以有了σ这一项以后,你就可以随著gradient的不同,每一个参数的gradient的不同,来自动的调整learning
rate的大小,那这个并不是,你今天会用的最终极的版本,</p>
<h3><span id="2rmsprop">（2）RMSProp</span></h3>
<p>刚才那个版本,就算是同一个参数,它需要的learning
rate,也会随著时间而改变,我们刚才的假设,好像是同一个参数,它的gradient的大小,就会固定是差不多的值,但事实上并不一定是这个样子的。举例来说我们来看,这个新月形的error
surface：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616191023874.png" alt="image-20220616191023874"></p>
<p>如果我们考虑横轴的话,考虑左右横的水平线的方向的话,你会发现说,在绿色箭头这个地方坡度<strong>比较陡峭,所以我们需要比较小的learning
rate</strong>。</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616191213346.png" alt="image-20220616191213346">
<figcaption aria-hidden="true">image-20220616191213346</figcaption>
</figure>
<p>但是走到了中间这一段，到了红色箭头的时候呢,坡度又变得平滑了起来,<strong>平滑了起来就需要比较大的learning
rate</strong>,所以就算是<strong>同一个参数同一个方向,我们也期待说,learning
rate是可以动态的调整的</strong>,于是就有了一个新的招数,这个招数叫做==RMS
Prop==。</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616191239648.png" alt="image-20220616191239648">
<figcaption aria-hidden="true">image-20220616191239648</figcaption>
</figure>
<p>RMS Prop这个方法,<strong>它的第一步跟刚才讲的Root Mean
Square,也就是那个Apagrad的方法,是一模一样的</strong> <span class="math display">\[
{σᵢ^0}=\sqrt{({g_i^0})^2}
\]</span>
我们看第二步,一样要算出σᵢ¹,只是我们现在算出σᵢ¹的方法跟刚才,算Root Mean
Square的时候不一样,刚才在算Root Mean
Square的时候,每一个gradient都有同等的重要性,但<strong>在RMS
Prop裡面,它决定你可以自己调整,现在的这个gradient,你觉得它有多重要</strong>
<span class="math display">\[
{σᵢ^1}=\sqrt[]{\alpha(σ_i^0)^2+(1-\alpha)(g_i^1)^2}
\]</span> 所以在RMS
Prop裡面,我们这个σᵢ¹它是前一步算出来的σᵢ⁰,裡面就是有gᵢ⁰,所以这个<strong>σᵢ⁰就代表了gᵢ⁰的大小</strong>,所以它是(σᵢ⁰)²,乘上α加上(1-α),乘上现在我们刚算出来的,新鲜热腾腾的gradient就是gᵢ¹</p>
<p>那这个<strong>α就像learning
rate一样,这个你要自己调它,它是一个hyperparameter</strong></p>
<ul>
<li>如果我今天<strong>α设很小趋近於0</strong>,就代表我觉得<strong>gᵢ¹相较於之前所算出来的gradient而言,比较重要</strong></li>
<li>我<strong>α设很大趋近於1</strong>,那就代表我觉得<strong>现在算出来的gᵢ¹比较不重要,之前算出来的gradient比较重要</strong></li>
</ul>
<p>所以同理在第三次update参数的时候,我们要算σᵢ²
,我们就把σᵢ¹拿出来取平方再乘上α,那σᵢ¹裡面有gᵢ¹跟σᵢ⁰
,σᵢ⁰裡面又有gᵢ⁰,所以你知道σᵢ¹裡面它有gᵢ¹有gᵢ⁰,
然后这个gᵢ¹跟gᵢ⁰呢他们会被乘上α,然后再加上1-α乘上这个(gᵢ²)² <span class="math display">\[
{σᵢ^2}=\sqrt[]{\alpha(σ_i^1)^2+(1-\alpha)(g_i^2)^2}
\]</span> 所以这个α就会决定说gᵢ²,它在整个σᵢ²裡面佔有多大的影响力</p>
<p>那同样的过程就反覆继续下去,σᵢᵗ等於根号α乘上(σᵢᵗ⁻¹)²,加上(1-α) (gᵢᵗ)²,
<span class="math display">\[
{σᵢ^t}=\sqrt[]{\alpha(σ_i^{t-1})^2+(1-\alpha)(g_i^t)^2}
\]</span>
你用α来决定现在刚算出来的gᵢᵗ,它有多重要,好那这个就是RMSProp。那RMSProp我们刚刚讲过说,透过α这一项你可以决定说,gᵢᵗ相较於之前存在,σᵢᵗ⁻¹裡面的gᵢᵗ到gᵢᵗ⁻¹而言,它的重要性有多大,如果你用RMS
Prop的话,你就可以动态调整σ这一项,我们现在假设从这个地方开始：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616194330541.png" alt="image-20220616194330541" style="zoom:80%;"></p>
<p>这个黑线是我们的error
surface,从这个地方开始你要update参数,好你这个球就从这边走到这边,那因為一路上都很平坦,很平坦就代表说g算出来很小,代表现在update参数的时候,我们会走比较大的步伐</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616194342782.png" alt="image-20220616194342782" style="zoom:67%;"></p>
<p>接下来继续滚,滚到这边以后我们gradient变大了,如果不是RMS
Prop,原来的Adagrad的话它反应比较慢,但如果你用RMS
Prop,然后呢你把α设小一点,你就是让新的,刚看到的gradient影响比较大的话,那你就可以很快的让σ的值变大,也可以很快的让你的步伐变小</p>
<p>你就可以踩一个煞车,本来很平滑走到这个地方,突然变得很陡,那RMS
Prop可以很快的踩一个煞车,把learning
rate变小,如果你没有踩剎车的话,你走到这裡这个地方,learning
rate太大了,那gradient又很大,两个很大的东西乘起来,你可能就很快就飞出去了,飞到很远的地方</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616194447003.png" alt="image-20220616194447003" style="zoom:80%;"></p>
<p>如果继续走,又走到平滑的地方了,因為这个σᵢᵗ
你可以调整α,让它比较看重於,最近算出来的gradient,所以你gradient一变小,σ可能就反应很快,它的这个值就变小了,然后呢你走的步伐就变大了,这个就是RMS
Prop,</p>
<h3><span id="3adam">（3）Adam</span></h3>
<p>那今天你最常用的,optimization的策略,有人又叫做optimizer,今天最常用的optimization的策略,就是Adam</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616194501101.png" alt="image-20220616194501101">
<figcaption aria-hidden="true">image-20220616194501101</figcaption>
</figure>
<p>Adam就是RMS
Prop加上Momentum,那Adam的演算法跟原始的论文https://arxiv.org/pdf/1412.6980.pdf</p>
<p>今天pytorch裡面,都帮你写得好好的了,所以这个你今天,不用担心这种optimization的问题,optimizer这个deep
learning的套件,往往都帮你做好了,然后这个optimizer裡面,也有一些参数需要调,也有一些hyperparameter,需要人工决定,但是你往往用预设的,那一种参数就够好了,你自己调有时候会调到比较差的,往往你直接copy,这个pytorch裡面,Adam这个optimizer,然后预设的参数不要随便调,就可以得到不错的结果了,关於Adam的细节,就留给大家自己研究</p>
<h2><span id="三-learning-rate-scheduling">三、Learning Rate Scheduling</span></h2>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616194539411.png" alt="image-20220616194539411" style="zoom:67%;"></p>
<p>我们刚才讲说这个简单的error
surface,我们都train不起来,现在我们来看一下,加上Adaptive Learning
Rate以后,train不train得起来。</p>
<p>那这边是採用,最原始的Adagrad那个做法啦,就是把过去看过的,这个learning
rate通通都,过去看过的gradient,通通都取平方再平均再开根号当作这个σ
,做起来是这个样子的</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616194548666.png" alt="image-20220616194548666" style="zoom:67%;"></p>
<p>这个走下来没有问题,然后接下来在左转的时候,这边也是update了十万次,之前update了十万次,只卡在左转这个地方</p>
<p>那现在有Adagrad以后,你可以再继续走下去,走到非常接近终点的位置,因為当你走到这个地方的时候,你因為这个左右的方向的,这个gradient很小,所以learning
rate会自动调整,左右这个方向的,learning
rate会自动变大,所以你这个步伐就可以变大,就可以不断的前进</p>
<p>接下来的问题就是,為什麼快走到终点的时候突然爆炸了呢？你想想看
我们在做这个σ的时候,我们是把过去所有看到的gradient,都拿来作平均</p>
<ul>
<li><p>所以这个纵轴的方向,在这个初始的这个地方,感觉gradient很大</p></li>
<li><p>可是这边走了很长一段路以后,这个纵轴的方向,gradient算出来都很小,所以纵轴这个方向,这个y轴的方向就累积了很小的σ</p></li>
<li><p>因為我们在这个y轴的方向,看到很多很小的gradient,所以我们就累积了很小的σ,累积到一个地步以后,这个step就变很大,然后就爆走就喷出去了</p></li>
<li><p>喷出去以后没关係,有办法修正回来,因為喷出去以后,就走到了这个gradient比较大的地方,走到gradient比较大的地方以后,这个σ又慢慢的变大,σ慢慢变大以后,这个参数update的距离,Update的步伐大小就慢慢的变小</p></li>
</ul>
<p>你就发现说走著走著,突然往左右喷了一下,但是这个喷了一下不会永远就是震盪,不会做简谐运动停不下来,这个力道慢慢变小,有摩擦力
让它慢慢地慢慢地,又回到中间这个峡谷来,然后但是又累计一段时间以后
又会喷,然后又慢慢地回来
怎麼办呢,<strong>有一个方法也许可以解决这个问题,这个叫做learning
rate的scheduling</strong></p>
<h4><span id="什麼是learningrate的scheduling呢">什麼是learning
rate的scheduling呢？</span></h4>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616194800158.png" alt="image-20220616194800158" style="zoom:80%;"></p>
<p>我们刚才这边还有一项η,这个η是一个固定的值,learning rate
scheduling的意思就是说,我们<strong>不要把η当一个常数,我们把它跟时间有关</strong></p>
<p><strong><font color="red"> 最常见的策略叫做==Learning Rate
Decay==,也就是说，随著时间的不断地进行,随著参数不断的update,我们这个η让它越来越小。</font></strong></p>
<p>那这个也就合理了,因為一开始我们距离终点很远,随著参数不断update,我们距离终点越来越近,所以我们把learning
rate减小,让我们参数的更新踩了一个煞车,让我们参数的更新能够慢慢地慢下来,所以刚才那个状况,如果加上Learning
Rate Decay有办法解决。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616195004172.png" alt="image-20220616195004172" style="zoom:80%;"></p>
<p>刚才那个状况,如果加上Learning Rate
Decay的话,我们就可以很平顺的走到终点,因為在这个地方,这个η已经变得非常的小了,虽然说它本来想要左右乱喷,但是因為乘上这个非常小的η,就停下来了
就可以慢慢地走到终点,那除了Learning Rate
Decay以外,还有另外一个经典，常用的Learning Rate
Scheduling的方式,叫做==Warm Up==</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616195017126.png" alt="image-20220616195017126" style="zoom:67%;"></p>
<p>Warm Up这个方法,听起来有点匪夷所思,这Warm
Up的方法是<strong>让learning rate,要先变大后变小</strong>,你会问说
变大要变到多大呢,变大速度要多快呢
，小速度要多快呢,<strong>这个也是hyperparameter</strong>,你要自己用手调的,但是大方向的大策略就是,learning
rate要先变大后变小,那这个方法听起来很神奇,就是一个黑科技这样,这个黑科技出现在,很多远古时代的论文裡面。</p>
<p>这个warm up,最近因為在训练BERT的时候,往往需要用到Warm
Up,所以又被大家常常拿出来讲,但它并不是有BERT以后,才有Warm Up的,Warm
Up这东西远古时代就有了,举例来说,Residual Network裡面是有Warm Up的</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616195035436.png" alt="image-20220616195035436" style="zoom:67%;"></p>
<p>这边是放了Residual
network,放在arXiv上面的文章连结啦,今天这种有关machine learning
的,文章往往在投conference之前,投国际会议之前,就先放到一个叫做arXiv的网站上,把它公开来让全世界的人都可以看。</p>
<p>你其实看这个arXiv的网址,你就可以知道,这篇文章是什麼时候放到网路上的,怎麼看呢
arXiv的前四个数字,这15代表年份,代表说residual
network这篇文章,是2015年放到arXiv上面的,后两个数字代表月份,所以它是15年的12月,15年的年底放在arXiv上面的</p>
<p>所以五六年前的文章,在deep
learning变化,这麼快速的领域裡面,五六年前就是上古时代,那在上古时代,这个Residual
Network裡面,就已经记载了Warm Up的这件事情,它说我们<strong>用learning
rate 0.01,取Warm Up,先用learning rate 0.01,再把learning
rate改成0.1</strong></p>
<p>用过去我们通常最常见的train,Learning Rate
Scheduling的方法,就是让learning rate越来越小,但是Residual
Network,这边特别註明它反其道而行,一开始要设0.01
接下来设0.1,还特别加一个註解说,一开始就用0.1反而就train不好,不知道為什麼
也没解释,反正就是train不好,需要Warm
Up这个黑科技。而在这个黑科技,在知名的Transformer裡面(这门课也会讲到),也用一个式子提了它。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616195112806.png" alt="image-20220616195112806" style="zoom:80%;"></p>
<p>它这边有一个式子说,它的learning
rate遵守这一个,神奇的function来设定,它的learning
rate,这个神奇的function,乍看之下会觉得 哇
在写什麼,不知道在写些什麼。这个东西你实际上,把这个function画出来,你实际上把equation画出来的话,就会发现它就是Warm
Up,learning rate会先增加,然后接下来再递减。所以你发现说Warm
Up这个技术,在很多知名的network裡面都有,被当作一个黑科技,就论文裡面不解释说,為什麼要用这个,但就偷偷在一个小地方,你没有注意到的小地方告诉你说,这个network要用这种黑科技,才能够把它训练起来。那為什麼需要warm
Up呢,这个仍然是今天,一个可以研究的问题啦。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616195206977.png" alt="image-20220616195206977" style="zoom:80%;"></p>
<p>这边有一个可能的解释是说,你想想看当我们在用Adam RMS
Prop,或Adagrad的时候,我们会需要计算σ,它是一个统计的结果,<strong>σ告诉我们,某一个方向它到底有多陡,或者是多平滑</strong>,那这个统计的结果,<strong>要看得够多笔数据以后,这个统计才精準,所以一开始我们的统计是不精準的</strong></p>
<p>一开始我们的σ是不精準的,所以我们一开始不要让我们的参数,走离初始的地方太远,先让它在初始的地方呢,做一些像是探索这样,所以<strong>一开始learning
rate比较小,是让它探索 收集一些有关error
surface的情报</strong>,先收集有关σ的统计数据,<strong>等σ统计得比较精準以后,在让learning
rate呢慢慢地爬升</strong></p>
<p>所以这是一个解释,為什麼我们需要warm
up的可能性,那如果你想要学更多,有关warm
up的东西的话,你其实可以看一篇paper,它是Adam的进阶版叫做RAdam,裡面对warm
up这件事情,有更多的理解。</p>
<h2><span id="四-summary-of-optimization">四、Summary of Optimization</span></h2>
<p>所以我们从最原始的gradient descent,进化到这一个版本：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616202420654.png" alt="image-20220616202420654" style="zoom:80%;"></p>
<p>这个版本裡面</p>
<ul>
<li><p>我们有Momentum,也就是说我们现在,不是完全顺著gradient的方向,现在不是完全顺著这一个时间点,算出来的gradient的方向,来update参数,而是把过去,所有算出来gradient的方向,做一个加总当作update的方向,这个是momentum</p></li>
<li><p>接下来应该要update多大的步伐呢,我们要除掉,gradient的Root Mean
Square</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616202442527.png" alt="image-20220616202442527" style="zoom:67%;"></p>
<p>那讲到这边可能有同学会觉得很困惑,这一个momentum是考虑,过去所有的gradient,这个σ也是考虑过去所有的gradient,一个放在分子一个放在分母,都考虑过去所有的gradient,不就是正好<strong>抵销了吗</strong>？,</p>
<p><strong>但是其实这个Momentum跟这个σ,它们在使用过去所有gradient的方式是不一样的</strong>：</p>
<ul>
<li><p><strong><font color="red">
Momentum是直接把所有的gradient通通都加起来,所以它有考虑方向,它有考虑gradient的正负号,它有考虑gradient是往左走还是往右走。</font></strong></p></li>
<li><p><strong><font color="red"> Root Mean
Square,它就不考虑gradient的方向了</font></strong>。它只考虑gradient的大小,记不记得我们在算σ的时候,我们都要取平方项,我们都要把gradient取一个平方项,我们是把平方的结果加起来,所以我们只考虑gradient的大小,不考虑它的方向,所以Momentum跟这个σ,算出来的结果并不会互相抵销掉。</p></li>
</ul></li>
<li><p>那最后我们还会加上,一个learning rate的scheduling：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616202650308.png" alt="image-20220616202650308" style="zoom:80%;"></p>
<p>那这个是今天<strong>optimization</strong>的,完整的版本了,这种Optimizer,除了Adam以外,Adam可能是今天最常用的,但除了Adam以外,还有各式各样的变形,但其实各式各样的变形都不脱,就是要嘛不同的方法算M,要嘛不同的方法算σ,要嘛不同的,Learning
Rate Scheduling的方式。</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/N29PC4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/N29PC4/" class="post-title-link" itemprop="url">模型训练（4）Batch and Momentum</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-09 11:34:59" itemprop="dateCreated datePublished" datetime="2022-06-09T11:34:59+08:00">2022-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 23:18:55" itemprop="dateModified" datetime="2022-07-13T23:18:55+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>21 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-batch-and-momentum">一、Batch and Momentum</span></h2>
<blockquote>
<p>python+numpy实现线性回归中梯度下降算法（对比sklearn官方demo） -
sciengieer的文章 - 知乎 https://zhuanlan.zhihu.com/p/390002941</p>
</blockquote>
<h3><span id="11-review-optimization-withbatch">1.1 Review： Optimization with
Batch</span></h3>
<p>上次我们有讲说,我们<strong>实际上在算微分的时候,并不是真的对所有 Data
算出来的 L 作微分</strong>,你是把所有的 Data 分成一个一个的
Batch,有的人是叫Mini Batch ,那我这边叫做
Batch,其实指的是一样的东西,助教投影片里面,是写 Mini Batch。</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616164437127.png" alt="image-20220616164437127">
<figcaption aria-hidden="true">image-20220616164437127</figcaption>
</figure>
<p>每一个 Batch 的大小呢,就是大 B 一笔的资料,我们每次<strong>在 Update
参数的时候,我们是拿大 B 一笔资料出来,算个 Loss,算个 Gradient,Update
参数</strong>,拿另外B一笔资料,再算个 Loss,再算个 Gradient,再 Update
参数,以此类推,所以我们不会拿所有的资料一起去算出 Loss,我们只会拿一个
Batch 的资料,拿出来算 Loss。</p>
<p><strong>所有的 Batch 看过一遍,叫做一个
Epoch</strong>,那事实上啊,你今天在做这些 Batch 的时候,你会做一件事情叫做
Shuffle</p>
<p>Shuffle
有很多不同的做法,但一个常见的做法就是,<strong><font color="red">
在每一个 Epoch 开始之前,会分一次 Batch,然后呢,每一个 Epoch 的 Batch
都不一样</font></strong>,就是第一个 Epoch,我们分这样子的 Batch,第二个
Epoch,会重新再分一次 Batch,所以哪些资料在同一个 Batch 里面,每一个 Epoch
都不一样的这件事情,叫做 <strong>Shuffle</strong>。</p>
<h3><span id="12-small-batch-vs-largebatch">1.2 Small Batch v.s. Large
Batch</span></h3>
<p>​ 我们先解释为什么要用 Batch,再说 Batch 对 Training
带来了什么样的帮助。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616164656382.png" alt="image-20220616164656382" style="zoom:50%;"></p>
<p>我们来比较左右两边这两个 Case,那假设现在我们有20笔训练资料</p>
<ul>
<li>左边的 Case 就是没有用 Batch,Batch
Size,直接设的跟我训练资料一样多,这种状况叫做 Full Batch,就是没有用 Batch
的意思</li>
<li>那右边的 Case 就是,Batch Size 等於1</li>
</ul>
<p>​ 这是两个最极端的状况</p>
<p>我们先来看左边的 Case,在左边 Case 里面,因为没有用 Batch,我们的 Model
必须把20笔训练资料都看完,才能够计算 Loss,才能够计算
Gradient,所以我们必须要把<strong>所有20笔 Example s
都看完以后,我们的参数才能够 Update
一次</strong>。就假设开始的地方在上边边,把所有资料都看完以后,Update
参数就从这里移动到下边。</p>
<p>如果 Batch Size 等於1的话,代表我们只需要拿一笔资料出来算
Loss,我们就可以 Update 我们的参数,所以每次我们 Update
参数的时候,看一笔资料就好,所以我们开始的点在这边,看一笔资料 就 Update
一次参数,再看一笔资料 就 Update 一次参数,如果今天总共有20笔资料的话
那<strong>在每一个 Epoch 里面,我们的参数会 Update
20次</strong>,那不过,因为我们现在是只看一笔资料,就 Update
一次参数,所以用一笔资料算出来的 Loss,显然是比较 Noisy 的,所以我们今天
Update 的方向,你会发现它是曲曲折折的</p>
<p>所以如果我们比较左边跟右边，哪一个比较好呢,他们有什么差别呢？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616165328450.png" alt="image-20220616165328450" style="zoom:50%;"></p>
<p>你会发现左边没有用 Batch
的方式,它蓄力的时间比较长,还有它技能冷却的时间比较长,你要把所有的资料都看过一遍,才能够
Update 一次参数</p>
<p>而右边的这个方法,Batch Size
等於1的时候,蓄力的时间比较短,每次看到一笔参数,每次看到一笔资料,你就会更新一次你的参数</p>
<p>所以今天假设有20笔资料,看完所有资料看过一遍,你已经更新了20次的参数,但是左边这样子的方法有一个优点,就是它这一步走的是稳的,那右边这个方法它的缺点,就是它每一步走的是不稳的</p>
<p>看起来左边的方法跟右边的方法,他们各自都有擅长跟不擅长的东西,左边是蓄力时间长,但是威力比较大,右边技能冷却时间短,但是它是比较不準的,看起来各自有各自的优缺点,但是你会觉得说,左边的方法技能冷却时间长,右边的方法技能冷却时间短,那只是你没有考虑并行运算的问题。</p>
<p><strong>实际上考虑并行运算的话,左边这个并不一定时间比较长</strong></p>
<h3><span id="13larger-batch-size-does-not-require-longer-time-to-compute-gradient">1.3
Larger batch size does not require longer time to compute gradient</span></h3>
<p>这边是真正的实验结果了,事实上,比较大的 Batch Size,你要算
Loss,再进而算 Gradient,所需要的时间,不一定比小的 Batch Size
要花的时间长</p>
<p>那以下是做在一个叫做 ==MNIST== 上面,MNIST (Mixed National Institute
of Standards and Technology
database)是美国国家标准与技术研究院收集整理的大型手写数字数据库,机器要做的事情,就是给它一张图片,然后判断这张图片,是0到9的哪一个数字,它要做数字的分类,那
MNIST 呢
是机器学习的helloworld,就是假设你今天,从来没有做过机器学习的任务,一般大家第一个会尝试的机器学习的任务,往往就是做
MNIST 做手写数字辨识,</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616165208136.png" alt="image-20220616165208136" style="zoom:50%;"></p>
<p>这边我们就是做了一个实验,我们想要知道说,给机器一个 Batch,它要计算出
Gradient,进而 Update 参数,到底需要花多少的时间</p>
<p>这边列出了 Batch Size 等於1 等於10,等於100 等於1000
所需要耗费的时间</p>
<p><strong>你会发现说 Batch Size
从1到1000,需要耗费的时间几乎是一样的,你可能直觉上认为有1000笔资料</strong>,那需要计算
Loss,然后计算
Gradient,花的时间不会是一笔资料的1000倍吗,但是实际上并不是这样的</p>
<p><strong><font color="red"> 因为在实际上做运算的时候,我们有
GPU,可以做并行运算,是因为你可以做平行运算的关係,这1000笔资料是平行处理的,所以1000笔资料所花的时间,并不是一笔资料的1000倍</font></strong>。当然
GPU 平行运算的能力还是有它的极限,当你的 Batch Size
真的非常非常巨大的时候,GPU 在跑完一个 Batch,计算出 Gradient
所花费的时间,还是会随著 Batch Size 的增加,而逐渐增长</p>
<p>所以今天如果 Batch Size
是从1到1000,所需要的时间几乎是一样的,但是当你的 Batch Size 增加到
10000,乃至增加到60000的时候,你就会发现 GPU 要算完一个 Batch,把这个 Batch
里面的资料都拿出来算 Loss,再进而算 Gradient,所要耗费的时间,确实有随著
Batch Size 的增加而逐渐增长,但你会发现这边用的是
V100,所以它挺厉害的,给它60000笔资料,一个 Batch
里面,塞了60000笔资料,它在10秒鐘之内,也是把 Gradient 就算出来</p>
<p>而那这个 Batch Size
的大小跟时间的关係,其实每年都会做这个实验,我特别把旧的投影片放在这边了,如果你有兴趣的话m,,可以看到这个时代的演进这样,17年的时候用的是那个980啊,2015年的时候用的是那个760啊,然后980要跑什么60000个
Batch,那要跑好几分鐘才跑得完啊,现在只要10秒鐘就可以跑得完了,你可以看到这个时代的演进,</p>
<h3><span id="14-smallerbatch-requires-longer-time-for-one-epoch">1.4 Smaller
batch requires longer time for one epoch</span></h3>
<p>所以 GPU 虽然有平行运算的能力,但它平行运算能力终究是有个极限,所以你
Batch Size 真的很大的时候,时间还是会增加的</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616165344706.png" alt="image-20220616165344706" style="zoom: 67%;"></p>
<p>但是因为有平行运算的能力,因此实际上,当你的 <strong>Batch Size
小的时候,你要跑完一个 Epoch,花的时间是比大的 Batch Size
还要多</strong>的,怎么说呢</p>
<p>如果今天假设我们的训练资料只有60000笔,那 Batch Size 设1,那你要60000个
Update 才能跑完一个 Epoch,如果今天是 Batch Size 等於1000,你要60个 Update
才能跑完一个 Epoch,假设今天一个 Batch Size 等於1000,要算 Gradient
的时间根本差不多,那60000次 Update,跟60次 Update
比起来,它的时间的差距量就非常可观了</p>
<p>所以左边这个图是 Update 一次参数,拿一个 Batch 出来计算一个
Gradient,Update 一次参数所需要的时间,右边这个图是,跑完一个完整的
Epoch,需要花的时间,你会发现左边的图跟右边的图,它的趋势正好是相反的,假设你
Batch Size 这个1,跑完一个 Epoch,你要 Update
60000次参数,它的时间是非常可观的,但是假设你的 Batch Size
是1000,你只要跑60次,Update 60次参数就会跑完一个 Epoch,所以你跑完一个
Epoch,看完所有资料的时间,如果你的 Batch Size 设1000,其实是比较短的,Batch
Size 设1000的时候,把所有的资料看过一遍,其实是比 Batch Size 设1
还要更快</p>
<p>所以如果我们看右边这个图的话,看完一个
Batch,把所有的资料看过一次这件事情,大的 Batch Size
反而是较有效率的,是不是跟你直觉想的不太一样</p>
<p>在没有考虑平行运算的时候,你觉得大的 Batch
比较慢,但实际上,在有考虑平行运算的时候,一个 Epoch 大的 Batch
花的时间反而是比较少的</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616165607814.png" alt="image-20220616165607814">
<figcaption aria-hidden="true">image-20220616165607814</figcaption>
</figure>
<p>我们如果要比较这个 Batch Size
大小的差异的话,看起来直接用技能时间冷却的长短,并不是一个精确的描述,看起来在技能时间上面,大的
Batch 并没有比较吃亏,甚至还佔到优势了.</p>
<p>所以事实上,20笔资料 Update 一次的时间,跟右边看一笔资料 Update
一次的时间,如果你用 GPU 的话,其实可能根本就是所以一样的,所以大的
Batch,它的技能时间,它技能冷却的时间,并没有比较长,那所以这时候你可能就会说,欸
那个大的 Batch 的劣势消失了,那难道它真的就,那这样看起来大的 Batch
应该比较好?</p>
<p>你不是说大的 Batch,这个 Update 比较稳定,小的 Batch,它的 Gradient
的方向比较 Noisy 吗,那这样看起来,大的 Batch 好像应该比较好哦,小的 Batch
应该比较差,因为现在大的 Batch
的劣势已经,因为平行运算的时间被拿掉了,它好像只剩下优势而已.</p>
<p>那神奇的地方是 <strong>Noisy 的 Gradient,反而可以帮助
Training</strong>,这个也是跟直觉正好相反的</p>
<p>如果你今天拿不同的 Batch
来训练你的模型,你可能会得到这样子的结果,左边是坐在 MNIST 上,右边是坐在
CIFAR-10 上,不管是 MNIST 还是 CIFAR-10,都是影像辨识的问题</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616165544738.png" alt="image-20220616165544738"></p>
<ul>
<li>横轴代表的是 Batch Size,从左到右越来越大</li>
<li>纵轴代表的是正确率,越上面正确率越高,当然正确率越高越好</li>
</ul>
<p>而如果你今天看 Validation Acc 上的结果，会发现说,Batch Size
越大,Validation Acc 上的结果越差,但这个不是 Overfitting,因为如果你看你的
Training 的话,会发现说 Batch Size 越大,Training
的结果也是越差的,而我们现在用的是同一个模型哦,照理说,它们可以表示的
Function 就是一模一样的</p>
<p>但是神奇的事情是,大的 Batch Size,往往在 Training
的时候,会给你带来比较差的结果</p>
<p>所以这个是什么样的问题,同样的 Model,所以这个不是 Model Bias
的问题,<strong>这个是 Optimization 的问题,代表当你用大的 Batch Size
的时候,你的 Optimization 可能会有问题</strong>,小的 Batch
Size,Optimization 的结果反而是比较好的,好 为什么会这样子呢</p>
<h3><span id="15-noisy-update-isbetter-for-training">1,5 “Noisy” update is
better for training</span></h3>
<p>为什么小的 Batch Size,在 Training Set 上会得到比较好的结果,为什么
Noisy 的 Update,Noisy 的 Gradient 会在 Training
的时候,给我们比较好的结果呢？一个可能的解释是这样子的</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626154658708.png" alt="image-20220626154658708" style="zoom:67%;"></p>
<p>假设你是 Full Batch,那你今天在 Update 你的参数的时候,你就是沿著一个
Loss Function 来 Update 参数,今天 Update 参数的时候走到一个 Local
Minima,走到一个 Saddle Point,显然就停下来了,Gradient
是零,如果你不特别去看Hession的话,那你用 Gradient Descent
的方法,你就没有办法再更新你的参数了</p>
<p>但是假如是 Small Batch 的话,因为我们每次是挑一个 Batch 出来,算它的
Loss,所以等於是,等於你每一次 Update 你的参数的时候,你用的 Loss Function
都是越有差异的,你选到第一个 Batch 的时候,你是用 L1 来算你的
Gradient,<strong>你选到第二个 Batch 的时候,你是用 L2 来算你的
Gradient,假设你用 L1 算 Gradient 的时候,发现 Gradient 是零,卡住了,但 L2
它的 Function 跟 L1 又不一样,L2 就不一定会卡住,所以 L1 卡住了
没关係,换下一个 Batch 来,L2 再算 Gradient。</strong></p>
<p><strong>你还是有办法 Training 你的 Model,还是有办法让你的 Loss
变小,所以今天这种 Noisy 的 Update 的方式,结果反而对
Training,其实是有帮助的。</strong></p>
<h3><span id="noisy-update-isbetter-for-generalization">“Noisy” update is
better for generalization</span></h3>
<p>那这边还有另外一个更神奇的事情，其实<strong>小的 Batch 也对 Testing
有帮助</strong>。</p>
<p>假设我们今天在 Training 的时候,都不管是大的 Batch 还小的 Batch,都
Training 到一样好,刚才的 Case是Training 的时候就已经 Training
不好了。</p>
<p>假设你有一些方法,你努力的调大的 Batch 的 Learning
Rate,然后想办法把大的 Batch,跟小的 Batch Training
得一样好,结果你会发现<strong>小的 Batch,居然在 Testing
的时候会是比较好的</strong>,那以下这个实验结果是引用自,On Large-Batch
Training For Deep Learning,Generalization Gap And Sharp
Minimahttps://arxiv.org/abs/1609.04836,这篇 Paper 的实验结果：</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220626154838373.png" alt="image-20220626154838373">
<figcaption aria-hidden="true">image-20220626154838373</figcaption>
</figure>
<p>那这篇 Paper 里面,作者 Train 了六个 Network 里面有 CNN 的,有 Fully
Connected Network 的,做在不同的 Cover
上,来代表这个实验是很泛用的,在很多不同的 Case
都观察到一样的结果,那它有小的 Batch,一个 Batch 里面有256笔 Example,大的
Batch 就是那个 Data Set 乘 0.1,Data Set 乘 0.1,Data Set
有60000笔,那你就是一个 Batch 里面有6000笔资料</p>
<p>然后他想办法,在大的 Batch 跟小的 Batch,都 Train 到差不多的 Training
的 Accuracy,所以刚才我们看到的结果是,Batch Size 大的时候,Training
Accuracy 就已经差掉了,这边不是想办法 Train 到大的 Batch 的时候,Training
Accuracy 跟小的 Batch,其实是差不多的</p>
<p>但是就算是在 Training 的时候结果差不多,Testing
的时候你还是看到了,小的 Batch 居然比大的 Batch 差,Training
的时候都很好,<strong>Testing 的时候大的 Batch 差,代表 Over
Fitting</strong>,这个才是 Over Fitting 对不对,好
那为什么会有这样子的现象呢？在这篇文章里面也给出了一个解释,</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626154919498.png" alt="image-20220626154919498" style="zoom:67%;"></p>
<p>假设这个是我们的 Training Loss,那在这个 Training Loss
上面呢,可能有很多个 Local Minima,有不只一个 Local Minima,那这些 Local
Minima 它们的 Loss 都很低,它们 Loss 可能都趋近於 0,但是这个
<strong>Local Minima,还是有好 Minima 跟坏 Minima 之分</strong></p>
<p>如果一个 Local Minima 它在一个峡谷里面,它是坏的
Minima,然后它在一个平原上,它是好的 Minima,为什么会有这样的差异呢</p>
<ul>
<li>因为假设现<strong>在 Training 跟 Testing 中间,有一个
Mismatch</strong>,Training 的 Loss 跟 Testing 的 Loss,它们那个 Function
不一样,有可能是本来你 Training 跟 Testing 的 Distribution就不一样。</li>
<li>那也有可能是因为 Training 跟 Testing,你都是从 Sample 的 Data
算出来的,也许 Training 跟 Testing,Sample 到的 Data
不一样,那所以它们算出来的 Loss,当然是有一点差距。</li>
</ul>
<p>那我们就假设说这个 Training 跟 Testing,它的差距就是把 Training 的
Loss,这个 Function
往右平移一点,这时候你会发现,对左边这个在一个盆地里面的 Minima
来说,它的在 Training 跟 Testing
上面的结果,不会差太多,只差了一点点,但是对右边这个在峡谷里面的 Minima
来说,一差就可以天差地远</p>
<p>它在这个 Training Set 上,算出来的 Loss 很低,但是因为 Training 跟
Testing 之间的不一样,所以 Testing 的时候,这个 Error Surface
一变,它算出来的 Loss 就变得很大,而很多人相信这个<strong>大的 Batch
Size,会让我们倾向於走到峡谷里面,而小的 Batch
Size,倾向於让我们走到盆地里面</strong></p>
<p>那他直觉上的想法是这样,就是小的 Batch,它有很多的 Loss,它每次 Update
的方向都不太一样,所以如果今天这个峡谷非常地窄,它可能一个不小心就跳出去了,因为每次
Update 的方向都不太一样,它的 Update
的方向也就随机性,所以一个很小的峡谷,没有办法困住小的 Batch</p>
<p>如果峡谷很小,它可能动一下就跳出去,之后停下来如果有一个非常宽的盆地,它才会停下来,那对於大的
Batch Size,反正它就是顺著规定
Update,然后它就很有可能,走到一个比较小的峡谷里面</p>
<p>但这只是一个解释,那也不是每个人都相信这个解释,那这个其实还是一个<strong>尚待研究的问题</strong>那这边就是比较了一下,大的
Batch 跟小的 Batch</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616171325318.png" alt="image-20220616171325318" style="zoom: 67%;"></p>
<p>左边这个是第一个 Column 是小的 Batch,第二个 Column 是大的 Batch</p>
<p>在有平行运算的情况下,小的 Batch 跟大的
Batch,其实运算的时间并没有太大的差距,除非你的大的 Batch
那个大是真的非常大,才会显示出差距来。但是一个 Epoch 需要的时间,小的
Batch 比较长,大的 Batch 反而是比较快的,所以从一个 Epoch
需要的时间来看,大的 Batch 其实是佔到优势的。</p>
<p>而小的 Batch,你会 Update 的方向比较 Noisy,大的 Batch Update
的方向比较稳定,但是 Noisy 的 Update 的方向,反而在 Optimization
的时候会佔到优势,而且在 Testing 的时候也会佔到优势,所以大的 Batch 跟小的
Batch,它们各自有它们擅长的地方。</p>
<p><strong><font color="red">所以 Batch Size,变成另外一个 你需要去调整的
Hyperparameter。</font></strong></p>
<p>那我们能不能够鱼与熊掌兼得呢,我们能不能够截取大的 Batch 的优点,跟小的
Batch 的优点,我们用大的 Batch Size
来做训练,用平行运算的能力来增加训练的效率,但是训练出来的结果同时又得到好的结果呢,又得到好的训练结果呢。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626155139635.png" alt="image-20220626155139635" style="zoom:67%;"></p>
<p>这是有可能的,有很多文章都在探讨这个问题,那今天我们就不细讲,我们把这些
Reference 列在这边给大家参考,那你发现这些
Paper,往往它想要做的事情都是什么,哇 76分鐘 Train BERT,15分鐘 Train
ResNet,一分鐘 Train Imagenet
等等,这为什么他们可以做到那么快,就是因为他们 Batch Size
是真的开很大,比如说在第一篇 Paper 里面,Batch Size 里面有三万笔 Example
这样,Batch Size 开很大,Batch Size 开大
真的就可以算很快,你可以在很短的时间内看到大量的资料,那他们需要有一些特别的方法来解决,Batch
Size 可能会带来的劣势。</p>
<h2><span id="二-momentum">二、Momentum</span></h2>
<p><strong><font color="red"> Momentum,这也是另外一个,有可能可以对抗
Saddle Point,或 Local Minima 的技术</font></strong>,Momentum
的运作是这个样子的，</p>
<h3><span id="21-small-gradient">2.1 Small Gradient</span></h3>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616171440148.png" alt="image-20220616171440148" style="zoom:80%;"></p>
<p>它的概念,你可以想像成在物理的世界里面,假设 Error Surface
就是真正的斜坡,而我们的参数是一个球,你把球从斜坡上滚下来,如果今天是
Gradient Descent,它走到 Local Minima 就停住了,走到 Saddle Point
就停住了</p>
<p>但是在物理的世界里,一个球如果从高处滚下来,从高处滚下来就算滚到 Saddle
Point,如果有<strong>惯性</strong>,它从左边滚下来,因为惯性的关係它还是会继续往右走,甚至它走到一个
Local
Minima,如果今天它的动量够大的话,它还是会继续往右走,甚至翻过这个小坡然后继续往右走</p>
<p>那所以今天在物理的世界里面,一个球从高处滚下来的时候,它并不会被 Saddle
Point,或 Local Minima卡住,不一定会被 Saddle Point,或 Local Minima
卡住,我们有没有办法运用这样子的概念,到 Gradient Descent
里面呢,那这个就是我们等一下要讲的,Momentum 这个技术</p>
<h3><span id="22-vanilla-gradient-descent">2.2 Vanilla Gradient Descent</span></h3>
<p>那我们先很快的复习一下,原来的 Gradient Descent 长得是什么样子,这个是
Vanilla 的 Gradient Descent,Vanilla
的意思就是一般的的意思,它直译是香草的,但就其实是一般的,一般的 Gradient
Descent 长什么样子呢？</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616171504634.png" alt="image-20220616171504634">
<figcaption aria-hidden="true">image-20220616171504634</figcaption>
</figure>
<p>​</p>
<p>一般的 Gradient Descent 是说,我们有一个初始的参数叫做 <span class="math inline">\(θ^0\)</span>,我们计算一下 Gradient,然后计算完这个
Gradient 以后呢,我们往 Gradient 的反方向去 Update 参数 <span class="math display">\[
θ^1 = θ^0 - {\eta}g^0
\]</span> 我们到了新的参数以后,再计算一次 Gradient,再往 Gradient
的反方向,再 Update 一次参数,到了新的位置以后再计算一次 Gradient,再往
Gradient 的反方向去 Update 参数,这个 Process 就一直这样子下去</p>
<h3><span id="12-gradient-descent-momentum">1.2 Gradient Descent + Momentum</span></h3>
<p>加上 Momentum 以后,每一次我们在移动我们的参数的时候,我们不是只往
Gradient Descent,我们不是只往 Gradient 的反方向来移动参数,我们是
<strong>Gradient
的反方向,加上前一步移动的方向,两者加起来的结果,去调整去到我们的参数,</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616171740139.png" alt="image-20220616171740139" style="zoom:50%;"></p>
<p>那具体说起来是这个样子,一样找一个初始的参数,然后我们假设前一步的参数的
Update 量呢,就设为 0 <span class="math display">\[
m^0 = 0
\]</span> 接下来在 <span class="math inline">\(θ^0\)</span> 的地方,计算
Gradient 的方向<span class="math inline">\(g^0\)</span>，然后接下来你要决定下一步要怎么走,它是
Gradient 的方向加上前一步的方向,不过因为前一步正好是
0,现在是刚初始的时候所以前一步是 0,所以 Update 的方向,跟原来的 Gradient
Descent 是一样的,这没有什么有趣的地方</p>
<p><span class="math display">\[
m^1 = {\lambda}m^0-{\eta}g^0\\\
θ^1 = θ^0 + m^1
\]</span> 但从第二步开始,有加上 Momentum
以后就不太一样了,从第二步开始,我们计算 <span class="math inline">\(g^1\)</span>,然后接下来我们 Update 的方向,不是
<span class="math inline">\(g^1\)</span>的反方向,而是根据上一次 Update
方向,也就是 m1 减掉 g1,当做我们新的 Update 的方向,这边写成 m2 <span class="math display">\[
m^2 = {\lambda}m^1-{\eta}g^1
\]</span> 那我们就看下面这个图</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616171749421.png" alt="image-20220616171749421">
<figcaption aria-hidden="true">image-20220616171749421</figcaption>
</figure>
<p>g1 告诉我们,<strong>Gradient
告诉我们要往红色反方向这边走</strong>,但是我们不是只听 Gradient
的话,加上 Momentum 以后,我们不是只根据 Gradient
的反方向,来调整我们的参数,我们<strong>也会看前一次 Update
的方向</strong></p>
<ul>
<li>如果前一次说要往<strong><span class="math inline">\(m^1\)</span>蓝色及蓝色虚线</strong>这个方向走</li>
<li>Gradient 说要往<strong>红色反方向这个方向</strong>走</li>
<li><strong>把两者相加起来</strong>,走两者的折中,也就是往<strong>蓝色<span class="math inline">\(m^2\)</span>这一个方向走</strong>,所以我们就移动了
m2,走到 θ2 这个地方</li>
</ul>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616171805517.png" alt="image-20220616171805517">
<figcaption aria-hidden="true">image-20220616171805517</figcaption>
</figure>
<p>接下来就反覆进行同样的过程,在这个位置我们计算出
Gradient,但我们不是只根据 Gradient
反方向走,我们看前一步怎么走,前一步走这个方向,走这个蓝色虚线的方向,我们把蓝色的虚线加红色的虚线,前一步指示的方向跟
Gradient 指示的方向,当做我们下一步要移动的方向</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616171811630.png" alt="image-20220616171811630">
<figcaption aria-hidden="true">image-20220616171811630</figcaption>
</figure>
<p>每一步的移动,我们都用 m 来表示,那这个 m
其实可以写成之前所有算出来的,Gradient 的 Weighted
Sum.从右边的这个式子,其实就可以轻易的看出来 <span class="math display">\[
m^0 = 0\\\
m^1 = -{\eta}g^0\\\
m^2 = -{\lambda}{\eta}g^0-{\eta}g^1\\\
...
\]</span> m0 我们把它设为 0,m1 是 m0 减掉 g0,m0 为 0,所以 m1 就是 g0
乘上负的 η,m2 是 λ 乘上 m1,λ 就是另外一个参数,就好像 η 是 Learning Rate
我们要调,λ 是另外一个参数,这个也是需要调的,m2 等於 λ 乘上 m1,减掉 η 乘上
g1,然后 m1 在哪里呢,m1 在这边,你把 m1 代进来,就知道说 m2,等於负的 λ 乘上
η 乘以 g0,减掉 η 乘上 g1,它是 g0 跟 g1 的 Weighted Sum</p>
<p>以此类推,所以你会发现说,现在这个加上 Momentum 以后,一<strong>个解读是
Momentum 是,Gradient
的负反方向加上前一次移动的方向</strong>,那但另外一个解读方式是,所谓的
Momentum,<strong>当加上 Momentum 的时候,我们 Update
的方向,不是只考虑现在的 Gradient,而是考虑过去所有 Gradient
的总合.</strong></p>
<p>​ 有一个更简单的例子,希望帮助你了解 Momentum</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616172323699.png" alt="image-20220616172323699" style="zoom:67%;"></p>
<p>那我们从这个地方开始 Update 参数,根据 Gradient
的方向告诉我们,应该往右 Update 参数,那现在没有前一次 Update
的方向,所以我们就完全按照 Gradient 给我们的指示,往右移动参数,好
那我们的参数,就往右移动了一点到这个地方</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616172349096.png" alt="image-20220616172349096" style="zoom:67%;"></p>
<p>Gradient
变得很小,告诉我们往右移动,但是只有往右移动一点点,但前一步是往右移动的,我们把前一步的方向用虚线来表示,放在这个地方,我们把之前
Gradient
告诉我们要走的方向,跟前一步移动的方向加起来,得到往右走的方向,那再往右走
走到一个 Local Minima,照理说走到 Local Minima,一般 Gradient Descent
就无法向前走了,因为已经没有这个 Gradient 的方向,那走到 Saddle Point
也一样,没有 Gradient 的方向已经无法向前走了</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616172359508.png" alt="image-20220616172359508" style="zoom:67%;"></p>
<p>但没有关係,如果有 Momentum 的话,你还是有办法继续走下去,因为 Momentum
不是只看 Gradient,Gradient 就算是
0,你还有前一步的方向,前一步的方向告诉我们向右走,我们就继续向右走,甚至你走到这种地方,Gradient
告诉你应该要往左走了,但是假设你前一步的影响力,比 Gradient
要大的话,你还是有可能继续往右走,甚至翻过一个小丘,搞不好就可以走到更好
Local Minima,这个就是 Momentum 有可能带来的好处。</p>
<h2><span id="concluding-remarks">Concluding Remarks</span></h2>
<ul>
<li><strong>Critical points have zero gradients.</strong></li>
<li>Critical points can be either <strong>saddle points or local
minima</strong>.
<ul>
<li>Can be determined by the Hessian matrix.</li>
<li>Local minima may be rare.</li>
<li>It is possible to escape saddle points along the direction of
eigenvectors of the Hessian matrix</li>
</ul></li>
<li>Smaller batch size and momentum help escape critical points.</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2N47FT1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2N47FT1/" class="post-title-link" itemprop="url">【Nan】General Guidance</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-09 11:34:59" itemprop="dateCreated datePublished" datetime="2022-06-09T11:34:59+08:00">2022-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 23:18:00" itemprop="dateModified" datetime="2022-07-13T23:18:00+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>17k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>30 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="general-guidance-overfit">General Guidance : overfit</span></h1>
<h2><span id="framework-of-ml">Framework of ML</span></h2>
<p>​
我们已经看了作业一了,其实之后好几个作业,它看起来的样子,基本上都是大同小异</p>
<figure>
<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311202919509.png" alt="image-20210311202919509">
<figcaption aria-hidden="true">image-20210311202919509</figcaption>
</figure>
<p>​
就是你会有一堆训练的资料,这些<strong>训练集</strong>裡面,会包含了<span class="math inline">\(x\)</span>跟<span class="math inline">\(y\)</span>的hat,<span class="math inline">\(x¹\)</span> 和跟它对应的<span class="math inline">\(ŷ¹\)</span>,<span class="math inline">\(x²\)</span> 跟它对应的<span class="math inline">\(ŷ²\)</span>,一直到<span class="math inline">\(xⁿ\)</span> 还有它对应的<span class="math inline">\(ŷⁿ\)</span></p>
<p>​
测试集,<strong>测试集就是你只有x没有y</strong>,其实在之后每一个作业,看起来都是非常类似的格式</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311203322900.png" alt="image-20210311203322900" style="zoom:67%;"></p>
<ul>
<li>作业二,其实是做<strong>语音辨识</strong>,我们的x就是非常小的一段声音讯号,其实这个不是真正完整的语音辨识系统,它是语音辨识系统的一个阉割版,ŷ是要去预测判断,这一小段声音讯号,它对应到哪一个phoneme。你不知道phoneme是什麼没有关係,你就把它想成是音标就可以了</li>
<li>作业三叫做<strong>图像识别</strong>,这个时候我们的x是一张图片,ŷ是机器要判断说这张图片裡面,有什麼样的东西</li>
<li>作业四是<strong>语者辨识</strong>,语者辨识要做的事情是,这个x也是一段声音讯号,ŷ现在不是phoneme,ŷ是现在是哪一个人在说话,这样的系统,现在其实非常的有用,如果你打电话去银行的客服,现在都有自动的语者辨认系统,它会听说现在打电话进来的人,是不是客户本人,就少了客服人员问你身份验证的时间</li>
<li>作业五是做<strong>机器翻译</strong>,x就是某一个语言,比如说,这是我唯一会的一句日文,痛みを知れ,它的ŷ就是另外一句话。</li>
</ul>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311203956745.png" alt="image-20210311203956745" style="zoom: 67%;"></p>
<p>​
训练集就要拿来训练我们的Model,训练Model的过程上週已经讲过了,训练的过程就是<strong>三个步骤</strong></p>
<ul>
<li>第一步,你要先写出一个有未知数的function,这个未知数,以后我们都用<span class="math inline">\(θ\)</span>来代表,一个Model裡面所有的未知函数,所以<span class="math inline">\(f_θ(x)\)</span>的意思就是说,我现在有一个function叫f(x),它裡面有一些未知的参数,这些未知的参数表示成<span class="math inline">\(θ\)</span>,它的input叫做x,这个input叫做feature</li>
<li>第二步，你要定一个东西叫做loss,loss是一个function,这个loss的输入就是一组参数,去判断说这一组参数是好还是不好</li>
<li>第三步，你要解一个,Optimization的problem,你要去找一个θ,这个θ
可以让loss的值越小越好,可以让loss的值 最小的那个<span class="math inline">\(θ\)</span>,我们叫做<span class="math inline">\(θ^*\)</span></li>
</ul>
<p>​ 有了<span class="math inline">\(θ^*\)</span>以后,那你就把它拿来用在测试集上,也就是你把<span class="math inline">\(θ^*\)</span>带入这些未知的参数,本来<span class="math inline">\(f_θ(x)\)</span>裡面有一些未知的参数,现在这个<span class="math inline">\(θ\)</span> 用<span class="math inline">\(θ^*\)</span>来取代,它的输入就是你现在的测试集,输出的结果
你就把它存起来,然后上传到Kaggle就结束了。</p>
<p>​ 接下来你就会遇到一个问题,直接执行助教的sample
code,往往只能够给你simple
baseline的结果而已,如果你想要做得更好,那应该要怎麼办,</p>
<h2><span id="general-guide">General Guide</span></h2>
<p>​
以下就是如何让你做得更好的攻略,它适用於前期所有的作业,这个攻略是怎麼走的呢?</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311205104719.png" alt="image-20210311205104719" style="zoom:67%;"></p>
<p>​
从最上面开始走起,第一个是你今天如果你觉得,你在Kaggle上的结果不满意的话,第一件事情你要做的事情是,<strong>检查你的training
data的loss</strong></p>
<p>​ 有人说"我在意的不是应该是,testing
data的loss吗,因為Kaggle上面的结果,呈现的是testing data的结果"</p>
<p>​ 但是你要先检查你的training data,看看你的model在training
data上面,有没有学起来,再去看testing的结果,如果你发现,你的<strong>training
data的loss很大</strong>,显然它<strong>在训练集上面也没有训练好</strong>,接下来你要分析一下,在训练集上面没有学好,是什麼样的原因,这边有两个可能,第一个可能是model的bias</p>
<h3><span id="model-bias">Model bias</span></h3>
<p>​ model的bias这件事情,我们在上週已经跟大家讲过了,所谓model
bias的意思是说,假设你的model太过简单。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311205956634.png" alt="image-20210311205956634" style="zoom:67%;"></p>
<p>​
举例来说,我们现在写了一个有未知parameter的function,这个未知的parameter,我们可以代各种不同的数字,你代θ¹
得到一个function <span class="math inline">\(f_{θ^1}(x)\)</span>,我们把那个function用这个,一个点来表示,θ²
得到另一个function <span class="math inline">\(f_{θ^1}(x)\)</span>,你把所有的function集合起来,得到一个function的set.</p>
<p>​
但是这个function的set太小了,这个function的set裡面,没有包含任何一个function,可以让我们的loss变低,即可以让loss变低的function,不在你的model可以描述的范围内。</p>
<p>​
在这个情况下,就算你找出了一个θ*,它是这些蓝色的function裡面,最好的那一个,也无济於事了,那个loss还是不够低。</p>
<p>​
这个状况就是你想要在<strong>大海裡面捞针</strong>,这个针指的是一个loss低的function,结果<strong>针根本就不在海裡</strong>,白忙一场,你怎麼捞都捞不出针,因為针根本就不在你的,这个function
set裡面,不在你的这个大海裡面,所以怎麼办？</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311212242288.png" alt="image-20210311212242288" style="zoom:67%;"></p>
<p>​
这个时候<strong>重新设计一个model,给你的model更大的弹性</strong>,举例来说,你可以增加你输入的features,我们上週说,本来我们输入的features,只有前一天的资讯,假设我们要预测接下来的这个,观看人数的话,我们用前一天的资讯,不够多,那用56天前的资讯,那model的弹性就比较大了</p>
<p>​ 你也可以用Deep
Learning,增加更多的弹性,所以如果你觉得,你的model的弹性不够大,那你可以增加更多features,可以设一个更大的model,可以用deep
learning,来增加model的弹性,这是第一个可以的解法。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311205104719.png" alt="image-20210311205104719" style="zoom:67%;"></p>
<p>​ 但是并不是training的时候,loss大就代表一定是model
bias,你可能会遇到另外一个问题,这个问题是什麼,这个问题是<strong>optimization做得不好</strong>,什麼意思呢？</p>
<h3><span id="optimization-issue">Optimization Issue</span></h3>
<p>​
我们知道说,我们今天用的optimization,在这门课裡面,我们其实都只会用到gradient
descent,这种optimization的方法,这种optimization的方法很多的问题。</p>
<p>​ 举例来说 我们上週也讲过说,你可能会卡在==local
minima==的地方,你没有办法找到一个,真的可以让loss很低的参数,如果用图具象化的方式来表示,就像这个样子</p>
<p>​
<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311213108040.png" alt="image-20210311213108040" style="zoom:67%;"></p>
<p>​ 蓝色部分是你的model可以表示的函式所形成的集合,你可以把θ
代入不同的数值,形成不同的function,把所有的function通通集合在一起,得到这个蓝色的set,这个蓝色的set裡面,确实包含了一些function,这些function它的loss是低的。</p>
<p>​ 但问题是<strong>gradient
descent这一个演算法,没办法帮我们找出,这个loss低的function</strong>,gradient
descent是解一个optimization的problem,找到θ* 然后就结束了</p>
<p>​ 但是这个<span class="math inline">\(θ^*\)</span>
它给我的loss不够低,这一个model裡面,存在著某一个function,它的loss是够低的,gradient
descent,没有给我们这一个function</p>
<p>​ 这就好像是说
我们想<strong>大海捞针,针确实在海裡,但是我们却没有办法把针捞起来</strong>,这边问题就来了</p>
<p>​ ==training data的loss不够低的时候,到底是model
bias,还是optimization的问题呢==</p>
<ul>
<li>找不到一个loss低的function,到底是因為我们的model的弹性不够,我们的海裡面没有针</li>
<li>还是说,我们的model的弹性已经够了,只是optimization gradient
descent不给力,它没办法把针捞出来</li>
</ul>
<p>​
到底是哪一个呢,到底我们的model已经够大了,还是它不够大,怎麼判断这件事呢</p>
<h4><span id="gaining-the-insights-fromcomparison">Gaining the insights from
comparison</span></h4>
<p>​
一个建议判断的方法,就是你可以<strong>透过比较不同的模型,来得知说,你的model现在到底够不够大</strong>,怎麼说呢</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210311214054168.png" alt="image-20210311214054168" style="zoom:67%;"></p>
<p>​ 我们这边举一个例子,这一个实验是从residual
network,那篇paper裡面节录出来的,我们把paper链接：http://arxiv.org/abs/1512.03385</p>
<p>​ 这篇paper一开头就跟你讲一个故事,它说 我想测2个networks</p>
<ul>
<li><strong>一个network有20层</strong></li>
<li><strong>一个network有56层</strong></li>
</ul>
<p>​
我们把它们测试在测试集上,这个<strong>横轴指的是training的过程</strong>,就是你参数update的过程,随著参数的update,当然你的loss会越来越低,但是结果<strong>20层的loss比较低,56层的loss还比较高</strong></p>
<p>​ 这个residual
network是比较早期的paper,2015年的paper,如果你现在大学生的话,那个时候你都还是高中生而已,所以那个时候大家对Deep
Learning,了解还没有那麼的透彻,大家对deep
learning,有各种奇怪的误解,很多人看到这张图都会说,这个代表overfitting,告诉你deep
learning不work,56层太深了 不work,根本就不需要那麼深</p>
<p>​ 那个时候大家也不是每个人都觉得deep
learning是好的,那时候还有很多,对deep
learning的质疑,所以看到这个实验有人就会说,最深没有比较好,所以这个叫做overfitting,但是这个是overfitting吗</p>
<p>​
<strong>这个不是overfitting</strong>,等一下会告诉你overfitting是什麼,<strong>并不是所有的结果不好,都叫做overfitting</strong></p>
<p>​
你要<strong>检查一下训练集上面的解释</strong>,你检查训练集的结果发现说,现在20层的network,跟56层的network比起来,在训练集上,20层的network
loss其实是比较低的,56层的network loss是比较高的</p>
<p>​
<strong>这代表56层的network,它的optimization没有做好</strong>,它的optimization不给力</p>
<p>​ 你可能问说,你怎麼知道是56层的optimization不给力,搞不好是model
bias,搞不好是56层的network,它的model弹性还不够大,它要156层才好,56层也许弹性还不够大,但是你比较56层跟20层,20层的loss都已经可以做到这样了,56层的弹性一定比20层更大对不对</p>
<p>​
如果今天<strong>56层的network要做到20层的network可以做到的事情</strong>,对它来说是轻而易举的</p>
<p>​
它<strong>只要前20层的参数,跟这个20层的network一样</strong>,剩下36层就什麼事都不做,identity
copy前一层的输出就好了,56层的network一定可以做到20层的network可以做到的事情,所以20层的network已经都可以走到这麼底的loss了,56层的network,它比20层的network弹性还要更大,所以没有道理</p>
<p>​
所以56层的network,如果你optimization成功的话,它应该要比20层的network,可以得到更低的loss,但结果在训练集上面没有,这个不是overfitting,这个也不是model
bias,因為56层network弹性是够的,这个问题是你的<strong>optimization不给力,optimization做得不够好</strong></p>
<h4><span id="startfrom-shallower-networks-or-other-models-which-are-easier-totrain">Start
from shallower networks (or other models), which are easier to
train.</span></h4>
<p>​
所以刚才那个例子就告诉我们,你怎麼知道你的optimization有没有做好,这边给大家的建议是</p>
<p>​
<strong>看到一个你从来没有做过的问题,也许你可以先跑一些比较小的,比较浅的network,或甚至用一些,不是deep
learning的方法</strong>,比如说 linear model,比如说support vector
machine,support vector
machine不知道是什麼也没有关係,它们可能是比较容易做Optimize的,它们比较不会有optimization失败的问题</p>
<p>​
也就是这些model它会竭尽全力的,在它们的能力范围之内,找出一组最好的参数,它们比较不会有失败的问题,所以你可以先train一些,比较浅的model,或者是一些比较简单的model,先知道
先有个概念说,这些简单的model,到底可以得到什麼样的loss</p>
<h4><span id="ifdeeper-networks-do-not-obtain-smaller-loss-on-training-data-then-thereis-optimization-issue">If
deeper networks do not obtain smaller loss on training data, then there
is optimization issue.</span></h4>
<p>​
接下来还缺一个深的model,如果你发现你深的model,跟浅的model比起来,深的model明明弹性比较大,但loss却没有办法比浅的model压得更低,那就代表说你的optimization有问题,你的gradient
descent不给力,那你要有一些其它的方法,来把optimization这件事情做得更好</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313203557412.png" alt="image-20210313203557412" style="zoom:67%;"></p>
<p>​
举例来说,我们上次看到的这个,观看人数预测的例子,我们说在训练集上面,2017年到2020年的资料是训练集,一层的network,它的loss是0.28k,2层就降到0.18k,3层就降到0.14k,4层就降到0.10k。</p>
<p>​ 但是 我测5层的时候结果变成0.34k,这是什麼问题</p>
<p>​ 我们现在loss很大，这显然不是model
bias的问题,因為4层都可以做到0.10k了,5层应该可以做得更低,这个是optimization的problem,这个是optimization的时候做得不好,才造成这样子的问题。</p>
<p>​
那如果optimization做得不好的话,怎麼办呢,这个我们下一节课,就会告诉大家要怎麼办,你现在就知道怎麼判断,现在如果你的training的loss大,到底是model
bias还是optimization,如果model bias
那就把model变大,如果是optimization失败了,那就看等一下的课程怎麼解这个问题。</p>
<p>​
<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313203829786.png" alt="image-20210313203829786" style="zoom:67%;"></p>
<p>​ 假设你现在经过一番的努力,你已经可以让你的,training
data的loss变小了,那接下来你就可以来看,testing data loss,如果testing data
loss也小,有比这个strong
baseline还要小就结束了,没什麼好做的就结束了。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313204055684.png" alt="image-20210313204055684" style="zoom:67%;"></p>
<p>​ 那但是如果你觉得还不够小呢,==如果training data上面的loss小,testing
data上的loss大,那你可能就是真的遇到overfitting的问题==</p>
<p>​ 你要注意 是training的loss小,testing的loss大
才叫做overfitting,很多同学每次一看到结果不好,在testing上的结果不好,就说这个是overfitting,不一定是overfitting。</p>
<p>​
你拿一个结果来问我说,老师这个结果要怎麼做得更好的时候,我第一个问题就会问你说,你在training
data上的loss,到底做得怎麼样,我发现十个同学有八个同学都说,要看training
data的loss吗,我没有把training data loss记下来,你要把training data
loss记下来,先确定说你的optimization没有问题,你的model够大了,然后接下来,才看看是不是testing的问题,<strong>如果是training的loss小,testing的loss大,这个有可能是overfitting</strong></p>
<h3><span id="overfitting">Overfitting</span></h3>
<p>​
為什麼会有overfitting这样的状况呢,為什麼有可能training的loss小,testing的loss大呢,这边就举一个极端的例子来告诉你说,為什麼会发生这样子的状况</p>
<p>​
<img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313204504732.png" alt="image-20210313204504732" style="zoom:67%;"></p>
<p>​ 这是我们的训练集,假设根据这些训练集,某一个很废的,machine
learning的方法,它找出了一个一无是处的function</p>
<p>​
这个一无是处的function说,<strong>如果今天x当做输入的时候,我们就去比对这个x,有没有出现在训练集裡面,如果x有出现在训练集裡面,就把它对应的ŷ当做输出,如果x没有出现在训练集裡面,就输出一个随机的值</strong></p>
<p>​
那你可以想像这个function啥事也没有干,它是一个一无是处的function,但虽然它是一个一无是处的function,它<strong>在training的data上，它的loss可是0呢</strong></p>
<p>​
你把training的data,通通丢进这个function裡面,它的输出跟你的训练集的level,是一模一样的,所以在training
data上面,这个一无是处的function,它的loss可是0呢,可是在testing
data上面,它的loss会变得很大,因為<strong>它其实什麼都没有预测</strong>,这是一个比较极端的例子,在一般的状况下,也有可能发生类似的事情。</p>
<p>​
举例来说,假设我们输入的feature叫做x,我们输出的level叫做y,那x跟y都是一维的</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313205555809.png" alt="image-20210313205555809" style="zoom: 50%;"></p>
<p>​
x跟y之间的关係,是这个二次的曲线,这个曲线我们刻意用虚线来表示,因為我们通常没有办法,直接观察到这条曲线,我们真正可以观察到的是什麼,我们真正可以观察到的,是我们的训练集,训练集
你可以想像成,就是从这条曲线上面,随机sample出来的几个点</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313205653097.png" alt="image-20210313205653097" style="zoom:50%;"></p>
<p>​ 今天的模型
它的能力非常的强,它的flexibility很大,它的弹性很大的话,你只给它这三个点,它会知道说,在这三个点上面我们要让loss低,所以今天你的model,它的这个曲线会通过这三个点,但是其它没有训练集做為限制的地方,它就会有freestyle,因為它的flexibility很大,它弹性很大,所以你的model,可以变成各式各样的function,你没有给它资料做為训练,它就会有freestyle，可以產生各式各样奇怪的结果。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313205918762.png" alt="image-20210313205918762" style="zoom:50%;"></p>
<p>​ 这个时候,如果你再丢进你的testing data,你的testing data
和training的data,当然不会一模一样,它们可能是从同一个,distribution
sample出来的,testing data是橙色的这些点,训练data是蓝色的这些点</p>
<p>​
用蓝色的这些点,找出一个function以后,你测试在橘色的这些点上,不一定会好,<strong>如果你的model它的自由度很大的话,它可以產生非常奇怪的曲线,导致训练集上的结果好,但是测试集上的loss很大</strong>,那至於更详细的背后的数学原理,為什麼这个比较有弹性的model,它就比较会,overfitting背后的数学原理,我们留待下下週</p>
<p>​ 那怎麼解决刚才那个,overfitting的问题呢,有两个可能的方向</p>
<ol type="1">
<li>第一个方向是,也许这个方向往往是最有效的方向,是<strong>增加你的训练集</strong></li>
</ol>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313210215733.png" alt="image-20210313210215733" style="zoom:67%;"></p>
<p>​
今天假设你自己,想要做一个application,你发现有overfitting的问题,其实我觉得,最简单解决overfitting的方法,就是增加你的训练集</p>
<p>​
所以今天如果训练集,蓝色的点变多了,那虽然你的model它的弹性可能很大,但是因為你这边的点非常非常的多,它就可以限制住,它看起来的形状还是会很像,產生这些资料背后的二次曲线,但是你在作业裡面,你是不能够使用这一招的,因為我们并不希望大家浪费时间,来收集资料</p>
<p>​ 那你可以做什麼呢,你可以做<strong>data
augmentation</strong>，这个方法并不算是使用了额外的资料。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313210406236.png" alt="image-20210313210406236" style="zoom:67%;"></p>
<p>​ Data
augmentation就是,你用一些你对於这个问题的理解,自己创造出新的资料。</p>
<p>​
举例来说在做影像辨识的时候,非常常做的一个招式是,假设你的训练集裡面有某一张图片,把它左右翻转,或者是把它其中一块截出来放大等等,你做左右翻转
你的资料就变成两倍,那这个就是data augmentation</p>
<p>​ 但是你要注意一下data augmentation,不能够随便乱做,这个augment
要augment得有道理,举例来说在影像辨识裡面,你就<strong>很少看到有人把影像上下颠倒</strong>当作augmentation,因為这些图片都是合理的图片,你把一张照片左右翻转,并不会影响到裡面是什麼样的东西,但你把它颠倒
那就很奇怪了,这可能不是一个训练集裡面,可能不是真实世界会出现的影像</p>
<p>​
那如果你给机器看这种,奇怪的影像的话,它可能就会学到奇怪的东西,所以<strong>data
augmentation,要根据你对资料的特性,对你现在要处理的问题的理解,来选择合适的,data
augmentation的方式</strong>,好 那这边是增加资料的部分</p>
<ol start="2" type="1">
<li>另外一个解法就是<strong>不要让你的模型,有那麼大的弹性</strong>,给它一些限制,</li>
</ol>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313210826241.png" alt="image-20210313210826241" style="zoom:67%;"></p>
<p>​ 举例来说 假设我们直接限制说,现在我们的model,我们somehow猜测出
知道说,x跟y背后的关係,其实就是一条二次曲线,只是我们不明确的知道这二次曲线,裡面的每一个参数长什麼样</p>
<p>​
那你说你怎麼会猜测出这样子的结果,你怎麼会知道说,<strong>要用多constrain的model才会好呢</strong>,那这就取决於你对这个问题的理解,因為这种model是你自己设计的,到底model要多constrain多flexible,结果才会好,那这个要问你自己,那要看这个设计出不同的模型,你就会得出不同的结果</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313211712938.png" alt="image-20210313211712938" style="zoom:67%;"></p>
<p>​
那现在假设我们已经知道说,模型就是二次曲线,那你就会给你,你就会在选择function的时候,有很大的限制,因為二次曲线要嘛就是这样子,来来去去就是那几个形状而已,所以当我们的训练集有限的时候,因為我们来来去去,只能够选那几个function</p>
<p>​
所以你可能,虽然说只给了三个点,但是因為我们能选择的function有限,你可能就会正好选到,跟真正的distribution,比较接近的function,然后在测试集上得到比较好的结果。</p>
<p>​
所以这是第二个方法,解决overfitting的问题,你要给你的model一些限制,最好你的model正好,跟背后產生资料的过程,你的process是一样的,那你可能就会,你就有机会得到好的结果</p>
<p>​ 有哪些方法可以给model製造限制呢,举例来说,</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313211849093.png" alt="image-20210313211849093" style="zoom:67%;"></p>
<ul>
<li>给它<strong>比较少的参数</strong>,如果是deep
learning的话,就给它比较少的神经元的数目,本来每层一千个神经元,改成一百个神经元之类的,或者是你可以让model共用参数,你可以让一些参数有一样的数值,那这个部分如果你没有很清楚的话,也没有关係,我们之后在讲CNN的时候,会讲到这个部分,所以这边先前情
先预告一下,就是我们之前讲的network的架构,叫做==fully-connected
network==,那fully-connected
network,其实是一个比较有弹性的架构,而==CNN是一个比较有限制的架构==,就说你可能会说,CNN不是比较厉害吗,大家都说做影像就是要CNN,比较厉害的model,难道它比较没有弹性吗,没错,它是一种比较没有弹性的model,它厉害的地方就是,它是针对影像的特性,来限制模型的弹性,所以你今天fully-connected的network,可以找出来的function所形成的集合,其实是比较大的,CNN这个model所找出来的function,它形成的集合其实是比较小的,其实包含在fully-connected的,network裡面的,但是就是因為CNN给了,比较大的限制,所以CNN在影像上,反而会做得比较好,那这个之后都还会再提到,</li>
<li>另外一个就是用<strong>比较少的features</strong>,那刚才助教已经示范过,本来给三天的资料,改成用给两天的资料,其实结果就好了一些,那这个是一个招数</li>
<li>还有一个招数叫做<strong>Early stopping</strong>,Early
stopping,Regularization跟Dropout,都是之后课程还会讲到的东西,那这三件事情在作业一的程式裡面,这个Early
stopping其实是有的,助教有写在它的code裡面,所以不知道这是什麼也没有关係,反正你直接执行sample
code,裡面就有了,Regularization,助教留下了一个空格给大家填,那你不知道什麼是regularization,没有关係,反正你可以过得了middle的baseline,那如果你想做得更好,也许你可以先自己survey一下,regularization是什麼,看有没有办法自己写</li>
<li><strong>Dropout</strong>,这是另外一个在Deep
Learning裡面,常用来限制模型的方法,那这个之后还会再提到</li>
</ul>
<p>​
<strong>但是我们也不要给太多的限制</strong>,為什麼不能给模型太多的限制呢</p>
<p>​
假设我们现在给模型更大的限制说,我们假设我们的模型,一定是Linear的Model,一定是写成y=a+bx,那你的model它能够產生的function,就一定是一条直线</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313212322570.png" alt="image-20210313212322570" style="zoom:67%;"></p>
<p>​
今天给三个点,没有任何一条直线,可以同时通过这三个点,但是你只能找到一条直线,这条直线跟这些点比起来,它们的距离是比较近的,但是你没有办法找到任何一条直线,同时通过这三个点,这个时候你的模型的限制就太大了,你在测试集上就不会得到好的结果</p>
<p>​ 但是 这个不是overfitting,因為你又回到了<strong>model
bias</strong>的问题,所以你现在这样在这个情况下,这个投影片的case上面你结果不好,并不是因為overfitting了,而是因為你给你模型太大的限制,大到你有了model
bias的问题。</p>
<p>​
所以你就会发现说,这边<strong>產生了一个矛盾的状况</strong>,今天你让你的模型的复杂的程度,或这样让你的模型的弹性越来越大,但是什麼叫做复杂的程度,什麼叫做弹性,在今天这堂课裡面,我们其实都没有给明确的定义,只给你一个概念上的叙述,那在下下週的课程裡面,你会真的认识到,什麼叫做一个模型很复杂,什麼叫做一个模型有弹性,怎麼真的衡量一个模型的弹性,复杂的程度有多大,那今天我们先用直观的来了解</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313212632577.png" alt="image-20210313212632577" style="zoom:50%;"></p>
<p>​
所谓比较复杂就是,<strong>它可以包含的function比较多,它的参数比较多,这个就是一个比较复杂的model</strong></p>
<p>​ 那一个比较复杂的model,如果你看它的training的loss,你会发现说
<strong>随著model越来越复杂,Training的loss可以越来越低</strong>,但是testing的时候呢,<strong>当model越来越复杂的时候,刚开始,你的testing的loss会跟著下降,但是当复杂的程度,超过某一个程度以后,Testing的loss就会突然暴增了</strong></p>
<p>​
那这就是因為说,当你的model越来越复杂的时候,复杂到某一个程度,overfitting的状况就会出现,所以你在training的loss上面,可以得到比较好的结果,那在Testing的loss上面,你会得到比较大的loss,那我们当然期待说,我们可以选一个中庸的模型,不是太复杂的
也不是太简单的,刚刚好可以在训练集上,给我们最好的结果,给我们最低的loss,给我们最低的testing
loss,怎麼选出这样的model呢</p>
<p>​ 一个很直觉的
你很有可能,没有人告诉你要怎麼做的话,你可能很直觉就会这麼做的,做法就是说,这个kaggle不是立刻上传,就可以知道答案了吗</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313213154441.png" alt="image-20210313213154441" style="zoom:67%;"></p>
<p>​
所以假设我们有三个模型,它们的复杂的程度不太一样,我不知道要选哪一个模型才会刚刚好,在测试集上得到最好的结果,因為你选太复杂的就overfitting,选太简单的有model
bias的问题,那怎麼选一个不偏不倚的,不知道 那怎麼办</p>
<p>​
把这三个模型的结果都跑出来,然后上传到kaggle上面,你及时的知道了你的分数,看看哪个分数最低,那个模型显然就是最好的模型</p>
<p>​ 但是并不建议你这麼做,為什麼不建议你这麼做呢</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313213301109.png" alt="image-20210313213301109" style="zoom: 67%;"></p>
<p>​
我们再举一个极端的例子,我们再把刚才那个极端的例子拿出来,假设现在有一群model,这一群model不知道為什麼都非常废,它们每一个model產生出来的,都是一无是处的function,我们有一到一兆个model,这一到一兆个model不知道為什麼,learn出来的function,都是一无是处的function</p>
<p>​
它们会做的事情就是,<strong>训练集裡面有的资料就把它记下来,训练集没看过的,就直接output随机的结果</strong></p>
<p>​
那你现在有一兆个模型,那你再把这一兆个模型的结果,通通上传到kaggle上面,你就得到一兆个分数,然后看这一兆的分数裡面,哪一个结果最好,你就觉得那个模型是最好的</p>
<p>​ 那虽然说每一个模型,它们在这个Testing data上面,这个testing
data它都没有看过啊,所以它输出的结果都是随机的,但虽然在testing
data上面,输出的结果都是随机的,但是<strong>你不断的随机,你总是会找到一个好的结果</strong>,所以也许编号五六七八九的那个模型,它找出来的function,正好在testing
data上面,就给你一个好的结果,那你就会很高兴觉得说,这个model编号五六七八九,是个好model,这个好model得到一个好function</p>
<p>​ 虽然它其实是随机的 但你不知道,但这个好function,在这个testing
data上面,给我们好的结果,所以你就觉得说 这个结果不错,就这样
我就<strong>选这一个model,这个function,当作我们最后上传的结果,当作我最后要用在,private
testing set上的结果</strong></p>
<p>​
但是<strong>如果你这样做,往往就会得到非常糟的结果</strong>,因為这个model毕竟是随机的,它<strong>恰好在public的testing
set data上面得到一个好结果,但是它在private的testing
set上,可能仍然是随机的</strong></p>
<p>​ 我们这个testing set,分成public的set跟private的set,你在看分数的时候
你只看得到public的分数，private的分数要deadline以后才知道,但假设你在挑模型的时候,你完全看你在public
set上面的,也就leaderboard上的分数,来选择你的模型的话,你可能就会这个样子：你在public的leaderboard上面排前十,但是deadline一结束,你就心态就崩了这样,你就掉到三百名之外,而且我们这修课的人这麼多,你搞不好会掉到一千名之外,也说不定。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313213904472.png" alt="image-20210313213904472" style="zoom:67%;"></p>
<p>​</p>
<p>​
而且这件事情并不是传说,并没有夸饰,每年都会有这样子的状况发生,那因為今年我们会看public,就是说我们在算分数的时候,你在public上面的结果好,还是会给你一点分数,我们不是只看private的分数而已,是public跟private的分数都看,那过去有些学期,是只看private的分数的时候,发生这种状况,你心态就会整个崩掉这样子,你就会非常非常的郁闷</p>
<p>​
<strong>那為什麼我们要把testing的set,分成public跟private呢</strong>,為什麼我们不能,就通通都分public就好呢,為什麼要為难大家呢,為什麼要让大家疑神疑鬼,不知道自己private上的结果是什麼</p>
<p>​
你自己想想看,假设所有的data都是public,那我刚才说,就算是一个一无是处的Model,得到了一无是处的function,它也有可能在public的data上面,得到好的结果,如果我们今天只有public的testing
set,没有private的testing
set,那你就回去写一个程式,不断random產生输出就好,然后不断把random的输出,上传到kaggle,然后看你什麼时候,可以random出一个好的结果,那这个作业就结束了</p>
<p>​ 这个显然没有意义,显然不是我们要的,而且因為如果今天
你想想看,然后这边有另外一个有趣的事情就是,你知道因為今天如果,public的testing
data是公开的,你可以知道public的,testing
data的结果,那你就算是一个很废的模型,產生了很废的function,也可能得到非常好的结果</p>
<p>​ 所以讲了这麼多,只是想要告诉大家说,我们為什麼要切public的testing
set,我為什麼要切private的testing
set,然后你其实不要花,不要用你public的testing
set,去调你的模型,因為你可能会在,private的testing
set上面,得到很差的结果,那不过因為今年,你在public
set上面的,好的结果也有算分数,所以怎麼办呢,為了避免你 就你可能会说,好
那我放弃private set的结果,就只拿public
set的结果,然后不断地產生随机的结果,去上传到Kaggle来,然后看看说能不能够,正好随机出一个好的结果,為了避免你浪费时间做这件事情,所以有每日上传的限制,让你不会说,我拿很废的模型只產生随机的结果,不断的测试public的testing的score</p>
<h2><span id="cross-validation">Cross Validation</span></h2>
<p>​
那到底要怎麼做才选择model,才是比较合理的呢,那界定的方法是这个样子的,那助教程式裡面也都帮大家做好了,你要<strong>把Training的资料分成两半,一部分叫作Training
Set,一部分是Validation Set</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313215102410.png" alt="image-20210313215102410" style="zoom:50%;"></p>
<p>​ 刚才助教程式裡面已经看到说,有90%的资料放在Training
Set裡面,有10%的资料,会被拿来做Validation Set,你在Training
Set上训练出来的模型,<strong>你在Validation
Set上面,去衡量它们的分数,你根据Validation
Set上面的分数,去挑选结果</strong>,再把这个结果上传到Kaggle上面,去看看你得到的public的分数,那因為你在挑分数的时候,是用Validation
Set来挑你的model,所以你的public的Testing
Set的分数,就可以反应你的,private Testing
Set的分数,就比较不会得到说,在public上面结果很好,但是在private上面结果很差,这样子的状况</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313215406332.png" alt="image-20210313215406332" style="zoom:50%;"></p>
<p>​
当我知道说,其实你看到public的结果以后,你就会去想要调它,你看到你现在弄了一堆模型,然后用Validation
Set检查一下,找了一个模型放到public
set上以后,发现结果不好,你其实不太可能不根据这一个结果,去调整你的模型,但是假设这一个route做太多次,你根据你的,public
Testing Set上的结果,去调整你的model太多次,你就又有可能fit在,你的public
Testing Set上面,然后在private Testing
Set上面,得到差的结果,不过还好反正我们有限制上传的次数,所以这个route,你也没有办法走太多次,可以避免你太过fit在,public的Testing
Set上面的结果。</p>
<p>​ 那我知道说今天因為,public的Testing
Set上面的结果,是大家都可以看到的,然后很多人都会,然后名字你又可以随便乱取,所以假设有一个人洗到第一名的话,他就会非常的得意,他就把自己的名字改成一些什麼,我第一次试就第一名了,或是
我其实只是个旁听,那其实他不是旁听的,那他改成说,我其实只是个旁听的,随便做就第一名了,那这个时候你就会觉得很紧张,尤其他如果是你认识的,隔壁小毛得到第一名,到处耀武扬威的时候,你就会开始有点紧张,你就会说,等一下
你不要得意,我等一下就去把你刷下来这样,那这个时候你要不要理他呢,你不要理他,根据过去的经验,就在public
leaderboard上排前几名的,往往private很容易就惨掉这样子,所以在public的Testing上面,得到太好的结果,也不用高兴得太早,其实
最好的做法,就是用Validation
loss,最小的直接挑就好了,就是你不要去管,你的public Testing Set的结果
这样,那我知道说在实作上,你不太可能这麼做,因為public
set的结果你有看到,所以它对你的模型的选择,可能还是会有些影响的,但是你要越少去看那个,public
Testing Set的结果越好</p>
<p>​
线上直播的同学,我复述一下刚才那个同学的问题,他的问题是说,所以我们不能去看,public
Testing Set的结果吗,理想上是,理想上你就用Validation
Set挑就好,然后上传以后 怎样就是怎样,有过那个strong
basseline以后,就不要再去动它了,那这样子就可以避免,你overfit在Testing
Set上面,好 那但是这边会有一个问题,就是怎麼分Training Set,跟Validation
Set呢,那如果在助教程式裡面,就是随机分的,但是你可能会说,搞不好我这个分
分得不好啊,搞不好我分到很奇怪的Validation Set,会导致我的结果很差,</p>
<h3><span id="n-fold-cross-validation">N-fold Cross Validation</span></h3>
<p>​ 如果你有这个担心的话,那你可以用N-fold Cross Validation,</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313215636180.png" alt="image-20210313215636180" style="zoom:50%;"></p>
<p>​ N-fold Cross
Validation就是你先<strong>把你的训练集切成N等份</strong>,在这个例子裡面我们切成三等份,切完以后,你拿其中<strong>一份当作Validation
Set</strong>,<strong>另外两份当Training
Set</strong>,然后这件事情你要<strong>重复三次</strong></p>
<p>​
也就是说,你先第一份第二份当Train,第三份当Validation,然后第一份第三份当Train,第二份当Validation,第一份当Validation,第二份第三份当Train</p>
<p>​ 然后接下来
你有三个模型,你不知道哪一个是好的,你就把这三个模型,在这三个setting下,在这三个Training跟Validation的,data
set上面,通通跑过一次,<strong>然后把这三个模型,在这三种状况的结果都平均起来</strong>,把每一个模型在这三种状况的结果,都平均起来,再看看谁的结果最好</p>
<p>​ 那假设现在model 1的结果最好,你用这三个fold得出来的结果是,这个model
1最好,然后你再把model 1,用在全部的Training
Set上,然后训练出来的模型,再用在Testing Set上面,好 那这个是N-fold Cross
Validation,好
那这个就是这门课前期的攻略,它可以带你打赢前期所有的副本</p>
<p>那接下来也许你要问的一个问题是,上週结束的时候,不是讲到预测2/26,也就是上週五的观看人数吗,到底结果做得怎麼样</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313220115627.png" alt="image-20210313220115627" style="zoom:50%;"></p>
<p>那这个就是我们要做的结果,上週比较多人选了三层的network,所以我们就把三层的network,拿来测试一下,以下是测试的结果,我们就没有再调参数了,大家决定用三层的就是下好离手了,就直接用上去了</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313220138585.png" alt="image-20210313220138585" style="zoom:50%;"></p>
<p>​ 得到的结果是这个样子了,这个图上 这个横轴就是从,2021年的1月1号开始
一直往下,然后红色的线是真实的数字,蓝色的线是预测的结果,2/26在这边
这个是今年2021年,观看人数最高的一天了,那机器的预测怎样呢,哇 非常的惨
差距非常的大,差距有2.58k这麼多,感谢大家
為了让这个模型不準,上週五花了很多力气,去点了这个video,所以这一天是,今年观看人数最多的一天,那你可能开始想说,那别的模型怎麼样呢,其实我也跑了一层二层跟四层的看看,所有的模型
都会惨掉,两层跟三层的错误率都是2点多k,其实四层跟一层比较好,都是1.8k左右,但是这四个模型不约而同的,觉得2/26应该是个低点,但实际上2/26是一个公值,那模型其实会觉得它是一个低点,也不能怪它,因為根据过去的资料,礼拜五就是没有人要学机器学习,礼拜五晚上大家都出去玩了对不对,礼拜五的观看人数是最少了,但是2/26出现了反常的状况,好
那这个就不能怪模型了,那我觉得出现这种状况,应该算是另外一种错误的形式,这种错误的形式,我们这边叫作mismatch。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313220235078.png" alt="image-20210313220235078" style="zoom:50%;"></p>
<p>​
那也有人会说,mismatch也算是一种Overfitting,这样也可以,这都只是名词定义的问题,那我这边想要表达的事情是,<strong>mismatch它的原因跟overfitting,其实不一样</strong>,一般的overfitting,你可以用搜集更多的资料来克服,但是<strong>mismatch意思是说,你今天的训练集跟测试集,它们的分佈是不一样的</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313220434830.png" alt="image-20210313220434830" style="zoom:67%;"></p>
<p>​
在训练集跟测试集,分佈是不一样的时候,你训练集再增加,其实也没有帮助了,那其实在多数的作业裡面,我们不会遇到这种mismatch的问题,我们都有把题目设计好了,所以资料跟测试集它的分佈差不多</p>
<p>​ 举例来说
以刚才作业一的,Covid19為例的话,假设我们今天资料在,分训练集跟测试集的时候,我们说<strong>2020年的资料是训练集,2021年的资料是测试集</strong>,那mismatch的问题可能就很严重了,这个我们其实有试过了
试了一下,如果今天用2020年当训练集,2021年当测试集,你就怎麼做都是惨了
就做不起来,训练什麼模型都会惨掉</p>
<p>​
因為2020年的资料跟2021年的资料,它们的背后的分佈其实都是不一样,所以你拿2020年的资料来训练,在2021年的作业一的资料上,你根本就预测不準,所以后来助教是用了别的方式,来分割训练集跟测试集,好
所以我们多数的作业,都不会有这种mismatch的问题,那除了作业十一。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210313220457880.png" alt="image-20210313220457880" style="zoom:67%;"></p>
<p>​
因為作业十一就是,针对mismatch的问题来设计的,作业十一也是一个影像分类的问题,这是它的训练集,看起来蛮正常的,但它测试集就是长这样子了,所以你知道这个时候,这个时候增加资料哪有什麼用呢,增加资料,你也没有办法让你的模型做得更好,所以这种问题要怎麼解决,那犹待作业十一的时候再讲,好
那你可能会问说
我怎麼知道,现在到底是不是mismatch呢,那我觉得知不知道是mismatch,那就要看你对这个资料本身的理解了,你可能要对你的训练集跟测试集,的產生方式有一些理解,你才能判断说,它是不是遇到了mismatch的状况,好
那这个就是我们作业的攻略,</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1HFEDWZ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1HFEDWZ/" class="post-title-link" itemprop="url">模型训练（5）Batch Normalization</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-09 11:34:59" itemprop="dateCreated datePublished" datetime="2022-06-09T11:34:59+08:00">2022-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 23:19:15" itemprop="dateModified" datetime="2022-07-13T23:19:15+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>22 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="quick-introduction-ofbatch-normalization">Quick Introduction of
Batch Normalization</span></h1>
<p>本篇是一个很快地介绍,Batch Normalization 这个技术</p>
<h3><span id="一-changinglandscape不变化的景观">一、Changing
Landscape（不变化的景观）</span></h3>
<p>之前才讲过说,我们能不能够直接改error surface 的 landscape,我们觉得说
error surface 如果很崎嶇的时候,它比较难
train,那我们能不能够直接把山剷平,让它变得比较好 train 呢？</p>
<p>==<strong>Batch Normalization</strong>==
就是其中一个,<strong>把山剷平的想法</strong>。我们一开始就跟大家讲说,不要小看
optimization 这个问题,有时候就算你的 error surface 是
convex的,它就是一个碗的形状,都不见得很好
train。假设你的两个参数啊,它们对 <strong>Loss
的斜率差别非常大</strong>,在 <span class="math inline">\(w_1\)</span>
这个方向上面,你的斜率变化很小,在 <span class="math inline">\(w_2\)</span> 这个方向上面斜率变化很大。</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616212321383.png" alt="image-20220616212321383">
<figcaption aria-hidden="true">image-20220616212321383</figcaption>
</figure>
<p>如果是<strong>固定的 learning
rate</strong>,你可能很难得到好的结果,所以我们才说你需要adaptive 的
learning rate、 Adam 等等比较进阶的 optimization
的方法,才能够得到好的结果</p>
<p>现在我们要从另外一个方向想,<strong>直接把难做的 error surface
把它改掉</strong>,看能不能够改得好做一点。在做这件事之前,也许我们第一个要问的问题就是,有这一种状况,$w_1
$ 跟 <span class="math inline">\(w_2\)</span>
它们的<strong>斜率差很多</strong>的这种状况,到底是从什麼地方来的</p>
<p>假设我现在有一个非常非常非常简单的 model,它的输入是 <span class="math inline">\(x_1\)</span> 跟 <span class="math inline">\(x_2\)</span>,它对应的参数就是 $ w_1 $ 跟 <span class="math inline">\(w_2\)</span>,它是一个 linear 的 model,没有
activation function</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616212341359.png" alt="image-20220616212341359"></p>
<p>$ w_1 $ 乘 <span class="math inline">\(x_1\)</span>,<span class="math inline">\(w_2\)</span> 乘 <span class="math inline">\(x_2\)</span> 加上 b 以后就得到 y,然后会计算 y 跟
<span class="math inline">\(\hat{y}\)</span> 之间的差距当做 e,把所有
training data e 加起来就是你的 Loss，然后去 minimize 你的
Loss，那什麼样的状况我们会產生像上面这样子,<strong>比较不好 train 的
error surface</strong> 呢？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616212421658.png" alt="image-20220616212421658" style="zoom:67%;"></p>
<p>当我们对 <strong>$ w_1 $ 有一个小小的改变</strong>,比如说加上 delta $
w_1 $ 的时候,那这个 L 也会有一个改变,那这个 $ w_1 $ 呢,是透过 $ w_1 $
改变的时候,你就改变了 y,y 改变的时候你就改变了
e,然后接下来就<strong>改变了 L</strong>。</p>
<p>那什麼时候 $ w_1 $ 的改变会对 L 的影响很小呢,也就是它在 error surface
上的斜率会很小呢？一个可能性是当你的 <strong>input
很小的时候</strong>,假设 <span class="math inline">\(x_1\)</span>
的值在不同的 training example 裡面,它的值都很小,那因為 <span class="math inline">\(x_1\)</span> 是直接乘上 $ w_1 $，如果 <span class="math inline">\(x_1\)</span> 的值都很小,$ w_1 $
有一个变化的时候,它得到的,它<strong>对 y 的影响也是小的</strong>,对 e
的影响也是小的,它对 L 的影响就会是小的。反之呢,如果今天是 <span class="math inline">\(x_2\)</span> 的话。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616212503953.png" alt="image-20220616212503953" style="zoom:67%;"></p>
<p>那假设 <strong><span class="math inline">\(x_2\)</span>
的值都很大</strong>,当你的 <span class="math inline">\(w_2\)</span>
有一个小小的变化的时候,虽然 <span class="math inline">\(w_2\)</span>
这个变化可能很小,但是因為它乘上了 <span class="math inline">\(x_2\)</span>,<span class="math inline">\(x_2\)</span> 的值很大,那 y 的变化就很大,那 e
的变化就很大,那 L 的变化就会很大,就会导致我们在 w
这个方向上,做变化的时候,我们把 w 改变一点点,那我们的 error surface
就会有很大的变化。</p>
<p>所以你发现说,既然在这个 linear 的 model 裡面,当我们 input 的
feature,<strong>每一个 dimension 的值,它的 scale
差距很大</strong>的时候,我们就可能產生像这样子的 error
surface,就可能產生<strong>不同方向,斜率非常不同,坡度非常不同的 error
surface</strong>。</p>
<p>所以怎麼办呢,我们有没有可能给feature 裡面<strong>不同的
dimension,让它有同样的数值的范围</strong>。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616212559236.png" alt="image-20220616212559236" style="zoom:67%;"></p>
<p>如果我们可以给不同的
dimension,同样的数值范围的话,那我们可能就可以製造比较好的 error
surface,让 training 变得比较容易一点</p>
<p>其实有很多不同的方法,这些不同的方法,往往就合起来统称為==Feature
Normalization==</p>
<h3><span id="feature-normalization">Feature Normalization</span></h3>
<p>以下所讲的方法只是Feature Normalization 的一种可能性,它<strong>并不是
Feature Normalization 的全部</strong>,假设 <span class="math inline">\(x^1\)</span> 到 <span class="math inline">\(x^R\)</span>,是我们所有的训练资料的 feature
vector</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221138101.png" alt="image-20220616221138101" style="zoom:67%;"></p>
<p>我们把所有训练资料的 feature vector ,统统都集合起来,那每一个 vector
,<span class="math inline">\(x_1\)</span> 裡面就 $x^1_1 $代表 <span class="math inline">\(x_1\)</span> 的第一个 element,$x^2_1 $,就代表
<span class="math inline">\(x_2\)</span> 的第一个 element,以此类推</p>
<p>那我们把<strong>不同笔资料即不同 feature vector,同一个
dimension</strong> 裡面的数值,把它取出来,然后去计算某一个 dimension 的
mean，它的 mean 呢 就是<span class="math inline">\(m_i\)</span>，我们计算第 i 个 dimension
的,standard deviation,我们用<span class="math inline">\(\sigma_i\)</span>来表示它</p>
<p>那接下来我们就可以做一种 normalization,那这种 normalization
其实叫做==<strong>标準化</strong>==,其实叫
==standardization==,不过我们这边呢,就等一下都统称 normalization 就好了
<span class="math display">\[
\tilde{x}^r_i ← \frac{x^r_i-m_i}{\sigma_i}
\]</span> 我们就是把这边的某一个数值x,减掉这一个 dimension 算出来的
mean,再除掉这个 dimension,算出来的 standard deviation,得到新的数值叫做
<span class="math inline">\(\tilde{x}\)</span></p>
<p>然后得到新的数值以后,<strong>再把新的数值把它塞回去</strong>,以下都用这个
tilde来代表有被 normalize 后的数值</p>
<p>那做完 normalize 以后有什麼好处呢？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221152221.png" alt="image-20220616221152221" style="zoom:67%;"></p>
<ul>
<li><p>做完 normalize 以后啊,这个 dimension 上面的数值就会平均是
0,然后它的 variance就会是 1,所以<strong>这一排数值的分布就都会在 0
上下</strong></p></li>
<li><p>对每一个 dimension都做一样的 normalization,就会发现所有 feature
不同 dimension 的数值都在 0 上下,那你可能就可以<strong>製造一个,比较好的
error surface</strong></p></li>
</ul>
<p>所以像这样子 Feature Normalization 的方式,往往对你的 training
有帮助,它可以让你在做 gradient descent 的时候,这个 gradient
descent,<strong>它的 Loss 收敛更快一点,可以让你的 gradient
descent,它的训练更顺利一点</strong>,这个是 Feature Normalization</p>
<h3><span id="considering-deep-learning">Considering Deep Learning</span></h3>
<p><span class="math inline">\(\tilde{x}\)</span> 代表 normalize 的
feature,把它丢到 deep network 裡面,去做接下来的计算和训练,所以把 <span class="math inline">\(x_1\)</span> tilde 通过第一个 layer 得到 <span class="math inline">\(z^1\)</span>,那你有可能通过 activation
function,不管是选 Sigmoid 或者 ReLU 都可以,然后再得到 <span class="math inline">\(a^1\)</span>,然后再通过下一层等等,那就看你有几层
network 你就做多少的运算</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221202386.png" alt="image-20220616221202386" style="zoom:67%;"></p>
<p>所以每一个 x 都做类似的事情,但是如果我们进一步来想的话,对 <span class="math inline">\(w_2\)</span> 来说</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221211793.png" alt="image-20220616221211793" style="zoom:67%;"></p>
<p>这边的 <span class="math inline">\(a^1\)</span> <span class="math inline">\(a^3\)</span> 这边的 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^3\)</span>,其实也是另外一种 input,如果这边 <span class="math inline">\(\tilde{x}\)</span>,虽然它已经做 normalize
了,但是通过 $ w_1 $ 以后它就<strong>没有做 normalize</strong>,如果 <span class="math inline">\(\tilde{x}\)</span> 通过 $ w_1 $ 得到是 <span class="math inline">\(z^1\)</span>,而 <span class="math inline">\(z^1\)</span> 不同的 dimension
间,它的数值的分布仍然有很大的差异的话,那我们要 train <span class="math inline">\(w_2\)</span> 第二层的参数,会不会也有困难呢</p>
<p>对 <span class="math inline">\(w_2\)</span> 来说,这边的 a 或这边的 z
其实也是一种 feature,我们应该要对这些 feature 也做 normalization</p>
<p>那如果你选择的是 Sigmoid,那可能比较推荐对 z 做 Feature
Normalization,因為Sigmoid 是一个 s 的形状,那它在 0
附近斜率比较大,所以如果你对 z 做 Feature Normalization,把所有的值都挪到
0 附近,那你到时候算 gradient 的时候,算出来的值会比较大</p>
<p>那不过因為你不见得是用 sigmoid ,所以你也不一定要把 Feature
Normalization放在 z
这个地方,如果是选别的,也许你选a也会有好的结果,也说不定，<strong>Ingeneral
而言,这个 normalization,要放在 activation function
之前,或之后都是可以的,在实作上,可能没有太大的差别</strong>,好
那我们这边呢,就是对 z 呢,做一下 Feature Normalization，</p>
<p>那怎麼对 z 做 Feature Normalization 呢</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221239128.png" alt="image-20220616221239128" style="zoom:67%;"></p>
<p>那你就把 z,想成是另外一种 feature ,我们这边有 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,我们就把 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span> 拿出来</p>
<ul>
<li><strong>算一下它的 mean</strong>，这边的 <span class="math inline">\(μ\)</span> 是一个 vector,我们就把 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,这三个 vector 呢,把它平均起来,得到
<span class="math inline">\(μ\)</span> 这个 vector</li>
<li><strong>算一个 standard deviation</strong>,这个 standard deviation
呢,这边这个成 <span class="math inline">\(\sigma\)</span>,它也代表了一个
vector,那这个 vector 怎麼算出来呢,你就把 <span class="math inline">\(z^i\)</span>减掉 <span class="math inline">\(μ\)</span>,然后取平方,这边的平方,这个 notation
有点 abuse 啊,这边的平方就是指,对每一个 element
都去做平方,然后再开根号,这边开根号指的是对每一个
element,向量裡面的每一个 element,都去做开根号,得到 <span class="math inline">\(\sigma\)</span>,反正你知道我的意思就好</li>
</ul>
<p>把这三个 vector,裡面的每一个 dimension,都去把它的 <span class="math inline">\(μ\)</span> 算出来,把它的 <span class="math inline">\(\sigma\)</span> 算出来,好
我这边呢,就不把那些箭头呢 画出来了,从 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,算出 <span class="math inline">\(μ\)</span>,算出 <span class="math inline">\(\sigma\)</span>。</p>
<p>接下来就把这边的每一个 z ,都去减掉 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,你把 <span class="math inline">\(z^i\)</span>减掉 <span class="math inline">\(μ\)</span>,除以 <span class="math inline">\(\sigma\)</span>,就得到 <span class="math inline">\(z^i\)</span>的 tilde。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221403703.png" alt="image-20220616221403703" style="zoom:50%;"></p>
<p>那这边的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,它都是<strong>向量</strong>,所以这边这个除的意思是<strong>element
wise 的相除</strong>,就是 <span class="math inline">\(z^i\)</span>减
<span class="math inline">\(μ\)</span>,它是一个向量,所以分子的地方是一个向量,分母的地方也是一个向量,把这个两个向量,它们对应的
element 的值相除,是我这边这个除号的意思,这边得到 Z 的 tilde。</p>
<p>所以我们就是把 <span class="math inline">\(z^1\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^1\)</span> tilde,同理 <span class="math inline">\(z^2\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^2\)</span> tilde,<span class="math inline">\(z^3\)</span> 减 <span class="math inline">\(μ\)</span> 除以 <span class="math inline">\(\sigma\)</span>,得到 <span class="math inline">\(z^3\)</span> tilde,那就把这个 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,做 Feature Normalization,变成 <span class="math inline">\(z^1\)</span> tilde,<span class="math inline">\(z^2\)</span> tilde 跟 <span class="math inline">\(z^3\)</span> 的 tilde。</p>
<p>接下来就看你爱做什麼 就做什麼啦,通过 activation function,得到其他
vector,然后再通过,再去通过其他 layer 等等,这样就可以了,这样你就等於对
<span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span>,做了 Feature Normalization,变成 <span class="math inline">\(\tilde{z}^1\)</span> <span class="math inline">\(\tilde{z}^2\)</span> <span class="math inline">\(\tilde{z}^3\)</span> 。</p>
<p>在这边有一件有趣的事情,这边的 <span class="math inline">\(μ\)</span>
跟 <span class="math inline">\(\sigma\)</span>,它们其实都是根据 <span class="math inline">\(z^1\)</span> <span class="math inline">\(z^2\)</span> <span class="math inline">\(z^3\)</span> 算出来的。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221515889.png" alt="image-20220616221515889" style="zoom: 67%;"></p>
<p>所以这边 <span class="math inline">\(z^1\)</span>
啊,它本来,如果我们没有做 Feature Normalization 的时候,你改变了 <span class="math inline">\(z^1\)</span> 的值,你会改变这边 a
的值,但是现在啊,当你改变 <span class="math inline">\(z^1\)</span>
的值的时候,<span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 也会跟著改变,<span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 改变以后,<span class="math inline">\(z^2\)</span> 的值 <span class="math inline">\(a^2\)</span> 的值,<span class="math inline">\(z^3\)</span> 的值 <span class="math inline">\(a^3\)</span> 的值,也会跟著改变。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221555203.png" alt="image-20220616221555203" style="zoom: 67%;"></p>
<p>所以<strong>之前</strong>,我们每一个 <span class="math inline">\(\tilde{x}_1\)</span> <span class="math inline">\(\tilde{x}_2\)</span> <span class="math inline">\(\tilde{x}_3\)</span>,它是<strong>独立分开处理的</strong>,但是我们在做
<strong>Feature Normalization 以后</strong>,这三个
example,它们变得<strong>彼此关联</strong>了。</p>
<p>我们这边 <span class="math inline">\(z^1\)</span> 只要有改变,接下来
<span class="math inline">\(z^2\)</span> <span class="math inline">\(a^2\)</span> <span class="math inline">\(z^3\)</span> <span class="math inline">\(a^3\)</span>,也都会跟著改变,所以这边啊,其实你要把,当你有做
Feature Normalization 的时候,你要把这一整个 process,就是有收集一堆
feature,把这堆 feature 算出 <span class="math inline">\(μ\)</span> 跟
<span class="math inline">\(\sigma\)</span> 这件事情,当做是 network
的一部分。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616221637917.png" alt="image-20220616221637917" style="zoom:67%;"></p>
<p>也就是说,你现在有一个比较大的 network</p>
<ul>
<li>你之前的 network,都只吃一个 input,得到一个 output</li>
<li>现在你有一个比较大的 network,这个大的 network,它是吃一堆
input,用这堆 input 在这个 network 裡面,要算出 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,然后接下来產生一堆 output</li>
</ul>
<p>那这个地方比较抽象,只可会意 不可言传这样子</p>
<p>那这边就会有一个问题了,因為你的训练资料裡面的 data 非常多,现在一个
data set,benchmark corpus 都上百万笔资料， GPU 的
memory,根本没有办法,把它整个 data set 的 data 都 load 进去。</p>
<p><strong><font color="red"> 在实作的时候,你不会让这一个 network
考虑整个 training data 裡面的所有 example,你只会考虑一个 batch 裡面的
example</font></strong>,举例来说,你 batch 设 64,那你这个巨大的
network,就是把 64 笔 data 读进去,算这 64 笔 data 的 <span class="math inline">\(μ\)</span>,算这 64 笔 data 的 <span class="math inline">\(\sigma\)</span>,对这 64 笔 data 都去做
normalization</p>
<p>因為我们在实作的时候,我们只对一个 batch 裡面的 data,做
normalization,所以这招叫做 ==<strong>Batch Normalization</strong>==</p>
<p>那这个 Batch Normalization,显然有一个问题
就是,<strong>你一定要有一个够大的 batch,你才算得出 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span></strong>,假设你今天,你 batch size
设 1,那你就没有什麼 <span class="math inline">\(μ\)</span> 或 <span class="math inline">\(\sigma\)</span> 可以算</p>
<p>所以这个 Batch Normalization,是适用於 batch size 比较大的时候,因為
batch size 如果比较大,<strong>也许这个 batch size 裡面的
data,就足以表示,整个 corpus
的分布</strong>,那这个时候你就可以,把这个本来要对整个 corpus,做 Feature
Normalization 这件事情,改成只在一个 batch,做 Feature Normalization,作為
approximation。</p>
<p><strong>在做 Batch Normalization
的时候,往往还会有这样的设计你算出这个 <span class="math inline">\(\tilde{z}\)</span> 以后</strong></p>
<ul>
<li><strong><font color="red"> 接下来你会把这个 <span class="math inline">\(\tilde{z}\)</span>,再乘上另外一个向量叫做 <span class="math inline">\(γ\)</span>,这个 <span class="math inline">\(γ\)</span> 也是一个向量,所以你就是把 <span class="math inline">\(\tilde{z}\)</span> 跟 <span class="math inline">\(γ\)</span> 做 element wise 的相乘,把 z
这个向量裡面的 element,跟 <span class="math inline">\(γ\)</span>
这个向量裡面的 element,两两做相乘。</font></strong></li>
<li><strong><font color="red"> 再加上 <span class="math inline">\(β\)</span> 这个向量,得到 <span class="math inline">\(\hat{z}\)</span>。</font></strong></li>
</ul>
<p><strong>而 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span>,你要把它想成是 network
的参数,它是另外再被learn出来的,</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616222053826.png" alt="image-20220616222053826" style="zoom:50%;"></p>
<h4><span id="那為什麼要加上-β-跟-γ-呢">那為什麼要加上 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span> 呢？</span></h4>
<p>有人可能会觉得说,如果我们做 normalization 以后,那这边的 <span class="math inline">\(\tilde{z}\)</span>,它的平均就一定是
0,那也许,<strong>今天如果平均是 0 的话,就是给那 network
一些限制</strong>,那<strong>也许这个限制会带来什麼负面的影响</strong>,所以我们把
<span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span> 加回去。</p>
<p>然后让 network 呢,现在它的 hidden layer 的 output平均不是 0
的话,他就自己去learn这个 <span class="math inline">\(β\)</span> 跟 <span class="math inline">\(γ\)</span>,来调整一下输出的分布,来调整这个 <span class="math inline">\(\hat{z}\)</span> 的分布</p>
<p>但讲到这边又会有人问说,刚才不是说做 Batch Normalization
就是,為了要让每一个不同的 dimension,它的 range
都是一样吗,现在如果加去乘上 <span class="math inline">\(γ\)</span>,再加上 <span class="math inline">\(β\)</span>,把 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 加进去,</p>
<h4><span id="这样不会不同dimension-的分布它的-range-又都不一样了吗">这样不会不同
dimension 的分布,它的 range 又都不一样了吗？</span></h4>
<p>有可能,但是你实际上在训练的时候,这个 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 的初始值啊</p>
<ul>
<li><strong>你会把这个 <span class="math inline">\(γ\)</span> 的初始值
就都设為 1,所以 <span class="math inline">\(γ\)</span>
是一个裡面的值,一开始其实是一个裡面的值,全部都是 1 的向量</strong></li>
<li><strong>那 <span class="math inline">\(β\)</span>
是一个裡面的值,全部都是 0 的向量,所以 <span class="math inline">\(γ\)</span> 是一个 one vector,都是 1 的向量,<span class="math inline">\(β\)</span> 是一个 zero vector,裡面的值都是 0
的向量</strong></li>
</ul>
<p>所以让你的 network 在一开始训练的时候,每一个 dimension
的分布,是比较接近的,也许训练到后来,你已经训练够长的一段时间,已经找到一个比较好的
error surface,走到一个比较好的地方以后,那再把 <span class="math inline">\(γ\)</span> 跟 <span class="math inline">\(β\)</span> 慢慢地加进去,好所以加 Batch
Normalization,往往对你的训练是有帮助的。</p>
<h3><span id="testing">Testing</span></h3>
<h4><span id="这个batch-normalization-在-inference或是-testing的时候会有什麼样的问题呢">这个
Batch Normalization 在 inference,或是 testing
的时候,会有什麼样的问题呢？</span></h4>
<p>在 testing 的时候,如果 当然如果今天你是在做作业,我们一次会把所有的
testing 的资料给你,所以你确实也可以在 testing 的资料上面,製造一个一个
batch。</p>
<p><strong>但是假设你真的有系统上线,你是一个真正的线上的
application,你可以说,我今天一定要等 30,比如说你的 batch size 设
64,我一定要等 64
笔资料都进来,我才一次做运算吗,这显然是不行的。</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616222338443.png" alt="image-20220616222338443" style="zoom: 67%;"></p>
<p>但是在做 Batch Normalization 的时候,一个 <span class="math inline">\(\tilde{x}\)</span>,一个 normalization 过的 feature
进来,然后你有一个 z,你的 z 呢,要减掉 <span class="math inline">\(μ\)</span> 跟除 <span class="math inline">\(\sigma\)</span>,那这个 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,是<strong>用一个 batch
的资料算出来的</strong></p>
<h4><span id="但如果今天在testing-的时候根本就没有-batch那我们要怎麼算这个-μ跟怎麼算这个-sigma-呢">但如果今天在
testing 的时候,根本就没有 batch,那我们要怎麼算这个 <span class="math inline">\(μ\)</span>,跟怎麼算这个 <span class="math inline">\(\sigma\)</span> 呢？</span></h4>
<p>所以真正的,这个实作上的解法是这个样子的,如果你看那个 PyTorch
的话呢,Batch Normalization 在 testing
的时候,你并不需要做什麼特别的处理,PyTorch 帮你处理好了</p>
<p><strong><font color="red"> 在 training 的时候,如果你有在做 Batch
Normalization 的话,在 training 的时候,你每一个 batch 计算出来的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,他都会拿出来算 ==moving
average==</font></strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616222438574.png" alt="image-20220616222438574" style="zoom:67%;"></p>
<p>你每一次取一个 batch 出来的时候,你就会算一个 <span class="math inline">\(μ^1\)</span>,取第二个 batch 出来的时候,你就算个
<span class="math inline">\(μ^2\)</span>,一直到取第 t 个 batch
出来的时候,你就算一个 <span class="math inline">\(μ^t\)</span>
。接下来你会算一个 moving average,你会把你现在算出来的 <span class="math inline">\(μ\)</span> 的一个平均值,叫做 <span class="math inline">\(μ\)</span> bar,乘上某一个
factor,那这也是一个常数,这个这也是一个 constant,这也是一个那个 hyper
parameter,也是需要调的。</p>
<p>在 PyTorch 裡面,我没记错 他就设 0.1,我记得他 P 就设 0.1,好,然后加上 1
减 P,乘上 <span class="math inline">\(μ^t\)</span> ,然后来更新你的 <span class="math inline">\(μ\)</span> 的平均值,然后最后在 testing
的时候,你就不用算 batch 裡面的 <span class="math inline">\(μ\)</span> 跟
<span class="math inline">\(\sigma\)</span> 了。</p>
<p>因為 testing 的时候,在真正 application 上,也没有 batch
这个东西,你就直接拿 <span class="math inline">\(\barμ\)</span> 跟 <span class="math inline">\(\bar\sigma\)</span> ,也就是 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span> 在训练的时候,得到的 moving
average,<span class="math inline">\(\barμ\)</span> 跟 <span class="math inline">\(\bar\sigma\)</span> ,来取代这边的 <span class="math inline">\(μ\)</span> 跟 <span class="math inline">\(\sigma\)</span>,这个就是 Batch Normalization,在
testing 的时候的运作方式。</p>
<h3><span id="comparison">Comparison</span></h3>
<p>好 那这个是从 Batch
Normalization,原始的文件上面截出来的一个实验结果,那在原始的文件上还讲了很多其他的东西,举例来说,我们今天还没有讲的是<strong>==,Batch
Normalization 用在 CNN
上,要怎麼用呢==</strong>,那你自己去读一下原始的文献,裡面会告诉你说,Batch
Normalization 如果用在 CNN 上,应该要长什麼样子。</p>
<blockquote>
<p><strong>卷积层上的BN使用</strong>，其实也是使用了<strong>类似权值共享的策略</strong>，<strong>把一整张特征图当做一个神经元进行处理</strong>。</p>
<p>卷积神经网络经过卷积后得到的是一系列的特征图，如果min-batch
sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch
sizes，f为特征图个数，p、q分别为特征图的宽高。</p>
<p>在cnn中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch
Normalization，mini-batch size 的大小就是：m * p *
q，于是对于每个特征图都只有一对可学习参数：γ、β。</p>
<p>相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。</p>
<ul>
<li><strong>nb在每一个特征图上的所有点沿着一个batch的样本数据的方向对数据进行求和，求平均等处理，不考虑不同特征图的数据间的运算。</strong></li>
<li><strong>lrb在每一个特征图上沿着不同特征图的方向对数据进行求和，求平均等处理，不考虑不同输入样本数据间的运算。</strong></li>
</ul>
</blockquote>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220616222534083.png" alt="image-20220616222534083">
<figcaption aria-hidden="true">image-20220616222534083</figcaption>
</figure>
<p>这个是原始文献上面截出来的一个数据</p>
<ul>
<li>横轴呢,代表的是训练的过程，纵轴代表的是 validation set 上面的
accuracy</li>
<li>那这个<strong>黑色</strong>的虚线是<strong>没有做 Batch
Normalization</strong> 的结果,它用的是 inception 的 network,就是某一种
network 架构啦,也是以 CNN 為基础的 network 架构</li>
<li>然后如果有做 Batch
Normalization,你会得到<strong>红色</strong>的这一条虚线,那你会发现说,红色这一条虚线,它<strong>训练的速度,显然比黑色的虚线还要快很多</strong>,虽然最后收敛的结果啊,就你只要给它足够的训练的时间,可能都跑到差不多的
accuracy,但是<strong>红色这一条虚线,可以在比较短的时间内,就跑到一样的
accuracy</strong>,那这边这个蓝色的菱形,代表说这几个点的那个 accuracy
是一样的</li>
<li><strong>粉红色</strong>的线是 sigmoid function,就 sigmoid function
一般的认知,我们虽然还没有讨论这件事啦,但一般都会选择 ReLu,而不是用
sigmoid function,因為 sigmoid function,它的 training
是比较困难的,但是这边想要强调的点是说,<strong>就算是 sigmoid
比较难搞的,加 Batch Normalization,还是 train 的起来</strong>,那这边没有
sigmoid,没有做 Batch Normalization
的结果,因為在这个实验上,作者有说,sigmoid 不加 Batch Normalization,根本连
train 都 train 不起来</li>
<li>蓝色的实线跟这个蓝色的虚线呢,是把 learning rate 设比较大一点,乘
5,就是 learning rate 变原来的 5 倍,然后乘 30,就是 learning rate 变原来的
30 倍,那因為<strong>如果你做 Batch Normalization 的话,那你的 error
surface 呢,会比较平滑
比较容易训练,所以你可以把你的比较不崎嶇,所以你就可以把你的 learning rate
呢,设大一点</strong></li>
</ul>
<h3><span id="internal-covariate-shift">Internal Covariate Shift?</span></h3>
<p><strong>好接下来的问题就是,Batch
Normalization,它為什麼会有帮助呢</strong>,在原始的 Batch
Normalization,那篇 paper 裡面,他提出来一个概念,叫做 ==internal covariate
shift==,==covariate
shift==(训练集和预测集样本分布不一致的问题就叫做“<em>covariate
shift</em>”现象) 这个词汇是原来就有的,internal covariate
shift,我认為是,Batch Normalization 的作者自己发明的。他认為说今天在
train network 的时候,会有以下这个问题,这个问题是这样。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616225043073.png" alt="image-20220616225043073" style="zoom: 67%;"></p>
<p>network 有很多层</p>
<ul>
<li><p>x 通过第一层以后 得到 a</p></li>
<li><p>a 通过第二层以后 得到 b</p></li>
<li><p>计算出 gradient 以后,把 A update 成 A′,把 B 这一层的参数 update
成 B′</p></li>
</ul>
<p>但是作者认為说,我们在计算 B,update 到 B′ 的 gradient
的时候,这个时候前一层的参数是 A 啊,或者是前一层的 output 是小 a 啊</p>
<p>那当前一层从 A 变成 A′ 的时候,它的 output 就从小 a 变成小 a′ 啊</p>
<p>但是我们计算这个 gradient 的时候,我们是根据这个 a 算出来的啊,所以这个
update 的方向,也许它<strong>适合用在 a 上,但不适合用在 a′
上面</strong></p>
<p>那如果说 Batch Normalization 的话,我们会让,因為我们每次都有做
normalization,我们就会让 a 跟 a′
呢,它的分布比较接近,也许这样就会对训练呢,有帮助。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616225149673.png" alt="image-20220616225149673" style="zoom:67%;"></p>
<p>但是有一篇 paper 叫做,How Does Batch Normalization,Help
Optimization,然后他就<strong>打脸了internal covariate shift
的这一个观点</strong>。</p>
<p>在这篇 paper 裡面,他从各式各样的面向来告诉你说,i<strong>nternal
covariate shift,首先它不一定是 training network 的时候的一个问题,然后
Batch Normalization,它会比较好,可能不见得是因為,它解决了 internal
covariate shift。</strong></p>
<p>那在这篇 paper
裡面呢,他做了很多很多的实验,比如说他比较了训练的时候,这个 a 的分布的变化
发现<strong>,不管有没有做 Batch
Normalization,它的变化都不大</strong>。</p>
<p>然后他又说,就算是变化很大,对 training
也没有太大的伤害,然后他又说,不管你是根据 a 算出来的 gradient,还是根据 a′
算出来的 gradient,方向居然都差不多。</p>
<p>所以他告诉你说,internal covariate shift,可能不是 training network
的时候,最主要的问题,它可能也不是,Batch Normalization
会好的一个的关键,那有关更多的实验,你就自己参见这篇文章。</p>
<h4><span id="為什麼-batchnormalization-会比较好呢">為什麼 Batch
Normalization 会比较好呢？</span></h4>
<p>那在这篇 How Does Batch Normalization,Help Optimization
这篇论文裡面,他从实验上,也从理论上,至少<strong>支持了 Batch
Normalization,可以改变 error surface,让 error surface
比较不崎嶇这个观点</strong>。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616225308853.png" alt="image-20220616225308853" style="zoom:67%;"></p>
<p>所以这个观点是有理论的支持,也有实验的佐证的,那在这篇文章裡面呢,作者还讲了一个非常有趣的话,他说他觉得啊,这个
Batch Normalization 的 positive impact。</p>
<p>因為他说,如果我们要让 network,这个 error surface
变得比较不崎嶇,<strong>其实不见得要做 Batch
Normalization,感觉有很多其他的方法,都可以让 error surface
变得不崎嶇</strong>,那他就试了一些其他的方法,发现说,跟 Batch
Normalization performance
也差不多,甚至还稍微好一点,所以他就讲了下面这句感嘆。</p>
<p>他觉得说,这个,positive impact of batchnorm on training,可能是
somewhat,<strong>serendipitous</strong>,什麼是 serendipitous
呢,这个字眼可能可以翻译成偶然的,但偶然并没有完全表达这个词汇的意思,这个词汇的意思是说,你发现了一个什麼意料之外的东西。</p>
<p>那这篇文章的作者也觉得,Batch Normalization
也像是盘尼西林一样,是一种偶然的发现,但无论如何,它是一个有用的方法。</p>
<h2><span id="to-learn-more">To learn more ……</span></h2>
<p>那其实 Batch Normalization,不是唯一的 normalization,normalization
的方法有一把啦,那这边就是列了几个比较知名的,</p>
<p>Batch Renormalization https://arxiv.org/abs/1702.03275 Layer
Normalization https://arxiv.org/abs/1607.06450 Instance Normalization
https://arxiv.org/abs/1607.08022 Group Normalization
https://arxiv.org/abs/1803.08494 Weight Normalization
https://arxiv.org/abs/1602.07868 Spectrum Normalization
https://arxiv.org/abs/1705.10941</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1T7T14B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1T7T14B/" class="post-title-link" itemprop="url">模型训练（6）Local Minimum And Saddle Point</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-09 11:34:59" itemprop="dateCreated datePublished" datetime="2022-06-09T11:34:59+08:00">2022-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 23:19:27" itemprop="dateModified" datetime="2022-07-13T23:19:27+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/" itemprop="url" rel="index"><span itemprop="name">训练技巧</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>19 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1><span id="when-gradient-is-small">When gradient is small</span></h1>
<h3><span id="一-critical-point">一、Critical Point</span></h3>
<h4><span id="11-training-fails-because">1.1 Training Fails because</span></h4>
<p>现在我们要讲的是Optimization的部分,所以我们要讲的东西基本上跟Overfitting没有什么太大的关联,我们只讨论Optimization的时候,怎么把gradient
descent做得更好,那为什么Optimization会失败呢？</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616161429257.png" alt="image-20220616161429257" style="zoom:67%;"></p>
<p>你常常在做Optimization的时候,你会发现,<strong>随著你的参数不断的update,你的training的loss不会再下降</strong>,但是你对这个loss仍然不满意,就像我刚才说的,你可以把deep的network,跟linear的model,或比较shallow
network
比较,发现说它没有做得更好,所以你觉得deepnetwork,没有发挥它完整的力量,所以Optimization显然是有问题的。</p>
<p><strong>但有时候你会甚至发现,一开始你的model就train不起来,一开始你不管怎么update你的参数,你的loss通通都掉不下去,那这个时候到底发生了什么事情呢？</strong></p>
<p>过去常见的一个猜想,是因为我们现在走到了一个地方,<strong>这个地方参数对loss的微分为零</strong>,当你的参数对loss微分为零的时候,gradient
descent就没有办法再update参数了,这个时候training就停下来了,loss当然就不会再下降了。</p>
<p>讲到gradient为零的时候,大家通常脑海中最先浮现的,可能就是==<strong>local
minima</strong>==,所以常有人说做deep learning,用gradient
descent会卡在local minima,然后所以gradient descent不work,所以deep
learning不work。</p>
<p><strong>但是如果有一天你要写,跟deep
learning相关paper的时候,你千万不要讲卡在local
minima这种事情,别人会觉得你非常没有水准,为什么？</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626143535169.png" alt="image-20220626143535169"></p>
<p>因为<strong>不是只有local
minima的gradient是零</strong>,还有其他可能会让gradient是零,比如说
==<strong>saddle point</strong>==,所谓的saddle
point,其实就是gradient是零,但是不是local minima,也不是local
maxima的地方,像在右边这个例子里面
红色的这个点,它在左右这个方向是比较高的,前后这个方向是比较低的,它就像是一个马鞍的形状,所以叫做saddle
point,那中文就翻成<strong>鞍点</strong>。</p>
<p>像saddle point这种地方,它也是gradient为零,但它不是local
minima,那像这种gradient为零的点,统称为==critical
point==,所以<strong>你可以说你的loss,没有办法再下降,也许是因为卡在了critical
point,但你不能说是卡在local minima,因为saddle
point也是微分为零的点</strong></p>
<p>但是今天如果你发现你的gradient,真的很靠近零,卡在了某个critical
point,我们有没有办法知道,到底是local minima,还是saddle
point？其实是有办法的</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616161554342.png" alt="image-20220616161554342" style="zoom:50%;"></p>
<p><strong>为什么我们想要知道到底是卡在local minima,还是卡在saddle
point呢</strong></p>
<ul>
<li>因为如果是<strong>卡在local
minima,那可能就没有路可以走了</strong>,因为四周都比较高,你现在所在的位置已经是最低的点,loss最低的点了,往四周走
loss都会比较高,你会不知道怎么走到其他的地方去</li>
<li>但saddle point就比较没有这个问题,如果你今天是<strong>卡在saddle
point的话,saddle
point旁边还是有路可以走的,</strong>还是有路可以让你的loss更低的,你只要逃离saddle
point,你就有可能让你的loss更低</li>
</ul>
<p><strong>所以鉴别今天我们走到,critical point的时候,到底是local
minima,还是saddle
point,是一个值得去探讨的问题,那怎么知道今天一个critical
point,到底是属于local minima,还是saddle point呢？</strong></p>
<h4><span id="12-warning-of-math">1.2 Warning of Math</span></h4>
<p>这边需要用到一点数学,以下这段其实没有很难的数学,就只是微积分跟线性代数,但如果你没有听懂的话,以下这段skip掉是没有关系的，那怎么知道说一个点,到底是local
minima,还是saddle point呢？</p>
<p>你要知道我们loss function的形状,可是我们怎么知道,loss
function的形状呢,network本身很复杂,用复杂network算出来的loss
function,显然也很复杂,我们怎么知道loss
function,长什么样子,虽然我们没有办法完整知道,整个loss function的样子</p>
<h4><span id="taylerseries-approximation"><strong><font color="red"> Tayler
Series Approximation</font></strong></span></h4>
<p>但是如果给定某一组参数,比如说蓝色的这个<span class="math inline">\(θ&#39;\)</span>,在<span class="math inline">\(θ&#39;\)</span>附近的loss
function,是有办法被写出来的,它写出来就像是这个样子：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626143850955.png" alt="image-20220626143850955" style="zoom:67%;"></p>
<p>所以这个<span class="math inline">\(L(θ)\)</span>完整的样子写不出来,但是它在<span class="math inline">\(θ&#39;\)</span>附近,你可以用这个式子来表示它,这个式子是,Tayler
Series
Appoximation泰勒级数展开,这个假设你在微积分的时候,已经学过了,所以我就不会细讲这一串是怎么来的,但我们就只讲一下它的概念,这一串里面包含什么东西呢?</p>
<ul>
<li><p>第一项是<span class="math inline">\(L(θ&#39;)\)</span>,就告诉我们说,当<span class="math inline">\(θ\)</span>跟<span class="math inline">\(θ&#39;\)</span>很近的时候,<span class="math inline">\(L(θ)\)</span>应该跟<span class="math inline">\(L(θ&#39;)\)</span>还蛮靠近的</p></li>
<li><p>第二项是<span class="math inline">\((θ-θ&#39;)^Tg\)</span></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626143927098.png" alt="image-20220626143927098" style="zoom:67%;"></p>
<p><strong><span class="math inline">\(g\)</span>是一个向量,这个g就是我们的gradient</strong>,我们用绿色的这个g来代表gradient,这个<strong>gradient会来弥补,<span class="math inline">\(θ&#39;\)</span>跟<span class="math inline">\(θ\)</span>之间的差距</strong>,我们虽然刚才说<span class="math inline">\(θ&#39;\)</span>跟<span class="math inline">\(θ\)</span>,它们应该很接近,但是中间还是有一些差距的,那这个差距,第一项我们用这个gradient,来表示他们之间的差距,有时候gradient会写成<span class="math inline">\(∇L(θ&#39;)\)</span>,这个地方的<span class="math inline">\(g\)</span>是一个向量,<strong>它的第i个component,就是θ的第i个component对L的微分</strong>,光是看g还是没有办法,完整的描述L(θ),你还要看第三项</p></li>
<li><p>第三项跟Hessian有关,这边有一个$H $</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626143956011.png" alt="image-20220626143956011" style="zoom:67%;"></p>
<p>这个<span class="math inline">\(H\)</span>叫做Hessian,它是一个矩阵,这个第三项是,再<span class="math inline">\((θ-θ&#39;)^TH(θ-θ&#39;)\)</span>,所以第三项会再补足,再加上gradient以后,与真正的L(θ)之间的差距.<strong>H里面放的是L的二次微分</strong>,<strong>它第i个row,第j个column的值,就是把θ的第i个component,对L作微分,再把θ的第j个component,对L作微分,再把θ的第i个component,对L作微分,做两次微分以后的结果</strong>
就是这个<span class="math inline">\(H_i{_j}\)</span></p></li>
</ul>
<p>如果这边你觉得有点听不太懂的话,也没有关系,反正你就记得这个<span class="math inline">\(L(θ)\)</span>,这个loss function,这个error
surface在<span class="math inline">\(θ&#39;\)</span>附近,可以写成这个样子,这个式子跟两个东西有关系,<strong>跟gradient有关系,跟hessian有关系,gradient就是一次微分,hessian就是里面有二次微分的项目</strong></p>
<h4><span id="hession">Hession</span></h4>
<p><strong>那如果我们今天走到了一个critical
point,意味著gradient为零,也就是绿色的这一项完全都不见了</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626144155017.png" alt="image-20220626144155017" style="zoom:50%;"></p>
<p><span class="math inline">\(g\)</span><strong>是一个zero
vector,绿色的这一项完全都不见了</strong>,只剩下红色的这一项,所以当在critical
point的时候,这个loss function,它可以被近似为<span class="math inline">\(L(θ&#39;)\)</span>,加上红色的这一项。我们可以<strong>根据红色的这一项来判断</strong>,在<span class="math inline">\(θ&#39;\)</span>附近的error
surface,到底长什么样子。知道error surface长什么样子,我就可以判断。</p>
<h4><span id="判断-θ39它是一个localminima还是一个saddle-point"><strong><font color="red">判断 <span class="math inline">\(θ&#39;\)</span>它是一个==local
minima==,还是一个==saddle point==</font>。</strong></span></h4>
<p>我们可以靠这一项来了解,这个error
surface的地貌,大概长什么样子,知道它地貌长什么样子,我们就可以知道说,现在是在什么样的状态,这个是Hessian。</p>
<p>那我们就来看一下怎么根据Hessian,怎么根据红色的这一项,来判断θ'附近的地貌。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626144347777.png" alt="image-20220626144347777" style="zoom:67%;"></p>
<p>我们现在为了等一下符号方便起见,我们<strong>把<span class="math inline">\((θ-θ&#39;)\)</span>用<span class="math inline">\(v\)</span>这个向量来表示</strong></p>
<ul>
<li>如果今天对任何可能的<span class="math inline">\(v\)</span><strong>,<span class="math inline">\(v^THv\)</span>都大于零</strong>,也就是说
现在θ不管代任何值,v可以是任何的v,也就是θ可以是任何值,不管θ代任何值,<strong>红色框框里面通通都大于零</strong>,那意味著说
<span class="math inline">\(L(θ)&gt;L(θ&#39;)\)</span>。<span class="math inline">\(L(θ)\)</span>不管代多少 只要在<span class="math inline">\(θ&#39;\)</span>附近,<span class="math inline">\(L(θ)\)</span>都大于<span class="math inline">\(L(θ&#39;)\)</span>,<strong>代表<span class="math inline">\(L(θ&#39;)\)</span>是附近的一个最低点,所以它是local
minima</strong></li>
<li>如果今天反过来说,对所有的<span class="math inline">\(v\)</span>而言,<strong><span class="math inline">\(v^THv\)</span>都小于零,也就是红色框框里面永远都小于零</strong>,也就是说<span class="math inline">\(θ\)</span>不管代什么值,红色框框里面都小于零,意味著说<span class="math inline">\(L(θ)&lt;L(θ&#39;)\)</span>,<strong>代表<span class="math inline">\(L(θ&#39;)\)</span>是附近最高的一个点,所以它是local
maxima</strong></li>
<li>第三个可能是假设,<strong><span class="math inline">\(v^THv\)</span>,有时候大于零
有时候小于零</strong>,你代不同的v进去
代不同的θ进去,红色这个框框里面有时候大于零,有时候小于零,意味著说在θ'附近,有时候L(θ)&gt;L(θ')
有时候L(θ)&lt;L(θ'),在L(θ')附近,有些地方高
有些地方低,这意味著什么,<strong>这意味著这是一个saddle
point</strong></li>
</ul>
<p>但是你这边是说我们要代所有的<span class="math inline">\(v\)</span>,去看<span class="math inline">\(v^THv\)</span>是大于零,还是小于零.我们怎么有可能把所有的v,都拿来试试看呢,所以有一个更简便的方法,去确认说这一个条件或这一个条件,会不会发生.</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626144425820.png" alt="image-20220626144425820" style="zoom:67%;"></p>
<p><strong><font color="red">
这个就直接告诉你结论,线性代数理论上是有教过这件事情的,如果今天对所有的v而言,<span class="math inline">\(v^THv\)</span>都大于零,那这种矩阵叫做positive
definite 正定矩阵,positive definite的矩阵,它所有的eigen
value特征值都是正的</font></strong></p>
<p>所以如果你今天算出一个hessian,你不需要把它跟所有的v都乘看看,你只要去直接看这个H的eigen
value,如果你发现：</p>
<ul>
<li><strong>所有eigen
value都是正的</strong>,那就代表说这个条件成立,就<span class="math inline">\(v^THv\)</span>,会大于零,也就代表说是一个local
minima。所以你从hessian metric可以看出,它是不是local
minima,你只要算出hessian metric算完以后,看它的eigen
value发现都是正的,它就是local minima。</li>
<li>那反过来说也是一样,如果今天在这个状况,对所有的v而言,<span class="math inline">\(v^THv\)</span>小于零,那H是negative
definite,那就代表所有<strong>eigen
value都是负的</strong>,就保证他是local maxima</li>
<li><strong>那如果eigen value有正有负</strong>,那就代表是saddle
point,</li>
</ul>
<p>那假设在这里你没有听得很懂的话,你就可以记得结论,<strong>你只要算出一个东西,这个东西的名字叫做hessian,它是一个矩阵,这个矩阵如果它所有的eigen
value,都是正的,那就代表我们现在在local
minima,如果它有正有负,就代表在saddle point。</strong></p>
<p>那如果刚才讲的,你觉得你没有听得很懂的话,我们这边举一个例子：</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220626145009069.png" alt="image-20220626145009069">
<figcaption aria-hidden="true">image-20220626145009069</figcaption>
</figure>
<p>我们现在有一个史上最废的network,输入一个x,它只有一个neuron，乘上<span class="math inline">\(w₁\)</span>,而且这个neuron,还没有activation
function,所以x乘上<span class="math inline">\(w₁\)</span>以后
之后就输出,然后再乘上<span class="math inline">\(w₂\)</span>
然后就再输出,就得到最终的数据就是y.总之这个function非常的简单 <span class="math display">\[
y= w₁×w₂×x
\]</span> 我们有一个史上最废的training set,这个data
set说,我们只有一笔data,这笔data是x,是1的时候,它的level是1
所以输入1进去,你希望最终的输出跟1越接近越好</p>
<p>而这个史上最废的training,它的error
surface,也是有办法直接画出来的,因为反正只有两个参数 w₁
w₂,连bias都没有,假设没有bias,只有w₁跟w₂两个参数,这个network只有两个参数
w₁跟w₂,那我们可以穷举所有w₁跟w₂的数值,算出所有w₁
w₂数值所代来的loss,然后就画出error surface 长这个样</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220626145041635.png" alt="image-20220626145041635">
<figcaption aria-hidden="true">image-20220626145041635</figcaption>
</figure>
<p>四个角落loss是高的,好 那这个图上你可以看出来说,有一些critical
point,这个黑点点的地方(0,0),<strong>原点的地方是critical
point</strong>,然后事实上,<strong>右上三个黑点也是一排critical
point,左下三个点也是一排critical
point</strong>。如果你更进一步要分析,他们是saddle point,还是local
minima的话,那圆心这个地方,<strong>原点这个地方 它是saddle
point</strong>,为什么它是saddle point呢？你往左上这个方向走
loss会变大,往右下这个方向走 loss会变大,往左下这个方向走
loss会变小,往右下这个方向走 loss会变小,它是一个saddle point。</p>
<p>而这两群critical point,它们都是local
minima,所以这个山沟里面,有一排local minima,这一排山沟里面有一排local
minima,然后在原点的地方,有一个saddle point,这个是我们把error
surface,暴力所有的参数,得到的loss
function以后,得到的loss的值以后,画出error
surface,可以得到这样的结论。</p>
<p>现在假设如果不暴力所有可能的loss,如果要直接算说一个点,是local
minima,还是saddle point的话 怎么算呢</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626145125121.png" alt="image-20220626145125121" style="zoom: 67%;"></p>
<p>我们可以把loss的function写出来,这个loss的function 这个L是 <span class="math display">\[
L=(\hat{y}-w_1 w_2 x)^2
\]</span> 正确答案 ŷ减掉model的输出,也就是w₁ w₂x,这边取square
error,这边<strong>只有一笔data,所以就不会summation over所有的training
data</strong>,因为反正只有一笔data,x代1
ŷ代1,我刚才说过只有一笔训练资料最废的,所以只有一笔训练资料,所以loss
function就是<span class="math inline">\(L=(\hat{y}-w_1 w_2
x)^2\)</span>,那你可以把这一个loss
function,它的gradient求出来,w₁对L的微分,w₂对L的微分写出来是这个样子
<span class="math display">\[
\frac{∂L}{∂w_1 }=2(1-w_1 w_2 )(-w_2 )
\]</span></p>
<p><span class="math display">\[
\frac{∂L}{∂w_2 }=2(1-w_1 w_2 )(-w_1 )
\]</span></p>
<p>​ 这个东西 <span class="math display">\[
\begin{bmatrix}
\frac{∂L}{∂w_1 }\\\
\frac{∂L}{∂w_2 }
\end{bmatrix}
\]</span>
就是所谓的g,所谓的gradient,什么时候gradient会零呢,什么时候会到一个critical
point呢?</p>
<p>举例来说 如果w₁=0 w₂=0,就在圆心这个地方,如果w₁代0 w₂代0,w₁对L的微分
w₂对L的微分,算出来就都是零
就都是零,这个时候我们就知道说,原点就是一个critical
point,但<strong>它是local maxima,它是local maxima,local
minima,还是saddle point呢,那你就要看hessian才能够知道了</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626145206428.png" alt="image-20220626145206428" style="zoom:67%;"></p>
<p>当然 我们刚才已经暴力所有可能的w₁
w₂了,所以你已经知道说,它显然是一个saddle
point,但是现在假设还没有暴力所有可能的loss,所以我们要看看能不能够用H,用Hessian看出它是什么样的critical
point,那怎么算出这个H呢？</p>
<p><strong>H它是一个矩阵,这个矩阵里面元素就是L的二次微分</strong>,所以这个矩阵里面第一个row,第一个coloumn的位置,就是w₁对L微分两次,第一个row
第二个coloumn的位置,就是先用w₂对L作微分,再用w₁对L作微分,然后这边就是w₁对L作微分,w₂对L作微分,然后w₂对L微分两次,这四个值组合起来,就是我们的hessian,那这个hessian的值是多少呢</p>
<p>这个hessian的式子,我都已经把它写出来了,你只要把w₁=0 w₂=0代进去,代进去
你就得到在原点的地方,hessian是这样的一个矩阵 <span class="math display">\[
\begin{bmatrix}
{0}&amp;-2\\\
{-2}&amp;0
\end{bmatrix}
\]</span> 这个hessian告诉我们,它是local minima,还是saddle
point呢,那你就要看这个矩阵的eigen value,算一下发现,这个矩阵有两个eigen
value,2跟-2 <strong>eigen value有正有负,代表saddle point</strong></p>
<p>所以我们现在就是用一个例子,跟你操作一下
告诉你说,你怎么从hessian看出一个点,它一个critical point 它是saddle
point,还是local minima</p>
<h3><span id="13-dont-afraid-of-saddlepoint">1.3 Don't afraid of saddle
point</span></h3>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626145319827.png" alt="image-20220626145319827" style="zoom:67%;"></p>
<p>如果今天你卡的地方是saddle
point,也许你就不用那么害怕了,因为如果你今天你发现,你停下来的时候,是因为saddle
point 停下来了,那其实就有机会可以放心了。</p>
<p>因为H它不只可以帮助我们判断,现在是不是在一个saddle
point,它还指出了我们参数,可以update的方向,就之前我们参数update的时候,都是看gradient
看g,但是我们走到某个地方以后,发现g变成0了 不能再看g了,g不见了
gradient没有了<strong>,但如果是一个saddle
point的话,还可以再看H,怎么再看H呢,H怎么告诉我们,怎么update参数呢</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626145640713.png" alt="image-20220626145640713" style="zoom:67%;"></p>
<p>我们这边假设<span class="math inline">\(\mu\)</span>是H的eigenvector特征向量,然后<span class="math inline">\(λ\)</span>是u的eigen
value特征值。如果我们把这边的<span class="math inline">\(v\)</span>换成<span class="math inline">\(\mu\)</span>的话,我们把<span class="math inline">\(\mu\)</span>乘在H的左边,跟H的右边,也就是<span class="math inline">\(\mu^TH\mu\)</span>, <span class="math inline">\(H\mu\)</span>会得到<span class="math inline">\(λ\mu\)</span>，因为<span class="math inline">\(\mu\)</span>是一个eigen vector。H乘上eigen
vector特征向量会得到特征向量λ eigen value乘上eigen vector即<span class="math inline">\(λ\mu\)</span></p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220626145734569.png" alt="image-20220626145734569">
<figcaption aria-hidden="true">image-20220626145734569</figcaption>
</figure>
<p>所以我们在这边得到uᵀ乘上λu,然后再整理一下,把uᵀ跟u乘起来,得到‖u‖²,所以得到λ‖u‖²</p>
<figure>
<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20220626145742180.png" alt="image-20220626145742180">
<figcaption aria-hidden="true">image-20220626145742180</figcaption>
</figure>
<p>假设我们这边v,代的是一个eigen vector,我们这边θ减θ',放的是一个eigen
vector的话,会发现说我们这个红色的项里面,其实就是λ‖u‖²</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626145756118.png" alt="image-20220626145756118" style="zoom:67%;"></p>
<p>那今天如果λ<strong>小于零</strong>,eigen
value小于零的话,那λ‖u‖²就会小于零,因为‖u‖²一定是正的,所以eigen
value是负的,那这一整项就会是<strong>负的</strong>,也就是u的transpose乘上H乘上u,它是负的,也就是<strong>红色这个框里是负的</strong>。所以这意思是说假设<span class="math inline">\(θ-θ&#39;=\mu\)</span>,那这一项<span class="math inline">\((θ-θ&#39;)^TH(θ-θ&#39;)\)</span>就是负的,也就是<span class="math inline">\(L(θ)&lt;L(θ&#39;)\)</span>。也就是说假设<span class="math inline">\(θ-θ&#39;=\mu\)</span>,也就是,<strong>你在θ'的位置加上u,沿著u的方向做update得到θ,你就可以让loss变小</strong>。</p>
<p>因为根据这个式子,你只要θ减θ'等于u,loss就会变小,所以你今天只要让θ等于θ'加u,你就可以让loss变小,你只要沿著u,也就是eigen
vector的方向,去更新你的参数 去改变你的参数,你就可以让loss变小了</p>
<p><strong><font color="red"> 所以虽然在critical
point没有gradient,如果我们今天是在一个saddle
point,你也不一定要惊慌,你只要找出负的eigen value,再找出它对应的eigen
vector,用这个eigen
vector去加θ',就可以找到一个新的点,这个点的loss比原来还要低。</font></strong></p>
<h5><span id="举具体的例子">举具体的例子：</span></h5>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626145931496.png" alt="image-20220626145931496" style="zoom: 67%;"></p>
<p>刚才我们已经发现,原点是一个critical
point,它的Hessian长这个样,那我现在发现说,这个Hessian有一个负的eigen
value,这个eigen value等于-2,那它对应的eigen
vector,它有很多个,其实是无穷多个对应的eigen
vector,我们就取一个出来,我们取<span class="math inline">\(\begin{bmatrix}{1} \\\
{1}\end{bmatrix}\)</span>是它对应的一个eigen
vector,那我们其实只要顺著这个u的方向,顺著<span class="math inline">\(\begin{bmatrix}{1} \\\
{1}\end{bmatrix}\)</span>这个vector的方向,去更新我们的参数,就可以找到一个,比saddle
point的loss还要更低的点。</p>
<p>如果以今天这个例子来看的话,你的saddle
point在(0,0)这个地方,你在这个地方会没有gradient,Hessian的eigen
vector告诉我们,只要往<span class="math inline">\(\begin{bmatrix}{1} \\\
{1}\end{bmatrix}\)</span>的方向更新,你就可以让loss变得更小,也就是说你可以逃离你的saddle
point,然后让你的loss变小,所以从这个角度来看,似乎saddle
point并没有那么可怕。如果你今天在training的时候,你的gradient你的训练停下来,你的gradient变成零,你的训练停下来,是因为saddle
point的话,那似乎还有解。</p>
<p><strong>但是当然实际上,在实际的implementation里面,你几乎不会真的把Hessian算出来</strong>,这个要是二次微分,要计算这个矩阵的computation,需要的运算量非常非常的大,更遑论你还要把它的eigen
value,跟 eigen
vector找出来,所以在实作上,你几乎没有看到,有人用这一个方法来逃离saddle
point。</p>
<p><strong>等一下我们会讲其他,也有机会逃离saddle
point的方法,他们的运算量都比要算这个H,还要小很多</strong>,那今天之所以我们把,这个saddle
point跟 eigen vector,跟Hessian的eigen
vector拿出来讲,是想要告诉你说,如果是卡在saddle
point,也许没有那么可怕,最糟的状况下你还有这一招,可以告诉你要往哪一个方向走.</p>
<h3><span id="14-saddle-point-vs-localminima">1.4 Saddle Point v.s. Local
Minima</span></h3>
<p>讲到这边你就会有一个问题了,这个问题是,那到底<strong>saddle
point跟local minima,谁比较常见呢</strong>,我们说,saddle
point其实并没有很可怕,那如果我们今天,常遇到的是saddle
point,比较少遇到local minima,那就太好了,那到底saddle point跟local
minima,哪一个比较常见呢?</p>
<p>总之这个<strong>从三维的空间来看,是没有路可以走的东西,在高维的空间中是有路可以走的,error
surface会不会也一样呢？</strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626150135469.png" alt="image-20220626150135469" style="zoom:67%;"></p>
<p>而经验上,如果你自己做一些实验的话,也支持这个假说</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220626150207285.png" alt="image-20220626150207285" style="zoom: 67%;"></p>
<p>这边是训练某一个network的结果,每一个点代表,训练那个network训练完之后,把它的Hessian拿出来进行计算,所以这边的每一个点,都代表一个network,就我们训练某一个network,然后把它训练训练,训练到gradient很小,卡在critical
point,把那组参数出来分析,看看它比较像是saddle point,还是比较像是local
minima</p>
<ul>
<li>纵轴代表training的时候的loss,就是我们今天卡住了,那个loss没办法再下降了,那个loss是多少,那很多时候,你的loss在还很高的时候,训练就不动了
就卡在critical point,那很多时候loss可以降得很低,才卡在critical
point,这是纵轴的部分</li>
<li>横轴的部分是minimum ratio,minimum ratio是<strong>eigen
value的数目分之正的eigen value的数目</strong>,又<strong>如果所有的eigen
value都是正的,代表我们今天的critical point,是local
minima,如果有正有负代表saddle
point</strong>,那在实作上你会发现说,你几乎找不到完全所有eigen
value都是正的critical point,你看这边这个例子里面,这个minimum
ratio代表eigen value的数目分之正的eigen
value的数目,最大也不过0.5到0.6间而已,代表说只有一半的eigen
value是正的,还有一半的eigen value是负的,</li>
</ul>
<p>所以今天虽然在这个图上,越往右代表我们的critical point越像local
minima,<strong>但是它们都没有真的,变成local
minima</strong>,就算是在最极端的状况,我们仍然有一半的case,我们的eigen
value是负的,这一半的case eigen
value是正的,代表说在所有的维度里面有一半的路,这一半的路
如果要让loss上升,还有一半的路可以让loss下降。</p>
<p><strong><font color="red"> 所以从经验上看起来,其实local
minima并没有那么常见,多数的时候,你觉得你train到一个地方,你gradient真的很小,然后所以你的参数不再update了,往往是因为你卡在了一个saddle
point。</font></strong></p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220616164405732.png" alt="image-20220616164405732" style="zoom: 33%;"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/35TG484/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/35TG484/" class="post-title-link" itemprop="url">特征工程（6）【Nan】时间序列处理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-06 20:36:41" itemprop="dateCreated datePublished" datetime="2022-06-06T20:36:41+08:00">2022-06-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-08 18:21:15" itemprop="dateModified" datetime="2022-06-08T18:21:15+08:00">2022-06-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>261</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="时间序列数据的预处理">时间序列数据的预处理</span></h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/466086665"><em>时间序列</em>数据的预<em>处理</em></a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/vyDZfDdaH2Y7k75NNsMNJA">如何在实际场景中使用异常检测？阿里云Prometheus智能检测算子来了</a></li>
</ul>
<blockquote>
<p>在本文中，我们将主要讨论以下几点：</p>
<ul>
<li>时间序列数据的定义及其重要性。</li>
<li>时间序列数据的预处理步骤。</li>
<li>构建时间序列数据，查找缺失值，对特征进行去噪，并查找数据集中存在的异常值。</li>
</ul>
</blockquote>
<h3><span id="时间序列的定义">时间序列的定义</span></h3>
<p><strong>时间序列是在特定时间间隔内记录的一系列均匀分布的观测值</strong>。时间序列的一个例子是黄金价格。在这种情况下，我们的观察是在固定时间间隔后一段时间内收集的黄金价格。时间单位可以是分钟、小时、天、年等。但是任何两个连续样本之间的时间差是相同的。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2MJ41K7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2MJ41K7/" class="post-title-link" itemprop="url">python-环境变量</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-06-06 15:22:38 / 修改时间：15:22:48" itemprop="dateCreated datePublished" datetime="2022-06-06T15:22:38+08:00">2022-06-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">工程</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%B7%A5%E7%A8%8B/%E6%B5%81%E7%A8%8B%E7%9A%84Python/" itemprop="url" rel="index"><span itemprop="name">流程的Python</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3JZF773/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3JZF773/" class="post-title-link" itemprop="url">恶意软件检测（7）【draft】CADE: Detecting and Explaining Concept Drift Samples for Security Applications</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-05 20:05:05" itemprop="dateCreated datePublished" datetime="2022-06-05T20:05:05+08:00">2022-06-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-19 15:50:46" itemprop="dateModified" datetime="2023-04-19T15:50:46+08:00">2023-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">应用</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/" itemprop="url" rel="index"><span itemprop="name">学术前沿</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%94%E7%94%A8/%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/" itemprop="url" rel="index"><span itemprop="name">网络安全</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="cadedetecting-and-explaining-concept-drift-samples-for-securityapplications">CADE:
Detecting and Explaining Concept Drift Samples for Security
Applications</span></h2>
<p>原文作者：Limin Yang, <em>University of Illinois at
Urbana-Champaign</em></p>
<p>原文链接：https://www.usenix.org/conference/usenixsecurity21/presentation/yang-limin</p>
<p>发表会议：USENIXSec 2021</p>
<p><strong>代码地址</strong>：https://github.com/whyisyoung/CADE</p>
<h3><span id="摘要">摘要</span></h3>
<p>概念漂移对部署机器学习模型来解决实际的安全问题提出了严峻的挑战。<strong>由于攻击者（和/或良性对手）的动态行为变化，随着时间的推移，测试数据分布往往会从原始的训练数据转移，从而导致部署的模型出现重大故障</strong>。</p>
<p>为了对抗概念漂移，我们提出了一种新的系统CADE，旨在（1）<strong>检测偏离现有类别的漂移样本</strong>；（2）<strong>解释检测到漂移的原因</strong>。与传统方法不同（需要大量新标签来统计确定概念漂移），我们的目标是在单个漂移样本到达时识别它们。认识到高维离群空间带来的挑战，我们建议将数据样本映射到低维空间，并自动学习距离函数来度量样本之间的相异性。通过对比学习，我们可以充分利用训练数据集中现有的标签来学习如何对样本进行比较和对比。<strong>为了解释检测到的漂移的意义，我们开发了一种基于距离的解释方法</strong>。我们表明，在这个问题背景下，解释“距离”比传统方法更有效，传统方法侧重于解释“决策边界”。我们通过两个案例来评估CADE：Android恶意软件分类和网络入侵检测。我们进一步与一家安全公司合作，在其恶意软件数据库上测试CADE。我们的结果表明，CADE可以有效地检测漂移样本，并提供语义上有意义的解释。</p>
<h3><span id="一-说明">一、说明</span></h3>
<p>由于概念漂移，部署基于机器学习的安全应用程序可能非常具有挑战性。无论是恶意软件分类、入侵检测还是在线滥用检测[6、12、17、42、48]，基于学习的模型都是在“封闭世界”假设下工作的，期望测试数据分布与训练数据大致匹配。然而，部署模型的环境通常会随着时间的推移而动态变化。这种变化可能既包括良性玩家的有机行为变化，也包括攻击者的恶意突变和适应。因此，测试数据分布从原始训练数据转移，这可能会导致模型出现严重故障[23]。</p>
<blockquote>
<p>[23] A survey on concept drift adaptation. ACM computing surveys
(CSUR), 2014.</p>
</blockquote>
<p>为了解决概念漂移问题，大多数基于学习的模型需要<strong>定期重新培训</strong>[36、39、52]。然而，再培训通常需要标记大量新样本（昂贵）。更重要的是，还很难确定何时应该对模型进行再培训。延迟的再培训会使过时的模型容易受到新的攻击。</p>
<p><font color="red"><strong>我们设想，对抗概念漂移需要建立一个监控系统来检查传入数据流和训练数据（和/或当前分类器）之间的关系</strong></font>。图1说明了高级思想。当原始分类器在生产空间中工作时，另一个系统应定期检查分类器对传入数据样本做出决策的能力。<strong>A检测模块(1)
可以过滤正在远离训练空间的漂移样本</strong>。更重要的是，为了<strong>解释漂移的原因（例如，攻击者突变、有机行为变化、以前未知的系统错误）</strong>，我们需要一种解释方法(2)
将检测决策与语义上有意义的特征联系起来。这两项功能对于为开放世界环境准备基于学习的安全应用程序至关重要。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550835.png" alt="image-20220605202728095" style="zoom: 67%;"></p>
<p>之前的工作已经探索了通过直接检查原始分类器（0）的预测置信度来检测漂移样本的方法
<strong>[32]</strong>。置信度较低可能表明传入样本是漂移样本。然而，该置信度得分是基于所有类别已知（封闭世界）的假设计算的概率（总和为1.0）。不属于任何现有类别的漂移样本可能会被分配到错误的类别，并具有很高的置信度（已通过现有工作验证[25、32、37]）。最近的一项工作提出了计算传入样本和每个现有类之间的不一致性度量的想法，以确定适合度[38]。<strong>该不合格度量基于距离函数计算，以量化样本之间的不相似性</strong>。<strong>然而，我们发现这种距离函数很容易失效，尤其是当数据稀疏且维数较高时。</strong></p>
<blockquote>
<p><strong>[32] A baseline for detecting misclassified and
out-of-distribution examples in neural networks.</strong></p>
</blockquote>
<p><strong>我们的方法</strong>。在本文中，我们提出了一种检测漂移样本的新方法，并结合一种解释检测决策的新方法。我们共同构建了一个称为CADE的系统，它是“用于漂移检测和解释的对比自动编码器
(“<strong>Contrastive Autoencoder for Drifting detection and
Explanation</strong>)”的缩写关键的挑战是<strong>推导一个有效的距离函数来衡量样本的相异性</strong>。我们没有随意选取距离函数，而是利用对比学习的思想[29]，根据现有的标签，从现有的训练数据中学习距离函数。给定原始分类器的训练数据（多个类别），我们将训练样本映射到低维潜在空间。映射函数通过对比样本来学习，以扩大不同类样本之间的距离，同时减少同一类样本之间的距离。<strong>我们证明了在潜在空间中得到的距离函数可以有效地检测和排序漂移样本。</strong></p>
<p>评价我们使用两个数据集评估我们的方法，包括<strong>Android恶意软件数据集[7]和2018年发布的入侵检测数据集[57]</strong>。我们的评估表明，我们的漂移检测方法具有很高的准确性，F1平均得分为0.96或更高，优于各种基线和现有方法。我们的分析还表明，使用对比学习可以减少检测决策的模糊性。对于解释模型，我们进行了定量和定性评估。案例研究还表明，所选特征与漂移样本的语义行为相匹配。</p>
<p>此外，我们还与一家安全公司的合作伙伴合作，在其内部恶意软件数据库上测试CADE。作为初步测试，我们从395个家庭中获得了2019年8月至2020年2月出现的20613个Windows
PE恶意软件样本。这使我们能够在不同的环境中测试更多恶意软件系列的系统性能。结果很有希望。<font color="red"><strong>例如，CADE在10个家庭中进行训练并在160个以前未见过的家庭中进行测试时，F1成绩达到0.95分。这使得人们有兴趣在生产系统中进一步测试和部署CADE。</strong>
</font></p>
<h4><span id="贡献">贡献：</span></h4>
<p>本文有三个主要贡献。</p>
<ul>
<li>我们提出CADE来补充现有的基于监督学习的安全应用程序，以对抗概念漂移。提出了<strong>一种基于对比表征学习的漂移样本检测方法</strong>。</li>
<li>我们说明了监督解释方法在解释异常样本方面的局限性，并<strong>介绍了一种基于距离的解释方法</strong>。</li>
<li>我们通过两个应用对所提出的方法进行了广泛的评估。我们与一家安保公司的初步测试表明，CADE是有效的。我们在此处发布了CADE代码1，以支持未来的研究。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
