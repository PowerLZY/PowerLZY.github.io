<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/24/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/24/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/24/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">263</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/349WYTC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/349WYTC/" class="post-title-link" itemprop="url">支持向量机（2）软间隔对偶性</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:38:05" itemprop="dateCreated datePublished" datetime="2022-03-15T22:38:05+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:37:45" itemprop="dateModified" datetime="2023-04-22T19:37:45+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">支持向量机</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong><font color="red"> SVM
是一个非常优雅的算法，具有完善的数学理论，虽然如今工业界用到的不多，但还是决定花点时间去写篇文章整理一下。</font></strong></p>
<p><strong>本质：SVM
想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</strong>为了对数据中的噪声有一定的容忍能力。<strong>以几何的角度，在丰富的数据理论的基础上，简化了通常的分类和回归问题。</strong></p>
<p><strong>几何意义</strong>：找到一个超平面将特征空间的正负样本分开，最大分隔（对噪音有一定的容忍能力）；</p>
<p><strong>间隔表示</strong>：划分超平面到属于不同标记的最近样本的距离之和；</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191157912.jpg" alt="【机器学习】支持向量机 SVM（非常详细）" style="zoom: 33%;"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/349WYTC/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/A9Y29H/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/A9Y29H/" class="post-title-link" itemprop="url">支持向量机（3）核函数</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:38:05" itemprop="dateCreated datePublished" datetime="2022-03-15T22:38:05+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:37:37" itemprop="dateModified" datetime="2023-04-22T19:37:37+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">支持向量机</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong><font color="red">SVM
是一个非常优雅的算法，具有完善的数学理论，虽然如今工业界用到的不多，但还是决定花点时间去写篇文章整理一下。</font></strong></p>
<p><strong>本质：SVM
想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</strong>为了对数据中的噪声有一定的容忍能力。<strong>以几何的角度，在丰富的数据理论的基础上，简化了通常的分类和回归问题。</strong></p>
<p><strong>几何意义</strong>：找到一个超平面将特征空间的正负样本分开，最大分隔（对噪音有一定的容忍能力）；</p>
<p><strong>间隔表示</strong>：划分超平面到属于不同标记的最近样本的距离之和；</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191157912.jpg" alt="【机器学习】支持向量机 SVM（非常详细）" style="zoom: 33%;"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/A9Y29H/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3MCYRQP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3MCYRQP/" class="post-title-link" itemprop="url">支持向量机（4）支持向量回归 & 多分类</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:38:05" itemprop="dateCreated datePublished" datetime="2022-03-15T22:38:05+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:44:29" itemprop="dateModified" datetime="2023-04-22T19:44:29+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">支持向量机</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong><font color="red"> SVM
是一个非常优雅的算法，具有完善的数学理论，虽然如今工业界用到的不多，但还是决定花点时间去写篇文章整理一下。</font></strong></p>
<p><strong>本质：SVM
想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</strong>为了对数据中的噪声有一定的容忍能力。<strong>以几何的角度，在丰富的数据理论的基础上，简化了通常的分类和回归问题。</strong></p>
<p><strong>几何意义</strong>：找到一个超平面将特征空间的正负样本分开，最大分隔（对噪音有一定的容忍能力）；</p>
<p><strong>间隔表示</strong>：划分超平面到属于不同标记的最近样本的距离之和；</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191157912.jpg" alt="【机器学习】支持向量机 SVM（非常详细）" style="zoom: 33%;"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/3MCYRQP/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2NSQX65/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2NSQX65/" class="post-title-link" itemprop="url">支持向量机（5）总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:38:05" itemprop="dateCreated datePublished" datetime="2022-03-15T22:38:05+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:49:07" itemprop="dateModified" datetime="2023-04-22T19:49:07+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">支持向量机</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="一支持向量机-svm">一、支持向量机 SVM</h3>
<p><strong><font color="red"> SVM
是一个非常优雅的算法，具有完善的数学理论，虽然如今工业界用到的不多，但还是决定花点时间去写篇文章整理一下。</font></strong></p>
<p><strong>本质：SVM
想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</strong>为了对数据中的噪声有一定的容忍能力。<strong>以几何的角度，在丰富的数据理论的基础上，简化了通常的分类和回归问题。</strong></p>
<p><strong>几何意义</strong>：找到一个超平面将特征空间的正负样本分开，最大分隔（对噪音有一定的容忍能力）；</p>
<p><strong>间隔表示</strong>：划分超平面到属于不同标记的最近样本的距离之和；</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191157912.jpg" alt="【机器学习】支持向量机 SVM（非常详细）" style="zoom: 33%;"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/2NSQX65/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2DSARR9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2DSARR9/" class="post-title-link" itemprop="url">聚类（1）K-means</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 17:29:21" itemprop="dateModified" datetime="2023-04-26T17:29:21+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">聚类</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-k-means">一、K-means</span></h3>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304211326577.jpg" alt="【机器学习】K-means（非常详细）" style="zoom:51%;"></p>
<p>K-means 聚类的迭代算法实际上是 EM 算法。EM
算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。在
K-means
中的隐变量是每个类别所属类别。k近邻法中，当<strong>训练集</strong>、<strong>距离度量</strong>、<strong>K值</strong>以及<strong>分类决策规则</strong>确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。</p>
<p><strong>K-均值是一个迭代算法，假设我们想要将数据聚类成 n
个组，其方法为:</strong></p>
<ul>
<li>首先选择𝐾个<strong>随机</strong>的点，称为<strong>聚类中心</strong>（cluster
centroids）；</li>
<li>对于数据集中的每一个数据，按照<strong>距离𝐾个中心点的距离</strong>，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</li>
<li>计算每一个组的平均值，将该组所<strong>关联的中心点移动到平均值</strong>的位置。</li>
<li>重复步骤，直至中心点不再变化。</li>
</ul>
<p>K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用
K-均值算法将数据分为三类，用于帮助确定将要生产的 T-恤衫的三种尺寸。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221641204.jpeg" alt="img" style="zoom: 50%;"></p>
<h4><span id="11-损失函数">1.1 损失函数</span></h4>
<p><strong>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和</strong>，因此
<strong>K-均值的代价函数（又称畸变函数 Distortion
function）为</strong>： <span class="math display">\[
J\left(c^{(1)}, c^{(2)}, \ldots, c^{(m)}, u_1, \ldots,
u_k\right)=\frac{1}{m} \sum_{i=1}^m\left\|X^{(1)}-u_{c^{(i)}}\right\|^2
\]</span> 其中 <span class="math inline">\(u_{c^{(i)}}\)</span> 代表与
<span class="math inline">\(x^{(i)}\)</span>
最近的聚类中心点。我们的的优化目标便是找出使得代价函数最小的 <span class="math inline">\(c^{(1)}, c^{(2)}, \ldots, c^{(m)}\)</span> 和
<span class="math inline">\(u_1, u_2, \ldots, u_k\)</span> 。</p>
<h4><span id="12-k值的选择-肘部法则">1.2 k值的选择 【肘部法则】</span></h4>
<p>在运行
K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：</p>
<ol type="1">
<li>我们应该选择𝐾 &lt;
𝑚，即聚类中心点的个数要小于所有训练集实例的数量。</li>
<li>随机选择𝐾个训练实例，然后令𝐾个聚类中心分别与这𝐾个训练实例相等K-均值的一个问题在于，它有可能会<strong>停留在一个局部最小值</strong>处，而这取决于初始化的情况。</li>
</ol>
<p>为了解决这个问题，我们通常需要多次运行
K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行
K-均值的结果，选择代价函数最小的结果。这种方法在𝐾较小的时候（2--10）还是可行的，<strong>但是如果𝐾较大，这么做也可能不会有明显地改善。</strong></p>
<p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用
K-均值算法聚类的动机是什么。有一个可能会谈及的方法叫作<strong>“肘部法则”</strong>。关
于“肘部法则”，我们所需要做的是改变𝐾值，也就是聚类类别数目的总数。我们用一个聚类来运行
K
均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后<strong>计算成本函数或者计算畸变函数</strong>𝐽。𝐾代表聚类数字。</p>
<p>[<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221641692.jpeg" alt="img"></p>
<p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。你会发现这种模式，它的畸变值会迅速下降，从
1 到 2，从 2 到 3 之后，你会在 3
的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用 3
个聚类来进行聚类是正确的，<strong>这是因为那个点是曲线的肘点，畸变值下降得很快，𝐾
= 3之后就下降得很慢，那么我们就选𝐾 =
3。</strong>当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p>
<h4><span id="13-knn与k-means区别">1.3 KNN与K-means区别？</span></h4>
<p>K最近邻(k-Nearest
Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。</p>
<h5><span id="区别">区别：</span></h5>
<table>
<colgroup>
<col style="width: 6%">
<col style="width: 46%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>算法</th>
<th>KNN</th>
<th>K-Means</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>类别</td>
<td>1.KNN是<strong>分类</strong>算法 2.属于<strong>监督学习</strong>
3.训练数据集是带label的数据</td>
<td>1.K-Means是<strong>聚类</strong>算法
2.属于<strong>非监督学习</strong>
3.训练数据集是无label的数据，是杂乱无章的，经过聚类后变得有序，先无序，后有序。</td>
</tr>
<tr class="even">
<td>训练模式</td>
<td>没有明显的前期训练过程，属于memory based learning</td>
<td>有明显的前期训练过程</td>
</tr>
<tr class="odd">
<td>k值的含义</td>
<td>K的含义：一个样本x，对它进行分类，就从训练数据集中，<strong>在x附近找离它最近的K个数据点</strong>，这K个数据点，类别c占的个数最多，就把x的label设为c。</td>
<td>K的含义：<strong>K是人工固定好的数字，假设数据集合可以分为K个蔟</strong>，那么就利用训练数据来训练出这K个分类。</td>
</tr>
</tbody>
</table>
<h5><span id="相似点"><strong>相似点</strong>：</span></h5>
<p>都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears
Neighbor)算法思想。</p>
<h4><span id="14-k-means优缺点及改进">1.4 K-Means优缺点及改进</span></h4>
<p>k-means：在大数据的条件下，<strong>会耗费大量的时间和内存</strong>。
优化k-means的建议：</p>
<ol type="1">
<li><p>减少聚类的数目K。因为，每个样本都要跟类中心计算距离。</p></li>
<li><p>减少样本的特征维度。比如说，<strong>通过PCA等进行降维</strong>。</p></li>
<li><p>考察其他的聚类算法，通过选取toy数据，去测试不同聚类算法的性能。</p></li>
<li><p><strong>hadoop集群</strong>，K-means算法是很容易进行并行计算的。</p></li>
<li><p>算法可能找到局部最优的聚类，而不是全局最优的聚类。使用改进的二分k-means算法。</p>
<p>二分k-means算法：首先将整个数据集看成一个簇，然后进行一次k-means（k=2）算法将该簇一分为二，并计算每个簇的误差平方和，选择平方和最大的簇迭代上述过程再次一分为二，直至簇数达到用户指定的k为止，此时可以达到的全局最优。</p></li>
</ol>
<h3><span id="二-k-means的调优与改进">二、K - means的调优与改进</span></h3>
<p>针对 K-means
算法的缺点，我们可以有很多种调优方式：如<strong>数据预处理</strong>（去除异常点），<strong>合理选择
K 值</strong>，<strong>高维映射</strong>等。以下将简单介绍：</p>
<h4><span id="21-数据预处理">2.1 数据预处理</span></h4>
<p>K-means
的本质是基于欧式距离的数据划分算法，均值和方差大的维度将对数据的聚类产生决定性影响。所以<strong>未做归一化处理和统一单位的数据是无法直接参与运算和比较</strong>的。常见的数据预处理方式有：<strong>数据归一化，数据标准化</strong>。</p>
<p>此外，离群点或者噪声数据会对均值产生较大的影响，导致中心偏移，因此我们还需要对数据进行异常点检测。</p>
<h4><span id="22-合理选择-k-值">2.2 合理选择 K 值</span></h4>
<p>K 值的选取对 K-means 影响很大，这也是 K-means 最大的缺点，常见的选取
K 值的方法有：<strong>手肘法、Gap statistic 方法</strong>。</p>
<p><strong>【手肘法】</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-5ca4a5fe0b06b25a2b97262abb401a16_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>当 K &lt; 3 时，曲线急速下降；当 K &gt; 3
时，曲线趋于平稳，通过手肘法我们认为拐点 3 为 K 的最佳值。</p>
<p>【<strong>Gap statistic</strong>】</p>
<p><span class="math display">\[
G a p(K)=\mathrm{E}\left(\log D_k\right)-\log D_k
\]</span> 其中 <span class="math inline">\(D_k\)</span> 为损失函数, 这里
<span class="math inline">\(E\left(\log D_k\right)\)</span> 指的是 <span class="math inline">\(\log D_k\)</span>
的期望。这个数值通常通过蒙特卡洛模拟产生, <strong>我们在样本
里所在的区域中按照均匀分布随机产生和原始样本数一样多的随机样本,
并对这个随机样本做 K-Means, 从而得到一个 <span class="math inline">\(D_k\)</span> 。如此往复多次, 通常 20 次,
我们可以得到 20 个 <span class="math inline">\(\log
D_k\)</span></strong> 。对这 20 个数值求平均值, 就得到了 <span class="math inline">\(E\left(\log D_k\right)\)</span>
的近似值。最终可以计算 Gap Statisitc。而 Gap statistic
取得最大值所对应的 K 就是最佳的 K。</p>
<p><img src="https://pic3.zhimg.com/80/v2-9a39a8dad143e5dd52a506d83c2cbb36_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>由图可见，当 K=3 时，Gap(K) 取值最大，所以最佳的簇数是 K=3。</p>
<p>Github 上一个项目叫 <a href="https://link.zhihu.com/?target=https%3A//github.com/milesgranger/gap_statistic">gap_statistic</a>
，可以更方便的获取建议的类簇个数。</p>
<h4><span id="23-采用核函数">2.3 采用核函数</span></h4>
<p><strong>基于欧式距离的 K-means
假设了了各个数据簇的数据具有一样的的先验概率并呈现球形分布</strong>，但这种分布在实际生活中并不常见。面对非凸的数据分布形状时我们可以引入核函数来优化，这时算法又称为核
K-means
算法，是核聚类方法的一种。<strong>核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。</strong>非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。</p>
<h4><span id="24-k-means">2.4 K-means++</span></h4>
<p><strong>K-means++
就是选择离已选中心点最远的点</strong>。这也比较符合常理,
聚类中心当然是互相离得越远越好。我们知道 初始值的选取对结果的影响很大,
对初始值选择的改进是很重要的一部分。在所有的改进算法中, K-means++ 最
有名。 K-means++ 算法步骤如下所示：</p>
<ol type="1">
<li>随机选取一个中心点 <span class="math inline">\(a_1\)</span>;</li>
<li>计算数据到之前 <span class="math inline">\(\mathrm{n}\)</span>
个聚类中心最远的距离 <span class="math inline">\(D(x)\)</span>,
并以一定概率 <span class="math inline">\(\frac{D(x)^2}{\sum
D(x)^2}\)</span> 选择新中心点 <span class="math inline">\(a_i\)</span>;</li>
<li>重复第二步。</li>
</ol>
<p>简单的来说, 就是 <strong>K-means++
就是选择离已选中心点最远的点</strong>。这也比较符合常理,
聚类中心当然是互相离得越 远越好。 但是这个算法的缺点在于,
难以并行化。所以 k-means II 改变取样策略, 并非按照 k-means++
那样每次遍历只取 样一个样本, 而是每次遍历取样 <span class="math inline">\(\mathrm{k}\)</span> 个, 重复该取样过程 <span class="math inline">\(\log (n)\)</span> 次, 则得到 <span class="math inline">\(k \log (n)\)</span> 个样本点组成的集合, 然后从
这些点中选取 k 个。当然一般也不需要 <span class="math inline">\(\log
(n)\)</span> 次取样, 5 次即可。</p>
<h4><span id="25-isodata">2.5 ISODATA</span></h4>
<p>ISODATA 的全称是<strong>迭代自组织数据分析法</strong>。它解决了 K
的值需要预先人为的确定这一缺点。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出
K 的大小。ISODATA
就是针对这个问题进行了改进，它的思想也很直观：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别。</p>
<h3><span id="三-收敛证明em算法">三、 收敛证明【EM算法】</span></h3>
<p>我们先来看一下 K-means
算法的步骤：先随机选择初始节点，然后计算每个样本所属类别，然后通过类别再跟新初始化节点。这个过程有没有想到之前介绍的
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/78311644">EM 算法</a> 。</p>
<p>我们需要知道的是 K-means 聚类的迭代算法实际上是 EM 算法。EM
算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。在
<strong>K-means 中的隐变量是每个样本所属类别</strong>。</p>
<p>K-means 算法迭代步骤中的每次确认中心点以后重新进行标记对应 EM
算法中的 <strong>E 步</strong>：<strong>求当前参数条件下的
Expectation</strong>。而根据标记重新求中心点 对应 EM 算法中的 <strong>M
步</strong>：<strong>求似然函数最大化时（损失函数最小时）对应的参数
。</strong></p>
<p>首先我们看一下损失函数的形式: <span class="math display">\[
J=\sum_{i=1}^C \sum_{j=1}^N r_{i j} \cdot \nu\left(x_j, \mu_i\right)
\]</span> 其中: <span class="math display">\[
\nu\left(x_j, \mu_i\right)=\left\|x_j-\mu_i\right\|^2, \quad r_{n k}=
\begin{cases}1 &amp; \text { if } x_n \in k \\ 0 &amp; \text { else
}\end{cases}
\]</span> 为了求极值，我们令损失函数求偏导数且等于 0 : <span class="math display">\[
\frac{\partial J}{\partial \mu_k}=2 \sum_{i=1}^N r_{i
k}\left(x_i-\mu_k\right)=0
\]</span> <span class="math inline">\(\mathrm{k}\)</span> 是指第 <span class="math inline">\(\mathrm{k}\)</span> 个中心点, 于是我们有: <span class="math display">\[
\mu_k=\frac{\sum_{i=1}^N r_{i k} x_i}{\sum_{i=1}^N r_{i k}}
\]</span> 可以看出，新的中心点就是所有该类的<strong>质心</strong>。</p>
<p><strong>EM 算法的缺点就是，容易陷入局部极小值，这也是 K-means
有时会得到局部最优解的原因。</strong></p>
<h3><span id="四-高斯混合模型gmm">四、高斯混合模型(GMM)</span></h3>
<h4><span id="41-gmm的思想">4.1 GMM的思想</span></h4>
<p>高斯混合模型（Gaussian Mixed
Model，GMM）也是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。<strong>高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的</strong>，当前<strong>数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。</strong></p>
<p>第一张图是一个数据分布的样例，如果只用一个高斯分布来拟合图中的数据，图
中所示的椭圆即为高斯分布的二倍标准差所对应的椭圆。直观来说，图中的数据
明显分为两簇，因此只用一个高斯分布来拟和是不太合理的，需要推广到用多个
高斯分布的叠加来对数据进行拟合。第二张图是用两个高斯分布的叠加来拟合得到的结果。<strong>这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。</strong>理论上，高斯混合模型可以拟合出任意类型的分布。</p>
<p>高斯混合模型的核心思想是, 假设数据可以看作从多个高斯分布中生成出来
的。在该假设下, 每个单独的分模型 都是标准高斯模型, 其均值 <span class="math inline">\(u_i\)</span> 和方差 <span class="math inline">\(\sum_i\)</span> 是待估计的参数。此外,
每个分模型都还有一个参数 <span class="math inline">\(\pi_i\)</span>,
可以理解为权 重或生成数据的概率。高斯混合模型的公式为: <span class="math display">\[
p(x)=\sum_{i=1}^k \pi_i N\left(x \mid u_i, \sum_i\right)
\]</span> 通常我们并不能直接得到高斯混合模型的参数,
而是观察到了一系列数据点, 给出一个类别的数量K后, 希望求得最佳的 <span class="math inline">\(K\)</span>
个高斯分模型。<strong>因此，高斯混合模型的计算，便成了最佳的均值 <span class="math inline">\(\mu\)</span>, 方差 <span class="math inline">\(\Sigma 、\)</span> 权重 <span class="math inline">\(\pi\)</span> 的寻找, 这类问题通常
通过最大似然估计来求解。遗憾的是, 此问题中直接使用最大似然估计,
得到的是一个复杂的非凸函数, 目标函数 是和的对数,
难以展开和对其求偏导。</strong></p>
<p><strong>在这种情况下，可以用EM算法。
</strong>EM算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。具体到高
斯混合模型的求解，EM算法的迭代过程如下。</p>
<p>首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。</p>
<ul>
<li>E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。</li>
<li>M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。</li>
</ul>
<p>高斯混合模型是一个生成式模型。可以这样理解数据的生成过程，假设一个最简单的情况，即只有两个一维标准高斯分布的分模型<em>N</em>(0,1)和<em>N</em>(5,1)，其权重分别为0.7和0.3。那么，在生成第一个数据点时，先按照权重的比例，随机选择一个分布，比如选择第一个高斯分布，接着从<em>N</em>(0,1)中生成一个点，如−0.5，便是第一个数据点。在生成第二个数据点时，随机选择到第二个高斯分布<em>N</em>(5,1)，生成了第二个点4.7。如此循环执行，便生成出了所有的数据点。</p>
<p>也就是说，我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个
数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不
变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。</p>
<h4><span id="42-gmm与k-means相比">4.2 GMM与K-Means相比</span></h4>
<p>高斯混合模型与K均值算法的相同点是：</p>
<ul>
<li><strong>都需要指定K值</strong>；</li>
<li><strong>都是使用EM算法来求解</strong>；</li>
<li>都往往只能收敛于局部最优。</li>
</ul>
<p>而它相比于K
均值算法的优点是，可以给出一个样本属于某类的<strong>概率</strong>是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；<strong>并且可以用于生成新的样本点</strong>。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/20463356">K-means
笔记（三）数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//sofasofa.io/forum_main_post.php%3Fpostid%3D1000282">K-means
怎么选 K?</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161733843">K-Means：隐变量、聚类、EM</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1YHD4BN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1YHD4BN/" class="post-title-link" itemprop="url">聚类（2）DBSCAN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 17:24:29" itemprop="dateModified" datetime="2023-04-22T17:24:29+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">聚类</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-dbscan算法基于密度">一、DBSCAN算法【基于密度】</span></h3>
<blockquote>
<p>（3）聚类算法之DBSCAN算法 - GISer.Wang的文章 - 知乎
https://zhuanlan.zhihu.com/p/77043965</p>
</blockquote>
<p>密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。其代表算法为<strong>DBSCAN算法</strong>和<strong>密度最大值</strong>算法。</p>
<h4><span id="21-dbscan算法原理">2.1 DBSCAN算法原理</span></h4>
<p><strong><font color="red"> DBCSAN（Density-Based Spatial Clustering
of Applications with
Noise）是一个比较有代表性的基于密度的聚类算法。</font></strong>与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并<strong>可在有“噪声”的数据中发现任意形状的聚类</strong>。</p>
<h4><span id="22-若干概念">2.2 若干概念</span></h4>
<p><strong>DBSCAN是基于一组邻域来描述样本集的紧密程度的, 参数 <span class="math inline">\((\epsilon, M i n P t s)\)</span>
用来描述邻域的样本分布紧密程度。其 中, <span class="math inline">\(\epsilon\)</span> 描述了某一数据点的邻域距离阈值
(半径)</strong>, MinPts 描述了数据点半径为 <span class="math inline">\(\epsilon\)</span> 的邻域中数据点个数的最
小个数。下面是与密度聚类相关的定义 (假设我的样本集是 <span class="math inline">\(D=\left\{x_1, x_2, \ldots, x_m\right\}\)</span>
):</p>
<ul>
<li><p><strong>对象的 <span class="math inline">\(\varepsilon\)</span>
领域</strong>：给定对象在半径 <span class="math inline">\(\varepsilon\)</span> 内的区域; 对于 <span class="math inline">\(x_j \in D\)</span>, 其 <span class="math inline">\(\epsilon\)</span>-邻域包含样本集 <span class="math inline">\(D\)</span> 中与 <span class="math inline">\(x_j\)</span> 的距离不大于 <span class="math inline">\(\epsilon\)</span> 的子样本集。即 <span class="math inline">\(N_\epsilon\left(x_j\right)=\left\{x_i \in D \mid
\operatorname{distance}\left(x_i, x_j\right) \leq
\epsilon\right\}\)</span> ，这个子样本集的个数记为 <span class="math inline">\(\left|N_\epsilon\left(x_j\right)\right|\)</span>
。 <span class="math inline">\(\epsilon\)</span>-邻域是
一个集合</p></li>
<li><p><strong>核心对象</strong>: 对于任一样本 <span class="math inline">\(x_j \in D\)</span>, 如果其 <span class="math inline">\(\epsilon\)</span>-邻域对应的 <span class="math inline">\(N_\epsilon\left(x_j\right)\)</span> 至少包含
MinPts 个样本, 即如果 <span class="math inline">\(\left|N_\epsilon\left(x_j\right)\right|
\geq\)</span> MinPts ，则 <span class="math inline">\(x_j\)</span>
是核心对象。</p></li>
<li><p><strong>直接密度可达</strong>: 如果 <span class="math inline">\(x_i\)</span> 位于 <span class="math inline">\(x_j\)</span> 的 <span class="math inline">\(\epsilon\)</span>-邻域中, 且 <span class="math inline">\(x_j\)</span> 是核心对象, 则称 <span class="math inline">\(x_i\)</span> 由 <span class="math inline">\(x_j\)</span> 密度直达。反之不一定成 立,
即此时不能说 <span class="math inline">\(x_j\)</span> 由 <span class="math inline">\(x_i\)</span> 密度直达，除非 <span class="math inline">\(x_i\)</span> 也是核心对象,
即<strong>密度直达不满足对称性</strong>。如图 <span class="math inline">\(\varepsilon=1, \mathrm{~m}=5, \mathrm{q}\)</span>
是一个核心对象, 从对象q出发到对象p是直接密度可达的。</p></li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221648998.jpg" alt="2019-05-18-061126.jpg" style="zoom:50%;"></p>
<ul>
<li>*密度可达<strong>: 对于 <span class="math inline">\(x_i\)</span> 和
<span class="math inline">\(x_j\)</span> ，如果存在样本样本序列 <span class="math inline">\(p_1, p_2, \ldots, p_T\)</span> ，满足 <span class="math inline">\(p 1=x_i, p_T=x_j\)</span> ，且 <span class="math inline">\(p_{t+1}\)</span> 由 <span class="math inline">\(p_t\)</span> 密度 直达, 则称 <span class="math inline">\(x_j\)</span> 由 <span class="math inline">\(x_i\)</span> 密度可达。也就是说,
密度可达满足传递性。此时序列中的传递样本 <span class="math inline">\(p_1, p_2, \ldots, p_{T-1}\)</span> 均 为核心对象,
因为只有核心对象才能使其他样本密度直达。</strong>密度可达也不满足对称性**,
这个可以由密度直达的 不对称性得出。</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221648520.jpg" alt="2019-05-18-061154.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>密度相连</strong>：对于 <span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(x_j\)</span> ，如果存在核心对象样本 <span class="math inline">\(x_k\)</span>, 使 <span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(x_j\)</span> 均由 <span class="math inline">\(x_k\)</span> 密度可达, 则称 <span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(x_j\)</span> 密度
相连。密度相连关系满足对称性。</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221648223.jpg" alt="2019-05-18-061202.jpg" style="zoom:50%;"></p>
<ul>
<li><p><strong>簇</strong>：一个基于密度的簇是最大的密度相连对象的集合。</p></li>
<li><p><strong>噪声</strong>：不包含在任何簇中的对象称为噪声。</p></li>
</ul>
<p>从下图可以很容易看出理解上述定义，图中 MinPts <span class="math inline">\(=5\)</span> ，红色的点都是核心对象, 因为其 <span class="math inline">\(\epsilon\)</span>-邻域至少有 5 个
样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的圆内,
如果不在圆内,
则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列，此序列是一个簇集。在这些密度
可达的样本序列的 <span class="math inline">\(\epsilon\)</span>-邻域内所有的样本相互都是密度相连的
(<strong>注意, 此图中有两个簇集</strong>)。</p>
<p><img src="https://pic2.zhimg.com/80/v2-7d15fc871942e0287be42a12d6d615dd_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<h4><span id="23-dbscan密度聚类思想">2.3 DBSCAN密度聚类思想</span></h4>
<p><strong>DBSCAN的聚类定义很简单：由密度可达关系导出的最大密度相连的样本集合,
即为我们最终聚类的一个类别,
或者说一个簇。（注意是密度相连的集合）</strong>,
簇里面可以有一个或者多个核心对象。<strong>如果只有一个核心对象,
则簇里其他的非核心对象样本都在这个核心对象的 <span class="math inline">\(\epsilon\)</span>-邻域里; 如果有多个核心对象,
则簇里的任意一个核心对象的 <span class="math inline">\(\epsilon\)</span>
-邻域中一定有一个其他的核心对象,
否则这两个核心对象无法密度可达</strong>。这些核心对象的 <span class="math inline">\(\epsilon\)</span>-邻域里所有的样本的
集合组成的一个DBSCAN聚类笶。</p>
<p>那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够<strong>密度可达</strong>的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找<strong>密度可达</strong>的样本集合，这样就得到另一个聚类簇
<strong>（这样的得到都肯定是密度相连的）</strong>。一直运行到<strong>所有核心对象都有类别为止。</strong></p>
<p>基本上这就是DBSCAN算法的主要内容了，是不是很简单？<strong>但是我们还是有三个问题没有考虑。</strong></p>
<ul>
<li><strong>异常点问题：</strong>一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。</li>
<li><strong>距离度量问题</strong>：<strong><font color="red">
即如何计算某样本和核心对象样本的距离</font></strong>。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量<strong>样本距离，比如欧式距离、曼哈顿距离</strong>等。</li>
<li><strong>数据点优先级分配问题</strong>：例如某些样本可能到两个核心对象的距离都小于
<span class="math inline">\(\epsilon\)</span>,
但是这两个核心对象由于不是 密度直达, 又不属于同一个聚类笶,
那么如果界定这个样本的类别呢? 一般来说, 此时 DBSCAN采用先来后 到,
先进行聚类的类别簇会标记这个样本为它的类别。也就是说<strong>DBSCAN的算法不是完全稳定的算法</strong>。</li>
</ul>
<h4><span id="24-算法步骤">2.4 算法步骤</span></h4>
<p><strong>输入: 样本集</strong> <span class="math inline">\(D=\left\{x_1, x_2, \ldots, x_m\right\}\)</span>,
<strong>邻域参数</strong> <span class="math inline">\((\epsilon\)</span>, MinPts <span class="math inline">\()\)</span></p>
<ol type="1">
<li>初始化核心对象集合 <span class="math inline">\(\Omega=\emptyset\)</span>, 初始化类别 <span class="math inline">\(k=0\)</span></li>
<li>遍历 <span class="math inline">\(D\)</span> 的元素, 如果是核心对象,
则将其加入到核心对象集合 <span class="math inline">\(\Omega\)</span>
中</li>
<li>如果核心对象集合 <span class="math inline">\(\Omega\)</span>
中元素都已经被访问, 则算法结束, 否则转入步骤4.</li>
<li>在核心对象集合 <span class="math inline">\(\Omega\)</span> 中,
随机选择一个末访问的核心对象 <span class="math inline">\(o\)</span>,
首先将 <span class="math inline">\(o\)</span> 标记为已访问, 然后将 <span class="math inline">\(o\)</span> 标记类别 <span class="math inline">\(k\)</span> , 最后将 <span class="math inline">\(o\)</span> 的 <span class="math inline">\(\epsilon\)</span>-邻域中末访问的数据，存放到种子集合
Seeds 中。</li>
<li>如果种子集合 <span class="math inline">\(S e e d
s=\emptyset\)</span>, 则当前聚类簇 <span class="math inline">\(C_k\)</span> 生成完毕，且 <span class="math inline">\(k=k+1\)</span>, 跳转到3。否则, 从种子集合 <span class="math inline">\(S e e d s\)</span> 中挑选一个种子点 seed,
首先将其标记为已访问、标记类别 <span class="math inline">\(k\)</span>,
然后判断 seed 是否为核心对象, 如果是将 seed
中末访问的种子点加入到种子集合中, 跳转到 5 。</li>
</ol>
<p><strong>从上述算法可知：</strong></p>
<ul>
<li><strong>每个簇至少包含一个核心对象</strong>；</li>
<li>非核心对象可以是簇的一部分，构成了簇的边缘（edge）；</li>
<li>包含过少对象的簇被认为是噪声；</li>
</ul>
<h4><span id="25-总结">2.5 总结</span></h4>
<h5><span id="优点">优点</span></h5>
<ol type="1">
<li><strong>可以对任意形状的稠密数据集进行聚类</strong>，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li><strong>可以在聚类的同时发现异常点</strong>，对数据集中的异常点不敏感。</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>
<h5><span id="缺点">缺点</span></h5>
<ol type="1">
<li><strong>不能处理密度差异过大（密度不均匀）的聚类</strong>：如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
<li>如果样本集较大时，聚类收敛时间较长;<strong>此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进；</strong></li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂，<strong>主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响</strong>。【OPTICS算法】</li>
<li><strong>边界点不完全确定性</strong></li>
</ol>
<h4><span id="26-optics算法">2.6 OPTICS算法</span></h4>
<p><strong>OPTICS主要针对输入参数 <span class="math inline">\(\epsilon\)</span> 过敏感做的改进</strong>,
OPTICS和DBSCNA的输入参数一样 ( <span class="math inline">\(\epsilon\)</span> 和 MinPts ), 虽然
OPTICS算法中也需要两个输入参数, 但该算法对 <span class="math inline">\(\epsilon\)</span> 输入不敏感（一般将 <span class="math inline">\(\epsilon\)</span> 固定为无穷大）,
同时该算法中并不显式的生成数据聚类, 只是对数据集合中的对象进行排序,
得到一个有序的对象列表, 通过该有序列表, 可以得到 一个决策图,
通过决策图可以不同 <span class="math inline">\(\epsilon\)</span>
参数的数据集中检测笶集, 即: 先通过固定的 MinPts 和无穷大的 <span class="math inline">\(\epsilon\)</span>
得到有序列表，然后得到决策图，通过决策图可以知道当 <span class="math inline">\(\epsilon\)</span> 取特定值时（比如 <span class="math inline">\(\epsilon=3\)</span> ) 数据的聚类情况。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/X8CSWX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/X8CSWX/" class="post-title-link" itemprop="url">聚类（3）HDBSCAN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 17:24:41" itemprop="dateModified" datetime="2023-04-22T17:24:41+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">聚类</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-hdbscan聚类">一、HDBSCAN聚类</span></h3>
<p><strong>HDBSCAN 是由 Campello、Moulavi 和 Sander
开发的聚类算法。它通过将 DBSCAN
转换为层次聚类算法，然后用一种稳定的聚类技术提取出一个扁平的聚类来扩展
DBSCAN</strong>。这篇文章的目标是让你大致了解这个算法的工作原理及其背后的动机。与
HDBSCAN 的原论文不一样，我们这里将不将 DBSCAN
进行对照分析。作者这里更倾向将这算法类比成一种扁平聚类提取方法（ Robust
Single Linkage ）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets <span class="keyword">as</span> data</span><br><span class="line">%matplotlib inline</span><br><span class="line">sns.set_context(<span class="string">&#x27;poster&#x27;</span>)</span><br><span class="line">sns.set_style(<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">sns.set_color_codes()</span><br><span class="line">plot_kwds = &#123;<span class="string">&#x27;alpha&#x27;</span> : <span class="number">0.5</span>, <span class="string">&#x27;s&#x27;</span> : <span class="number">80</span>, <span class="string">&#x27;linewidths&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">moons, _ = data.make_moons(n_samples=<span class="number">50</span>, noise=<span class="number">0.05</span>)</span><br><span class="line">blobs, _ = data.make_blobs(n_samples=<span class="number">50</span>, centers=[(-<span class="number">0.75</span>,<span class="number">2.25</span>), (<span class="number">1.0</span>, <span class="number">2.0</span>)], cluster_std=<span class="number">0.25</span>)</span><br><span class="line">test_data = np.vstack([moons, blobs])</span><br><span class="line">plt.scatter(test_data.T[<span class="number">0</span>], test_data.T[<span class="number">1</span>], color=<span class="string">&#x27;b&#x27;</span>, **plot_kwds)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> hdbscan</span><br><span class="line">clusterer = hdbscan.HDBSCAN(min_cluster_size=<span class="number">5</span>, gen_min_span_tree=<span class="literal">True</span>)</span><br><span class="line">clusterer.fit(test_data)</span><br></pre></td></tr></table></figure>
<p><strong>现在我们已经对数据聚类完了——但实际发生了什么我们还不知道。我们将拆解成下面5个步骤来进行分析：</strong></p>
<ol type="1">
<li>根据 密度/稀疏度 进行<strong>空间转换</strong>。</li>
<li>构建基于加权距离图的最小生成树。</li>
<li>构建组件之间的层次簇结构。</li>
<li>用最小簇大小压缩层次聚类。</li>
<li>利用压缩好的生成树进行分类。</li>
</ol>
<h4><span id="11-空间转换">1.1 空间转换</span></h4>
<p><strong>聚类时我们希望在稀疏带噪声的数据中找到密度更高的族类——噪声的假设很重要</strong>：因为在真实情况下，数据都比较复杂，会有异常值的、缺失的数据和噪声等情况。算法的核心是单链聚类，它对噪声非常敏感：如果噪声数据点放在两个岛屿之间，可能就会将它们连在一起（也就是本来是两个族类的被分成一个）。显然，我们希望我们的算法对噪声具有鲁棒性，因此我们需要在运行单链算法之前找到一种方法来减少噪声。（作者用岛屿来比喻族类，海洋来表示噪声，下面“海洋”和“岛屿”代表这个意思。）</p>
<p><strong>我们要如何在不进行聚类的情况下找出“海洋”和“岛屿”？</strong>只要我们能够估算出样本集的密度，我们就可以认为密度较低的点都是“海洋”。要注意的是这里的目标不是完全区分出“海洋”和“岛屿”——现在只是聚类的初始步骤，并不是最终的输出——现在只是为了使我们的聚类中心对噪声更加鲁棒。因此，要识别出“海洋”的话，我们可以降低海平面（也就是加大容错范围）。出于实际目的，这意味着使每个“海洋”之间以及“海洋”与“岛屿”之间的距离会增加。</p>
<p>当然这只是直觉。它在实际中是如何工作的？我们需要一个计算量少的密度估计方式，简单到只要计算
k 个最近邻点的距离就可以。<strong><font color="red">
我们可以直接从一个距离矩阵（不管怎样后面都要生成的）中地读取到这个距离；或者，如果我们的指标支持（并且维度较低），用
<a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/neighbors.html%23k-d-tree">kd-trees</a>
来做这种检索就很适合。</font></strong>下面正式将点 x 的参数 k
定义为<strong>核心距离, 并表示为 <span class="math inline">\(\operatorname{core}_k(x)\)</span> (与DBSCAN、LOF
和 HDBSCAN 文献一样）。现在我们需要一种降维方法来拉开点之
间的距离（相对高维距离）。简单的方法是定义一种新距离公式，我们将其称为（与论文一样)相互可达距离
(mutual reachability distance)</strong>。相互可达距离的定义如下: <span class="math display">\[
d_{\mathrm{mreach}-k}(a, b)=\max \left\{\operatorname{core}_k(a),
\operatorname{core}_k(b), d(a, b)\right\}
\]</span> <strong>其中 d(a,b) 是 a 和 b
之间的原始距离</strong>。在这个度量下，密集点（具有低核心距离）之间的距离保持不变，但稀疏的点与其他点的距离被拉远到用core距离来计算。这有效地“降低了海平面”，减少稀疏的“海”点，同时使“陆地”保持原状。需要注意的是，显然
k 取值很关键；较大的 k
值将更多的点圈到“海”中。下面用图片来解析更容易理解，先让k=5。然后对于给定的点，我们可以为核心距离绘制一个圆刚好圈到第六个临近点（包括点本身），如下所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-dbf4559853b0d81f1c5ae2205a7b97a9_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p><strong>再选择另一个点</strong>，进行同样的操作，这次选到另一组临近点集合（其中一些点可能是上一组的临近点）。</p>
<p><img src="https://pic3.zhimg.com/80/v2-308c1bb09e8f779232aac217bee2507e_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>我们再选一个点再做一遍，得到第三组临近点。</p>
<p><img src="https://pic4.zhimg.com/80/v2-0e0e83ecf594b202cea6d3c208e45f0f_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>如果我们现在想知道蓝绿两组之间的相互可达距离，我们可以像下图，先画一个箭头连接蓝绿两个圆心：它穿过蓝色圆心，但不穿过绿色圆圈——绿色的核心距离大于蓝色和绿色之间的距离。<strong>因此，我们认为蓝色和绿色之间的相互可达距离更大——也就是绿色圆圈的半径（如果我们将一端设在绿色点上，则最容易想象）。</strong></p>
<p>实际上，有<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1506.06422v2.pdf">基础理论</a>可以证明<strong>相互可达距离</strong>作为一种变换，在允许单链接聚类的情况下，更接近层次水平上的真实密度分布。</p>
<h4><span id="12-构建最小生成树"><strong>1.2 构建最小生成树</strong></span></h4>
<p><strong>为了从密集的数据集上找到“岛屿”，现在我们有了新的指标：相互可达性</strong>。当然密集区是相对的，不同的“岛屿”可能会有不同的密度。<strong><font color="red">
理论上，我们要做的是：将数据当成是一个加权图，以数据点作为顶点，任意两点之间的边的权重等于这些点的相互可达距离。</font></strong></p>
<p>现在考虑一个阈值，一开始很高，然后逐渐变小。丢弃权重高于该阈值的任何边。我们删除边的同时，连接的组件从图里断开。最终，我们将拥有不同阈值级别的连接元件（从完全连接到完全断开）的层次结构。</p>
<p>实际当中，这样操纵非常耗时：有 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/412918565/edit#">n^2</a>
条边，我们不想多次计算连通组件算法。正确的做法是找到最小的边集，从这个集合中删除任何边都会导致组件的连接断开。但是我们还需要找到更多这样的边，使得找不到更小的边来连接组件。幸运的是，图论为我们提供了这样的东西：<strong>图的最小生成树</strong>。</p>
<p>我们可以<strong>通过 Prim
的算法非常有效地构建最小生成树</strong>——我们一次构建一条边，每次都把当前最小权重的边去连接一个尚未加入到树中的节点。可以看到下面HDBSCAN构造的树
；请注意，这是相互可达距离的最小生成树，与图中的纯距离不同。在这种情况下，我们的
k 值为 5。 在这个例子中，存在一种更快的方法，例如用 <strong>Dual Tree
Boruvka 来构建最小生成树。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">clusterer.minimum_spanning_tree_.plot(edge_cmap=<span class="string">&#x27;viridis&#x27;</span>, </span><br><span class="line">                                      edge_alpha=<span class="number">0.6</span>, </span><br><span class="line">                                      node_size=<span class="number">80</span>, </span><br><span class="line">                                      edge_linewidth=<span class="number">2</span>)   </span><br></pre></td></tr></table></figure>
<p><img src="https://pic2.zhimg.com/80/v2-a4b387ac9c43b1f681589529c82fe68d_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h4><span id="13-构建层次聚类">1.3 <strong>构建层次聚类</strong></span></h4>
<p><strong>给定最小生成树，下一步是将其转换为层次结构的组件</strong>。这最容易以相反的顺序完成：按距离（按递增顺序）对树的边进行排序，然后迭代，为每条边新建一个合并后的簇。这里唯一困难是识别每条边将连接在哪两个簇上，但这通过联合查找数据结构很容易。我们可以将结果视为树状图，如下所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-33a8cb27ae3ea0009e1ad77b438cf458_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>这图可以告诉我们这个鲁棒的单一链接会在哪挺下来。我们想知道；层次结构结构的聚类虽好，但我们想要的是一个扁平的聚类。我们可以通过在上图中画一条水平线并选择它穿过的聚类，来做到这一点。这实际上是
DBSCAN
里用到的操作（将只能切割成单一集群的作为噪声）。<strong>问题是，我们怎么知道在哪里画这条线？
DBSCAN 只是把它作为一个（非常不直观的）参数</strong>。</p>
<p>更糟糕的是，我们真的要用来处理可变密度的聚类，并希望任何切割线都是通过相互可达距离选出来的，并且今后固定在一个密度水平上。理想情况下，我们希望能够在不同的地方切割树，来选择我们的聚类。这是
HDBSCAN 下一步开始的地方，并与鲁棒的单一链接产生差异。</p>
<h4><span id="14-压缩聚类树">1.4 <strong>压缩聚类树</strong></span></h4>
<p>压缩聚类的第一步是将庞大而复杂的聚类层次结构压缩成一个较小的树，每个节点附加更多的数据。正如您在上面的层次结构中看到的那样，簇分裂通常是从一个簇中分离出一个或两个点；这就是关键点——与其将其视为一个聚类分裂成两个新聚类，不如将其视为一个“有损”的单个持久聚类。</p>
<p><strong>为了具体化，我们需要一个最小簇大小的概念，作为 HDBSCAN
的参数</strong>。一旦我们有了最小簇大小的值，我们现在可以遍历层次结构，并在每次拆分时询问拆分创建的新簇之一是否比最小簇大小少。如果我们的点数少于最小簇大小，我们将其声明为“离群点”，并让较大的簇保留父级的簇标识，标记哪些点“离群”
，以及需要的距离值。另一方面，如果要分裂成两个簇，每个簇至少与最小簇大小一样大才能进行分割。</p>
<p>在遍历整个层次结构并执行此操作后，我们最终得到了一个更小节点的树，每个节点都有关于该节点处的簇大小如何随距离变化而减小的数据。我们可以将其可视化为类似于上面的树状图——同样，我们可以让线的宽度代表簇中的点数。对于我们使用最小簇大小
5 的数据，结果如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clusterer.condensed_tree_.plot()</span><br></pre></td></tr></table></figure>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221714234.jpeg" alt="img" style="zoom:50%;"></p>
<p>这更容易观察和处理，尤其像我们当前测试数据一样简单的时候。然而，我们仍然需要挑出选聚类来用作平面聚类。上面的图应该会给你启发。</p>
<h4><span id="15-聚类抽取">1.5 <strong>聚类抽取</strong></span></h4>
<p>直觉上, 我们希望选择的簇持久且具有更长的生命周期;
短暂的簇最终可能只是单一链接方法的产 物。可以再看看之前的图, 我们可以说,
我们想要选择图中具有最大墨水面积的那些簇。为了进行 平面聚类,
我们需要添加进一步的要求：即如果您选择了一个簇,
则您不能选择它的任何它子簇。 当然我们需要明确下HDBSCAN实际的算法。首先,
我们需要将它转成一个具体的算法。首先, 我
们需要一个与距离不同的度量来衡量簇的持久性;</p>
<p>我们定义 <span class="math inline">\(\lambda=\frac{1}{\text {
distance }}\)</span> 。对于给定的簇, 我们可 以定义值 <span class="math inline">\(\lambda_{b i r t h}\)</span> and <span class="math inline">\(\lambda_{\text {death }}\)</span>
分别是对应笶分裂并成为自己的笟时的 <span class="math inline">\(\lambda\)</span> ，以及簇分裂成更小的簇时的 <span class="math inline">\(\lambda\)</span> 值 (如果有）。反过来,
对于给定的簇, 对于该簇中的每个点 <span class="math inline">\(\mathrm{p}\)</span>, 我们可以将值 <span class="math inline">\(\lambda_p\)</span> 定义为该点“离 群"的 lambda 值,
这是一个介于 <span class="math inline">\(\lambda_{b i r t h}\)</span> 和
<span class="math inline">\(\lambda_{\text {death }}\)</span> 之间的值,
因为离群点要么在簇生命周期的某个 时刻离开簇,
或者在簇分裂成两个较小的簇时离开簇。现在, 对于每个簇计算稳定性为 <span class="math display">\[
\sum_{p \in \text { cluster }}\left(\lambda_p-\lambda_{\text {birth
}}\right)
\]</span>
将所有叶节点声明为选定的簇。现在遍历树（反向拓扑排序）。如果子簇的稳定性之和大于笶的稳
定性, 那么我们将簇的稳定性设置为子笶的稳定性之和。另一方面,
如果簇的稳定性大于其子笶的 总和, 则我们将簇固定为一个簇,
并取消选择其所有子簇。一旦我们到达根节点, 我们将当前选定
的簇集称为我们的平面聚类并返回它。</p>
<p>好的, 解析了这么多,
但它实际上只是根据前面我们提到的“选择图中总墨水面积最大的簇"规则来
进行选择。我们可以通过这个算法在压缩树树图中选择簇,
最终会的到你想要的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clusterer.condensed_tree_.plot(select_clusters=<span class="literal">True</span>, selection_palette=sns.color_palette())</span><br></pre></td></tr></table></figure>
<p><strong>现在我们有了聚类, 用 sklearn API
可以很容易给每个簇打上标签</strong>。任何不在所选聚类中的点都只是一个噪声点（标为
-1）。不过, 我们可以做更多：对于每个簇, 我们有该笶中每个点 <span class="math inline">\(p\)</span> 的 <span class="math inline">\(\lambda
\_p\)</span> 值; 如果我们简单地将这些值归一化（它们的取值范围从 0 到
1），那么我们就可以衡量簇中每个 点的属于该笶的概率。</p>
<p>hdbscan
库将此作为分类对象的probabilities_属性返回。有了标签和对应的概率，我们就可以画个图,
不同的颜色代表不同的分类, 并根据概率大小降低该颜色的饱和度
（离群的点为纯灰色）。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221716022.jpeg" alt="img" style="zoom:50%;"></p>
<p>这就是 HDBSCAN
的工作原理。这可能看起来有些复杂——算法有相当多的部分——但实际上每个部分都非常简单，并可以容易掌握。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ol type="1">
<li><strong>图解HDBSCANS - Mr.g的文章</strong> - 知乎
https://zhuanlan.zhihu.com/p/412918565</li>
<li><font color="blue">原文: <a href="https://link.zhihu.com/?target=https%3A//nbviewer.jupyter.org/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/How%20HDBSCAN%20Works.ipynb">How
HDBSCAN works</a></font></li>
<li>聚类算法(Clustering Algorithms)之层次聚类(Hierarchical Clustering) -
小玉的文章 - 知乎 https://zhuanlan.zhihu.com/p/363879425</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1P85MAS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1P85MAS/" class="post-title-link" itemprop="url">聚类（4）总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 17:26:15" itemprop="dateModified" datetime="2023-04-22T17:26:15+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">聚类</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-聚类算法">一、聚类算法</span></h3>
<blockquote>
<p>常用聚类算法 - 小胡子的文章 - 知乎
https://zhuanlan.zhihu.com/p/104355127</p>
<p><strong>K-means, K-medians, K-mediods and K-centers</strong> -
仲基的文章 - 知乎 https://zhuanlan.zhihu.com/p/398600714</p>
</blockquote>
<p>什么是聚类算法？聚类是一种机器学习技术，它涉及到数据点的分组。给定一组数据点，我们可以使用聚类算法将每个数据点划分为一个特定的组。理论上，同一组中的数据点应该具有相似的属性和/或特征，而不同组中的数据点应该具有高度不同的属性和/或特征。<strong>聚类是一种无监督学习的方法</strong>，是许多领域中常用的统计数据分析技术。</p>
<p><strong>聚类算法主要包括以下五类：</strong></p>
<ul>
<li><strong>基于分层的聚类（hierarchical methods）</strong></li>
</ul>
<p>这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。<strong>代表算法有：<em>BIRCH算法</em>（1996）、<em>CURE算法</em>、CHAMELEON算法等。</strong></p>
<blockquote>
<p>层次聚类通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层，树的顶层是一个聚类的根节点。</p>
<p><strong>最小距离的层次聚类算法</strong>通过自下而上合并创建聚类树，合并算法通过计算两类数据点间的欧式距离来计算不同类别数据点间的相似度，对所有数据点中最为相似的两个数据点进行组合，组合后，最小距离（Single
Linkage）的计算方法是将两个组合数据点中距离最近的两个数据点间的距离作为这两个组合数据点的距离。并反复迭代这一过程。</p>
</blockquote>
<ul>
<li><strong>基于划分的聚类（partitioning methods）</strong></li>
</ul>
<p>给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：<strong><em>K-means算法</em>、<em>K-medoids算法</em>、<em>CLARANS算法</em></strong></p>
<ul>
<li><strong>基于密度的聚类（density-based methods）</strong></li>
</ul>
<p>基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有<strong>：<em>DBSCAN（Density-Based
Spatial Clustering of Applic with
Noise）算法（1996）</em>、<em>OPTICS（Ordering Points to Identify
Clustering
Structure）算法（1999）</em>、<em>DENCLUE算法（1998）</em>、<em>WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据）</em></strong></p>
<ul>
<li><strong>基于网格的聚类（grid-based methods）</strong></li>
</ul>
<p>这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：<strong><em>STING（Statistical
Information Grid）</em>、<em>CLIQUE（Clustering In
Quest）算法（1998）</em>、<em>WaveCluster算法</em>。</strong>其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。</p>
<ul>
<li><strong>基于模型的聚类（model-based methods）</strong></li>
</ul>
<p>基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。</p>
<h3><span id="二-聚类评估">二、聚类评估</span></h3>
<p>由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。例
如，K均值聚类可以用<strong>误差平方</strong>和来评估，但是基于密度的数据簇可能不是球形，
误差平方和则会失效。在许多情况下，判断聚类算法结果的好坏强烈依赖于主观解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。</p>
<p>聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结
果的质量。这一过程又分为三个子任务。</p>
<ol type="1">
<li><p><strong>估计聚类趋势。</strong></p>
<p>这一步骤是检测数据分布中是否存在非随机的簇结构。如果数据是基本随机
的，那么聚类的结果也是毫无意义的。我们可以观察聚类误差是否随聚类类别数
量的增加而单调变化，如果数据是基本随机的，即不存在非随机簇结构，那么聚
类误差随聚类类别数量增加而变化的幅度应该较不显著，并且也找不到一个合适
的K对应数据的真实簇数。</p></li>
<li><p><strong>判定数据簇数。</strong></p>
<p>确定聚类趋势之后，我们需要找到与真实数据分布最为吻合的簇数，据此判定聚类结果的质量。数据簇数的判定方法有很多，例如<strong>手肘法</strong>和<strong>Gap
Statistic</strong>方
法。需要说明的是，用于评估的最佳数据簇数可能与程序输出的簇数是不同的。
例如，有些聚类算法可以自动地确定数据的簇数，但可能与我们通过其他方法确定的最优数据簇数有所差别。</p></li>
<li><p><strong>测定聚类质量。</strong></p>
<p>在无监督的情况下，我们可以通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。定义评估指标可以展现面试者实际解决和分析问题的能力。事实上测量指标可以有很多种，以下列出了几种常用的度量指标，更多的指标可以阅读相关文献。</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3RD5TWZ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3RD5TWZ/" class="post-title-link" itemprop="url">集成学习（1）Random Forest</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:14:35" itemprop="dateCreated datePublished" datetime="2022-03-11T21:14:35+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 19:54:38" itemprop="dateModified" datetime="2023-04-21T19:54:38+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-random-forestbagging">一 、Random Forest（Bagging）</span></h3>
<p><strong>Random Forest（随机森林），用随机的方式建立一个森林。RF
算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。</strong></p>
<p>对于分类问题，其输出的类别是由个别树输出的众数所决定的。在回归问题中，把每一棵决策树的输出进行平均得到最终的回归结果。</p>
<ol type="1">
<li>随机森林具有<strong>防止过拟合能力</strong>，精度比大多数单个算法要好；</li>
<li>随机森林分类器可以<strong>处理缺失值</strong>；</li>
<li><strong>于有袋外数据(OOB)，可以在模型生成过程中取得真实误差的无偏估计，且不损失训练数据量在训练过程中，能够检测到feature间的互相影响，且可以得出feature的重要性，具有一定参考意义；</strong></li>
<li>每棵树可以独立、同时生成，容易做成<strong>并行化方法</strong>；</li>
<li>具有一定的特征选择能力。</li>
</ol>
<h4><span id="11-思想">1.1 思想</span></h4>
<p><strong>Random Forest (随机森林）是 Bagging 的扩展变体,
它在以决策树为基学习器构建 Bagging 集成的基础上, 进
一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF
包括四个部分</strong>：</p>
<ul>
<li><strong>样本随机</strong>：假设训练数据集共有 <span class="math inline">\(M\)</span> 个对象的数据,
从样本数据中采取有放回（Boostrap) 随机抽取 <span class="math inline">\(N\)</span> 个 样本 (因为是有放回抽取,
有些数据可能被选中多次, 有些数据可能不被选上), 每一次取出的样本不完全相
同，这些样本组成了决策树的训练数据集；</li>
<li><strong>特征随机</strong>：假设每个样本数据都有 <span class="math inline">\(K\)</span> 个特征, 从所有特征中随机地选取 <span class="math inline">\(k(k&lt;=K)\)</span> 个特征, 选择最佳分割
属性作为节点建立CART决策树, 决策树成长期间 <span class="math inline">\(k\)</span>
的大小始终不变（<strong>在Python中构造随机森林模型的时
候，默认取特征的个数 <span class="math inline">\(k\)</span> 是 <span class="math inline">\(K\)</span> 的平方根, 即 <span class="math inline">\(\sqrt{K}\)</span></strong> );</li>
<li>重复前面的步骤, 建立 <span class="math inline">\(m\)</span>
棵CART树, 这些树都要完全的成长且不被修剪, 这些树形成了森林;</li>
<li>根据这些树的预测结果进行投票,
决定样本的最后预测类别。（针对回归模型, 是根据这些决策树模型的平均
值来获取最终的结果)</li>
</ul>
<p>随机选择样本和 Bagging 相同，采用的是 Bootstrap
自助采样法；<strong>随机选择特征是指在每个节点在分裂过程中都是随机选择特征的</strong>（区别与每棵树随机选择一批特征）。</p>
<blockquote>
<p>这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。</p>
</blockquote>
<p>随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算不剪枝也不会出现过拟合。</p>
<h4><span id="12-处理缺失值的方法">1.2 处理缺失值的方法</span></h4>
<ul>
<li>方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是<strong>分类变量(categorical
var)缺失，用众数补上</strong>，如果是<strong>连续型变量(numerical
var)缺失，用中位数补</strong>。</li>
<li>方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity
matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用<strong>proximity矩阵进行加权平均的方法补缺失值</strong>。然后迭代4-6次，这个补缺失值的思想和KNN有些类似。</li>
</ul>
<h4><span id="13-优缺点">1.3 优缺点</span></h4>
<h5><span id="优点"><strong>优点</strong>：</span></h5>
<ol type="1">
<li><p><strong>模型准确率高</strong>：随机森林既可以处理分类问题，也可以处理回归问题，即使存在部分数据缺失的情况，随机森林也能保持很高的分类精度。</p></li>
<li><p><strong>能够处理数量庞大的高维度的特征</strong>，且不需要进行降维（因为特征子集是随机选择的）；</p></li>
<li><p><strong>易于并行化</strong>，在大数据集上有很大的优势；</p></li>
<li><p><strong>可解释性</strong>：可以生成树状结构，判断各个特征的重要性；</p>
<blockquote>
<p>在sklearn中，随机森林<strong>基于每棵树分裂时的GINI指数下降量</strong>来判断各个特征的重要性。但是这种方法存在一个问题：当特征是连续的时候或者是类别很多的离散特征时，该方法会将这些特征的重要性增加。</p>
<p>解决方法：对特征编码，使得特征的取值数量相近。</p>
</blockquote></li>
<li><p><strong>对异常值、缺失值不敏感；</strong></p></li>
<li><p><strong>随机森林有袋外数据（OOB），因此不需要单独划分交叉验证集</strong>。</p></li>
</ol>
<h5><span id="缺点">缺点：</span></h5>
<ul>
<li>随机森林解决回归问题的效果不如分类问题；（因为它的预测不是天生连续的，在解决回归问题时，随机森林并不能为训练数据以外的对象给出答案）</li>
<li><strong>树之间的相关性越大，错误率就越大</strong>；</li>
<li><strong>当训练数据噪声较大时，容易产生过拟合现象。</strong></li>
</ul>
<h4><span id="14-基学习期的选择">1.4 基学习期的选择？</span></h4>
<h5><span id="为什么集成学习的基分类器通常是决策树还有什么">为什么集成学习的基分类器通常是决策树？还有什么？</span></h5>
<p>基分类器通常是决策树：样本权重、方便调节、随机性；</p>
<ul>
<li><strong>决策树可以较方便地将样本权重整合到训练过程中，而不需要通过过采样来调整样本权重。</strong></li>
<li>树的表达能力和泛化能力，<strong>方便调节</strong>（可以通过树的层数来调节）</li>
<li>样本的扰动对决策树的影响较大，<strong><font color="red">
因此不同子样本集合生成的决策树基分类器随机性较大。这样的不稳定的分类器更适合作为基分类器。</font></strong>此外树节点分类时随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li>
</ul>
<h5><span id="可以将随机森林的基分类器由决策树替换成线性分类器或k-nn吗">可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></h5>
<p>Bagging主要好处是集成后的方差，比基分类器小。bagging采用的基分类，最好是本身对样本分布较为敏感。而线性分类器和K-NN都是较为稳定的分类器（参数模型？）甚至<strong>可能因为采样，而导致他们再训练中更难收敛，从而增大了集成分类器的偏差。</strong></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/36WS4DK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/36WS4DK/" class="post-title-link" itemprop="url">集成学习（2）Adaboost</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:14:35" itemprop="dateCreated datePublished" datetime="2022-03-11T21:14:35+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 21:38:43" itemprop="dateModified" datetime="2023-04-21T21:38:43+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-adaboost-boosting">一、Adaboost (Boosting)</span></h3>
<p>AdaBoost（Adaptive
Boosting，自适应增强），其自适应在于：<strong>前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。</strong></p>
<h4><span id="11-思想">1.1 思想</span></h4>
<p><strong>Adaboost 迭代算法有三步：</strong></p>
<ul>
<li>初始化训练样本的权值分布，每个样本具有相同权重；</li>
<li>训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；</li>
<li>将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，<strong>加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重</strong>。</li>
</ul>
<h4><span id="12-细节">1.2 细节</span></h4>
<h5><span id="121-损失函数"><strong>1.2.1 损失函数 </strong></span></h5>
<p>Adaboost
模型是<strong>加法模型</strong>，学习算法为<strong>前向分步学习算法</strong>，损失函数为<strong>指数函数的分类问题</strong>。</p>
<p><strong>加法模型</strong>：最终的强分类器是由若干个弱分类器<strong>加权平均</strong>得到的。</p>
<p><strong>前向分布学习算法</strong>：算法是通过一轮轮的弱学习器学习，<strong>利用前一个弱学习器的结果来更新后一个弱学习器的训练集权重</strong>。第
k 轮的强学习器为： <span class="math display">\[
F_{k}(x)=\sum_{i=1}^{k} \alpha_{i} f_{i}(x)=F_{k-1}(x)+\alpha_{k}
f_{k}(x)
\]</span></p>
<p><strong>定义损失函数为 <span class="math inline">\(\mathbf{n}\)</span>
个样本的指数损失函数：</strong> <span class="math display">\[
L(y, F)=\sum_{i=1}^n \exp \left(-y_i F_k\left(x_i\right)\right)
\]</span> 利用前向分布学习算法的关系可以得到： <span class="math display">\[
\begin{aligned}
L(y, F) &amp; =\sum_{i=1}^m \exp
\left[\left(-y_i\right)\left(F_{k-1}\left(x_i\right)+\alpha_k
f_k\left(x_i\right)\right)\right] \\
&amp; =\sum_{i=1}^m \exp \left[-y_i F_{k-1}\left(x_i\right)-y_i \alpha_k
f_k\left(x_i\right)\right] \\
&amp; =\sum_{i=1}^m \exp \left[-y_i F_{k-1}\left(x_i\right)\right] \exp
\left[-y_i \alpha_k f_k\left(x_i\right)\right]
\end{aligned}
\]</span> <strong>因为 <span class="math inline">\(F_{k-1}(x)\)</span>
已知, 所以令 <span class="math inline">\(w_{k, i}=\exp \left(-y_i
F_{k-1}\left(x_i\right)\right)\)</span>
，随着每一轮迭代而将这个式子带入损失函数, 损失函数转 化为:</strong>
<span class="math display">\[
L(y, F(x))=\sum_{i=1}^m w_{k, i} \exp \left[-y_i \alpha_k
f_k\left(x_i\right)\right]
\]</span> 我们求 <span class="math inline">\(f_k(x)\)</span> ，可以得到:
<span class="math display">\[
f_k(x)=\operatorname{argmin} \sum_{i=1}^m w_{k, i} I\left(y_i \neq
f_k\left(x_i\right)\right)
\]</span> 将 <span class="math inline">\(f_k(x)\)</span> 带入损失函数,
并对 <span class="math inline">\(\alpha\)</span> 求导, 使其等于
0，则就得到了: <span class="math display">\[
\alpha_k=\frac{1}{2} \log \frac{1-e_k}{e_k}
\]</span> 其中, <span class="math inline">\(e_k\)</span>
即为我们前面的<strong>分类误差率</strong>。 <span class="math display">\[
e_k=\frac{\sum_{i=1}^m w_{k i}^{\prime} I\left(y_i \neq
f_k\left(x_i\right)\right)}{\sum_{i=1}^m w_{k i}^{\prime}}=\sum_{i=1}^m
w_{k i} I\left(y_i \neq f_k\left(x_i\right)\right)
\]</span> 最后看样本权重的更新。利用 <span class="math inline">\(F_k(x)=F_{k-1}(x)+\alpha_k f_k(x)\)</span> 和
<span class="math inline">\(w_{k+1, i}=w_{k, i} e x p\left[-y_i \alpha_k
f_k(x, i)\right]\)</span>, 即可得: <span class="math display">\[
w_{k+1, i}=w_{k i} \exp \left[-y_i \alpha_k f_k\left(x_i\right)\right]
\]</span> 这样就得到了样本权重更新公式。</p>
<h5><span id="122-正则化">1.2.2 正则化</span></h5>
<p><strong>为了防止 Adaboost
过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长（learning
rate)</strong> 。 对于前面的弱学习器的迭代,加上正则化项 <span class="math inline">\(\mu\)</span> 我们有: <span class="math display">\[
F_k(x)=F_{k-1}(x)+\mu \alpha_k f_k(x)
\]</span> <span class="math inline">\(\mu\)</span> 的取值范围为 <span class="math inline">\(0&lt;\mu \leq 1\)</span>
。对于同样的训练集学习效果, 较小的 <span class="math inline">\(\mu\)</span> 意味着我们需要更多的弱学习器的迭代次
数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>
<h4><span id="13-优缺点">1.3 优缺点</span></h4>
<h5><span id="131-优点"><strong>1.3.1 优点</strong></span></h5>
<ol type="1">
<li>分类精度高；</li>
<li>可以<strong>用各种回归分类模型来构建弱学习器，非常灵活</strong>；</li>
<li>不容易发生过拟合。</li>
</ol>
<h5><span id="132-缺点"><strong>1.3.2 缺点</strong></span></h5>
<ol type="1">
<li>对异常点敏感，异常点会获得较高权重。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/23/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/">25</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/25/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
