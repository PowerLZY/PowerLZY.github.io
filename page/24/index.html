<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/24/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/24/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/24/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">254</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/36WS4DK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/36WS4DK/" class="post-title-link" itemprop="url">集成学习（2）Adaboost</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:14:35" itemprop="dateCreated datePublished" datetime="2022-03-11T21:14:35+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 21:38:43" itemprop="dateModified" datetime="2023-04-21T21:38:43+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-adaboost-boosting">一、Adaboost (Boosting)</span></h3>
<p>AdaBoost（Adaptive
Boosting，自适应增强），其自适应在于：<strong>前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。</strong></p>
<h4><span id="11-思想">1.1 思想</span></h4>
<p><strong>Adaboost 迭代算法有三步：</strong></p>
<ul>
<li>初始化训练样本的权值分布，每个样本具有相同权重；</li>
<li>训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；</li>
<li>将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，<strong>加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重</strong>。</li>
</ul>
<h4><span id="12-细节">1.2 细节</span></h4>
<h5><span id="121-损失函数"><strong>1.2.1 损失函数 </strong></span></h5>
<p>Adaboost
模型是<strong>加法模型</strong>，学习算法为<strong>前向分步学习算法</strong>，损失函数为<strong>指数函数的分类问题</strong>。</p>
<p><strong>加法模型</strong>：最终的强分类器是由若干个弱分类器<strong>加权平均</strong>得到的。</p>
<p><strong>前向分布学习算法</strong>：算法是通过一轮轮的弱学习器学习，<strong>利用前一个弱学习器的结果来更新后一个弱学习器的训练集权重</strong>。第
k 轮的强学习器为： <span class="math display">\[
F_{k}(x)=\sum_{i=1}^{k} \alpha_{i} f_{i}(x)=F_{k-1}(x)+\alpha_{k}
f_{k}(x)
\]</span></p>
<p><strong>定义损失函数为 <span class="math inline">\(\mathbf{n}\)</span>
个样本的指数损失函数：</strong> <span class="math display">\[
L(y, F)=\sum_{i=1}^n \exp \left(-y_i F_k\left(x_i\right)\right)
\]</span> 利用前向分布学习算法的关系可以得到： <span class="math display">\[
\begin{aligned}
L(y, F) &amp; =\sum_{i=1}^m \exp
\left[\left(-y_i\right)\left(F_{k-1}\left(x_i\right)+\alpha_k
f_k\left(x_i\right)\right)\right] \\
&amp; =\sum_{i=1}^m \exp \left[-y_i F_{k-1}\left(x_i\right)-y_i \alpha_k
f_k\left(x_i\right)\right] \\
&amp; =\sum_{i=1}^m \exp \left[-y_i F_{k-1}\left(x_i\right)\right] \exp
\left[-y_i \alpha_k f_k\left(x_i\right)\right]
\end{aligned}
\]</span> <strong>因为 <span class="math inline">\(F_{k-1}(x)\)</span>
已知, 所以令 <span class="math inline">\(w_{k, i}=\exp \left(-y_i
F_{k-1}\left(x_i\right)\right)\)</span>
，随着每一轮迭代而将这个式子带入损失函数, 损失函数转 化为:</strong>
<span class="math display">\[
L(y, F(x))=\sum_{i=1}^m w_{k, i} \exp \left[-y_i \alpha_k
f_k\left(x_i\right)\right]
\]</span> 我们求 <span class="math inline">\(f_k(x)\)</span> ，可以得到:
<span class="math display">\[
f_k(x)=\operatorname{argmin} \sum_{i=1}^m w_{k, i} I\left(y_i \neq
f_k\left(x_i\right)\right)
\]</span> 将 <span class="math inline">\(f_k(x)\)</span> 带入损失函数,
并对 <span class="math inline">\(\alpha\)</span> 求导, 使其等于
0，则就得到了: <span class="math display">\[
\alpha_k=\frac{1}{2} \log \frac{1-e_k}{e_k}
\]</span> 其中, <span class="math inline">\(e_k\)</span>
即为我们前面的<strong>分类误差率</strong>。 <span class="math display">\[
e_k=\frac{\sum_{i=1}^m w_{k i}^{\prime} I\left(y_i \neq
f_k\left(x_i\right)\right)}{\sum_{i=1}^m w_{k i}^{\prime}}=\sum_{i=1}^m
w_{k i} I\left(y_i \neq f_k\left(x_i\right)\right)
\]</span> 最后看样本权重的更新。利用 <span class="math inline">\(F_k(x)=F_{k-1}(x)+\alpha_k f_k(x)\)</span> 和
<span class="math inline">\(w_{k+1, i}=w_{k, i} e x p\left[-y_i \alpha_k
f_k(x, i)\right]\)</span>, 即可得: <span class="math display">\[
w_{k+1, i}=w_{k i} \exp \left[-y_i \alpha_k f_k\left(x_i\right)\right]
\]</span> 这样就得到了样本权重更新公式。</p>
<h5><span id="122-正则化">1.2.2 正则化</span></h5>
<p><strong>为了防止 Adaboost
过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长（learning
rate)</strong> 。 对于前面的弱学习器的迭代,加上正则化项 <span class="math inline">\(\mu\)</span> 我们有: <span class="math display">\[
F_k(x)=F_{k-1}(x)+\mu \alpha_k f_k(x)
\]</span> <span class="math inline">\(\mu\)</span> 的取值范围为 <span class="math inline">\(0&lt;\mu \leq 1\)</span>
。对于同样的训练集学习效果, 较小的 <span class="math inline">\(\mu\)</span> 意味着我们需要更多的弱学习器的迭代次
数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>
<h4><span id="13-优缺点">1.3 优缺点</span></h4>
<h5><span id="131-优点"><strong>1.3.1 优点</strong></span></h5>
<ol type="1">
<li>分类精度高；</li>
<li>可以<strong>用各种回归分类模型来构建弱学习器，非常灵活</strong>；</li>
<li>不容易发生过拟合。</li>
</ol>
<h5><span id="132-缺点"><strong>1.3.2 缺点</strong></span></h5>
<ol type="1">
<li>对异常点敏感，异常点会获得较高权重。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/F4BZ62/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/F4BZ62/" class="post-title-link" itemprop="url">集成学习（3）GBDT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:14:35" itemprop="dateCreated datePublished" datetime="2022-03-11T21:14:35+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 20:50:47" itemprop="dateModified" datetime="2023-04-21T20:50:47+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-gradient-boostingdecision-tree">一、Gradient Boosting
Decision Tree</span></h3>
<blockquote>
<p>https://www.cnblogs.com/modifyrong/p/7744987.html</p>
</blockquote>
<p><strong>GBDT（Gradient Boosting Decision
Tree）是一种迭代的决策树算法，该算法由多棵决策树组成，从名字中我们可以看出来它是属于
Boosting 策略。GBDT 是被公认的泛化能力较强的算法。</strong></p>
<h4><span id="11-思想">1.1 思想</span></h4>
<p><strong>GBDT是boosting算法的一种，按照boosting的思想，在GBDT算法的每一步，用一棵决策树去拟合当前学习器的残差，获得一个新的弱学习器。将这每一步的决策树组合起来，就得到了一个强学习器</strong>。GBDT
由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即
GB），和 Shrinkage（一个重要演变）</p>
<h5><span id="111-回归树regressiondecision-tree">1.1.1 回归树（Regression
Decision Tree）</span></h5>
<p>如果认为 GBDT
由很多分类树那就大错特错了（虽然调整后也可以分类）。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。GBDT
的核心在于累加所有树的结果作为最终结果，所以 GBDT
中的树都是<strong>回归树</strong>，不是分类树，这一点相当重要。</p>
<p><strong><font color="red">
回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。</font></strong></p>
<h5><span id="112-梯度迭代gradientboosting"><strong>1.1.2 梯度迭代（Gradient
Boosting）</strong></span></h5>
<p>上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，GBDT
的每一棵树都是以之前树得到的<strong>残差【负梯度】</strong>来更新目标值，这样每一棵树的<strong>值加起来</strong>即为
GBDT 的预测值。</p>
<p>模型的预测值可以表示为： <span class="math display">\[
F_{k}(x)=\sum_{i=1}^{k} f_{i}(x)
\]</span> <span class="math inline">\(f_i(x)\)</span>
为基模型与其权重的乘积, 模型的训练目标是使预测值 <span class="math inline">\(F_k(x)\)</span> 逼近真实值 <span class="math inline">\(\mathrm{y}\)</span>,
也就是说要让每个基模型的预测值逼近各自要预测的部分真实值。贪心的解决手段：每次只训练一个基模型。那么,
现在改写整体模型为迭代式: <span class="math display">\[
F_{k}(x)=F_{k-1}(x)+f_{k}(x)
\]</span>
其实很简单，其<strong>残差其实是最小均方损失函数关于预测值的反向梯度(划重点)</strong>：<strong>用负梯度的解作为样本新的真实值</strong>。基于残差
GBDT 容易对异常值敏感。 <span class="math display">\[
-\frac{\partial\left(\frac{1}{2}\left(y-F_{k}(x)\right)^{2}\right)}{\partial
F_{k}(x)}=y-F_{k}(x)
\]</span> 很明显后续的模型会对第 4
个值关注过多，这不是一种好的现象，所以一般回归类的损失函数会用<strong>绝对损失或者
Huber 损失函数</strong>来代替平方损失函数。 <span class="math display">\[
L(y, F)=|y-F|
\]</span></p>
<p><span class="math display">\[
L(y, F)= \begin{cases}\frac{1}{2}(y-F)^{2} &amp; |y-F| \leq \delta \\
\delta(|y-F|-\delta / 2) &amp; |y-F|&gt;\delta\end{cases}
\]</span></p>
<p>GBDT 的 Boosting 不同于 Adaboost 的 Boosting，<strong>GBDT
的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于
0</strong>，这样后面的树就能专注于那些被分错的样本。</p>
<blockquote>
<p><strong><font color="red">
最后补充一点拟合残差的问题，无论损失函数是什么形式，每个决策树拟合的都是负梯度。只有当损失函数是均方损失时，负梯度刚好是残差</font></strong>，也就是说<strong>拟合残差只是针对均方损失的特例</strong>，并不能说GBDT的迭代的过程是拟合残差。</p>
</blockquote>
<h5><span id="113缩减shrinkage添加权重-基数增大"><strong>1.1.3
缩减（Shrinkage）</strong>添加权重、基数增大</span></h5>
<blockquote>
<p><strong><font color="red">
gbdt中的步长和参数中的学习率作用是什么？详细讲一讲？</font></strong></p>
<ul>
<li>参数中的学习率用于梯度下降</li>
</ul>
</blockquote>
<p>Shrinkage
的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。</p>
<p>Shrinkage
不直接用残差修复误差，而是只修复一点点，把大步切成小步。<strong>本质上
Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight
降低时，基模型数会配合增大</strong>。</p>
<h4><span id="12-优缺点">1.2 优缺点</span></h4>
<h5><span id="优点"><strong>优点</strong></span></h5>
<ol type="1">
<li>可以自动进行特征组合，拟合非线性数据；在稠密数据集上泛化能力和表达能力很好。</li>
<li>可以灵活处理各种类型的数据，不需要对数据预处理和归一化。</li>
<li>预测可以并行，计算数据很快。</li>
</ol>
<h5><span id="缺点"><strong>缺点</strong></span></h5>
<ol type="1">
<li>对异常点敏感。</li>
</ol>
<h4><span id="13-gbdt-与-adaboost-的对比">1.3 GBDT 与 Adaboost 的对比</span></h4>
<h4><span id="相同"><strong>相同：</strong></span></h4>
<ol type="1">
<li>都是 Boosting 家族成员，使用弱分类器；</li>
<li>都使用前向分布算法；</li>
</ol>
<h4><span id="不同"><strong>不同：</strong></span></h4>
<ol type="1">
<li><strong>迭代思路不同</strong>：Adaboost
是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT
是通过算梯度来弥补模型的不足（利用残差）；</li>
<li><strong>损失函数不同</strong>：AdaBoost
采用的是<strong>指数损失</strong>，GBDT
使用的是<strong>绝对损失</strong>或者 <strong>Huber
损失函数</strong>；</li>
</ol>
<h4><span id="14gbdt算法用于分类问题"><strong><font color="red"> 1.4
GBDT算法用于分类问题</font></strong></span></h4>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/46445201</p>
</blockquote>
<p>将GBDT应用于回归问题，相对来说比较容易理解。因为<strong>回归问题的损失函数一般为平方差损失函数</strong>，这时的残差，恰好等于预测值与实际值之间的差值。每次拿一棵决策树去拟合这个差值，使得残差越来越小，这个过程还是比较intuitive的。</p>
<p><strong><font color="red">
将GBDT用于分类问题，类似于逻辑回归、FM模型用于分类问题，其实是在用一个线性模型或者包含交叉项的非
线性模型, 去拟合所谓的对数几率 <span class="math inline">\(\ln
\frac{p}{1-p}\)</span> 。</font></strong>而GBDT也是一样,
只是用一系列的梯度提升树去拟合这个对数几 率,
实际上最终得到的是一系列CART回归树。其分类模型可以表达为： <span class="math display">\[
P(y=1 \mid x)=\frac{1}{1+e^{-\sum_{m=0}^M h_m(x)}}
\]</span> 其中 <span class="math inline">\(h_m(x)\)</span>
就是学习到的决策树。清楚了这一点之后, 我们便可以参考逻辑回归, 单样本
<span class="math inline">\(\left(x_i, y_i\right)\)</span> 的损失函数可
以表达为<strong>交叉熵</strong>： <span class="math display">\[
\operatorname{loss}\left(x_i, y_i\right)=-y_i \log
\hat{y_i}-\left(1-y_i\right) \log \left(1-\hat{y_i}\right)
\]</span> 假设第 <span class="math inline">\(\mathrm{K}\)</span>
步迭代之后当前学习器为 <span class="math inline">\(F(x)=\sum_{m=0}^k
h_m(x)\)</span>, 将 <span class="math inline">\(\hat{y}_i\)</span>
的表达式带入之后, 可将损失函数写为: <span class="math display">\[
\operatorname{loss}\left(x_i, y_i \mid F(x)\right)=y_i \log
\left(1+e^{-F\left(x_i\right)}\right)+\left(1-y_i\right)\left[F\left(x_i\right)+\log
\left(1+e^{-F\left(x_i\right)}\right)\right]
\]</span> <strong>可以求得损失函数相对于当前学习器的负梯度为</strong>：
<span class="math display">\[
-\left.\frac{\partial l o s s}{\partial F(x)}\right|_{x_i,
y_i}=y_i-\frac{1}{1+e^{-F\left(x_i\right)}}=y_i-\hat{y_i}
\]</span> 可以看到, 同回归问题很类似, 下一棵决策树的训练样本为: <span class="math inline">\(\left\{x_i,
y_i-\hat{y}_i\right\}_{i=1}^n\)</span>, 其所需要拟合的残差为真实标签
与预测概率之差。于是便有下面GBDT应用于二分类的算法：</p>
<ul>
<li><span class="math inline">\(F_0(x)=h_0(x)=\log
\frac{p_1}{1-p_1}\)</span>, 其中 <span class="math inline">\(p_1\)</span> 是训练样本中 <span class="math inline">\(\mathrm{y}=1\)</span> 的比例,
利用先验信息来初始化学习器</li>
<li>For <span class="math inline">\(m=1,2, \ldots, M\)</span> :
<ul>
<li>计算 <span class="math inline">\(g_i=\hat{y}_i-y_i\)</span>,
并使用训练集 <span class="math inline">\(\left\{\left(x_i,-g_i\right)\right\}_{i=1}^n\)</span>
<strong>训练一棵回归树</strong> <span class="math inline">\(t_m(x)\)</span>, 其中 <span class="math inline">\(\hat{y}_i=\frac{1}{1+e^{-F_{m-1}(x)}}\)</span></li>
<li>通过一维最小化损失函数找到树的最优权重: <span class="math inline">\(\quad
\rho_m=\underset{\rho}{\operatorname{argmin}} \sum_i l o s s\left(x_i,
y_i \mid F_{m-1}(x)+\rho t_m(x)\right)\)</span></li>
</ul></li>
<li><strong>考虑shrinkage,</strong> 可得这一轮迭代之后的学习器 <span class="math inline">\(F_m(x)=F_{m-1}(x)+\alpha \rho_m t_m(x),
\alpha\)</span> 为学习率</li>
<li>得到最终学习器为： <span class="math inline">\(F_M(x)\)</span></li>
</ul>
<p>以上就是将GBDT应用于二分类问题的算法流程。类似地, 对于多分类问题,
则需要考虑以下softmax模型： <span class="math display">\[
\begin{array}{r}
P(y=1 \mid x)=\frac{e^{F_1(x)}}{\sum_{i=1}^k e^{F_i(x)}} \\
P(y=2 \mid x)=\frac{e^{F_2(x)}}{\sum_{i=1}^k e^{F_i(x)}} \\
\ldots \cdots \\
P(y=k \mid x)=\frac{e^{F_k(x)}}{\sum_{i=1}^k e^{F_i(x)}}
\end{array}
\]</span> <strong>其中 <span class="math inline">\(F_1 ... F_k\)</span>
是 <span class="math inline">\(k\)</span> 个不同的tree
ensemble。每一轮的训练实际上是训练了 <span class="math inline">\(k\)</span> 棵树去拟合softmax的每一个分支
模型的负梯度</strong>【one-hot中的一维】。softmax模型的单样本损失函数为:
<span class="math display">\[
\text { loss }=-\sum_{i=1}^k y_i \log P\left(y_i \mid
x\right)=-\sum_{i=1}^k y_i \log \frac{e^{F_i(x)}}{\sum_{j=1}^k
e^{F_j(x)}}
\]</span> 这里的 <span class="math inline">\(y_i(i=1 \ldots k)\)</span>
是样本label在 <span class="math inline">\(\mathbf{k}\)</span>
个类别上作one-hot编码之后的取值, 只有一维为 1 , 其余都是 <span class="math inline">\(\mathbf{0}\)</span> 。由以上表 达式不难推导: <span class="math display">\[
-\frac{\partial l o s s}{\partial
F_q}=y_q-\frac{e^{F_q(x)}}{\sum_{j=1}^k e^{F_j(x)}}=y_q-\hat{y_q}
\]</span> 可见, 这k棵树同样是拟合了样本的真实标签与预测概率之差,
与二分类的过程非常类似。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/SMMBFY/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/SMMBFY/" class="post-title-link" itemprop="url">集成学习（7）总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:14:35" itemprop="dateCreated datePublished" datetime="2022-03-11T21:14:35+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 21:39:13" itemprop="dateModified" datetime="2023-04-21T21:39:13+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-集成学习">一、集成学习</span></h2>
<p>常见的集成学习框架有三种：Bagging，Boosting 和
Stacking。三种集成学习框架在基学习器的产生和综合结果的方式上会有些区别，我们先做些简单的介绍。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304211722979.jpg" alt="【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细）" style="zoom: 67%;"></p>
<p>本文主要介绍基于集成学习的决策树，其主要通过不同学习框架生产基学习器，并综合所有基学习器的预测结果来改善单个基学习器的识别率和泛化性。</p>
<p><strong><font color="red">
模型的准确度可由偏差和方差共同决定：</font></strong> <span class="math display">\[
\text { Error }=\text { bias }^{2}+\operatorname{var}+\xi
\]</span></p>
<p><strong>模型总体期望：</strong> <span class="math display">\[
\begin{aligned}
E(F) &amp;=E\left(\sum_{i}^{m} r_{i} f_{i}\right) \\
&amp;=\sum_{i}^{m} r_{i} E\left(f_{i}\right)
\end{aligned}
\]</span> <strong>模型总体方差</strong>: <span class="math display">\[
\begin{aligned}
\operatorname{Var}(F) &amp;=\operatorname{Var}\left(\sum_{i}^{m} r_{i}
f_{i}\right) \\
&amp;=\sum_{i}^{m} \operatorname{Var}\left(r_{i} f_{i}\right)+\sum_{i
\neq j}^{m} \operatorname{Cov}\left(r_{i} f_{i}, r_{j} f_{j}\right) \\
&amp;=\sum_{i}^{m} r_{i}{ }^{2}
\operatorname{Var}\left(f_{i}\right)+\sum_{i \neq j}^{m} \rho r_{i}
r_{j} \sqrt{\operatorname{Var}\left(f_{i}\right)}
\sqrt{\operatorname{Var}\left(f_{j}\right)} \\
&amp;=m r^{2} \sigma^{2}+m(m-1) \rho r^{2} \sigma^{2} \\
&amp;=m r^{2} \sigma^{2}(1-\rho)+m^{2} r^{2} \sigma^{2} \rho
\end{aligned}
\]</span></p>
<table>
<colgroup>
<col style="width: 4%">
<col style="width: 31%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>集成学习</th>
<th>Bagging</th>
<th>Boosting</th>
<th>Stacking</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>思想</td>
<td>对训练集进行<strong>有放回抽样</strong>得到子训练集</td>
<td>基模型的训练是有<strong>顺序</strong>的，每个基模型都会在前一个基模型学习的基础上进行学习；基于贪心策略的前向加法</td>
<td><strong>预测值</strong>将作为训练样本的特征值，进行训练得到最终预测结果。</td>
</tr>
<tr class="even">
<td>样本抽样</td>
<td>有放回地抽取数据集</td>
<td>训练集不变</td>
<td></td>
</tr>
<tr class="odd">
<td>样本权重</td>
<td>样本权重相等</td>
<td>不断调整样本的权重</td>
<td></td>
</tr>
<tr class="even">
<td>优化目标</td>
<td>减小的是方差</td>
<td>减小的是偏差</td>
<td></td>
</tr>
<tr class="odd">
<td>基模型</td>
<td><strong>强模型（偏差低，方差高）</strong></td>
<td><strong>弱模型（偏差高，方差低）</strong>而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</td>
<td><strong>强模型（偏差低，方差高）</strong></td>
</tr>
<tr class="even">
<td>相关性</td>
<td></td>
<td>对于 Boosting
来说，由于基模型共用同一套训练集，所以基模型间具有强相关性，故模型间的相关系数近似等于
1</td>
<td></td>
</tr>
<tr class="odd">
<td>模型偏差</td>
<td><strong>整体模型的偏差与基模型近似</strong>。(<span class="math inline">\(\mu\)</span>)</td>
<td>基于贪心策略的前向加法，随着基模型数的增多，偏差减少。</td>
<td></td>
</tr>
<tr class="even">
<td>模型方差</td>
<td>随着<strong>模型的增加可以降低整体模型的方差</strong>，故其基模型需要为强模型；(<span class="math inline">\(\frac{\sigma^{2}(1-\rho)}{m}+\sigma^{2}
\rho\)</span>)</td>
<td><strong>整体模型的方差与基模型近似</strong>（<span class="math inline">\(\sigma^{2}\)</span>）</td>
<td></td>
</tr>
</tbody>
</table>
<h4><span id="11-bagging">1.1 Bagging</span></h4>
<p>Bagging 全称叫 <strong>Bootstrap
aggregating</strong>，，每个基学习器都会对训练集进行<strong>有放回抽样</strong>得到子训练集，比较著名的采样法为
0.632
自助法（<strong>Bootstrap</strong>）。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging
常用的综合方法是<strong>投票法</strong>，票数最多的类别为预测类别。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304211727148.jpg" alt="img" style="zoom: 50%;"></p>
<h4><span id="12-boosting">1.2 Boosting</span></h4>
<p><strong>Boosting
训练过程为阶梯状，基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果</strong>，用的比较多的综合方式为加权法。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304211727241.jpg" alt="img" style="zoom: 33%;"></p>
<h4><span id="13-stacking">1.3 Stacking</span></h4>
<p><strong>Stacking
是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行的预测，其预测值将作为训练样本的特征值，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终预测结果。</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304211728902.jpg" alt="img" style="zoom: 33%;"></p>
<p><font color="red">
那么，为什么集成学习会好于单个学习器呢？原因可能有三：</font></p>
<ul>
<li><p>训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；</p></li>
<li><p>假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；</p></li>
<li><p>可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。</p></li>
</ul>
<h4><span id="14stacking-vs-神经网络"><font color="red"> <strong>1.4
Stacking</strong> vs <strong>神经网络</strong></font></span></h4>
<blockquote>
<ul>
<li>https://zhuanlan.zhihu.com/p/32896968</li>
</ul>
<p><strong>本文的核心观点是提供一种对于stacking的理解，即与神经网络对照来看。</strong>当然，在<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/59769987/answer/269367049">阿萨姆：为什么做stacking之后，准确率反而降低了？</a>中我已经说过stacking不是万能药，但往往很有效。通过与神经网络的对比，读者可以从另一个角度加深对stacking的理解。</p>
</blockquote>
<h5><span id="141stacking是一种表示学习representation-learning">1.4.1
Stacking是一种表示学习(representation learning)</span></h5>
<p><strong>表示学习指的是模型从原始数据中自动抽取有效特征的过程</strong>，比如深度学习就是一种表示学习的方法。关于表示学习的理解可以参考：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/264417928/answer/283087276">阿萨姆：人工智能（AI）是如何处理数据的？</a></p>
<p>原始数据可能是杂乱无规律的。在stacking中，通过第一层的多个学习器后，有效的特征被学习出来了。从这个角度来看，stacking的第一层就是特征抽取的过程。在[1]的研究中，上排是未经stacking的数据，下排是经过stacking(多个无监督学习算法)处理后的数据，我们显著的发现红色和蓝色的数据在下排中分界更为明显。<strong>数据经过了压缩处理。这个小例子说明了，有效的stacking可以对原始数据中的特征有效的抽取</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304211729620.jpg" alt="img" style="zoom: 50%;"></p>
<h5><span id="142stacking和神经网络从某种角度看有异曲同工之妙神经网络也可以被看作是集成学习">1.4.2
<strong>Stacking和神经网络从某种角度看有异曲同工之妙，神经网络也可以被看作是集成学习</strong></span></h5>
<p>承接上一点，stacking的学习能力主要来自于对于特征的表示学习，这和神经网络的思路是一致的。这也是为什么我说“第一层”，“最后一层”。</p>
<p>而且神经网络也可以被看做是一种集成学习，主要取决于不同神经元、层对于不同特征的理解不同。从浅层到深层可以理解为一种从具体到抽象的过程。</p>
<p><strong>Stacking中的第一层可以等价于神经网络中的前
n-1层，而stacking中的最终分类层可以类比于神经网络中最后的输出层。</strong>不同点在于，<strong>stacking中不同的分类器通过异质来体现对于不同特征的表示</strong>，神经网络是从同质到异质的过程且有分布式表示的特点(distributed
representation)。Stacking中应该也有分布式的特点，主要表现在多个分类器的结果并非完全不同，而有很大程度的相同之处。</p>
<p>但同时这也提出了一个挑战，多个分类器应该尽量在保证效果好的同时尽量不同，stacking集成学习框架的对于基分类器的两个要求：</p>
<ul>
<li>差异化(diversity)要大</li>
<li>准确性(accuracy)要高</li>
</ul>
<h5><span id="143-stacking的输出层为什么用逻辑回归"><strong>1.4.3 Stacking
的输出层为什么用逻辑回归？</strong></span></h5>
<blockquote>
<p><strong>表示学习的过拟合问题</strong>：</p>
<ul>
<li>仅包含学习到的特征</li>
<li>交叉验证</li>
<li>简单模型：<strong>逻辑回归</strong></li>
</ul>
</blockquote>
<p>如果你看懂了上面的两点，你应该可以理解stacking的有效性主要来自于特征抽取。<strong>而表示学习中，如影随形的问题就是过拟合，试回想深度学习中的过拟合问题。</strong></p>
<p>在[3]中，周志华教授也重申了stacking在使用中的过拟合问题。因为第二层的特征来自于对于第一层数据的学习，那么第二层数据中的特征中不该包括原始特征，<strong>以降低过拟合的风险</strong>。举例：</p>
<ul>
<li>第二层数据特征：仅包含学习到的特征</li>
<li>第二层数据特征：包含学习到的特征 + 原始特征</li>
</ul>
<p>另一个例子是，stacking中一般都用交叉验证来避免过拟合，足可见这个问题的严重性。</p>
<p>为了降低过拟合的问题，第二层分类器应该是较为简单的分类器，广义线性如逻辑回归是一个不错的选择。<strong>在特征提取的过程中，我们已经使用了复杂的非线性变换，因此在输出层不需要复杂的分类器</strong>。这一点可以对比神经网络的激活函数或者输出层，都是很简单的函数，一点原因就是不需要复杂函数并能控制复杂度。</p>
<h5><span id="144stacking是否需要多层第一层的分类器是否越多越好"><strong>1.4.4
Stacking是否需要多层？第一层的分类器是否越多越好？</strong></span></h5>
<p>通过以上分析，stacking的表示学习不是来自于多层堆叠的效果，而是<strong>来自于不同学习器对于不同特征的学习能力</strong>，并有效的结合起来。一般来看，2层对于stacking足够了。多层的stacking会面临更加复杂的过拟合问题，且收益有限。</p>
<p>第一层分类器的数量对于特征学习应该有所帮助，<strong>经验角度看越多的基分类器越好。即使有所重复和高依赖性，我们依然可以通过特征选择来处理</strong>，问题不大。</p>
<h3><span id="二-偏差与方差">二、偏差与方差</span></h3>
<p>上节介绍了集成学习的基本概念，这节我们主要介绍下如何从偏差和方差的角度来理解集成学习。</p>
<h4><span id="21-集成学习的偏差与方差">2.1 集成学习的偏差与方差</span></h4>
<p><font color="red">
偏差（Bias）描述的是预测值和真实值之差；方差（Variance）描述的是预测值作为随机变量的离散程度。</font>放一场很经典的图：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304211727341.jpg" alt="img" style="zoom: 50%;"></p>
<p><strong>模型</strong>的<strong>偏差</strong>与<strong>方差</strong></p>
<ul>
<li><strong>偏差：</strong>描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的
High
Variance，点会很分散。低偏差对应的点都打在靶心附近，所以喵的很准，但不一定很稳；</li>
<li><strong>方差：</strong>描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图
High
Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。</li>
</ul>
<p>我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型，<strong>但并不是所有集成学习框架中的基模型都是弱模型</strong>。<strong>Bagging
和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting
中的基模型为弱模型（偏差高，方差低）</strong>。</p>
<h4><span id="22-bagging-的偏差与方差">2.2 Bagging 的偏差与方差</span></h4>
<ul>
<li><strong>整体模型的期望等于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。</strong></li>
<li><strong>整体模型的方差小于等于基模型的方差，当且仅当相关性为 1
时取等号，随着基模型数量增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。</strong>但是，模型的准确度一定会无限逼近于
1
吗？并不一定，当基模型数增加到一定程度时，方差公式第一项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。</li>
</ul>
<h4><span id="23-boosting-的偏差与方差">2.3 Boosting 的偏差与方差</span></h4>
<ul>
<li>整体模型的方差等于基模型的方差，如果基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting
框架中的基模型必须为弱模型。</li>
<li>此外 Boosting
框架中采用基于贪心策略的前向加法，整体模型的期望由基模型的期望累加而成，所以随着基模型数的增多，整体模型的期望值增加，整体模型的准确度提高。</li>
</ul>
<h4><span id="24-小结">2.4 小结</span></h4>
<ul>
<li>我们可以使用<strong>模型的偏差和方差来近似描述模型的准确度</strong>；</li>
<li>对于 Bagging
来说，整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型；</li>
<li>对于 Boosting
来说，整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</li>
</ul>
<h3><span id="三-gbdt-xgboost-lightgbm">三、GBDT、XGBoost、LIghtGBM</span></h3>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304212131924.jpg" alt="img" style="zoom: 50%;"></p>
<table style="width:100%;">
<colgroup>
<col style="width: 10%">
<col style="width: 29%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Boosting 算法</th>
<th>GBDT</th>
<th>XGBoost</th>
<th>LightGBM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>思想</strong></td>
<td>回归树、梯度迭代、缩减;<strong>GBDT
的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于
0</strong></td>
<td><strong>二阶导数、线性分类器、正则化</strong>、缩减、<strong>列抽样、并行化</strong></td>
<td><strong>更快的训练速度和更低的内存使用</strong></td>
</tr>
<tr class="even">
<td>目标函数</td>
<td><span class="math inline">\(y-F_{k}(x)\)</span></td>
<td><span class="math inline">\(\Omega\left(f_{t}\right)=\gamma
T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}\)</span></td>
<td><span class="math inline">\(\Omega\left(f_{t}\right)=\gamma
T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}\)</span></td>
</tr>
<tr class="odd">
<td>损失函数</td>
<td>最小均方损失函数、<strong>绝对损失或者 Huber 损失函数</strong></td>
<td>【线性】最小均方损失函数、sigmod和softmax</td>
<td>-</td>
</tr>
<tr class="even">
<td>基模型</td>
<td>CART模型</td>
<td>CART模型/ 回归模型</td>
<td>-</td>
</tr>
<tr class="odd">
<td>抽样算法</td>
<td>无</td>
<td><strong>列抽样</strong>：借鉴了<strong>随机森林</strong>的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</td>
<td><strong>单边梯度抽样算法；</strong>根据样本梯度来对梯度小的这边样本进行采样，一部分大梯度和随机分布</td>
</tr>
<tr class="even">
<td><strong>切分点算法</strong></td>
<td>CART模型</td>
<td><strong>预排序</strong>、<strong>贪心算法</strong>、<strong>近似算法（</strong>加权分位数缩略图<strong>）</strong></td>
<td><strong>直方图算法</strong>：内存消耗降低，计算代价减少；（不需要记录特征到样本的索引）</td>
</tr>
<tr class="odd">
<td><strong>缺失值算法</strong></td>
<td>CART模型</td>
<td><strong>稀疏感知算法</strong>：选择增益最大的枚举项即为最优<strong>缺省方向</strong>。【<strong><font color="red">
稀疏数据优化不足</font></strong>】【<strong>gblinear 补0</strong>】</td>
<td><strong>互斥特征捆绑算法</strong>：<strong>互斥</strong>指的是一些特征很少同时出现非0值。<strong>稀疏感知算法</strong>；【<strong>gblinear
补0</strong>】</td>
</tr>
<tr class="even">
<td><strong>建树策略</strong></td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Leaf-wise</strong>：每次分裂增益最大的叶子节点，直到达到停止条件。</td>
</tr>
<tr class="odd">
<td><strong>正则化</strong></td>
<td>无</td>
<td>L1 和 L2 正则化项</td>
<td>L1 和 L2 正则化项</td>
</tr>
<tr class="even">
<td><strong>Shrinkage（缩减）</strong></td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr class="odd">
<td>类别特征优化</td>
<td>无</td>
<td>无</td>
<td><strong>类别特征最优分割</strong>：<strong>many-vs-many</strong></td>
</tr>
<tr class="even">
<td>并行化设计</td>
<td>无</td>
<td><strong>块结构设计</strong>、</td>
<td><strong>特征并行</strong>、
<strong>数据并行</strong>、<strong>投票并行</strong></td>
</tr>
<tr class="odd">
<td><strong>缓存优化</strong></td>
<td>无</td>
<td>为每个线程分配一个连续的缓存区、<strong>“核外”块计算</strong></td>
<td>1、所有的特征都采用相同的方法获得梯度；2、其次，因为不需要存储特征到样本的索引，降低了存储消耗</td>
</tr>
<tr class="even">
<td><strong>缺点</strong></td>
<td>对异常点敏感；</td>
<td><strong>预排序</strong>：仍需要遍历数据集；<font color="red">
不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。</font></td>
<td><strong>内存更小</strong>： 索引值、特征值边bin、互斥特征捆绑;
<strong>速度更快</strong>：遍历直方图；单边梯度算法过滤掉梯度小的样本；基于
Leaf-wise
算法的增长策略构建树，减少了很多不必要的计算量；特征并行、数据并行方法加速计算</td>
</tr>
</tbody>
</table>
<h3><span id="四-集成学习qampa">四、集成学习Q&amp;A</span></h3>
<h4><span id="41为什么gbdt和随机森林稍好点都不太适用直接用高维稀疏特征训练集"><strong><font color="red"> 4.1
为什么gbdt和随机森林(稍好点)都不太适用直接用高维稀疏特征训练集？</font></strong></span></h4>
<h5><span id="原因">原因：</span></h5>
<p>gbdt这类boosting或者rf这些bagging集成分类器模型的算法，是典型的贪心算法，在当前节点总是选择对当前数据集来说最好的选择</p>
<p>一个6层100树的模型，要迭代2^(5 4 3 2 1
0)*100次<strong>,每次都根据当前节点最大熵或者最小误差分割来选择变量</strong></p>
<p><strong>那么，高维稀疏数据集里很多“小而美”的数据就被丢弃了</strong>，因为它对当前节点来说不是最佳分割方案(比如，关联分析里，支持度很低置信度很高的特征)</p>
<p>但是高维数据集里面，对特定的样本数据是有很强预测能力的，比如你买叶酸，买某些小的孕妇用品品类，对应这些人6个月后买奶粉概率高达40%，但叶酸和孕妇用品销量太小了，用户量全网万分之一都不到，这种特征肯定是被树算法舍弃的，哪怕这些特征很多很多。。它仍是被冷落的份。。。</p>
<h5><span id="方法lightgbm-互斥捆绑算法">方法：【LightGBM 互斥捆绑算法】</span></h5>
<h5><span id="选择svm和lr这种能提供最佳分割平面的算法可能会更好">选择svm和lr这种能提供最佳分割平面的算法可能会更好；</span></h5>
<p>但如果top.特征已经能够贡献很大的信息量了，比如刚才孕妇的案例，你用了一个孕妇用品一级类目的浏览次数购买金额购买次数这样的更大更强的特征包含了这些高维特征的信息量，那可能gbdt会更好</p>
<p>实际情况的数据集是，在数据仓库里的清洗阶段，你可以选择把它做成高维的特征，也可以选择用算法把它做成低维的特征，一般有</p>
<p>1-在数据清洗阶段，或用类目升级(三级类目升级到二三级类目)范围升级的方式来做特征，避免直接清洗出来高维特征</p>
<p>2-在特征生成后，<strong>利用数据分析结论简单直接的用多个高维特征合并</strong>(<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加减乘除&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A574865175%7D">加减乘除</a>逻辑判断都行，随你合并打分)的方式来做特征，前提你hold得住工作量判断量，但这个如果业务洞察力强效果有可能特别好</p>
<p>3-在特征工程的特征处理阶段，我们可以用<strong>PCA因子构建等降维算法做特征整合</strong>，对应训练集，也这么搞，到时候回归或预测的时候，就用这个因子或者主成分的值来做特征</p>
<h4><span id="41为什么集成学习的基分类器通常是决策树还有什么">4.1
为什么集成学习的基分类器通常是决策树？还有什么？</span></h4>
<p>基分类器通常是决策树：样本权重、方便调节、随机性；</p>
<ul>
<li><strong>决策树可以较方便地将样本权重整合到训练过程中，而不需要通过过采样来调整样本权重。</strong></li>
<li>树的表达能力和泛化能力，<strong>方便调节</strong>（可以通过树的层数来调节）</li>
<li>样本的扰动对决策树的影响较大，<strong><font color="red">
因此不同子样本集合生成的决策树基分类器随机性较大。这样的不稳定的分类器更适合作为基分类器。</font></strong>此外树节点分类时随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li>
</ul>
<h4><span id="42可以将随机森林的基分类器由决策树替换成线性分类器或k-nn吗">4.2
可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></h4>
<p>Bagging主要好处是集成后的方差，比基分类器小。bagging采用的基分类，最好是本身对样本分布较为敏感。而线性分类器和K-NN都是较为稳定的分类器（参数模型？）甚至可能因为采样，而导致他们再训练中更难收敛，从而增大了集成分类器的偏差。</p>
<h4><span id="43为什么可以利用gbdt算法实现特征组合和筛选gbdtlr"><strong><font color="red"> 4.3
为什么可以利用GBDT算法实现特征组合和筛选？【GBDT+LR】</font></strong></span></h4>
<p>GBDT模型是有一组有序的树模型组合起来的，前面的树是由对大多数样本有明显区分度的特征分裂构建而成，经过前面的树，仍然存在少数残差较大的样本，后面的树主要由能对这些少数样本有区分度的特征分裂构建。优先选择对整体有区分度的特征，然后再选择对少数样本有区分度的特征，这样才更加合理，所以<strong>GBDT子树节点分裂是一个特征选择的过程，而子树的多层结构则对特征组合的过程，最终实现特征的组合和筛选。</strong></p>
<p><strong>GBDT+LR融合方案：</strong></p>
<p>（1）利用GBDT模型训练数据，最终得到一系列弱分类器的cart树。</p>
<p>（2）<strong>生成新的训练数据。将原训练数据重新输入GBDT模型，对于每一个样本，都会经过模型的一系列树，对于每棵树，将样本落到的叶子节点置为1，其他叶子为0，然后将叶子节点的数字从左至右的拼接起来，形成该棵树的特征向量，最后将所有树的特征向量拼接起来，形成新的数据特征，之后保留原样本标签形成新的训练数据。</strong></p>
<p>（3）将上一步得到的训练数据作为输入数据输入到LR模型中进行训练</p>
<h3><span id="参考文献">参考文献</span></h3>
<ul>
<li><p><font color="red"> 【机器学习】决策树（中）——Random
Forest、Adaboost、GBDT
（非常详细）</font>:https://zhuanlan.zhihu.com/p/86263786</p></li>
<li><p>https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble</p></li>
<li><p>机器学习算法中GBDT与Adaboost的区别与联系是什么？ -
Frankenstein的回答 - 知乎
https://www.zhihu.com/question/54626685/answer/140610056</p></li>
<li><p>GBDT学习笔记 - 许辙的文章 - 知乎
https://zhuanlan.zhihu.com/p/169382376</p></li>
<li><p>GBDT - 王多鱼的文章 - 知乎
https://zhuanlan.zhihu.com/p/38057220</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3VQ3FZD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3VQ3FZD/" class="post-title-link" itemprop="url">集成学习（5）LightGBM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:13:30" itemprop="dateCreated datePublished" datetime="2022-03-11T21:13:30+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-21 21:19:58" itemprop="dateModified" datetime="2023-04-21T21:19:58+08:00">2023-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-lightgbm">一、LightGBM</span></h2>
<blockquote>
<ul>
<li>《<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=Lightgbm%3A+A+highly+efficient+gradient+boosting+decision+tree&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22165627712%22%7D">Lightgbm:
A highly efficient gradient boosting decision tree</a>》</li>
<li>《A communication-efficient <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=parallel+algorithm+for+decision+tree&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22165627712%22%7D">parallel
algorithm for decision tree</a>》</li>
</ul>
</blockquote>
<p>LightGBM 由微软提出，主要用于解决 GDBT
在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中。从 LightGBM
名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost
具有<strong>训练速度快、内存占用低</strong>的特点。下图分别显示了
XGBoost、XGBoost_hist（利用梯度直方图的 XGBoost） 和 LightGBM
三者之间针对不同数据集情况下的内存和训练时间的对比：</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304212057167.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>那么 LightGBM
到底如何做到<strong>更快的训练速度和更低的内存</strong>使用的呢？</p>
<p><strong><font color="red"> LightGBM
为了解决这些问题提出了以下几点解决方案：</font></strong></p>
<ol type="1">
<li><p><strong>【减小内存、最优分类点】直方图算法</strong>；【特征离散化
+ 内存占用 + 方差减少】</p></li>
<li><p><strong>【样本维度】 单边梯度抽样算法</strong>；</p>
<ul>
<li><p>【根据样本梯度来对梯度小的这边样本进行采样，一部分大梯度和随机分布】</p></li>
<li><p>一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。</p></li>
</ul></li>
<li><p><strong>【特征维度】互斥特征捆绑算法</strong>；【特征稀疏行优化
+分箱 】</p></li>
<li><p><strong>【分裂算法】基于最大深度的 Leaf-wise
的垂直生长算法</strong>；【深度限制的最大分裂收益的叶子】</p></li>
<li><p><strong>类别特征最优分割</strong>；</p></li>
<li><p><strong>特征并行和数据并行</strong>；</p></li>
<li><p><strong>缓存优化。</strong></p></li>
</ol>
<h3><span id="11-数学原理">1.1 数学原理</span></h3>
<h4><span id="111-直方图算法"><strong>1.1.1 直方图算法</strong></span></h4>
<h5><span id="1-直方图算法"><strong>(1) 直方图算法</strong></span></h5>
<p><strong><font color="red"> 直方图算法的基本思想是将连续的特征离散化为
k （默认256, 1字节）个离散特征，同时构造一个宽度为 k
的直方图用于统计信息（含有 k 个
bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin
即可找到最佳分裂点。</font></strong></p>
<p>我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以
k=256 为例）：</p>
<ul>
<li><strong>内存占用更小：</strong>XGBoost 需要用 32
位的浮点数去存储特征值，并用 32 位的整形去存储排序索引，而 LightGBM
只需要用 8 位去存储直方图，相当于减少了 1/8；</li>
<li><strong>计算代价更小：</strong>计算特征分裂增益时，XGBoost
需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k
次，直接将时间复杂度从代价是O( feature *
distinct_values_of_the_feature); 而 histogram 只需要计算 bins次, 代价是(
feature * bins)。distinct_values_of_the_feature &gt;&gt; bins</li>
</ul>
<p><strong>（2）直方图优化算法流程:</strong></p>
<ol type="1">
<li><strong>直方图优化算法需要在训练前预先把特征值转化为bin
value</strong>，也就是对每个特征的取值做个分段函数，将所有样本在该特征上的取值划分到某一段（bin）中。最终把特征取值从连续值转化成了离散值。需要注意得是：feature
value对应的bin value在整个训练过程中是不会改变的。</li>
<li><strong>最外面的 for
循环表示的意思是对当前模型下所有的叶子节点处理</strong>，需要遍历所有的特征，来找到增益最大的特征及其划分值，以此来分裂该叶子节点。</li>
<li>在某个叶子上，第二个 for
循环就开始遍历所有的特征了。<strong>对于每个特征，首先为其创建一个直方图
(new Histogram()
)</strong>。这个直方图存储了两类信息，分别是<strong><font color="red">
每个bin中样本的梯度之和 <span class="math inline">\(H[ f.bins[i]
].g\)</span>
</font></strong>，还有就是<strong>每个bin中样本数量</strong><span class="math inline">\(（H[f.bins[i]].n）\)</span></li>
<li>第三个 for
循环遍历所有样本，累积上述的两类统计值到样本所属的bin中。即直方图的每个
bin 中包含了一定的样本，在此计算每个 bin 中的样本的梯度之和并对 bin
中的样本记数。</li>
<li>最后一个for循环, 遍历所有bin, 分别以当前bin作为分割点,
累加其左边的bin至当前bin的梯度和（ <span class="math inline">\(\left.S_{L}\right)\)</span> 以及样本数量 <span class="math inline">\(\left(n_{L}\right)\)</span>,
并与父节点上的总梯度和 <span class="math inline">\(\left(S_{p}\right)\)</span> 以及总样本数量 <span class="math inline">\(\left(n_{p}\right)\)</span> 相减, 得到右边
所有bin的梯度和 <span class="math inline">\(\left(S_{R}\right)\)</span>
以及样本数量 <span class="math inline">\(\left(n_{R}\right)\)</span>,
带入公式, 计算出增益, 在遍历过程中取最大的增 益,
以此时的特征和bin的特征值作为分裂节点的特征和分裂特征取值。</li>
</ol>
<h5><span id="3-源码分析">(3) 源码分析</span></h5>
<blockquote>
<p>https://blog.csdn.net/anshuai_aw1/article/details/83040541</p>
<p><strong><font color="red">
『我爱机器学习』集成学习（四）LightGBM</font></strong>：https://www.hrwhisper.me/machine-learning-lightgbm/</p>
</blockquote>
<p>问题一：<strong>如何将特征映射到bin呢？即如何分桶？对于连续特征和类别特征分别怎么样处理？</strong></p>
<p>问题二：<strong>如何构建直方图？直方图算法累加的g是什么？难道没有二阶导数h吗？</strong></p>
<h5><span id="特征分桶">特征分桶：</span></h5>
<blockquote>
<p><strong>特征分桶的源码</strong>在<strong>bin.cpp</strong>文件和<strong>bin.h</strong>文件中。由于LGBM可以处理类别特征，因此对连续特征和类别特征的处理方式是不一样的。</p>
</blockquote>
<h5><span id="连续特征">连续特征:</span></h5>
<p>在<strong>bin.cpp</strong>中，我们可以看到<strong>GreedyFindBin</strong>函数和<strong>FindBinWithZeroAsOneBin</strong>函数，这两个函数得到了数值型特征取值（负数，0，正数）的各个bin的切分点，即bin_upper_bound。</p>
<h5><span id="greedyfindbin数值型根据特征不同取值的个数划分类别型">GreedyFindBin:
数值型根据特征不同取值的个数划分，类别型？？</span></h5>
<ul>
<li><em>特征取值计数的数组</em>、<em>特征的不同的取值的数组</em>、<em>特征有多少个不同的取值</em></li>
<li><strong>bin_upper_bound就是记录桶分界的数组</strong></li>
<li>特征取值数比max_bin数量少，直接取distinct_values的中点放置</li>
<li>特征取值数比max_bin来得大，说明几个特征值要共用一个bin
<ul>
<li>如果一个特征值的数目比mean_bin_size大，那么这些特征需要单独一个bin</li>
<li>剩下的特征取值的样本数平均每个剩下的bin：mean size for one bin</li>
</ul></li>
</ul>
<h5><span id="构建直方图">构建直方图：</span></h5>
<p>给定一个特征的值，我们现在已经可以转化为对应的bin了。现在我们就可以构建直方图了。</p>
<h5><span id="constructhistogram"><strong>ConstructHistogram</strong>：</span></h5>
<ul>
<li><strong>累加了一阶、二阶梯度和还有个数</strong></li>
<li>当然还有其它的版本，当is_constant_hessianis_constant_hessian为true的时候是不用二阶梯度的</li>
</ul>
<h5><span id="寻找最优切分点-缺失值处理-gain和xgb一样">寻找最优切分点 :
缺失值处理 + Gain和XGB一样</span></h5>
<h5><span id="4直方图算法优点"><strong><font color="red">
（4）直方图算法优点：</font></strong></span></h5>
<ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于
直方图算法，则只需要(1x样本数x维
度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的
bin
值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p></li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为<span class="math inline">\(k\)</span>的树的时间复杂度：对特征所有取值的排序为<span class="math inline">\(O(NlogN)\)</span>，<span class="math inline">\(N\)</span>为样本点数目，若有<span class="math inline">\(D\)</span>维特征，则<span class="math inline">\(O(kDNlogN)\)</span>，而直方图算法需要<span class="math inline">\(O(kD \times bin)\)</span> (bin是histogram
的横轴的数量，一般远小于样本数量<span class="math inline">\(N\)</span>)。</p></li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>两个维度</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的<span class="math inline">\(k\)</span>个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p></li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost
的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost
提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM
所使用直方图算法对 Cache
天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在
Cache Miss的问题。</p></li>
<li><p><strong>数据并行优化</strong>，用 histgoram
可以大幅降低通信代价。用 pre-sorted
算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst
在并行的时候也使用 histogram 进行通信。</p></li>
</ul>
<h5><span id="5直方图算法缺点">（5）直方图算法缺点：</span></h5>
<p><strong>当然，直方图算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。</strong>但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；<strong>较粗的分割点也有正则化的效果，可以有效地防止过拟合</strong>；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（GradientBoosting）的框架下没有太大的影响。</p>
<h4><span id="112-单边梯度抽样算法"><strong>1.1.2 单边梯度抽样算法</strong></span></h4>
<p><font color="red">
<strong>直方图算法仍有优化的空间</strong>，建立直方图的复杂度为O(<strong>feature
×
data</strong>)，如果能<strong>降低特征数</strong>或者<strong>降低样本数</strong>，训练的时间会大大减少。</font></p>
<p><strong>GBDT
算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法</strong>（Gradient-based
One-Side Sampling,
GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算锅中只需关注梯度高的样本，极大的减少了计算量。</p>
<ol type="1">
<li>根据<strong>梯度的绝对值</strong>将样本进行<strong>降序</strong>排序</li>
<li>选择前a×100%的样本，这些样本称为A</li>
<li>剩下的数据(1−a)×100的数据中，随机抽取b×100%的数据，这些样本称为B</li>
<li>在计算增益的时候，放大样本B中的梯度 (1−a)/b 倍</li>
<li>关于g，在具体的实现中是一阶梯度和二阶梯度的乘积，见Github的实现（LightGBM/src/boosting/goss.hpp）</li>
</ol>
<blockquote>
<p>a%（大梯度）+ (1-a)/ b * b % 的大梯度</p>
</blockquote>
<p><strong>使用GOSS进行采样,
使得训练算法更加的关注没有充分训练(under-trained)的样本,
并且只会稍微的改变原有的数据分布</strong>。原有的在特征值为 <span class="math inline">\(\mathrm{d}\)</span> 处分数据带来的增益可以定义为：
<span class="math display">\[
V_{j \mid O}(d)=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in O: x_{i
j} \leq d} g_{i}\right)^{2}}{n_{l \mid
O}^{j}(d)}+\frac{\left(\sum_{x_{i} \in O: x_{i j}&gt;d}
g_{i}\right)^{2}}{n_{r \mid O}^{j}(d)}\right)
\]</span> 其中: - O为在决策树待分裂节点的训练集 - <span class="math inline">\(n_{o}=\sum I\left(x_{i} \in O\right)\)</span> -
<span class="math inline">\(n_{l \mid O}^{j}(d)=\sum I\left[x_{i} \in O:
x_{i j} \leq d\right]\)</span> and <span class="math inline">\(n_{r \mid
O}^{j}(d)=\sum I\left[x_{i} \in O: x_{i j}&gt;d\right]\)</span></p>
<p><strong>而使用GOSS后, 增益定义为：</strong> <span class="math display">\[
V_{j \mid O}(d)=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in A_{l}}
g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}}
g_{i}\right)^{2}}{n_{l}^{j}(d)}+\frac{\left(\sum_{x_{i} \in A_{r}}
g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}}
g_{r}\right)^{2}}{n_{r}^{j}(d)}\right)
\]</span> 其中: - <span class="math inline">\(A_{l}=\left\{x_{i} \in A:
x_{i j} \leq d\right\}, A_{r}=\left\{x_{i} \in A: x_{i
j}&gt;d\right\}\)</span> - <span class="math inline">\(B_{l}=\left\{x_{i} \in B: x_{i j} \leq d\right\},
B_{r}=\left\{x_{i} \in B: x_{i j}&gt;d\right\}\)</span></p>
<p>实验表明，该做法并没有降低模型性能，反而还有一定提升。究其原因，应该是采样也会增加弱学习器的多样性，从而潜在地提升了模型的泛化能力，稍微有点像深度学习的dropout。</p>
<h4><span id="113互斥特征捆绑算法冲突小的特征可能与多个特征包组合特征集合"><strong>1.1.3
互斥特征捆绑算法</strong>【冲突小的特征可能与多个特征包组合】[特征集合]</span></h4>
<blockquote>
<p><strong>互斥指的是一些特征很少同时出现非0值</strong>【<strong>类似one-hot特征</strong>】</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/366234433">详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）</a></p>
</blockquote>
<p><strong><font color="red"> 互斥特征捆绑算法（Exclusive Feature
Bundling,
EFB）指出如果将一些特征进行合并，则可以降低特征数量。</font></strong>高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。</p>
<p><strong>1）首先介绍如何判定哪些特征应该捆绑在一起？</strong></p>
<p>EFB算法采用<strong>构图（build
graph）</strong>的思想，将特征作为节点，不互斥的特征之间进行连边，然后从图中找出所有的捆绑特征集合。其实学过数据结构里的图算法就了解过，这个问题基本就是<a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%9B%BE%E7%9D%80%E8%89%B2%E9%97%AE%E9%A2%98/8928655%3Ffr%3Daladdin">图着色问题</a>。但是图着色问题是一个<strong>NP-hard问题</strong>，不可能在多项式时间里找到最优解。</p>
<p>因此EFB采用了一种近似的贪心策略解决办法。<strong>它允许特征之间存在少数的样本点并不互斥（比如某些对应的样本
点之间并不同时为非 0 ）</strong>, 并设置一个最大冲突阈值 <span class="math inline">\(K\)</span> 。我们选择合适的 <span class="math inline">\(K\)</span> 值, 可以在准确度和训绩效率上获
得很好的trade-off (均衡)。</p>
<p><strong>下面给出EFB的特征捆绑的贪心策略流程：</strong></p>
<blockquote>
<p>（1）将特征作为图的顶点，对于<strong>不互斥的特征进行相连</strong>（存在同时不为0的样本），特征同时不为0的样本个数作为边的权重；
（2）根据顶点的度对特征进行降序排序，度越大表明特征与其他特征的冲突越大（越不太可能与其他特征进行捆绑）；【<strong>入度排序，转化为非零值个数排序</strong>】
（3）设置<strong>最大冲突阈值K</strong>，外层循环先对每一个上述排序好的特征，遍历已有的特征捆绑簇，如果发现该特征加入到该特征簇中的冲突数不会超过最大阈值K，则将该特征加入到该簇中。否则新建一个特征簇，将该特征加入到新建的簇中。</p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-743681d9fd6cebee11f0dcc607f2f687_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>上面时间的复杂度为 <span class="math inline">\(O(N^2)\)</span>，n为特征的数量，时间其实主要花费在建图上面，两两特征计算互斥程度的时间较长（2层for循环）。对于百万级别的特征数量来说，该复杂度仍是<strong>不可行的</strong>。为了提高效率，可以不再构建图，将特征直接按照非零值个数排序，将特征<strong>非零值个数</strong>类比为节点的度（即冲突程度)，因为更多的非零值更容易引起冲突。只是改进了排序策略，不再构建图，下面的for循环是一样的。</p>
<p><strong>2）如何将特征捆绑簇里面的所有特征捆绑（合并）为一个特征？</strong>【<strong>直方图偏移</strong>】</p>
<p>如何进行合并，最关键的是如何能将原始特征从合并好的特征进行分离出来。EFB采用的是加入一个<strong>偏移常量</strong>（offset）来解决。</p>
<blockquote>
<p>举个例子，我们绑定两个特征A和B，A取值范围为[0, 10)，B取值范围为[0,
20)。则我们可以加入一个偏移常量10，即将B的取值范围变为[10,30），然后合并后的特征范围就是[0,
30)，并且能很好的分离出原始特征~</p>
</blockquote>
<p>因为lgb中<strong>直方图算法</strong>对特征值进行了<strong>分桶</strong>（bin）操作，导致合并互斥特征变得更为简单。从上面伪码看到偏移常量offset直接对每个特征桶的数量累加就行，然后放入偏移常数数组（binRanges）中。</p>
<h4><span id="114-带深度限制的leaf-wise-算法"><strong>1.1.4 带深度限制的
Leaf-wise 算法</strong></span></h4>
<h5><span id="level-wise">Level-wise</span></h5>
<p>大多数GBDT框架使用的按层生长 (level-wise)
的决策树生长策略，Level-wise遍历一次数据可以同时分裂同一层的叶子，容易进行<strong>多线程优化</strong>，也好<strong>控制模型复杂度，不容易过拟合</strong>。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
<h5><span id="leaf-wise">Leaf-wise</span></h5>
<p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-76f2f27dd24fc452a9a65003e5cdd305_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4><span id="115lightgbm类别特征最优分割"><strong>1.1.5
LightGBM类别特征最优分割</strong></span></h4>
<blockquote>
<p>LightGBM中只需要提前将类别映射到非负整数即可(<code>integer-encoded categorical features</code>)</p>
</blockquote>
<p><strong>我们知道，LightGBM可以直接处理类别特征，而不需要对类别特征做额外的one-hot
encoding。那么LGB是如何实现的呢？</strong></p>
<p>类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足,
LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。<strong>LightGBM
采用 many-vs-many
的切分方式将类别特征分为两个子集，实现类别特征的最优切分</strong>。假设某维
特征有 k 个类别，则有<span class="math inline">\(2^k - 1\)</span>种可能,
时间复杂度为<span class="math inline">\(o(2^k)\)</span> ,LightGBM 基于
Fisher的 《<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=On+Grouping+For+Maximum+Homogeneity&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22165627712%22%7D">On
Grouping For Maximum Homogeneity</a>》论文实现了 O(klogk) 的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=时间复杂度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22165627712%22%7D">时间复杂度</a>。</p>
<p><strong>算法流程如下图所示</strong>，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序;
然后按照排序的结果依次枚举最优分割点。从下图可以看到, <span class="math inline">\(\frac{\operatorname{Sum}(y)}{\operatorname{Count}(y)}\)</span>为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。</p>
<p><img src="https://pic1.zhimg.com/v2-0f1b7024e9da8f09c75b7f8e436a5d24_b.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>在Expo数据集上的实验结果表明，相比0/1展开的方法，使用LightGBM支持的类别特征可以使训练速度加速8倍，并且精度一致。</strong>更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。</p>
<h3><span id="12-工程实现-并行计算">1.2 工程实现 - 并行计算</span></h3>
<h4><span id="121-特征并行优化最优划分点"><strong>1.2.1 特征并行</strong>【优化
最优划分点】</span></h4>
<p>传统的特征并行算法在于对数据进行垂直划分，然后使用<strong>不同机器找到不同特征的最优分裂点</strong>，<strong>基于通信整合得到最佳划分点</strong>，然后基于通信告知其他机器划分结果。在本小节中，<strong>工作的节点称为worker</strong></p>
<h5><span id="传统"><strong>传统：</strong></span></h5>
<ul>
<li>垂直划分数据<strong>（对特征划分）</strong>，<strong>不同的worker有不同的特征集</strong></li>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li><strong>具有全局最佳切分点的worker进行节点分裂，然后广播切分后的结果</strong>（<strong>左右子树的instance
indices</strong>）</li>
<li>其它worker根据收到的instance indices也进行划分</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-b0d10c5cd832402e4503e2c1220f7376_r.jpg" alt="preview" style="zoom: 67%;"></p>
<p><strong>传统的特征并行方法有个很大的缺点</strong>：</p>
<ul>
<li><strong>需要告知每台机器最终划分结果，增加了额外的复杂度</strong>（因为对数据进行垂直划分，每台机器所含数据不同，划分结果需要通过通信告知）；</li>
<li>无法加速split的过程，该过程复杂度为O(#data)O(#data)，当数据量大的时候效率不高；</li>
</ul>
<h5><span id="lightgbm"><strong>LightGBM</strong></span></h5>
<p><strong>LightGBM
则不进行数据垂直划分，每台机器都有训练集完整数据</strong>，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。</p>
<ul>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li>每个worker根据全局最佳切分点进行节点分裂</li>
</ul>
<h5><span id="缺点">缺点：</span></h5>
<ul>
<li>split过程的复杂度仍是O(#data)，当数据量大的时候效率不高</li>
<li><strong>每个worker保存所有数据，存储代价高</strong></li>
</ul>
<h4><span id="122-数据并行"><strong>1.2.2 数据并行</strong></span></h4>
<h5><span id="传统方法">传统方法：</span></h5>
<p>数据并行目标是并行化整个决策学习的过程：</p>
<ul>
<li>水平切分数据，<strong>不同的worker拥有部分数据</strong></li>
<li>每个worker根据本地数据构建局部直方图</li>
<li>合并所有的局部直方图得到全部直方图</li>
<li>根据全局直方图找到最优切分点并进行分裂</li>
</ul>
<figure>
<img src="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-data-parallelization.png" alt="LightGBM-data-parallelization">
<figcaption aria-hidden="true">LightGBM-data-parallelization</figcaption>
</figure>
<p>在第3步中，有两种合并的方式：</p>
<ul>
<li>采用点对点方式(point-to-point communication
algorithm)进行通讯，每个worker通讯量为$O(machine * feature * bin
$)。</li>
<li>采用collective communication algorithm(如“All
Reduce”)进行通讯（相当于有一个中心节点，通讯后在返回结果），每个worker的通讯量为$O(machine
* feature * bin $)。</li>
</ul>
<h5><span id="lightgbm中的数据并行">LightGBM中的数据并行</span></h5>
<ol type="1">
<li><strong>使用“Reduce
Scatter”将不同worker的不同特征的直方图合并，然后workers在局部合并的直方图中找到局部最优划分，最后同步全局最优划分。</strong></li>
<li>前面提到过，可以通过直方图作差法得到兄弟节点的直方图，因此只需要通信一个节点的直方图。</li>
</ol>
<p>传统的数据并行策略主要为水平划分数据，然后本地构建直方图并整合成全局直方图，最后在全局直方图中找出最佳划分点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为
$O(machine * feature * bin $); 如果使用集成的通信, 则通讯开销为 <span class="math inline">\(O(2 * feature * b i n)\)</span>.</p>
<p><strong>LightGBM 采用分散规约（Reduce
scatter）的方式将直方图整合的任务分摊到不同机器上，从而降低通信代价，并通过直方图做差进一步降低不同机器间的通信。</strong></p>
<h4><span id="123-投票并行"><strong>1.2.3 投票并行</strong></span></h4>
<p>LightGBM采用一种称为<strong>PV-Tree</strong>的算法进行投票并行(Voting
Parallel)，其实这本质上也是一种<strong>数据并行</strong>。PV-Tree和普通的决策树差不多，只是在寻找最优切分点上有所不同。</p>
<p>其算法伪代码描述如下：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304212108095.png" alt="LightGBM-pv-tree" style="zoom:50%;"></p>
<ol type="1">
<li>水平切分数据，不同的worker拥有部分数据。</li>
<li>Local voting:
<strong>每个worker构建直方图，找到top-k个最优的本地划分特征。</strong></li>
<li>Global voting:
<strong>中心节点聚合得到最优的top-2k个全局划分特征（top-2k是看对各个worker选择特征的个数进行计数，取最多的2k个）。</strong></li>
<li><strong>Best Attribute Identification</strong>：
<strong>中心节点向worker收集这top-2k个特征的直方图，并进行合并，然后计算得到全局的最优划分。</strong></li>
<li>中心节点将全局最优划分广播给所有的worker，worker进行本地划分。</li>
</ol>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304212107125.png" alt="LightGBM-voting-parallelization" style="zoom: 50%;"></p>
<p><strong>可以看出，PV-tree将原本需要 $O(machine * feature * bin $)
变为了 <span class="math inline">\(O(2 * feature * b i
n)\)</span>，通信开销得到降低。此外，可以证明，当每个worker的数据足够多的时候，top-2k个中包含全局最佳切分点的概率非常高。</strong></p>
<h4><span id="124-缓存优化"><strong>1.2.4 缓存优化</strong></span></h4>
<p>上边说到 XGBoost
的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost
提出缓存访问优化算法进行改进。</p>
<p>而 LightGBM 所使用直方图算法对 Cache 天生友好：</p>
<ol type="1">
<li>首先，<strong>所有的特征都采用相同的方法获得梯度</strong>（区别于不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中；</li>
<li>其次，因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在
Cache Miss的问题。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/21JWWCP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/21JWWCP/" class="post-title-link" itemprop="url">集成学习（4）XGBoost</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:13:30" itemprop="dateCreated datePublished" datetime="2022-03-11T21:13:30+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 17:28:11" itemprop="dateModified" datetime="2023-04-26T17:28:11+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>33 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-xgboost">一、XGBoost</span></h3>
<p>XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源
boosting tree 工具包，比常见的工具包快 10 倍以上。Xgboost 和 GBDT
两者都是 boosting
方法，除了工程实现、解决问题上的一些差异外，最大的不同就是<strong>目标函数</strong>的定义。故本文将从数学原理和工程实现上进行介绍，并在最后介绍下
Xgboost 的优点。</p>
<h3><span id="11-数学原理">1.1 数学原理</span></h3>
<h4><span id="111-目标函数"><strong>1.1.1 目标函数</strong></span></h4>
<p>我们知道 XGBoost 是由<span class="math inline">\(k\)</span>个基模型组成的一个加法运算式： <span class="math display">\[
\hat{y}_{i}=\sum_{t=1}^{k} f_{t}\left(x_{i}\right)
\]</span> <strong>损失函数：</strong> <span class="math display">\[
L=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)
\]</span>
我们知道模型的预测精度由模型的<strong>偏差</strong>和<strong>方差</strong>共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的<strong>损失函数<span class="math inline">\(L\)</span></strong>与抑<strong>制模型复杂度的正则项
<span class="math display">\[\Omega\]</span></strong>组成。支持<strong>决策树</strong>也支持<strong>线性模型</strong>。
<span class="math display">\[
O b j=\sum_{i=1}^{n} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{t=1}^{k}
\Omega\left(f_{t}\right)
\]</span> <strong>Boosting模型是向前加法：</strong> <span class="math display">\[
\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)
\]</span> 目标函数就可以写成： <span class="math display">\[
\begin{aligned}
O b j^{(t)} &amp;=\sum_{i=1}^{n} l\left(y_{i},
\hat{y}_{i}^{t}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\
&amp;=\sum_{i=1}^{n} l\left(y_{i},
\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+\sum_{i=1}^{t}
\Omega\left(f_{i}\right)
\end{aligned}
\]</span> 求此时最优化目标函数，就相当于求解 <span class="math display">\[f_{t}\left(x_{i}\right)\]</span>。根据泰勒展开式：
<span class="math display">\[
f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime
\prime}(x) \Delta x^{2}
\]</span> <font color="red"> <strong>我们把<span class="math inline">\(\hat{y}_{i}^{t-1}\)</span>,视为x， <span class="math inline">\(f_{t}\left(x_{i}\right)\)</span>视为<span class="math inline">\(\Delta
x\)</span>，故可以将目标函数写成</strong>：</font> <span class="math display">\[
O b j^{(t)}=\sum_{i=1}^{n}\left[l\left(y_{i},
\hat{y}_{i}^{t-1}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i}
f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t}
\Omega\left(f_{i}\right)
\]</span>
由于<strong>第一项为常数，对优化没有影响，所以我们只需要求出每一步损失函数的一阶导和二阶导的值</strong>【前t-1的结果和标签求】，然后最优化目标函数，就可以得到每一步的f(x),最后根据加法模型得到一个整体模型。
<span class="math display">\[
O b j^{(t)} \approx \sum_{i=1}^{n}\left[g_{i}
f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i}
f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t}
\Omega\left(f_{i}\right)
\]</span></p>
<blockquote>
<p>以<strong>平方损失函数</strong>【绝对值、hubor损失】为例（GBDT
残差）：</p>
<p><span class="math display">\[
  \sum_{i=1}^n\left(y_i-\left(\hat{y}_i^{t-1}+f_t\left(x_i\right)\right)\right)^2
  \]</span></p>
<p>其中 <span class="math inline">\(g_i\)</span> 为损失函数的一阶导,
<span class="math inline">\(h_i\)</span> 为损失函数的二阶导,
注意这里的导是对 <span class="math inline">\(\hat{y}_i^{t-1}\)</span>
求导。 <span class="math display">\[
  \begin{aligned}
  g_i &amp; =\frac{\partial\left(\hat{y}^{t-1}-y_i\right)^2}{\partial
\hat{y}^{t-1}}=2\left(\hat{y}^{t-1}-y_i\right) \\
  h_i &amp;
=\frac{\partial^2\left(\hat{y}^{t-1}-y_i\right)^2}{\hat{y}^{t-1}}=2
  \end{aligned}
  \]</span></p>
</blockquote>
<h4><span id="112基于决策树的目标函数"><strong>1.1.2
基于决策树的目标函数</strong></span></h4>
<p>我们知道 <strong>Xgboost
的基模型不仅支持决策树，还支持线性模型</strong>，这里我们主要介绍基于决策树的目标函数。</p>
<p>我们可以将决策树定义为 <span class="math inline">\(f_t(x)=w_{q(x)}\)</span>, <span class="math inline">\(x\)</span> 为某一样本, 这里的 <span class="math inline">\(q(x)\)</span> 代表了该样本在哪个叶子结点上, 而
<span class="math inline">\(w_q\)</span> 则 代表了叶子结点取值 <span class="math inline">\(w\)</span>, 所以 <span class="math inline">\(w_{q(x)}\)</span> 就代表了每个样本的取值 <span class="math inline">\(w\)</span> (即预测值)。</p>
<p><strong>决策树的复杂度可由叶子数 <span class="math inline">\(T\)</span> 组成</strong>, 叶子节点越少模型越简单,
此外叶子节点也不应该含有过高的权重 <span class="math inline">\(w\)</span> (类 比 <span class="math inline">\(L
R\)</span> 的每个变量的权重), 所以目标函数的正则项可以定义为: <span class="math display">\[
\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T}
w_{j}^{2}
\]</span>
即<strong>决策树模型的复杂度</strong>由生成的所有<strong>决策树的叶子节点数量</strong>，和所有<strong>节点权重所组成的向量的<span class="math inline">\(L2\)</span>范式</strong>共同决定。</p>
<p><img src="https://pic1.zhimg.com/80/v2-e0ab9287990a6098e4cdbc5a8cff4150_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>我们设 <span class="math inline">\(I_j=\left\{i \mid
q\left(x_i\right)=j\right\}\)</span> 为第 <span class="math inline">\(j\)</span> 个叶子节点的样本集合,
故我们的目标函数可以写成: <span class="math display">\[
\begin{aligned}
O b j^{(t)} &amp; \approx \sum_{i=1}^n\left[g_i
f_t\left(x_i\right)+\frac{1}{2} h_i
f_t^2\left(x_i\right)\right]+\Omega\left(f_t\right) \\
&amp; =\sum_{i=1}^n\left[g_i w_{q\left(x_i\right)}+\frac{1}{2} h_i
w_{q\left(x_i\right)}^2\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^T
w_j^2 \\
&amp; =\sum_{j=1}^T\left[\left(\sum_{i \in I_j} g_i\right)
w_j+\frac{1}{2}\left(\sum_{i \in I_j} h_i+\lambda\right)
w_j^2\right]+\gamma T
\end{aligned}
\]</span> 第二步是遍历所有的样本后求每个样本的损失函数,
但样本最终会落在叶子节点上, 所以我们也可以遍历叶子节 点,
然后获取叶子节点上的样本集合, 最后在求损失函数。即我们之前样本的集合,
现在都改写成叶子结点的集 合, 由于一个叶子结点有多个样本存在, 因此才有了
<span class="math inline">\(\sum_{i \in I_j} g_i\)</span> 和 <span class="math inline">\(\sum_{i \in I_j} h_i\)</span> 这两项, <span class="math inline">\(w_j\)</span> 为第 <span class="math inline">\(j\)</span> 个叶子节点取值。 <span class="math display">\[
O b j^{(t)}=\sum_{j=1}^T\left[G_j
w_j+\frac{1}{2}\left(H_j+\lambda\right) w_j^2\right]+\gamma T
\]</span> <font color="red"> 这里我们要注意 <span class="math inline">\(G_j\)</span> 和 <span class="math inline">\(H_j\)</span> 是前 <span class="math inline">\(t-1\)</span> 步得到的结果, 其值已知可视为常数,
只有最后一棵树的叶子节点 <span class="math inline">\(w_j\)</span> 不
确定，那么将目标函数对 <span class="math inline">\(w_j\)</span>
求一阶导，并令其等于 0 ，则可以求得叶子结点 <span class="math inline">\(j\)</span> 对应的权值：</font> <span class="math display">\[
w_j^*=-\frac{G_j}{H_j+\lambda}
\]</span> 所以<strong>目标函数可以化简为</strong>: <span class="math display">\[
\text { Obj }=-\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda}+\gamma
T
\]</span>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304212034506.jpg" alt="img" style="zoom: 67%;"></p>
<p>上图给出目标函数计算的例子, 求每个节点每个样本的一阶导数 <span class="math inline">\(g_i\)</span> 和二阶导数 <span class="math inline">\(h_i\)</span>, 然后针对每个节点对所含样
本求和得到的 <span class="math inline">\(G_j\)</span> 和 <span class="math inline">\(H_j\)</span>,
最后遍历决策树的节点即可得到目标函数。</p>
<h4><span id="113最优切分点划分算法"><strong>1.1.3
最优切分点划分算法</strong></span></h4>
<p><strong><font color="red"> 在决策树的生长过程中,
一个非常关键的问题是如何找到叶子的节点的最优切分点</font>,</strong>
Xgboost 支持两种分裂节点 的方法一一贪心算法和近似算法。</p>
<p><strong>1）贪心算法</strong></p>
<ol type="1">
<li><strong>从深度为 0 的树开始,
对每个叶节点枚举所有的可用特征;</strong></li>
<li>针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列,
通过线性扫描的方式来决定该特征的最
佳分裂点，并记录该特征的分裂收益;</li>
<li>选择收益最大的特征作为分裂特征, 用该特征的最佳分裂点作为分裂位置,
在该节点上分裂出左右两个新的叶 节点, 并为每个新节点关联对应的样本集?
?</li>
<li>回到第 1 步, 递归执行到满足特定条件为止。（树的深度、gamma）</li>
</ol>
<p><strong>那么如何计算每个特征的分裂收益呢?</strong></p>
<p>假设我们在某一节点完成特征分裂，则分列前的目标函数可以写为: <span class="math display">\[
O b
j_1=-\frac{1}{2}\left[\frac{\left(G_L+G_R\right)^2}{H_L+H_R+\lambda}\right]+\gamma
\]</span> 分裂后的目标函数为: <span class="math display">\[
O b
j_2=-\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}\right]+2
\gamma
\]</span> 则对于目标函数来说，分裂后的收益为：<strong>MAX【obj1 -
obj2（分裂后越小越好）】</strong> <span class="math display">\[
\text { Gain
}=\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{\left(G_L+G_R\right)^2}{H_L+H_R+\lambda}\right]-\gamma
\]</span> 注意该特征收益也可作为特征重要性输出的重要依据。</p>
<p>我们可以发现对于所有的分裂点 <span class="math inline">\(a\)</span>,
我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 <span class="math inline">\(G_L\)</span> 和 <span class="math inline">\(G_R\)</span>
。然后用上面的公式计算每个分割方案的分数就可以了。<strong><font color="red">
观察分裂后的收益, 我们会发现节点划分不一定会使得结 果变好,
因为我们有一个引入新叶子的偍罚项（gamma),
也就是说引入的分割带来的增益如果小于一个阀值的 时候,
我们可以剪掉这个分割。</font></strong></p>
<p><strong>2）近似算法</strong>【<strong>加权分位划分点</strong>】</p>
<p><strong>贪婪算法可以的到最优解，但当数据量太大时则无法读入内存进行计算</strong>，近似算法主要针对贪婪算法这一缺点给出了近似最优解。</p>
<p>对于每个特征，只考察分位点可以减少计算复杂度。该算法会首先根据<strong>特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中</strong>，然后聚合统计信息找到所有区间的最佳分裂点。在提出候选切分点时有两种策略：</p>
<ul>
<li><strong>Global</strong>：<strong>学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割</strong>；</li>
<li><strong>Local</strong>：每次分裂前将重新提出候选切分点。</li>
</ul>
<p><strong>下图给出近似算法的具体例子，以三分位为例：</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-5d1dd1673419599094bf44dd4b533ba9_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的<span class="math inline">\(G, H\)</span> 值，最终求解节点划分的增益。</p>
<h4><span id="114加权分位数缩略图xgboost-直方图算法"><strong>1.1.4
加权分位数缩略图</strong>[XGBoost 直方图算法]</span></h4>
<ul>
<li>第一个 for 循环：对特征 <span class="math inline">\(k\)</span>
根据该特征分布的分位数找到切割点的候选集合【直方图】 <span class="math inline">\(S_k=\left\{s_{k 1}, s_{k 2}, \ldots, s_{k
l}\right\}\)</span> 。XGBoost 支持 Global 策略和 Local 策略。</li>
<li>第二个 for 循环：针对每个特征的候选集合,
将样本映射到由该特征对应的候选点集构成的分桶区间中, 即 <span class="math inline">\(s_{k, v} \geq x_{j k}&gt;s_{k, v-1}\)</span> ，
对每个桶统计 <span class="math inline">\(G, H\)</span> 值,
最后在这些统计量上寻找最佳分裂点。</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-161382c979557b8bae1563a459cd1ed4_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>事实上， <strong>XGBoost
不是简单地按照样本个数进行分位，而是以二阶导数值<span class="math inline">\(h_{i}\)</span>作为样本的权重进行划分</strong>，如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-5f16246289eaa2a3ae72f971db198457_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<h5><span id="那么问题来了为什么要用h_i进行样本加权">那么问题来了：为什么要用
<span class="math inline">\(h_{i}\)</span>进行样本加权？</span></h5>
<p>我们知道模型的目标函数为: <span class="math display">\[
O b j^{(t)} \approx \sum_{i=1}^n\left[g_i
f_t\left(x_i\right)+\frac{1}{2} h_i
f_t^2\left(x_i\right)\right]+\sum_{i=1}^t \Omega\left(f_i\right)
\]</span> 我们稍作整理，便可以看出 <span class="math inline">\(h_i\)</span> 有对 loss 加权的作用。 <span class="math display">\[
\begin{aligned}
O b j^{(t)} &amp; \approx \sum_{i=1}^n\left[g_i
f_t\left(x_i\right)+\frac{1}{2} h_i
f_t^2\left(x_i\right)\right]+\sum_{i=1}^t \Omega\left(f_i\right) \\
&amp; =\sum_{i=1}^n\left[g_i f_t\left(x_i\right)+\frac{1}{2} h_i
f_t^2\left(x_i\right)+\frac{1}{2}
\frac{g_i^2}{h_i}\right]+\Omega\left(f_t\right)+C \\
&amp; =\sum_{i=1}^n \frac{1}{2}
h_i\left[f_t\left(x_i\right)-\left(-\frac{g_i}{h_i}\right)\right]^2+\Omega\left(f_t\right)+C
\end{aligned}
\]</span> 其中 <span class="math inline">\(\frac{1}{2}
\frac{g_i^2}{h_i}\)</span> 与 <span class="math inline">\(C\)</span>
皆为常数。我们可以看到 <span class="math inline">\(h_i\)</span>
就是平方损失函数中样本的权重。</p>
<p>对于样本权值相同的数据集来说，找到候选分位点已经有了解决方案（GK
算法），但是当样本权值不一样时，该如何找到候选分位点呢？（作者给出了一个
Weighted Quantile Sketch 算法，这里将不做介绍。）</p>
<h4><span id="xgboost的近似直方图算法也类似于lightgbm这里的直方图算法为什么慢"><strong><font color="red">
xgboost的近似直方图算法也类似于lightgbm这里的直方图算法?
为什么慢？</font></strong></span></h4>
<ul>
<li><strong>xgboost在每一层都动态构建直方图</strong>，
因为<strong>xgboost的直方图算法不是针对某个特定的feature</strong>，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而<strong>lightgbm中对每个特征都有一个直方图</strong>，所以构建一次直方图就够了。</li>
<li><strong>lightgbm有一些工程上的cache优化</strong></li>
</ul>
<h4><span id="115稀疏感知算法缺失值的处理"><strong>1.1.5
稀疏感知算法</strong>【<strong>缺失值的处理</strong>】</span></h4>
<blockquote>
<ul>
<li><strong>特征值缺失的样本无需遍历只需直接分配到左右节点</strong></li>
<li><strong>如果训练中没有数据缺失，预测时出现了数据缺失，则默认被分类到右节点.</strong>？
<ul>
<li>看c++源码是默认向左方向</li>
</ul></li>
</ul>
</blockquote>
<p>在决策树的第一篇文章中我们介绍 CART
树在应对数据缺失时的分裂策略【<strong>缺失代理</strong>】，XGBoost
也给出了其解决方案。XGBoost
在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。</p>
<p><strong>XGBoost提出的是在计算分割后的分数时，遇到缺失值，分别将缺失值带入左右两个分割节点，然后取最大值的方向为其默认方向。</strong>至于如何学到缺省值的分支，其实很简单，<strong>分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。</strong></p>
<p>在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而<strong>特征值缺失的样本无需遍历只需直接分配到左右节点</strong>，故算法所需遍历的样本量减少，下图可以看到稀疏感知算法比
basic 算法速度块了超过 50 倍。</p>
<p><img src="https://pic1.zhimg.com/80/v2-e065bea4b424ea2d13b25ed2e7004aa8_1440w.jpg" alt="img" style="zoom:67%;"></p>
<h4><span id="116-缩减与列采样"><strong>1.1.6 缩减与列采样</strong></span></h4>
<p>除了在目标函数中引入正则项，为了防止过拟合，XGBoost还引入了缩减(shrinkage)和列抽样（column
subsampling），通过在每一步的boosting中引入缩减系数，降低每个树和叶子对结果的影响；列采样是借鉴随机森林中的思想，根据反馈，列采样有时甚至比行抽样效果更好，同时，通过列采样能加速计算。</p>
<h3><span id="12-工程实现">1.2 工程实现</span></h3>
<h4><span id="121-块结构设计"><strong>1.2.1 块结构设计</strong></span></h4>
<p>我们知道，决策树的学习<strong>最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序</strong>。而
<strong><font color="red"> XGBoost
在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed
Sparse Columns
Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</font></strong></p>
<blockquote>
<p>预排序 + 块设计【独立】 + 稀疏矩阵存储</p>
</blockquote>
<ul>
<li><strong>每一个块结构包括一个或多个已经排序好的特征</strong>；</li>
<li><strong>缺失特征值将不进行排序</strong>；</li>
<li>每个特征会存储指向<strong>样本梯度统计值</strong>的索引，方便计算一阶导和二阶导数值；</li>
</ul>
<p>这种块结构存储的特征之间相互独立，方便计算机进行并行计算。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个<strong>特征的增益计算可以同时进行</strong>，这也是
Xgboost 能够实现分布式或者多线程计算的原因。</p>
<h4><span id="122缓存访问优化算法索引访问梯度统计-gt-缓存空间不连续"><strong>1.2.2
缓存访问优化算法</strong>【索引访问梯度统计 -&gt; 缓存空间不连续】</span></h4>
<p>块结构的设计可以减少节点分裂时的计算量，但<strong>特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续</strong>，这样会造成缓存命中率低，从而影响到算法的效率。</p>
<p>为了解决缓存命中率低的问题，XGBoost
提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。此外适当调整块大小，也可以有助于缓存优化。</p>
<p>于exact greedy算法中, 使用<strong>缓存预取（cache-aware
prefetching）</strong>。具体来说，<strong>对每个线程分配一个连续的buffer</strong>，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化）</p>
<h4><span id="123-核外块计算"><strong>1.2.3 “核外”块计算</strong></span></h4>
<p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，<strong>XGBoost
独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行</strong>。</p>
<p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p>
<ul>
<li><strong>块压缩：</strong>对 Block
进行按列压缩，并在读取时进行解压；</li>
<li><strong>块拆分：</strong>将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li>
</ul>
<h4><span id="124-xgboost损失函数">1.2.4 <strong>XGBoost损失函数</strong></span></h4>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_32103261/article/details/106664227">不平衡处理：xgboost
中scale_pos_weight、给样本设置权重weight、 自定义损失函数 和
简单复制正样本的区别</a></p>
</blockquote>
<p><strong>损失函数</strong>：损失函数描述了预测值和真实标签的差异，通过对损失函数的优化来获得对学习任务的一个近似求解方法。boosting类算法的损失函数的作用：
Boosting的框架, 无论是GBDT还是Adaboost, 其在每一轮迭代中,
<strong>根本没有理会损失函数具体是什么,
仅仅用到了损失函数的一阶导数通过随机梯度下降来参数更新</strong>。XGBoost是用了牛顿法进行的梯度更新。通过对损失进行分解得到一阶导数和二阶导数并通过牛顿法来迭代更新梯度。</p>
<h5><span id="1自定义损失函数">（1）<strong>自定义损失函数</strong></span></h5>
<p><strong>XGBOOST是一个非常灵活的模型</strong>，允许使用者根据实际使用场景调整<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=损失函数&amp;spm=1001.2101.3001.7020">损失函数</a>，对于常见的二分类问题一般使用的binary：logistic损失函数，其形式为：
<span class="math display">\[
J(\theta)=-\frac{1}{m} \sum_{i=1}^n\left(y^{(i)} \log
h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log
\left(1-h_\theta\left(x^{(i)}\right)\right)\right)
\]</span>
这个损失函数对于正类错分和负类错分给予的惩罚时相同的，但是<strong>对于不平衡数据集，或者某些特殊情况（两类错分代价不一样）的时候这样的损失函数就不再合理了。</strong></p>
<p>基于XGBoost的损失函数的分解求导，可以知道XGBoost的除正则项以外的核心影响因子是损失函数的1阶导和2阶导，所以对于任意的学习任务的损失函数，可以对其求一阶导数和二阶导数带入到XGBoost的自定义损失函数范式里面进行处理。
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">custom_obj</span>(<span class="params">pred, dtrain</span>):<span class="comment"># pred 和 dtrain 的顺序不能弄反</span></span><br><span class="line">    <span class="comment"># STEP1 获得label</span></span><br><span class="line">    label = dtrain.get_label()</span><br><span class="line">    <span class="comment"># STEP2 如果是二分类任务，需要让预测值通过sigmoid函数获得0～1之间的预测值</span></span><br><span class="line">    <span class="comment"># 如果是回归任务则下述任务不需要通过sigmoid</span></span><br><span class="line">    <span class="comment"># 分类任务sigmoid化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    sigmoid_pred = sigmoid(原始预测值)</span><br><span class="line">    <span class="comment">#回归任务</span></span><br><span class="line">    pred = 原始预测值</span><br><span class="line">    <span class="comment"># STEP3 一阶导和二阶导</span></span><br><span class="line">    grad = 一阶导</span><br><span class="line">    hess = 二阶导</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> grad, hess</span><br></pre></td></tr></table></figure></p>
<p>非平衡分类学习任务，例如首笔首期30+的风险建模任务，首期30+的逾期率比例相对ever30+的逾期率为1/3左右，<strong>通过修正占比少的正样本权重来对影响正样本对损失函数的贡献度，可以进一步提升模型的效果</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_binary_cross_entropy</span>(<span class="params">pred, dtrain,imbalance_alpha=<span class="number">10</span></span>):</span><br><span class="line">    <span class="comment"># retrieve data from dtrain matrix</span></span><br><span class="line">    label = dtrain.get_label()</span><br><span class="line">    <span class="comment"># compute the prediction with sigmoid</span></span><br><span class="line">    sigmoid_pred = <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-pred))</span><br><span class="line">    <span class="comment"># gradient</span></span><br><span class="line">    grad = -(imbalance_alpha ** label) * (label - sigmoid_pred)</span><br><span class="line">    hess = (imbalance_alpha ** label) * sigmoid_pred * (<span class="number">1.0</span> - sigmoid_pred)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> grad, hess </span><br></pre></td></tr></table></figure>
<h5><span id="2focal-loss">（2）Focal Loss</span></h5>
<p><strong>Focal Loss for Dense Object Detection 是ICCV2017的Best
student
paper,文章思路很简单但非常具有开拓性意义，效果也非常令人称赞。</strong></p>
<ul>
<li>大家还可以看知乎的讨论：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/63581984">如何评价 Kaiming 的 Focal
Loss for Dense Object Detection？</a></li>
<li>[机器学习] XGBoost
自定义损失函数-FocalLoss：https://blog.csdn.net/zwqjoy/article/details/109311133</li>
</ul>
<p>Focal
Loss的引入主要是为了解决难易样本数量不平衡（注意，有区别于正负样本数量不平衡）的问题，实际可以使用的范围非常广泛，为了方便解释，拿目标检测的应用场景来说明</p>
<p><strong>Focal Loss的主要思想就是改变损失函数.Focal
loss是在交叉熵损失函数基础上进行的修改</strong></p>
<p>单阶段的目标检测器通常会产生高达100k的候选目标，只有极少数是正样本，正负样本数量非常不平衡。我们在计算分类的时候常用的损失——交叉熵。<span class="math inline">\(y&#39;\)</span>是经过激活函数的输出，所以在0-1之间。可见普通的交叉熵对于正样本而言，输出概率越大损失越小。对于负样本而言，输出概率越小则损失越小。此时的损失函数在大量简单样本的迭代过程中比较缓慢且可能无法优化至最优。</p>
<p>为了解决<strong>正负样本不平衡</strong>的问题，我们通常会在交叉熵损失的前面加上一个参数<strong>平衡因子alpha</strong>，用来平衡正负样本本身的比例不均.
文中alpha取0.25，即正样本要比负样本占比小，这是因为负例易分。</p>
<h3><span id="13-优缺点">1.3 优缺点</span></h3>
<h4><span id="131-优点"><strong>1.3.1 优点</strong></span></h4>
<ol type="1">
<li><strong>精度更高：</strong>GBDT
只用到一阶<strong>泰勒展开</strong>，而 XGBoost
对损失函数进行了二阶泰勒展开。<strong>XGBoost
引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数</strong>；</li>
<li><strong>灵活性更强：</strong>GBDT 以 CART
作为<strong>基分类器</strong>，XGBoost 不仅支持 CART
还支持线性分类器，（使用线性分类器的 <strong>XGBoost 相当于带 L1 和 L2
正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）</strong>）。此外，XGBoost
工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li>
<li><strong>正则化：</strong>XGBoost
在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的
L2
范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</li>
<li><strong>Shrinkage（缩减）：</strong>相当于学习速率。XGBoost
在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li>
<li><strong>列抽样：</strong>XGBoost
借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</li>
<li><strong>缺失值处理：</strong>XGBoost
采用的稀疏感知算法极大的加快了节点分裂的速度；</li>
<li><strong>可以并行化操作：</strong>块结构可以很好的支持并行计算。</li>
</ol>
<h4><span id="132-缺点"><strong>1.3.2 缺点</strong></span></h4>
<ol type="1">
<li>虽然利用<strong>预排序</strong>和<strong>近似算法</strong>可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要<strong>遍历数据集</strong>；</li>
<li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要<strong>存储特征对应样本的梯度统计值的索引</strong>，相当于消耗了两倍的内存。</li>
</ol>
<h3><span id="二-xgboost常用参数">二、XGBoost常用参数</span></h3>
<h4><span id="xgboost的参数一共分为三类">XGBoost的参数一共分为三类：</span></h4>
<p><a target="_blank" rel="noopener" href="https://xgboost.apachecn.org/#/">完整参数请戳官方文档</a></p>
<p>1、<strong>通用参数</strong>：宏观函数控制。</p>
<p>2、<strong>Booster参数</strong>：控制每一步的booster(tree/regression)。booster参数一般可以调控模型的效果和计算代价。我们所说的调参，很这是大程度上都是在调整booster参数。</p>
<p>3、<strong>学习目标参数</strong>：控制训练目标的表现。我们对于问题的划分主要体现在学习目标参数上。比如我们要做分类还是回归，做二分类还是多分类，这都是目标参数所提供的。</p>
<h4><span id="通用参数">通用参数</span></h4>
<ol type="1">
<li><strong>booster</strong>：我们有两种参数选择，<code>gbtree</code>、<code>dart</code>和<code>gblinear</code>。gbtree、dart是采用树的结构来运行数据，而gblinear是基于线性模型。</li>
<li><strong>silent</strong>：静默模式，为<code>1</code>时模型运行不输出。</li>
<li><strong>nthread</strong>:
使用线程数，一般我们设置成<code>-1</code>,使用所有线程。如果有需要，我们设置成多少就是用多少线程。</li>
</ol>
<h4><span id="booster参数">Booster参数</span></h4>
<ol type="1">
<li><p><strong>n_estimator</strong>:
也作<code>num_boosting_rounds</code>这是生成的<strong>最大树的数目</strong>，也是最大的迭代次数。</p></li>
<li><p><strong>learning_rate</strong>:
有时也叫作<code>eta</code>，系统默认值为<code>0.3</code>,。<strong>每一步迭代的步长</strong>，很重要。太大了运行准确率不高，太小了运行速度慢。我们一般使用比默认值小一点，<code>0.1</code>左右就很好。</p></li>
<li><p><strong>gamma</strong>：系统默认为<code>0</code>,我们也常用<code>0</code>。在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。<code>gamma</code>指定了节点分裂所需的<strong>最小损失函数下降值</strong>。
这个参数的值越大，算法越保守。因为<code>gamma</code>值越大的时候，损失函数下降更多才可以分裂节点。所以树生成的时候更不容易分裂节点。范围:
<code>[0,∞]</code></p></li>
<li><p><strong>subsample</strong>：系统默认为<code>1</code>。这个参数控制对于每棵树，<strong>随机采样的比例</strong>。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。
典型值：<code>0.5-1</code>，<code>0.5</code>代表平均采样，防止过拟合.
范围: <code>(0,1]</code>，<strong>注意不可取0</strong></p></li>
<li><p><strong>colsample_bytree</strong>：系统默认值为1。我们一般设置成0.8左右。用来控制每棵<strong>随机采样的列数的占比</strong>(每一列是一个特征)。
典型值：<code>0.5-1</code>范围: <code>(0,1]</code></p></li>
<li><p><strong>colsample_bylevel</strong>：默认为1,我们也设置为1.这个就相比于前一个更加细致了，它指的是每棵树每次节点分裂的时候列采样的比例</p></li>
<li><p><strong>max_depth</strong>：
系统默认值为<code>6</code>，我们常用<code>3-10</code>之间的数字。这个值为<strong>树的最大深度</strong>。这个值是用来控制过拟合的。<code>max_depth</code>越大，模型学习的更加具体。设置为<code>0</code>代表没有限制，范围:
<code>[0,∞]</code></p></li>
<li><p><strong>max_delta_step</strong>：默认<code>0</code>,我们常用<code>0</code>.这个参数限制了<strong>每棵树权重改变的最大步长</strong>，如果这个参数的值为<code>0</code>,则意味着没有约束。如果他被赋予了某一个正值，则是这个算法更加保守。通常，这个参数我们不需要设置，但是<strong>当个类别的样本极不平衡的时候，这个参数对逻辑回归优化器是很有帮助的。</strong></p></li>
<li><p><strong>lambda</strong>:也称<code>reg_lambda</code>,默认值为<code>0</code>。<strong>权重的L2正则化项</strong>。(和Ridge
regression类似)。这个参数是用来控制XGBoost的正则化部分的。这个参数在减少过拟合上很有帮助。</p></li>
<li><p><strong>alpha</strong>:也称<code>reg_alpha</code>默认为<code>0</code>,权重的L1正则化项。(和Lasso
regression类似)。
可以应用在很高维度的情况下，使得算法的速度更快。</p></li>
<li><p><strong>scale_pos_weight</strong>：默认为<code>1</code>在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。通常可以将其设置为<strong>负样本的数目与正样本数目的比值</strong>。<strong>xgboost中scale_pos_weight、对样本进行weight设置和简单复制正样本得到的结果是一样的，本质上都是改变了训练的损失函数。通过自定义设置损失函数可得到验证。实际上基本思想都是通过过采样的方法处理不平衡数据。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (label  <span class="number">1.0</span>f) &#123;</span><br><span class="line">    w *= scale_pos_weight;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 见源码 src/objective/regression_obj.cu</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>在DMatrix里边设置每个样本的weight 是
怎样改变训练过程的呢，其实是改变训练的损失函数，源代码里的代码如下，可以看到对不同的样本赋予不同的权重实际上是影响了该样本在训练过程中贡献的损失，进而改变了一阶导和二阶导。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">_out_gpair[_idx] = GradientPair(Loss::FirstOrderGradient(p, label) * w,</span><br><span class="line">                   Loss::SecondOrderGradient(p, label) * w);</span><br><span class="line"><span class="comment"># 见源码 src/objective/regression_obj.cu</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h4><span id="学习目标参数">学习目标参数</span></h4>
<h5><span id="objective-缺省值reglinear">objective [缺省值=reg:linear]</span></h5>
<ul>
<li><code>reg:linear</code>– <strong>线性回归</strong></li>
<li><code>reg:logistic</code> – <strong>逻辑回归</strong></li>
<li><code>binary:logistic</code> – 二分类逻辑回归，输出为概率</li>
<li><code>binary:logitraw</code> – 二分类逻辑回归，输出的结果为wTx</li>
<li><code>count:poisson</code> –
计数问题的poisson回归，输出结果为poisson分布。在poisson回归中，max_delta_step的缺省值为0.7
(used to safeguard optimization)</li>
<li><code>multi:softmax</code> – 设置 XGBoost
使用softmax目标函数做多分类，需要设置参数num_class（类别个数）</li>
<li><code>multi:softprob</code> –
如同softmax，但是输出结果为ndata*nclass的向量，其中的值是每个数据分为每个类的概率。</li>
</ul>
<h5><span id="eval_metric缺省值通过目标函数选择">eval_metric
[缺省值=通过目标函数选择]</span></h5>
<ul>
<li><code>rmse</code>: <strong>均方根误差</strong></li>
<li><code>mae</code>: <strong>平均绝对值误差</strong></li>
<li><code>logloss</code>: negative log-likelihood</li>
<li><code>error</code>:
二分类错误率。其值通过错误分类数目与全部分类数目比值得到。对于预测，预测值大于0.5被认为是正类，其它归为负类。
error@t: 不同的划分阈值可以通过 ‘t’进行设置</li>
<li><code>merror</code>: 多分类错误率，计算公式为(wrong cases)/(all
cases)</li>
<li><code>mlogloss</code>: 多分类log损失</li>
<li><code>auc</code>: 曲线下的面积</li>
<li><code>ndcg</code>: Normalized Discounted Cumulative Gain</li>
<li><code>map</code>: 平均正确率</li>
</ul>
<p>一般来说，我们都会使用<code>xgboost.train(params, dtrain)</code>函数来训练我们的模型。这里的<code>params</code>指的是<code>booster</code>参数。</p>
<h3><span id="三-xgboostqampa">三、XGBoostQ&amp;A</span></h3>
<ul>
<li>推荐收藏 |
又有10道XGBoost面试题送给你：https://cloud.tencent.com/developer/article/1518305</li>
</ul>
<h4><span id="1-xgboost模型如果过拟合了怎么解决"><strong>1、XGBoost模型如果过拟合了怎么解决?</strong></span></h4>
<ul>
<li><strong>正则项</strong>：叶子结点的数目和叶子结点权重的L2模的平方</li>
<li><strong>列抽样</strong>：训练的时候只用一部分特征，不仅可以降低过拟合，还可以加速</li>
<li><strong>子采样</strong>：每轮计算可以不使用全部样本</li>
<li><strong>shrinkage</strong>:
步长(学习率)，消弱训练出的每棵树的影响，让后面的训练有更大的学习空间</li>
</ul>
<p>当出现过拟合时，有两类参数可以缓解：</p>
<p>第一类参数：用于<strong>直接控制模型的复杂度</strong>。包括<code>max_depth,min_child_weight,gamma</code>
等参数</p>
<p>第二类参数：用于<strong>增加随机性</strong>，从而使得模型在训练时对于噪音不敏感。包括<code>subsample,colsample_bytree</code></p>
<p>还有就是直接减小<code>learning rate</code>，但需要同时增加<code>estimator</code>
参数。</p>
<h4><span id="2-怎么理解决策树-xgboost能处理缺失值而有的模型svm对缺失值比较敏感呢">2、怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感呢?</span></h4>
<blockquote>
<p>微调的回答 - 知乎
https://www.zhihu.com/question/58230411/answer/242037063</p>
<p>XGBoost是一种<strong>boosting</strong>的集成学习模型：支持的弱学习器（即单个的学习器，也称基学习器）有<strong>树模型</strong>和<strong>线性模型</strong>（<strong>gblinear</strong>），默认为<strong>gbtree</strong>。</p>
<ul>
<li><p><strong>gblinear</strong>，<strong>由于线性模型不支持缺失值，会将缺失值填充为0</strong>；</p></li>
<li><p><strong>gbtree</strong>或者<strong>dart</strong>，则支持缺失值；</p></li>
</ul>
</blockquote>
<ul>
<li>工具包自动处理数据缺失<strong>不代表</strong>具体的算法可以<strong>处理缺失项</strong></li>
<li>对于有缺失的数据：以<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=决策树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A242037063%7D">决策树</a>为原型的模型<strong>优于</strong>依赖距离度量的模型</li>
</ul>
<h4><span id="3-histogram-vs-pre-sorted">3、 Histogram VS Pre-sorted</span></h4>
<h5><span id="pre-sorted">Pre-sorted</span></h5>
<p><strong>预排序还是有一定优点的，如果不用预排序的话，在分裂节点的时候，选中某一个特征后，需要对A按特征值大小进行排序，然后计算每个阈值的增益，这个过程需要花费很多时间</strong>。</p>
<p>预排序算法在计算最优分裂时，各个特征的增益可以并行计算，并且能精确地找到分割点。但是<strong>预排序后需要保存特征值及排序后的索引，因此需要消耗两倍于训练数据的内存，时间消耗大</strong>。另外预排序后，<strong>特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化，时间消耗也大</strong>。最后，在每一层，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样。</p>
<h5><span id="historgram">Historgram</span></h5>
<p>首先需要指出的是，XGBoost在寻找树的分裂节点的也是支持直方图算法的，就是论文中提到的近视搜索算法（Approximate
Algorithm）。<strong>只是，无论特征值是否为0，直方图算法都需要对特征的分箱值进行索引，因此对于大部分实际应用场景当中的稀疏数据优化不足。</strong></p>
<p>回过头来，为了能够发挥直方图算法的优化威力，LightGBM提出了另外两个新技术：<strong>单边梯度采样（Gradient-based
One-Side Sampling</strong>）和<strong>互斥特征合并（Exclusive Feature
Bundling）</strong>，<strong><font color="red">
在减少维度和下采样上面做了优化以后才能够将直方图算法发挥得淋漓尽致。</font></strong></p>
<h5><span id="4-xgboost中的树如何剪枝">4、<strong>Xgboost中的树如何剪枝？</strong></span></h5>
<p><strong>在loss中增加了正则项</strong>：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度在每次分裂时，如果分裂后增益小于设置的阈值，则不分裂，则对应于Gain需要大于0才会分裂。(预剪枝)</p>
<p>则对于目标函数来说，分裂后的收益为：<strong>MAX</strong>【<strong>obj1
- obj2 （分裂后越小越好）</strong>】</p>
<p><span class="math display">\[
\text { Gain
}=\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{\left(G_L+G_R\right)^2}{H_L+H_R+\lambda}\right]-\gamma
\]</span> 注意该特征收益也可作为特征重要性输出的重要依据。</p>
<p>我们可以发现对于所有的分裂点 <span class="math inline">\(a\)</span>,
我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 <span class="math inline">\(G_L\)</span> 和 <span class="math inline">\(G_R\)</span>
。然后用上面的公式计算每个分割方案的分数就可以了。<font color="red">观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入<strong>新叶子的惩罚项（gamma)</strong>，也就是说引入的分割带来的<strong>增益如果小于一个阀值</strong>的时候，我们可以剪掉这个分割。
</font></p>
<ul>
<li>当一次分裂后，计算新生成的左、右叶子节点样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会收回此次分裂。</li>
<li>完成完整一棵树的分裂之后，再从底到顶反向检查是否有不满足分裂条件的结点，进行回溯剪枝。</li>
</ul>
<h4><span id="5-xgboost采样是有放回还是无放回的">5、<strong>Xgboost采样是有放回还是无放回的？</strong></span></h4>
<p>xgboost时基于boosting的方法，样本是不放回的 ，每轮样本不重复。</p>
<h4><span id="6-xgboost在工程上有哪些优化为什么要做这些工程化优化">6、<strong>Xgboost在工程上有哪些优化？为什么要做这些工程化优化？</strong></span></h4>
<h5><span id="61-块结构设计"><strong>6.1 块结构设计</strong></span></h5>
<p>我们知道，决策树的学习<strong>最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序</strong>。而
<strong><font color="red"> XGBoost
在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed
Sparse Columns
Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</font></strong></p>
<blockquote>
<p>预排序 + 块设计【独立】 + 稀疏矩阵存储</p>
</blockquote>
<ul>
<li><strong>每一个块结构包括一个或多个已经排序好的特征</strong>；</li>
<li><strong>缺失特征值将不进行排序</strong>；</li>
<li>每个特征会存储指向<strong>样本梯度统计值</strong>的索引，方便计算一阶导和二阶导数值；</li>
</ul>
<p>这种块结构存储的特征之间相互独立，方便计算机进行并行计算。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个<strong>特征的增益计算可以同时进行</strong>，这也是
Xgboost 能够实现分布式或者多线程计算的原因。</p>
<h5><span id="62缓存访问优化算法索引访问梯度统计-gt-缓存空间不连续"><strong>6.2
缓存访问优化算法</strong>【索引访问梯度统计 -&gt; 缓存空间不连续】</span></h5>
<p>块结构的设计可以减少节点分裂时的计算量，但<strong>特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续</strong>，这样会造成缓存命中率低，从而影响到算法的效率。</p>
<p>为了解决缓存命中率低的问题，XGBoost
提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。此外适当调整块大小，也可以有助于缓存优化。</p>
<p>于exact greedy算法中, 使用<strong>缓存预取（cache-aware
prefetching）</strong>。具体来说，<strong>对每个线程分配一个连续的buffer</strong>，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化）</p>
<h5><span id="63-核外块计算"><strong>6.3 “核外”块计算</strong></span></h5>
<p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，<strong>XGBoost
独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行</strong>。</p>
<p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p>
<ul>
<li><strong>块压缩：</strong>对 Block
进行按列压缩，并在读取时进行解压；</li>
<li><strong>块拆分：</strong>将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li>
</ul>
<h4><span id="7-xgboost与gbdt有什么联系和不同基模型-算法-工程设计">7、<strong>Xgboost与GBDT有什么联系和不同？</strong>【基模型、算法、工程设计】</span></h4>
<ol type="1">
<li><strong>基分类器</strong>：GBDT 以 CART
作为基分类器，而Xgboost的基分类器不仅支持CART决策树，还支持线性分类器，此时Xgboost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。</li>
<li><strong>导数信息</strong>：GBDT只用了一阶导数信息，Xgboost中对损失函数进行二阶泰勒展开，引入二阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶和二阶可导即可。</li>
<li><strong>正则项</strong>：Xgboost的目标函数加入正则项(叶子结点的数目和叶子结点权重的L2模的平方)，相当于分裂预剪枝过程，降低过拟合。</li>
<li><strong>列抽样</strong>：Xgboost支持列采样，与随机森林类似，用于防止过拟合且加速。(列采样就是训练的时候随机使用一部分特征)，也同时支持子采样，即每轮迭代计算可以不使用全部样本，对样本数据进行采样。</li>
<li><strong>缺失值处理</strong>：Xgboost可以处理缺失值(具体，查看上方问答)</li>
<li><strong>并行化</strong>：Xgboost可以在特征维度进行并行化，在训练前预先将每个特征按照特征值大小进行预排序，按块的形式存储，后续可以重复使用这个结构，减小计算量，分裂时可以用多线程并行计算每个特征的增益，最终选增益最大的那个特征去做分裂，提高训练速度。</li>
</ol>
<h4><span id="8-xgboost特征重要性">8、<strong><font color="red">
XGBoost特征重要性</font></strong></span></h4>
<blockquote>
<p><strong>何时使用shap value分析特征重要性？</strong> - 知乎
https://www.zhihu.com/question/527570173/answer/2472253431</p>
</blockquote>
<p>这一思路，通常被用来做<strong>特征筛选</strong>。剔除贡献度不高的尾部特征，增强模型的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=鲁棒性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22355884348%22%7D">鲁棒性</a>的同时，起到特征降维的作用。另一个方面，则是用来做<strong>模型的可解释性</strong>。我们期望的结果是：重要的特征是符合业务直觉的；符合业务直觉的特征排名靠前。</p>
<h5><span id="xgb内置的三种特征重要性计算方法">XGB内置的三种特征重要性计算方法</span></h5>
<ul>
<li><strong>weight</strong>：<code>xgb.plot_importance</code>,<strong>子树模型分裂时，用到的特征次数。这里计算的是所有的树。</strong></li>
<li><strong>gain</strong>:<code>model.feature_importances_</code>,信息增益的泛化概念。这里是指，<strong>节点分裂时，该特征带来信息增益（目标函数）优化的平均值。</strong></li>
<li><strong>cover</strong>:<code>model = XGBRFClassifier(importance_type = 'cover')</code>
这个计算方法，需要在定义模型时定义。之后再调用<code>model.feature_importances_</code>
得到的便是基于<code>cover</code>得到的贡献度。<strong>树模型在分裂时，特征下的叶子结点涵盖的样本数除以特征用来分裂的次数。分裂越靠近根部，cover
值越大。</strong></li>
</ul>
<h5><span id="其他重要性计算方法">其他重要性计算方法</span></h5>
<ul>
<li><strong>permutation</strong>:<strong>如果这个特征很重要，那么我们打散所有样本中的该特征，则最后的优化目标将折损。这里的折损程度，就是特征的重要程度。</strong></li>
<li><strong>shap</strong>:<strong>轮流去掉每一个特征，算出剩下特征的贡献情况，以此来推导出被去除特征的边际贡献。该方法是目前唯一的逻辑严密的特征解释方法</strong></li>
</ul>
<h4><span id="9-xgboost增量训练">9、XGBoost增量训练</span></h4>
<p><strong>XGBoost</strong>：XGBoost提供两种增量训练的方式，一种是在当前迭代树的基础上增加新树，原树不变；另一种是当前迭代树结构不变，重新计算叶节点权重，同时也可增加新树。</p>
<h4><span id="10-xgboost线性模型">10、XGBoost线性模型</span></h4>
<p><strong>线性模型</strong>：Xgboost通过泰勒公式的二阶展开迭代的残差是1导/2导，线性回归迭代的是标签，xgboost需要串行多个线性回归，预测结果为多个象形线性回归的累积值......，除了用到了线性回归的原理方程式，他们两的损失函数，下降梯度都不一样，几乎没有什么共同点</p>
<h4><span id="11-xgboost用泰勒展开优势在哪">11、XGBoost
用泰勒展开优势在哪？</span></h4>
<p>https://www.zhihu.com/question/61374305</p>
<ul>
<li><strong>xgboost是以mse为基础推导出来的</strong>，在mse的情况下，xgboost的目标函数展开就是一阶项+二阶项的形式，而其他类似logloss这样的目标函数不能表示成这种形式。为了后续推导的统一，所以将<strong>目标函数进行二阶泰勒展开，就可以直接自定义损失函数了，只要二阶可导即可，增强了模型的扩展性</strong>。</li>
<li><strong>二阶信息能够让梯度收敛的更快，类似牛顿法比SGD收敛更快</strong>。一阶信息描述梯度变化方向，二阶信息可以描述梯度变化方向是如何变化的。</li>
</ul>
<h3><span id="参考链接">参考链接</span></h3>
<ul>
<li><p><strong>XGBoost官方文档</strong>：https://xgboost.readthedocs.io/en/latest/index.html</p></li>
<li><p>LightGBM算法梳理：https://zhuanlan.zhihu.com/p/78293497</p></li>
<li><p>详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）：https://zhuanlan.zhihu.com/p/366234433</p></li>
<li><p>【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）：https://zhuanlan.zhihu.com/p/87885678</p></li>
<li><p>xgboost面试题整理:
https://xiaomindog.github.io/2021/06/22/xgb-qa/</p></li>
<li><p><strong>深入理解XGBoost</strong>：https://bailingnan.github.io/post/shen-ru-li-jie-xgboost/</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3TFM6N7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3TFM6N7/" class="post-title-link" itemprop="url">线性模型（1）线性回归</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-08 18:56:01" itemprop="dateCreated datePublished" datetime="2022-03-08T18:56:01+08:00">2022-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 11:02:23" itemprop="dateModified" datetime="2023-04-26T11:02:23+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">线性模型</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="一线性回归">一、线性回归</h2>
<p><strong>线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。</strong>这样就可以表达特征与结果之间的非线性关系。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/3TFM6N7/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2N1XDRQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2N1XDRQ/" class="post-title-link" itemprop="url">线性模型（3）正则化</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-08 18:56:01" itemprop="dateCreated datePublished" datetime="2022-03-08T18:56:01+08:00">2022-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 11:23:00" itemprop="dateModified" datetime="2023-04-26T11:23:00+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">线性模型</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-正则化l1和l2"><strong><font color="red">一、正则化
L1和L2</font></strong></span></h3>
<blockquote>
<ul>
<li><strong>本质其实是为了模型参数服从某一分布</strong>；</li>
<li><strong>正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现；</strong></li>
</ul>
<p><strong>为什么希望参数具有稀疏性？</strong></p>
<p>相当于对模型进行了一次特征选择，只留下比较重要的特征，提高模型的泛化能力；</p>
</blockquote>
<p><strong><font color="red">
正则化是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。</font></strong>如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。</p>
<p>正则化一般会采用 L1 范式或者 L2 范式, 其形式分别为 <span class="math inline">\(\Phi(w)=\|x\|_1\)</span> 和 <span class="math inline">\(\Phi(w)=\|x\|_2 。\)</span></p>
<p><img src="https://pic3.zhimg.com/80/v2-a352bc374e80df1299a4d63d39ce4606_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h4><span id="11-l1-正则化零均值拉普拉斯分布">1.1 <strong>L1 正则化</strong>
【零均值拉普拉斯分布】</span></h4>
<p><strong>LASSO 回归, 相当于为模型添加了这样一个先验知识</strong>：
<span class="math inline">\(\mathbf{w}\)</span>
服从<strong>零均值拉普拉斯分布</strong>。首先看看拉普拉斯分布长什
么样子: <span class="math display">\[
f(w \mid \mu, b)=\frac{1}{2 b} \exp \left(-\frac{|w-\mu|}{b}\right)
\]</span> 由于引入了先验知识, 所以似然函数这样写: <span class="math display">\[
\begin{aligned}
L(w) &amp; =P(y \mid w, x) P(w) \\
&amp; =\prod_{i=1}^N
p\left(x_i\right)^{y_i}\left(1-p\left(x_i\right)\right)^{1-y_i}
\prod_{j=1}^d \frac{1}{2 b} \exp
\left(-\frac{\left|w_j\right|}{b}\right)
\end{aligned}
\]</span> 取 <span class="math inline">\(\log\)</span>
再取负，得到目标函数: <span class="math display">\[
-\ln L(w)=-\sum_i\left[y_i \ln p\left(x_i\right)+\left(1-y_i\right) \ln
\left(1-p\left(x_i\right)\right)\right]+\frac{1}{2 b^2}
\sum_j\left|w_j\right|
\]</span> 等价于原始损失函数的后面加上了 L1 正则, 因此 L1
正则的本质其实是为模型增加了“模型参数服从零均值拉普拉
斯分布"这一先验知识。</p>
<h4><span id="12-l2正则化零均值正态分布"><strong>1.2 L2
正则化</strong>【零均值正态分布】</span></h4>
<p>Ridge 回归, 相当于为模型添加了这样一个先验知识： <span class="math inline">\(w\)</span> 服从<strong>零均值正态分布</strong>。
首先看看正态分布长什么样子: <span class="math display">\[
f(w \mid \mu, \sigma)=\frac{1}{\sqrt{2 \pi} \sigma} \exp
\left(-\frac{(w-\mu)^2}{2 \sigma^2}\right)
\]</span> 由于引入了先验知识, 所以似然函数这样写: <span class="math display">\[
\begin{aligned}
L(w) &amp; =P(y \mid w, x) P(w) \\
&amp; =\prod_{i=1}^N
p\left(x_i\right)^{y_i}\left(1-p\left(x_i\right)\right)^{1-y_i}
\prod_{j=1}^d \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{w_j^2}{2
\sigma^2}\right) \\
&amp; =\prod_{i=1}^N
p\left(x_i\right)^{y_i}\left(1-p\left(x_i\right)\right)^{1-y_i}
\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{w^T w}{2
\sigma^2}\right)
\end{aligned}
\]</span> 取 In 再取负，得到目标函数: <span class="math display">\[
-\ln L(w)=-\sum_i\left[y_i \ln p\left(x_i\right)+\left(1-y_i\right) \ln
\left(1-p\left(x_i\right)\right)\right]+\frac{1}{2 \sigma^2} w^T w
\]</span> 等价于原始的损失函数后面加上了 <span class="math inline">\(L
2\)</span> 正则, 因此 <span class="math inline">\(L 2\)</span>
正则的本质其实是为模型增加了““<strong>模型参数服从零均值正态分
布</strong>"这一先验知识。</p>
<h4><span id="13-l1-和-l2-的区别">1.3 L1 和 L2 的区别</span></h4>
<blockquote>
<ul>
<li><strong><font color="red"> 解空间约束条件 ： KKT条件【互斥松弛条件 +
约束条件大于0】</font></strong></li>
<li><strong><font color="red">
函数叠加：（0点成为最值的可能）导数为0的可能性</font></strong></li>
<li><strong><font color="red"> 贝叶斯先验：分布图像</font></strong></li>
</ul>
</blockquote>
<p><strong>L1 正则化</strong>增加了所有权重 w
参数的绝对值之和<strong>逼迫更多 w 为零</strong>，也就是变稀疏（ L2
因为其导数也趋 0, 奔向零的速度不如 L1
给力了）。对<strong>稀疏规则趋之若鹜</strong>的一个关键原因在于它能<strong>实现特征的自动选择</strong>。L1
正则化的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为
0。</p>
<p><strong>L2 正则化</strong>中增加所有权重 w 参数的平方之和，逼迫所有
<strong>w 尽可能趋向零但不为零</strong>（L2 的导数趋于零）。因为在未加入
L2
正则化发生过拟合时，拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的变化很剧烈，也就是某些
w 值非常大。为此，L2 正则化的加入就惩罚了权重变大的趋势。</p>
<h4><span id="14l1正则化使得模型参数具有稀疏性的原理"><strong><font color="red"> 1.4
L1正则化使得模型参数具有稀疏性的原理？</font></strong></span></h4>
<h5><span id="1解空间约束条件-kkt条件互斥松弛条件-约束条件大于0">(1)
解空间约束条件 ： KKT条件【互斥松弛条件 + 约束条件大于0】</span></h5>
<p><strong>KKT
条件是指优化问题在最优处（包括基本型的最优值，对偶问题的最优值）必须满足的条件</strong>。</p>
<blockquote>
<p><strong>参考线性支持向量机的 KKT 条件:</strong></p>
<ul>
<li><strong>主问题可行</strong>: <span class="math inline">\(g_{i}\left(u^{\star}\right)=1-y_{i}\left(w^{\star
\top} x_{i}+b^{\star}\right) \leq 0\)</span> ；</li>
<li><strong><font color="red"> 对偶问题可行: <span class="math inline">\(\alpha_{i}^{\star} \geq
0\)</span></font></strong>;</li>
<li><strong>主变量最优</strong>: <span class="math inline">\(w^{\star}=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i},
\sum_{i=1}^{m} \alpha_{i} y_{i}=0\)</span>;</li>
<li><strong><font color="red"> 互补松弛: <span class="math inline">\(\alpha_{i}^{\star}
g_{i}\left(u^{\star}\right)=\alpha_{i}^{\star}\left(1-y_{i}\left(w^{\star
\top} x_{i}+b^{\star}\right)\right)=0\)</span> ；</font></strong></li>
</ul>
</blockquote>
<p><strong>原函数曲线等高线（同颜色曲线上，每一组<span class="math inline">\(w_1,w_2\)</span>带入后值都相同)</strong>：</p>
<p><img src="https://pic4.zhimg.com/80/v2-efc752bd6d1ce09dbf2e18b9766570eb_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>当加入 L1 正则化的时候, 我们先画出 <span class="math inline">\(\left|w_1\right|+\left|w_2\right|=F\)</span>
的图像, 也就是一个菱形, 代表这些曲线上的点算出来的
要使得这个菱形越小越好 ( <span class="math inline">\(F\)</span>
越小越好)。那么还和原来一样的话, 过中心紫色圈圈的那个菱形明显很大,
因此我 们要取到一个恰好的值。那么如何求值呢?</p>
<ol type="1">
<li>以同一条原曲线目标等高线来说, 现在以最外圈的红色等高线为例,
我们看到, 对于红色曲线上的每个点都可 做一个菱形, 根据上图可知,
当这个菱形与某条等高线相切（仅有一个交点）的时候, 这个菱形最小, 上图相
割对比较大的两个菱形对应的 L1 范数更大。用公式说这个时候能使得在相同的
<span class="math inline">\(\frac{1}{N} \sum_{i=1}^N\left(y_i-w^T
x_i\right)^2\)</span>, 由于 相切的时候的 <span class="math inline">\(C||
w \|_1\)</span> 小, 即 <span class="math inline">\(\left|w_1\right|+\left|w_2\right|\)</span>
所以能够使得 <span class="math inline">\(\frac{1}{N} \sum
i=1^N\left(y_i-w^T x_i\right)^2+C|| w \|_1\)</span> 更小;</li>
<li>有了第一条的说明我们可以看出, 最终加入 <span class="math inline">\(L
1\)</span> 范数得到的解一定是某个菱形和某条原函数等高线的切点。现
在有个比较重要的结论来了, <font color="red"> 我们经过观察可以看到,
几乎对于很多原函数等高曲线, 和某个菱形相交的时 候及其容易相交在坐标轴
(比如上图) , 也就是说最终的结果, 解的某些维度及其容易是 0,
比如上图最终解 是 <span class="math inline">\(w=(0, x)\)</span>
，这也就是我们所说的 L1 更容易得到稀疏解（解向量中 0
比较多)的原因;</font></li>
<li>当加入 <span class="math inline">\(L 2\)</span> 正则化的时候, 分析和
<span class="math inline">\(L 1\)</span> 正则化是类似的,
也就是说我们仅仅是从菱形变成了圆形而已, 同样还
是求原曲线和圆形的切点作为最终解。<strong>当然与 <span class="math inline">\(L 1\)</span> 范数比, 我们这样求的 <span class="math inline">\(L 2\)</span> 范数的从图上来看, 不容易交在
坐标轴上, 但是仍然比较靠近坐标轴。因此这也就是我们老说的, L2
范数能让解比较小 (靠近 0), 但是比 较平滑（不等于 0)。</strong></li>
</ol>
<h5><span id="2函数叠加0点成为最值的可能导数为0的可能性">(2)
函数叠加：（0点成为最值的可能)导数为0的可能性</span></h5>
<p>我们接下来从更严谨的方式来证明,
简而言之就是假设现在我们是一维的情况下 <span class="math inline">\(h(w)=f(w)+C|w|\)</span>, 其中 <span class="math inline">\(h(w)\)</span> 是目标函数, <span class="math inline">\(f(w)\)</span> 是没加 <span class="math inline">\(\mathrm{L} 1\)</span> 正则化项前的目标函数, <span class="math inline">\(C|w|\)</span> 是 <span class="math inline">\(\mathrm{L}\)</span> 正则项, 要使得 0
点成为最值可能的点, <strong>虽然在 0 点不可导, 但是我们只需要让 0
点左右的导数异号, 即 <span class="math inline">\(h_l^{\prime}(0)
h_r^{\prime}(0)=\left(f^{\prime}(0)+C\right)\left(f^{\prime}(0)-C\right)&lt;0\)</span>
即可也就 是 <span class="math inline">\(C&gt;\left|f^{\prime}(0)\right|\)</span> 的情况下,
0 点都是可能的最值点</strong>。相反, L2正则项在原点处的导数是0,
只要原目标函数在原点 处导数不为 0 , 那么最小值点就不会在原点, 所以 <span class="math inline">\(L 2\)</span> 只有减小w最对值的作用,
对解空间的稀疏性没有贡献。</p>
<h5><span id="3-贝叶斯先验分布图像">(3) 贝叶斯先验：分布图像</span></h5>
<p><img src="https://pic3.zhimg.com/80/v2-a352bc374e80df1299a4d63d39ce4606_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>从贝叶斯加入先验分布的角度解释，L1正则化相当于对模型参数引入拉普拉斯先验，L2正则化相当于与引入了高斯先验。高斯分布在0点是平滑的，拉普拉斯在0点处是一个尖峰。</strong></p>
<h4><span id="15-简单总结">1.5 <strong>简单总结</strong></span></h4>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 59%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>正则化</th>
<th>L1 正则化</th>
<th>L2 正则化</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>服从分布</td>
<td>零均值拉普拉斯分布</td>
<td>零均值正态分布</td>
</tr>
<tr class="even">
<td>损失函数变化</td>
<td><span class="math inline">\(\frac{1}{2 b^2} \sum_j\left
|w_j\right|\)</span></td>
<td><span class="math inline">\(\frac{1}{2 \sigma^2} w^T w\)</span></td>
</tr>
<tr class="odd">
<td>模型参数w效果</td>
<td>逼迫更多 w 为零，【稀疏解】<strong><font color="red">
（解空间约束条件、函数叠加、贝叶斯先验）</font></strong></td>
<td>趋向零但不为零【平滑】</td>
</tr>
<tr class="even">
<td>作用</td>
<td>【特征选择】【降低模型复杂度】</td>
<td>【降低模型复杂度】</td>
</tr>
</tbody>
</table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1J1QH0W/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1J1QH0W/" class="post-title-link" itemprop="url">线性模型（2）逻辑回归</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-08 18:56:01" itemprop="dateCreated datePublished" datetime="2022-03-08T18:56:01+08:00">2022-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 11:24:06" itemprop="dateModified" datetime="2023-04-26T11:24:06+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">线性模型</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>30 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>Logistic Regression</strong>
虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic
Regression 因其简单、可并行化、可解释强深受工业界喜爱。<strong>Logistic
回归的本质是：假设数据服从这个Logistic分布，然后使用极大似然估计做参数的估计。</strong></p>
<p><strong>逻辑回归的思路</strong>是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/1J1QH0W/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3BVGDDE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3BVGDDE/" class="post-title-link" itemprop="url">深度学习-NLP（2）Word2vec*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-08 17:34:12" itemprop="dateCreated datePublished" datetime="2022-03-08T17:34:12+08:00">2022-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-19 19:28:10" itemprop="dateModified" datetime="2022-07-19T19:28:10+08:00">2022-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-word2vec">一、Word2Vec</span></h2>
<blockquote>
<ul>
<li><p><strong>nlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert</strong>：https://zhuanlan.zhihu.com/p/56382372</p></li>
<li><p>Word2Vec算法梳理🔥 - 杨航锋的文章 - 知乎
https://zhuanlan.zhihu.com/p/58290018</p></li>
<li><p><a target="_blank" rel="noopener" href="https://plushunter.github.io/2018/02/14/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9AWord2Vec/">Free
will：Word2Vec</a></p></li>
<li><p>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26306795">NLP]
秒懂词向量<em>Word2vec</em>的本质</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/61635013"><em>Word2Vec</em>详解</a></p></li>
<li><p><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53425736"><em>word2vec</em>详解（CBOW，skip-gram，负采样，分层Softmax）</a></strong></p></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89637281">快速入门词嵌入之<em>word2vec</em></a></p></li>
</ul>
<p><strong>word2vec 相比之前的 Word Embedding
方法好在什么地方？</strong></p>
<ul>
<li><strong>极快的训练速度</strong>。以前的语言模型优化的目标是MLE，只能说词向量是其副产品。Mikolov应该是第一个提出抛弃MLE（和困惑度）指标，就是要学习一个好的词嵌入。如果不追求MLE，模型就可以大幅简化，去除隐藏层。再利用HSoftmax以及负采样的加速方法，可以使得训练在小时级别完成。而原来的语言模型可能需要几周时间。</li>
<li><strong>一个很酷炫的man-woman=king-queen的示例</strong>。这个示例使得人们发现词嵌入还可以这么玩，并促使词嵌入学习成为了一个研究方向，而不再仅仅是神经网络中的一些参数。</li>
<li><strong>word2vec里有大量的tricks，比如噪声分布如何选？如何采样？如何负采样？</strong>等等。这些tricks虽然摆不上台面，但是对于得到一个好的词向量至关重要。</li>
</ul>
</blockquote>
<p>谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种<strong>浅层的神经网络模型</strong>，它有两种网络结构，分别是<strong>连续词袋</strong>（CBOW）和<strong>跳字</strong>(Skip-Gram)模型。</p>
<blockquote>
<p>CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好</p>
</blockquote>
<h3><span id="11-介绍cbow">1.1 介绍CBOW</span></h3>
<p>CBOW，全称Continuous
Bag-of-Word，中文叫做连续词袋模型：<strong>以上下文来预测当前词</strong>
<span class="math inline">\(w_t\)</span> 。CBOW模型的目的是预测 $P(w_t|
w_{t-k}, , w_{t-1}, w_{t+1}, , w_{t+k}) $</p>
<figure>
<img src="https://pic4.zhimg.com/v2-27f3e577618f84c0026968d273d823ef_b.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4><span id="前向传播过程">前向传播过程</span></h4>
<ul>
<li><p><strong>输入层</strong>: 输入C个单词<span class="math inline">\(x\)</span>： $x_{1k}, , x_{Ck} $，并且每个 <span class="math inline">\(x\)</span> 都是用 <strong>One-hot</strong>
编码表示，每一个 <span class="math inline">\(x\)</span> 的维度为
V（词表长度）。</p></li>
<li><p><strong>输入层到隐层</strong></p>
<ul>
<li>首先，共享矩阵为 <span class="math inline">\(W_{V \times N}\)</span>
，<strong>V表示词表长度</strong>，W的每一行表示的就是一个N维的向量（训练结束后，W的每一行就表示一个词的词向量）。</li>
<li>然后，我们把所有<strong>输入的词转<span class="math inline">\(x\)</span>化为对应词向量</strong>，然后<strong>取平均值</strong>，这样我们就得到了隐层输出值
( 注意，隐层中无激活函数，也就是说这里是线性组合)。 其中，隐层输出 <span class="math inline">\(h\)</span> 是一个N维的向量 。</li>
</ul>
<p><span class="math display">\[
  h = \frac{1}{C} W^T(x_1 + x_2 + \cdots + x_c)
  \]</span></p></li>
<li><p><strong>隐层到输出层</strong>：隐层的输出为N维向量 <span class="math inline">\(h\)</span> ， 隐层到输出层的权重矩阵为 <span class="math inline">\(W&#39;_{N \times V}\)</span>
。然后，通过矩阵运算我们得到一个 $V $ 维向量 <span class="math display">\[
  u = W&#39;^{T} * h
  \]</span></p></li>
</ul>
<p>其中，向量 <span class="math inline">\(u\)</span> 的第 <span class="math inline">\(i\)</span> 行表示词汇表中第 <span class="math inline">\(i\)</span>
个词的可能性，然后我们的目的就是取可能性最高的那个词。<strong>因此，在最后的输出层是一个softmax
层获取分数最高的词</strong>，那么就有我们的最终输出： <span class="math display">\[
P(w_j| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})}
\]</span></p>
<h4><span id="损失函数">损失函数</span></h4>
<p>我们假定 <span class="math inline">\(j^*\)</span>
是真实单词在词汇表中的下标，那么根据极大似然法，则目标函数定义如下：
<span class="math display">\[
E = -log \, p(W_O |W_I) = -log \, \frac{exp({u_j})}{\sum_{k \in V}
exp({u_k})} =  log  \sum_{k \in V} exp(u_{k})  -u_j
\]</span></p>
<h3><span id="12-skip-gram模型">1.2 Skip-gram模型</span></h3>
<p>Skip-Gram的基本思想是：<strong>通过当前词 <span class="math inline">\(w_t\)</span> 预测其上下文 <span class="math inline">\(w_{t-i}, \cdots , w_{t+i}\)</span></strong>
，模型如下图所示：</p>
<figure>
<img src="https://pic2.zhimg.com/v2-42ef75691c18a03cfda4fa85a8409e19_b.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4><span id="前向传播过程">前向传播过程</span></h4>
<ul>
<li><p><strong>输入层</strong>： 输入的是一个单词，其表示形式为
<strong>One-hot</strong> ，我们将其表示为V维向量 <span class="math inline">\(x_k\)</span> ，其中 <span class="math inline">\(V\)</span> 为词表大小。然后，通过词向量矩阵 <span class="math inline">\(W_{V \times N}\)</span> 我们得到一个N维向量 <span class="math display">\[
  h = W^T * x_k = v^{T}_{w_I}
  \]</span></p></li>
<li><p><strong>隐层</strong>：
而隐层中没有激活函数，也就是说输入=输出，因此隐藏的输出也是 <span class="math inline">\(h\)</span> 。</p></li>
<li><p><strong>隐层到输出层</strong>：</p>
<ul>
<li><p><strong>首先</strong>，因为要输出C个单词，因此我们此时的输出有C个分布：
<strong>$y_1, y_C $，且每个分布都是独立的</strong>，我们需要单独计算，
其中 <span class="math inline">\(y_i\)</span> 表示窗口的第 <span class="math inline">\(i\)</span> 个单词的分布。
【<strong>独立性假设</strong>】</p></li>
<li><p><strong>其次</strong>， 因为矩阵 <span class="math inline">\(W&#39;_{N \times V}\)</span>
是共享的，因此我们得到的 <span class="math inline">\(V \times 1\)</span>
维向量 <span class="math inline">\(u\)</span> 其实是相同的，也就是有
<span class="math inline">\(u_{c,j} = u_j\)</span> ，这里 <span class="math inline">\(u\)</span> 的每一行同 CBOW
中一样，表示的也是评分。</p></li>
<li><p><strong>最后</strong>，每个分布都经过一个 softmax 层，不同于
CBOW，我们此处产生的是第 <span class="math inline">\(i\)</span>
个单词的分布（共有C个单词），如下：</p></li>
</ul>
<p><span class="math display">\[
  P(w_{i,j}| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V}
exp({u_k})}
  \]</span></p></li>
</ul>
<h4><span id="损失函数">损失函数</span></h4>
<p>假设 <span class="math inline">\(j^*\)</span>
是真实单词在词汇表中的下标，那么根据<strong>极大似然法</strong>，则目标函数定义如下：
<span class="math display">\[
\begin{split} E &amp;= - log \, p(w_1, w_2, \cdots, w_C | w_I)   \\
&amp;= - log \prod_{c=1}^C P(w_c|w_i) \\ &amp;= - log  \prod_{c=1}^{C}
\frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \\ &amp;= -
\sum_{c=1}^C u_{j^*_c} + C \cdot log \sum_{k=1}^{V} exp(u_k) \end{split}
\]</span></p>
<h2><span id="二-word2vec-优化">二、Word2Vec 优化</span></h2>
<p>以上我们讨论的模型（二元模型，CBOW和skip-gram）都是他们的原始形式，没有加入任何优化技巧。对于这些模型，每个单词存在两类向量表达：<strong>输入向量</strong><img src="https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%7D" alt="[公式]">，<strong>输出向量</strong><img src="https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%27%7D" alt="[公式]">（这也是为什么word2vec的名称由来：1个单词对应2个向量表示)。学习得到输入向量比较简单；但<strong>要学习输出向量是很困难</strong>的。</p>
<h3><span id="21-hierarchical-softmax">==2.1 Hierarchical Softmax==</span></h3>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/56139075</p>
</blockquote>
<p>Hierarchical
Softmax对原模型的改进主要有两点，第一点是从输入层到隐藏层的映射，没有采用原先的与矩阵W相乘然后相加求平均的方法，而是<strong>直接对所有输入的词向量求和</strong>。假设输入的词向量为（0，1，0，0）和（0,0,0,1），那么隐藏层的向量为（0,1,0,1）。</p>
<p><strong>Hierarchical Softmax</strong>的第二点改进是采用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=哈夫曼树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2261635013%22%7D">哈夫曼树</a>来替换了原先的从隐藏层到输出层的矩阵W’。<strong>哈夫曼树的叶节点个数为词汇表的单词个数V</strong>，一个叶节点代表一个单词，而从根节点到该叶节点的路径确定了这个单词最终输出的词向量。</p>
<p><img src="https://pic4.zhimg.com/v2-3db7e66f36db0a9e6e6bc2f348dece47_b.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>具体来说，这棵哈夫曼树除了根结点以外的所有非叶节点中都含有一个由参数θ确定的sigmoid函数，不同节点中的θ不一样</strong>。训练时==<strong>隐藏层的向量</strong>==与这个<strong>==sigmoid函数==</strong>进行运算，根据结果进行分类，若分类为负类则沿左子树向下传递，编码为0；若分类为正类则沿右子树向下传递，编码为1。</p>
<p><strong>每个叶子节点代表语料库中的一个词</strong>，<strong>于是每个词语都可以被01唯一的编码，并且其编码序列对应一个事件序列，于是我们可以计算条件概率
<img src="https://www.zhihu.com/equation?tex=p%28w+%7C+C+o+n+t+e+x+t%28w%29%29" alt="[公式]"></strong> 。</p>
<p><strong>在开始计算之前，还是得引入一些符号：</strong></p>
<ol type="1">
<li><p><img src="https://www.zhihu.com/equation?tex=p%5E%7Bw%7D" alt="[公式]"> :从根结点出发到达w对应叶子结点的路径</p></li>
<li><p><img src="https://www.zhihu.com/equation?tex=l%5E%7Bw%7D" alt="[公式]"> :路径中包含结点的个数</p></li>
<li><p><img src="https://www.zhihu.com/equation?tex=p_%7B1%7D%5E%7Bw%7D%2C+p_%7B2%7D%5E%7Bw%7D%2C+%5Ccdots%2C+p_%7Bl%5E%7Bw%7D%7D%5E%7Bw%7D" alt="[公式]"> :路径 <img src="https://www.zhihu.com/equation?tex=p%5E%7Bw%7D" alt="[公式]">
中的各个节点</p></li>
<li><p><img src="https://www.zhihu.com/equation?tex=d_%7B2%7D%5E%7Bw%7D%2C+d_%7B3%7D%5E%7Bw%7D%2C+%5Ccdots%2C+d_%7Bl+w%7D%5E%7Bw%7D+%5Cin%5C%7B0%2C1%5C%7D" alt="[公式]"> :词w的编码， <img src="https://www.zhihu.com/equation?tex=d_%7Bj%7D%5E%7Bw%7D" alt="[公式]"> 表示路径 <img src="https://www.zhihu.com/equation?tex=p%5E%7Bw%7D" alt="[公式]">
第j个节点对应的编码（根节点无编码）</p></li>
<li><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7B1%7D%5E%7Bw%7D%2C+%5Ctheta_%7B2%7D%5E%7Bw%7D%2C+%5Ccdots%2C+%5Ctheta_%7Bl%5E%7Bw%7D-1%7D%5E%7Bw%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm%7D" alt="[公式]"> :路径 <img src="https://www.zhihu.com/equation?tex=p%5E%7Bw%7D" alt="[公式]">
中非叶节点对应的<strong>参数向量</strong></p></li>
</ol>
<p>于是可以给出w的条件概率：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=p%28w+%7C+C+o+n+t+e+x+t%28w%29%29%3D%5Cprod_%7Bj%3D2%7D%5E%7Bl%5E%7Bw%7D%7D+p%5Cleft%28d_%7Bj%7D%5E%7Bw%7D+%7C+%5Cmathbf%7Bx%7D_%7Bw%7D%2C+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>这是个简单明了的式子，从根节点到叶节点经过了 <img src="https://www.zhihu.com/equation?tex=l%5E%7Bw%7D-1" alt="[公式]">
个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1)。</strong></p>
<p>其中，每一项是一个<strong>逻辑斯谛回归</strong>：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=p%5Cleft%28d_%7Bj%7D%5E%7Bw%7D+%7C+%5Cmathbf%7Bx%7D_%7Bw%7D%2C+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%7B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%2C%7D+%26+%7Bd_%7Bj%7D%5E%7Bw%7D%3D0%7D+%5C%5C+%7B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%2C%7D+%26+%7Bd_%7Bj%7D%5E%7Bw%7D%3D1%7D%5Cend%7Barray%7D%5Cright." alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>考虑到d只有0和1两种取值，我们可以用指数形式方便地将其写到一起：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=p%5Cleft%28d_%7Bj%7D%5E%7Bw%7D+%7C+%5Cmathbf%7Bx%7D_%7Bw%7D%2C+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%3D%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5E%7B1-d_%7Bj%7D%5E%7Bw%7D%7D+%5Ccdot%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5E%7Bd%5E%7Bw%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>我们的目标函数取对数似然</strong>：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D%5Csum_%7Bw+%5Cin+%5Cmathcal%7BC%7D%7D+%5Clog+p%28w+%7C+C+o+n+t+e+x+t%28w%29%29" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>将 <img src="https://www.zhihu.com/equation?tex=p%28w+%7C+C+o+n+t+e+x+t%28w%29%29" alt="[公式]"> 代入上式，有:</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D%5Csum_%7Bw+%5Cin+%5Cmathcal%7BC%7D%7D+%5Clog+%5Cprod_%7Bj%3D2%7D%5E%7Bl%5E%7Bw%7D%7D%5Cleft%5C%7B%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5E%7B1-d_%7Bj%7D%5E%7Bw%7D%7D+%5Ccdot%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5E%7Bd_%7Bj%7D%5E%7Bw%7D%7D%5Cright%5C%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%3D%5Csum_%7Bw+%5Cin+%5Cmathcal%7BC%7D%7D+%5Csum_%7Bj%3D2%7D%5E%7Bl%5E%7Bw%7D%7D%5Cleft%5C%7B%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29+%5Ccdot+%5Clog+%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%2Bd_%7Bj%7D%5E%7Bw%7D+%5Ccdot+%5Clog+%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5Cright%5C%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>这也很直白，连乘的对数换成求和。不过还是有点长，我们把每一项简记为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28w%2C+j%29%3D%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29+%5Ccdot+%5Clog+%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%2Bd_%7Bj%7D%5E%7Bw%7D+%5Ccdot+%5Clog+%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="wordvec极大化化目标函数使用的算法是是随机梯度上升法"><strong><font color="red">
<em>WordVec</em>
极大化化目标函数使用的算法是是随机梯度上升法</font></strong></span></h4>
<p>每一项有两个参数，一个是每个节点的参数向量 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"> ，另一个是输出层的输入 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> ，我们分别对其求偏导数：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D%28w%2C+j%29%7D%7B%5Cpartial+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%7D%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%7D%5Cleft%5C%7B%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29+%5Ccdot+%5Clog+%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%2Bd_%7Bj%7D%5E%7Bw%7D+%5Ccdot+%5Clog+%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%5Cright%5C%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>因为sigmoid函数的导数有个很棒的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B%5Cprime%7D%28x%29%3D%5Csigma%28x%29%5B1-%5Csigma%28x%29%5D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>于是代入上上式得到：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D+%5Cmathbf%7Bx%7D_%7Bw%7D-d_%7Bj%7D%5E%7Bw%7D+%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29+%5Cmathbf%7Bx%7D_%7Bw%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>合并同类项得到：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cleft%5B1-d_%7Bj%7D%5E%7Bw%7D-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D+%5Cmathbf%7Bx%7D_%7Bw%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>于是 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]">的更新表达式就得到了：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D+%3A%3D%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%2B%5Ceta%5Cleft%5B1-d_%7Bj%7D%5E%7Bw%7D-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D+%5Cmathbf%7Bx%7D_%7Bw%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Ceta" alt="[公式]">
是学习率，通常取0-1之间的一个值。学习率越大训练速度越快，但目标函数容易在局部区域来回抖动。</p>
<p><strong>再来 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 的偏导数</strong>，注意到 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28w%2C+j%29%3D%5Cleft%281-d_%7Bj%7D%5E%7Bw%7D%5Cright%29+%5Ccdot+%5Clog+%5Cleft%5B%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D%2Bd_%7Bj%7D%5E%7Bw%7D+%5Ccdot+%5Clog+%5Cleft%5B1-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D" alt="[公式]"> 中 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"> 是对称的，所有直接将 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"> 的偏导数中的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]"> 替换为 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> ，得到关于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 的偏导数：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D%28w%2C+j%29%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D_%7Bw%7D%7D%3D%5Cleft%5B1-d_%7Bj%7D%5E%7Bw%7D-%5Csigma%5Cleft%28%5Cmathbf%7Bx%7D_%7Bw%7D%5E%7B%5Ctop%7D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D%5Cright%29%5Cright%5D+%5Ctheta_%7Bj-1%7D%5E%7Bw%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>不过 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]">
是上下文的词向量的和，不是上下文单个词的词向量。怎么把这个更新量应用到单个词的词向量上去呢？word2vec采取的是直接将
<img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D_%7Bw%7D" alt="[公式]"> 的更新量整个应用到每个单词的词向量上去</strong>：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D%28%5Cwidetilde%7Bw%7D%29+%3A%3D%5Cmathbf%7Bv%7D%28%5Cwidetilde%7Bw%7D%29%2B%5Ceta+%5Csum_%7Bj%3D2%7D%5E%7Bl%5E%7Bw%7D%7D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D%28w%2C+j%29%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D_%7Bw%7D%7D%2C+%5Cquad+%5Cwidetilde%7Bw%7D+%5Cin+%5Ctext+%7B+Context+%7D%28w%29" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D%28%5Cwidetilde%7Bw%7D%29" alt="[公式]">
代表上下文中某一个单词的词向量。我认为应该也可以将其平均后更新到每个词向量上去，无非是学习率的不同，欢迎指正。</p>
<h3><span id="22-negative-sampling">2.2 Negative Sampling</span></h3>
<blockquote>
<p>Negative Sampling - 素轻的文章 - 知乎
https://zhuanlan.zhihu.com/p/56106590</p>
</blockquote>
<blockquote>
<p><strong>为了解决数量太过庞大的输出向量的更新问题，我们就不更新全部向量，而只更新他们的一个样本</strong>。</p>
<p>训练神经网络
意味着输入一个训练样本调整weight，让它预测这个训练样本更准。换句话说，每个训练样本将会影响网络中所有的weight。<strong>Negative
sampling
解决了这个问题，每次我们就修改了其中一小部分weight，而不是全部。</strong></p>
</blockquote>
<p><strong>负采样是另一种用来提高Word2Vec效率的方法</strong>，它是基于这样的观察：训练一个神经网络意味着使用一个训练样本就要稍微调整一下神经网络中所有的权重，这样才能够确保预测训练样本更加精确，如果能设计一种方法每次只更新一部分权重，那么计算复杂度将大大降低。</p>
<p>如果 vocabulary 大小为10000时， 当输入样本 ( "fox", "quick")
到神经网络时， <strong>“ fox” 经过 one-hot 编码，在输出层我们期望对应
“quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出
0</strong>。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为
negative word. negative sampling 的想法也很直接
，<strong>将随机选择一小部分的 negative words，比如选 10个 negative
words 来更新对应的权重参数。</strong></p>
<p><strong>假设原来模型每次运行都需要300×10,000(其实没有减少数量，但是运行过程中，减少了需要载入的数量。)
现在只要300×(1+10)减少了好多。</strong></p>
<h4><span id="问题来了如何选择10个negativesample呢">==问题来了，如何选择10个negative
sample呢？==</span></h4>
<p><strong>negative
sample也是根据他们出现概率来选的，而这个概率又和他们出现的频率有关。更常出现的词，更容易被选为negative
sample。</strong></p>
<p>这个概率用一个公式表示，每个词给了一个和它频率相关的权重。这个概率公式为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=P%28w_i%29+%3D+%5Cfrac%7Bf%28w_i%29%5E%7B0.75%7D+%7D%7B+%5Csum_%7Bj%3D0%7D%5E%7Bn%7D%28f%28w_j%29%5E%7B0.75%7D%7D%29" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>在paper中说0.75这个超参是试出来的，这个函数performance比其他函数好。</p>
<p><strong><font color="red">
负采样算法实际上就是一个带权采样过程，负例的选择机制是和单词词频联系起来的。</font></strong>具体做法是以
<code>N+1</code> 个点对区间 <code>[0,1]</code>
做非等距切分，并引入的一个在区间 <code>[0,1]</code> 上的 <code>M</code>
等距切分，其中 <code>M &gt;&gt; N。</code>源码中取 M =
10^8。然后对两个切分做投影，得到映射关系：采样时，每次生成一个 [1, M-1]
之间的整数 i，则 Table(i)
就对应一个样本；当采样到正例时，跳过（<strong>拒绝采样</strong>）。</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-36547a4cd05365292830ad4b22ba4c93_1440w.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://pic3.zhimg.com/80/v2-a788595cc2611b0bfdac9e039a2e82fe_1440w.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cfe67c913af37a9435f3331139abeab8_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2><span id="三-word2vec-qampa">三、Word2vec Q&amp;A</span></h2>
<h3><span id="31-word2vec与lda的区别">3.1 Word2Vec与LDA的区别</span></h3>
<ol type="1">
<li><p>LDA是利用文档中<strong>单词的共现关系</strong>来对单词按<strong>主题聚类</strong>，也可以理解为对“<strong>文档-单词</strong>”矩阵进行<strong>分解</strong>，得到“<strong>文档-主题</strong>”和“<strong>主题-单词</strong>”两个<strong>概率分布</strong>。</p></li>
<li><p>Word2Vec是利用<strong>上下文-单词</strong>“矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说，如果两个单词所对应的word2vec向量相似度较高，那么它们很可能经常在同样的上下文中出现。</p></li>
<li><p>LDA模型是一种基于<strong>概率图模型</strong>的<strong>生成式模型</strong>，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题）；</p></li>
<li><p>而Word2Vec模型一般表达为<strong>神经网络</strong>的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。</p></li>
</ol>
<h3><span id="32-word2vec存在的问题是什么">3.2 Word2Vec存在的问题是什么？</span></h3>
<ul>
<li>对每个local context window单独训练，没有利用包 含在global
co-currence矩阵中的统计信息。</li>
<li>对多义词无法很好的表示和处理，因为使用了唯一的词向量</li>
</ul>
<h3><span id="32-ood-of-word2vec">3.2 OOD of word2vec</span></h3>
<p>其它单词认定其为Unknow，编号为0</p>
<h3><span id="34-项目中的word2vec">3.4 项目中的word2vec</span></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">feature_asm2vec</span>(<span class="params">data_type, inter_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Feature engineering for asm2vec feature.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> data_type == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">        <span class="comment"># TODO : 模型空判断</span></span><br><span class="line">        <span class="comment"># Train a Word2vec model by mixing traing set and test set</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;------------------------ 训练asm2vec模型 ------------------------&quot;</span>)</span><br><span class="line">        sentences = PathLineSentences(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/semantic/&quot;</span>)</span><br><span class="line">        model = Word2Vec(sentences=sentences, vector_size=<span class="number">1024</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, workers=<span class="number">5</span>)</span><br><span class="line">        model.wv.save_word2vec_format(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/models/asm2vec.bin&quot;</span>, binary=<span class="literal">True</span>, sort_attr=<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load the trained Word2vec model</span></span><br><span class="line">    model_wv = KeyedVectors.load_word2vec_format(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/models/asm2vec.bin&quot;</span>, binary=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------------------------ 生成asm2vec特征 ------------------------&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/<span class="subst">&#123;data_type&#125;</span>_filename.txt&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        filename = fp.read().split()</span><br><span class="line">    <span class="comment"># Feature engineering for generating string vector features</span></span><br><span class="line">    obj = StringVector()</span><br><span class="line">    arr = np.zeros((<span class="built_in">len</span>(filename), obj.dim))</span><br><span class="line">    <span class="keyword">with</span> tqdm(total=<span class="built_in">len</span>(filename), ncols=<span class="number">80</span>, desc=obj.name) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> i, file <span class="keyword">in</span> <span class="built_in">enumerate</span>(filename):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/semantic/<span class="subst">&#123;file&#125;</span>.txt&quot;</span>, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                stringz = f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">            lines = <span class="string">&#x27; &#x27;</span>.join(stringz.split(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line">            raw_words = <span class="built_in">list</span>(<span class="built_in">set</span>(lines.split()))</span><br><span class="line">            arr[i, :] = obj.feature_vector((model_wv, raw_words))</span><br><span class="line">            pbar.update(<span class="number">1</span>)</span><br><span class="line">    arr[np.isnan(arr)] = <span class="number">0</span></span><br><span class="line">    np.save(<span class="string">f&quot;<span class="subst">&#123;inter_path&#125;</span>/feature/<span class="subst">&#123;data_type&#125;</span>_semantic.npy&quot;</span>, arr)</span><br></pre></td></tr></table></figure>
<h3><span id="35-tf-idf-word2vec和bert-比较">3.5 Tf-Idf、Word2Vec和BERT 比较</span></h3>
<blockquote>
<p>从算法本质来说 word2vec
一旦训练好了是<strong>没法处理未登录词（OOV）</strong>的，一般的做法是给OOV一个默认的向量，下面是一个类的封装（仅列出核心部分）</p>
</blockquote>
<p>https://www.leiphone.com/category/yanxishe/TbZAzc3CJAMs815p.html</p>
<ul>
<li><strong>词袋法</strong>：用scikit-learn进行特征工程、特征选择以及机器学习，测试和评估，用lime解释。</li>
<li><strong>词嵌入法</strong>：用gensim拟合Word2Vec，用tensorflow/keras进行特征工程和深度学习，测试和评估，用Attention机制解释。</li>
<li><strong>语言模型</strong>：用transformers进行特征工程，用transformers和tensorflow/keras进行预训练BERT的迁移学习，测试和评估。</li>
</ul>
<h4><span id="概要">概要</span></h4>
<p>在本文中，我将使用NLP和Python来解释3种不同的文本多分类策略：老式的词袋法（tf-ldf），著名的词嵌入法（Word2Vec）和最先进的语言模型（BERT）。</p>
<figure>
<img src="https://static.leiphone.com/uploads/new/images/20200930/5f73ddf75200f.png?imageView2/2/w/740" alt="NLP之文本分类：「Tf-Idf、Word2Vec和BERT」三种模型比较">
<figcaption aria-hidden="true">NLP之文本分类：「Tf-Idf、Word2Vec和BERT」三种模型比较</figcaption>
</figure>
<p>NLP（自然语言处理）是人工智能的一个领域，它研究计算机和人类语言之间的交互作用，特别是如何通过计算机编程来处理和分析大量的自然语言数据。NLP常用于文本数据的分类。文本分类是指根据文本数据内容对其进行分类的问题。</p>
<p>我们有多种技术从原始文本数据中提取信息，并用它来训练分类模型。本教程比较了传统的<strong>词袋法</strong>（与简单的机器学习算法一起使用）、流行的<strong>词嵌入模型</strong>（与深度学习神经网络一起使用）和最先进的语言模型（和基于<strong>attention</strong>的<strong>transformers</strong>模型中的<strong>迁移学习</strong>一起使用），语言模型彻底改变了NLP的格局。</p>
<p>我将介绍一些有用的Python代码，这些代码可以轻松地应用在其他类似的案例中（仅需复制、粘贴、运行），并对代码逐行添加注释，以便你能复现这个例子（下面是全部代码的链接）。</p>
<p><strong>词袋法</strong>：文件越多，词汇表越大，因此特征矩阵将是一个巨大的稀疏矩阵。</p>
<h4><span id="bert比之word2vec有哪些进步呢">Bert比之Word2Vec,有哪些进步呢？</span></h4>
<ul>
<li><p><strong>静态到动态：一词多义问题</strong></p></li>
<li><p><strong>词的多层特性</strong>：一个好的语言表示出了建模一词多义现象以外，还需要能够体现词的复杂特性，包括语法
(syntax)、语义 (semantics) 等。</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2J7SQ0E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2J7SQ0E/" class="post-title-link" itemprop="url">决策树（1）ID3</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-25 17:29:37" itemprop="dateCreated datePublished" datetime="2022-02-25T17:29:37+08:00">2022-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 19:27:50" itemprop="dateModified" datetime="2023-04-22T19:27:50+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/" itemprop="url" rel="index"><span itemprop="name">决策树</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>
<h2><span id="一-id3删特征">一、ID3【删特征】</span></h2>
<p>ID3
算法是建立在奥卡姆剃刀[<strong>“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情”</strong>]（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。</p>
<!--more-->
<h3><span id="11-思想">1.1 思想</span></h3>
<p>从信息论的知识中我们知道：信息熵越大，从而样本纯度越低，。ID3
算法的核心思想就是以<strong>信息增益</strong>来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5
也是贪婪搜索）。 其大致步骤为：</p>
<ol type="1">
<li>初始化特征集合和数据集合；</li>
<li>计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；</li>
<li>更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；</li>
<li>重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。</li>
</ol>
<h3><span id="12-划分标准">1.2 划分标准</span></h3>
<p>ID3 使用的分类标准是信息增益，它表示得知特征 A
的信息而使得样本集合不确定性减少的程度。</p>
<p>数据集的<strong>信息熵</strong>：</p>
<p><span class="math display">\[
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2}
\frac{\left|C_{k}\right|}{|D|}
\]</span></p>
<p>其中<span class="math inline">\(C_{k}\)</span>表示集合 D 中属于第 k
类样本的样本子集。针对某个特征 A，对于数据集 D 的条件熵 <span class="math inline">\(H(D \mid A)\)</span>为：</p>
<p><span class="math display">\[
\begin{aligned} H(D \mid A) &amp;=\sum_{i=1}^{n}
\frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right) \\
&amp;=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|}\left(\sum_{k=1}^{K}
\frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2}
\frac{\left|D_{i k}\right|}{\left|D_{i}\right|}\right) \end{aligned}
\]</span></p>
<p><strong>信息增益</strong> = 信息熵 - 条件熵。信息增益越大表示使用特征
A 来划分所获得的“纯度提升越大”。 <span class="math display">\[
\operatorname{Gain}(D, A)=H(D)-H(D \mid A)
\]</span></p>
<p>信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</p>
<h3><span id="13缺点没有剪枝-特征偏好-缺失值">1.3
缺点【没有剪枝、特征偏好、缺失值】</span></h3>
<ul>
<li>ID3 没有剪枝策略，容易过拟合；</li>
<li>信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于
1；</li>
<li>只能用于处理离散分布的特征；</li>
<li>没有考虑缺失值。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/23/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/">25</a><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/25/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
