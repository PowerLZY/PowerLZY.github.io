<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/11/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/11/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/11/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">117</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/171MAZN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/171MAZN/" class="post-title-link" itemprop="url">机器学习（17）KNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:48" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:48+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-10 19:48:26" itemprop="dateModified" datetime="2022-07-10T19:48:26+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="1-什么是knnkd树-siftbbf算法">1. 什么是KNN【KD树 + SIFT+BBF算法】</span></h2><blockquote>
<p>  KNN与KD树：<a target="_blank" rel="noopener" href="https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272">https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272</a></p>
<p>  【数学】kd 树算法之详细篇 - 椰了的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23966698">https://zhuanlan.zhihu.com/p/23966698</a></p>
<p>  <strong>KNN是生成式模型还是判别式的</strong>，为什么？ - 风控算法小白的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/475072467/answer/2027766449">https://www.zhihu.com/question/475072467/answer/2027766449</a></p>
</blockquote>
<h3><span id="11-knn的通俗解释">1.1 KNN的通俗解释</span></h3><p>何谓K近邻算法，即K-Nearest Neighbor algorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。</p>
<p>用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，<strong>在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。</strong></p>
<p>​                                                                     <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067"><img src="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067" alt="img"></a></p>
<p>​                                                                <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67"><img src="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67" alt="img"></a></p>
<p>如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），KNN就是解决这个问题的。</p>
<p>如果<strong>K=3</strong>，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>红色</strong>的三角形一类。</p>
<p>如果<strong>K=5</strong>，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>蓝色</strong>的正方形一类。</p>
<p><strong>于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</strong></p>
<h3><span id="12-近邻的距离度量">1.2 近邻的距离度量</span></h3><p>我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。</p>
<p><strong>有哪些距离度量的表示法</strong>(普及知识点，可以跳过)：</p>
<ol>
<li><p><strong>==欧氏距离==</strong>，最常见的两点之间或多点之间的距离表示法，又称之为<strong>欧几里得度量</strong>，它定义于欧几里得空间中，如点 x = (x1,…,xn) 和 y = (y1,…,yn) 之间的距离为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0699ac91c0bb89297e4cf418ad7d46cac175652d1a1de30fccc66a16a0eef254/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6428782c79293d2535437371727425374228785f312d795f3129253545322b28785f322d795f3229253545322b2e2e2e2b28785f6e2d795f6e29253545322537443d2535437371727425374225354373756d5f253742693d312537442535452537426e25374428785f692d795f692925354532253744"><img src="https://camo.githubusercontent.com/0699ac91c0bb89297e4cf418ad7d46cac175652d1a1de30fccc66a16a0eef254/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6428782c79293d2535437371727425374228785f312d795f3129253545322b28785f322d795f3229253545322b2e2e2e2b28785f6e2d795f6e29253545322537443d2535437371727425374225354373756d5f253742693d312537442535452537426e25374428785f692d795f692925354532253744" alt="img"></a></p>
<ul>
<li><p>二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b21b48fd9bce0b9f47d16d8e5505a6b600a02e1032632df73a77d5ba43012e2a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f322925354532253744"><img src="https://camo.githubusercontent.com/b21b48fd9bce0b9f47d16d8e5505a6b600a02e1032632df73a77d5ba43012e2a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f322925354532253744" alt="img"></a></p>
</li>
<li><p>三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ebf7fbc4dd6e6eb0d80576e7e5cd19d38ea7a8b9cd801680fc9b39f242367515/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f3229253545322b287a5f312d7a5f322925354532253744"><img src="https://camo.githubusercontent.com/ebf7fbc4dd6e6eb0d80576e7e5cd19d38ea7a8b9cd801680fc9b39f242367515/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f3229253545322b287a5f312d7a5f322925354532253744" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/62f0123f5d52ed280b23c7972c60f635d52a02d0b4958bd5bf0a7c61328f3225/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374225354373756d5f2537426b3d312537442535452537426e25374428785f253742316b2537442d785f253742326b2537442925354532253744"><img src="https://camo.githubusercontent.com/62f0123f5d52ed280b23c7972c60f635d52a02d0b4958bd5bf0a7c61328f3225/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374225354373756d5f2537426b3d312537442535452537426e25374428785f253742316b2537442d785f253742326b2537442925354532253744" alt="img"></a></p>
<p>也可以用表示成向量运算的形式：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8e7528374c22f456904f868c6619a94ba0e51180896315eb3b019f9d7b358742/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228612d622928612d622925354554253744"><img src="https://camo.githubusercontent.com/8e7528374c22f456904f868c6619a94ba0e51180896315eb3b019f9d7b358742/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228612d622928612d622925354554253744" alt="img"></a></p>
</li>
</ul>
</li>
<li><p><strong>曼哈顿距离</strong>，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在<strong>==欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和==</strong>。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为： <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/eeacd6c593ac3eadb18eebfdaf9e27626a54b8f8bdb9f53bc11a90713e8b0bf8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253743785f312d785f322537432b253743795f312d795f32253743"><img src="https://camo.githubusercontent.com/eeacd6c593ac3eadb18eebfdaf9e27626a54b8f8bdb9f53bc11a90713e8b0bf8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253743785f312d785f322537432b253743795f312d795f32253743" alt="img"></a>，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。</p>
<p>通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，此即曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。</p>
<ul>
<li><p>二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/c914feff78b9ccb3f920a67c8dba69b110d641abde5f2533f184a656780cb94d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d253743785f312d785f322537432b253743795f312d795f32253743"><img src="https://camo.githubusercontent.com/c914feff78b9ccb3f920a67c8dba69b110d641abde5f2533f184a656780cb94d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d253743785f312d785f322537432b253743795f312d795f32253743" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/eb4a219d2fbe50979339a7e1a83464ec27957562f106da7a3b08da8cc3394795/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d25354373756d5f2537426b3d312537442535452537426e253744253743785f253742316b2537442d785f253742326b253744253743"><img src="https://camo.githubusercontent.com/eb4a219d2fbe50979339a7e1a83464ec27957562f106da7a3b08da8cc3394795/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d25354373756d5f2537426b3d312537442535452537426e253744253743785f253742316b2537442d785f253742326b253744253743" alt="img"></a></p>
</li>
</ul>
</li>
<li><p><strong>切比雪夫距离</strong>，若二个向量或二个点p 、and q，其座标分别为Pi及qi，则两者之间的切比雪夫距离定义如下：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/3a305bb0cf8cdcd6a77f9e2e2cc1085542ae46f8c70c5b43c67be9728d3d99d6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686562797368657625374428702c71293d6d61785f6928253743705f692d715f6925374329"><img src="https://camo.githubusercontent.com/3a305bb0cf8cdcd6a77f9e2e2cc1085542ae46f8c70c5b43c67be9728d3d99d6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686562797368657625374428702c71293d6d61785f6928253743705f692d715f6925374329" alt="img"></a></p>
<p>这也等于以下Lp度量的极值： <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8fe6945bfb5795e12cfe28b72758c59a598e0c5a89b1ffb874f01cc8ff77a7ba/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d392d32345f32322d31392d34312e706e67"><img src="https://camo.githubusercontent.com/8fe6945bfb5795e12cfe28b72758c59a598e0c5a89b1ffb874f01cc8ff77a7ba/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d392d32345f32322d31392d34312e706e67" alt="img"></a>，因此切比雪夫距离也称为L∞度量。</p>
<p>以数学的观点来看，切比雪夫距离是由一致范数（uniform norm）（或称为上确界范数）所衍生的度量，也是超凸度量（injective metric space）的一种。</p>
<p>在平面几何中，若二点p及q的直角坐标系坐标为(x1,y1)及(x2,y2)，则切比雪夫距离为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2c68232279b3d8c857a5ebf5bcb91f6792abeb47daedeb22214391611680dd97/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686573732537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329"><img src="https://camo.githubusercontent.com/2c68232279b3d8c857a5ebf5bcb91f6792abeb47daedeb22214391611680dd97/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686573732537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329" alt="img"></a></p>
<p><strong>玩过国际象棋的朋友或许知道，国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？。你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。</strong></p>
<ul>
<li><p>二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离 ：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/9387d181258b2a722bcabf891beb2d616a2ec297f35bade87679099641f27f09/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329"><img src="https://camo.githubusercontent.com/9387d181258b2a722bcabf891beb2d616a2ec297f35bade87679099641f27f09/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ea4a630eab19efb4c0270f3808dde70fec7bb5f21d37f76ab016316277fbb730/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d61785f6928253743785f25374231692537442d785f253742326925374425374329"><img src="https://camo.githubusercontent.com/ea4a630eab19efb4c0270f3808dde70fec7bb5f21d37f76ab016316277fbb730/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d61785f6928253743785f25374231692537442d785f253742326925374425374329" alt="img"></a></p>
</li>
</ul>
</li>
</ol>
<p><strong>==简单说来，各种“距离”的应用场景简单概括为：==</strong></p>
<ul>
<li><strong>空间：欧氏距离</strong>，</li>
<li><strong>路径：曼哈顿距离，国际象棋国王：切比雪夫距离</strong>，</li>
<li>以上三种的统一形式:闵可夫斯基距离，</li>
<li>加权：标准化欧氏距离，</li>
<li>排除量纲和依存：马氏距离，</li>
<li>向量差距：夹角余弦，</li>
<li><strong>编码差别：汉明距离</strong>，</li>
<li>集合近似度：杰卡德类似系数与距离，</li>
<li>相关：相关系数与相关距离。</li>
</ul>
<h3><span id="13-k值选择">1.3 K值选择</span></h3><ol>
<li>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></li>
<li>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且<strong>K值的增大就意味着整体的模型变得简单。</strong></li>
<li>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</li>
</ol>
<p>在实际应用中，K值一般取一个比较小的数值，<strong>例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</strong></p>
<h3><span id="14-knn最近邻分类算法的过程">1.4 KNN最近邻分类算法的过程</span></h3><ol>
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前 k 个最小距离的样本；</li>
<li>根据这 k 个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<h2><span id="关于knn的一些问题">关于KNN的一些问题</span></h2><ol>
<li><p>在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用<strong>曼哈顿距离</strong>？</p>
<p><strong>答：</strong>我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向的运动。</p>
</li>
<li><p>KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?</p>
<p>答：极大的节约了时间成本．点线距离如果 &gt;　最小点，无需回溯上一层，如果&lt;,则再上一层寻找。</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/16HRCXQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/16HRCXQ/" class="post-title-link" itemprop="url">机器学习（12）降维</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-29 16:31:00" itemprop="dateModified" datetime="2023-01-29T16:31:00+08:00">2023-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="降维">降维</span></h2><blockquote>
<ul>
<li>数据降维算法: <a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1194552337170214912">https://www.zhihu.com/column/c_1194552337170214912</a></li>
</ul>
</blockquote>
<p><img src="https://pic2.zhimg.com/v2-e47296e78fff3d97eea11d0657ddcb81_1440w.jpg?source=172ae18b" alt="【机器学习】降维——PCA（非常详细）" style="zoom:51%;"></p>
<h2><span id="一-pca">一、PCA</span></h2><blockquote>
<p>  <strong><font color="red"> 降维问题的优化目标：将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。</font></strong></p>
<p>  要找的 <strong>==P 是能让原始协方差矩阵对角化的 P==</strong>。换句话说，优化目标变成了<strong>寻找一个矩阵 P，满足</strong> <img src="https://www.zhihu.com/equation?tex=PCP%5ET" alt="[公式]"> <strong>是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件</strong>。</p>
</blockquote>
<p><strong>PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量</strong>。PCA 的数学推导可以从<strong>==最大可分型==</strong>和<strong>最近重构性</strong>两方面进行，前者的优化条件为划分后方差最大，后者的优化条件为点到划分平面距离最小，这里我将从最大可分性的角度进行证明。</p>
<h3><span id="1-向量表示与基变换">1. 向量表示与基变换</span></h3><p>我们先来介绍些线性代数的基本知识。</p>
<h3><span id="11-内积">1.1 内积</span></h3><p><strong>两个向量的 A 和 B 内积</strong>我们知道形式是这样的：</p>
<p><img src="https://www.zhihu.com/equation?tex=%28a_1%2Ca_2%2C%5Ccdots%2Ca_n%29%5Ccdot+%28b_1%2Cb_2%2C%5Ccdots%2Cb_n%29%5E%5Cmathsf%7BT%7D%3Da_1b_1%2Ba_2b_2%2B%5Ccdots%2Ba_nb_n+%5C%5C" alt="[公式]"></p>
<p>内积运算将两个向量映射为实数，其计算方式非常容易理解，但我们无法看出其物理含义。接下来我们从几何角度来分析，为了简单起见，我们假设 A 和 B 均为二维向量，则：</p>
<p><img src="https://www.zhihu.com/equation?tex=A%3D%28x_1%2Cy_1%29%EF%BC%8CB%3D%28x_2%2Cy_2%29+%5C+A+%5Ccdot+B+%3D+%7CA%7C%7CB%7Ccos%28%5Calpha%29+%5C%5C" alt="[公式]"></p>
<p>其几何表示见下图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-cf4c0041c8459d2894b9a57d8f679a0a_1440w.jpg" alt="img"></p>
<p>我们看出 A 与 B 的内积等于 <strong>A 到 B 的投影长度乘以 B 的模</strong>。</p>
<p>如果假设 B 的模为 1，即让 <img src="https://www.zhihu.com/equation?tex=%7CB%7C%3D1" alt="[公式]"> ，那么就变成了：</p>
<p><img src="https://www.zhihu.com/equation?tex=A%5Ccdot+B%3D%7CA%7Ccos%28a%29+%5C%5C" alt="[公式]"></p>
<p>也就是说，<strong>A 与 B 的内积值等于 A 向 B 所在直线投影的标量大小。</strong></p>
<h3><span id="12-基">1.2 基</span></h3><p>在我们常说的坐标系种，向量 (3,2) 其实隐式引入了一个定义：以 x 轴和 y 轴上正方向长度为 1 的向量为标准。向量 (3,2) 实际是说在 x 轴投影为 3 而 y 轴的投影为 2。<strong>注意投影是一个标量，所以可以为负。</strong></p>
<p>所以，对于向量 (3, 2) 来说，如果我们想求它在 <img src="https://www.zhihu.com/equation?tex=%281%2C0%29%2C%280%2C1%29" alt="[公式]"> 这组基下的坐标的话，分别内积即可。当然，内积完了还是 (3, 2)。</p>
<p>所以，我们大致可以得到一个结论，我们<strong>要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了</strong>。为了方便求坐标，我们希望这组基向量模长为 1。因为向量的内积运算，当模长为 1 时，内积可以直接表示投影。然后还需要这组基是线性无关的，我们一般用正交基，非正交的基也是可以的，不过正交基有较好的性质。</p>
<h3><span id="13-基变换的矩阵表示">1.3 基变换的矩阵表示</span></h3><p>这里我们先做一个练习：对于向量 (3,2) 这个点来说，在 <img src="https://www.zhihu.com/equation?tex=%28%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%2C%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%28-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%2C+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 这组基下的坐标是多少？</p>
<p>我们拿 (3,2) 分别与之内积，得到 <img src="https://www.zhihu.com/equation?tex=%28%5Cfrac%7B5%7D%7B%5Csqrt%7B2%7D%7D%2C-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 这个新坐标。</p>
<p>我们可以用矩阵相乘的形式简洁的表示这个变换：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D++1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+3+%5C%5C+2+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+5%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D++%5Cend%7Bpmatrix%7D++%5C%5C" alt="[公式]"></p>
<p>左边矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。推广一下，如果我们有 m 个二维向量，只要将二维向量按列排成一个两行 m 列矩阵，然后用“基矩阵”乘以这个矩阵就可以得到了所有这些向量在新基下的值。例如对于数据点 <img src="https://www.zhihu.com/equation?tex=%281%2C1%29%EF%BC%8C%282%2C2%29%EF%BC%8C%283%2C3%29" alt="[公式]"> 来说，想变换到刚才那组基上，则可以这样表示：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+1+%26+2+%26+3+%5C%5C+1+%26+2+%26+3+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+2%2F%5Csqrt%7B2%7D+%26+4%2F%5Csqrt%7B2%7D+%26+6%2F%5Csqrt%7B2%7D+%5C%5C+0+%26+0+%26+0+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>我们可以把它写成通用的表示形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+p_1+%5C%5C+p_2+%5C%5C+%5Cvdots+%5C%5C+p_R+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+a_1+%26+a_2+%26+%5Ccdots+%26+a_M+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+p_1a_1+%26+p_1a_2+%26+%5Ccdots+%26+p_1a_M+%5C%5C+p_2a_1+%26+p_2a_2+%26+%5Ccdots+%26+p_2a_M+%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C+p_Ra_1+%26+p_Ra_2+%26+%5Ccdots+%26+p_Ra_M+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 是一个行向量，表示第 i 个基， <img src="https://www.zhihu.com/equation?tex=a_j" alt="[公式]"> 是一个列向量，表示第 j 个原始数据记录。实际上也就是做了一个向量矩阵化的操作。</p>
<p>==上述分析给矩阵相乘找到了一种物理解释：<strong>两个矩阵相乘的意义是将右边矩阵中的每一列向量</strong> <img src="https://www.zhihu.com/equation?tex=a_i" alt="[公式]"> <strong>变换到左边矩阵中以每一行行向量为基所表示的空间中去。</strong>也就是说一个矩阵可以表示一种线性变换。==</p>
<h3><span id="2-最大可分性">2. 最大可分性</span></h3><p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，<strong>如果基的数量少于向量本身的维数，则可以达到降维的效果</strong>。</p>
<p><strong>但是我们还没回答一个最关键的问题：如何选择基才是最优的。或者说，如果我们有一组 N 维向量，现在要将其降到 K 维（K 小于 N），那么我们应该如何选择 K 个基才能最大程度保留原有的信息？</strong></p>
<p>一种直观的看法是：<strong><font color="red"> 希望投影后的投影值尽可能分散，因为如果重叠就会有样本消失。当然这个也可以从熵的角度进行理解，熵越大所含信息越多。</font></strong></p>
<h4><span id="21-方差">2.1 方差</span></h4><p>我们知道数值的分散程度，可以用数学上的方差来表述。<strong>一个变量的方差可以看做是每个元素与变量均值的差的平方和的均值</strong>，即：</p>
<p><img src="https://www.zhihu.com/equation?tex=Var%28a%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu%29%5E2%7D+%5C%5C" alt="[公式]"></p>
<p><strong>为了方便处理，我们将每个变量的均值都化为 0</strong> ，因此方差可以直接用每个元素的平方和除以元素个数表示：</p>
<p><img src="https://www.zhihu.com/equation?tex=Var%28a%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%5C%5C" alt="[公式]"></p>
<p>于是上面的问题被形式化表述为：<strong>寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</strong></p>
<h4><span id="22-协方差">2.2 协方差</span></h4><p>在一维空间中我们可以用方差来表示数据的分散程度。而对于高维数据，我们用协方差进行约束，<strong>协方差可以表示两个变量的相关性</strong>。<strong><font color="red"> 为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性</font></strong>，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。</p>
<p>协方差公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm-1%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu_a%29%28b_i-%5Cmu_b%29%7D+%5C%5C" alt="[公式]"></p>
<p>由于均值为 0，所以我们的协方差公式可以表示为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C" alt="[公式]"></p>
<p>当样本数较大时，不必在意其是 m 还是 m-1，为了方便计算，我们分母取 m。</p>
<p><strong><font color="red"> 协方差为 0 时，表示两个变量完全不相关</font></strong>。为了让协方差为 0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。</p>
<p>（<strong>补充</strong>：协方差为 0 时，两个变量只是线性不相关。完全独立是有问题的，才疏学浅，还望见谅。）</p>
<p><strong><font color="red"> 至此，我们得到了降维问题的优化目标：将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。</font></strong></p>
<h4><span id="23-协方差矩阵">2.3 协方差矩阵</span></h4><p>针对我们给出的优化目标，接下来我们将从数学的角度来给出优化目标。我们看到，最终要达到的目的与<strong>变量内方差及变量间协方差</strong>有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们有：</p>
<p>假设我们只有 a 和 b 两个变量，那么我们将它们按行组成矩阵 X：</p>
<p><img src="https://www.zhihu.com/equation?tex=X%3D%5Cbegin%7Bpmatrix%7D++a_1+%26+a_2+%26+%5Ccdots+%26+a_m+%5C%5C+b_1+%26+b_2+%26+%5Ccdots+%26+b_m++%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>然后：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D%3D+%5Cbegin%7Bpmatrix%7D++%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Bb_i%5E2%7D++%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D++Cov%28a%2Ca%29+%26+Cov%28a%2Cb%29+%5C%5C++Cov%28b%2Ca%29+%26+Cov%28b%2Cb%29+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>我们可以看到这个矩阵对角线上的分别是两个变量的方差，而其它元素是 a 和 b 的协方差。两者被统一到了一个矩阵里。</p>
<p><strong>设我们有 m 个 n 维数据记录，将其排列成矩阵</strong> <img src="https://www.zhihu.com/equation?tex=X_%7Bn%2Cm%7D" alt="[公式]"> <strong>，设</strong> <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5ET" alt="[公式]"> <strong>，则 C 是一个对称矩阵，其对角线分别对应各个变量的方差，而第 i 行 j 列和 j 行 i 列元素相同，表示 i 和 j 两个变量的协方差</strong>。</p>
<h4><span id="24-矩阵对角化">2.4 矩阵对角化</span></h4><p>根据我们的优化条件，<strong>我们需要将除对角线外的其它元素化为 0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大）</strong>，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系。</p>
<p>设原始数据矩阵 X 对应的协方差矩阵为 C，而 P 是一组基按行组成的矩阵，设 Y=PX，则 Y 为 X 对 P 做基变换后的数据。设 Y 的协方差矩阵为 D，我们推导一下 D 与 C 的关系：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++D+%26+%3D++%5Cfrac%7B1%7D%7Bm%7DYY%5ET+%5C%5C++%26+%3D+%5Cfrac%7B1%7D%7Bm%7D%28PX%29%28PX%29%5ET+%5C%5C+%26+%3D+%5Cfrac%7B1%7D%7Bm%7DPXX%5ETP%5ET+%5C%5C++%26+%3D+P%28%5Cfrac%7B1%7D%7Bm%7DXX%5ET%29P%5ET+%5C%5C++%26+%3D+PCP%5ET++%5Cend%7Baligned%7D++%5C%5C" alt="[公式]"></p>
<p>这样我们就看清楚了，我们要找的 <strong>==P 是能让原始协方差矩阵对角化的 P==</strong>。换句话说，优化目标变成了<strong>寻找一个矩阵 P，满足</strong> <img src="https://www.zhihu.com/equation?tex=PCP%5ET" alt="[公式]"> <strong>是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件</strong>。</p>
<p>至此，我们离 PCA 还有仅一步之遥，我们还需要完成对角化。</p>
<p><strong>由上文知道，协方差矩阵 C 是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质：</strong></p>
<ol>
<li><strong>实对称矩阵不同特征值对应的特征向量必然正交</strong>。</li>
<li><strong>设特征向量 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> 重数为 r，则必然存在 r 个线性无关的特征向量对应于 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> ，因此可以将这 r 个特征向量单位正交化。</strong></li>
</ol>
<p><strong>由上面两条可知，一个 n 行 n 列的实对称矩阵一定可以找到 n 个单位正交特征向量，设这 n 个特征向量为 <img src="https://www.zhihu.com/equation?tex=e_1%2Ce_2%2C%5Ccdots%2Ce_n" alt="[公式]"> ，我们将其按列组成矩阵： <img src="https://www.zhihu.com/equation?tex=E%3D%28e_1+%2C+e_2+%2C+%5Ccdots+%2C+e_n+%29" alt="[公式]"> 。</strong></p>
<p>则对协方差矩阵 C 有如下结论：</p>
<p><img src="https://www.zhihu.com/equation?tex=E%5ETCE%3D%5CLambda%3D%5Cbegin%7Bpmatrix%7D+%5Clambda_1+%26+%26+%26+%5C%5C+%26+%5Clambda_2+%26+%26+%5C%5C+%26+%26+%5Cddots+%26+%5C%5C+%26+%26+%26+%5Clambda_n+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。到这里，我们发现我们已经找到了需要的矩阵 P： <img src="https://www.zhihu.com/equation?tex=P%3DE%5E%5Cmathsf%7BT%7D" alt="[公式]"> 。</p>
<p><strong>P 是协方差矩阵的特征向量单位化后按行排列出的矩阵</strong>，其中每一行都是 C 的一个特征向量。如果设 P 按照 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 中特征值的从大到小，将特征向量从上到下排列，则用 P 的前 K 行组成的矩阵乘以原始数据矩阵 X，就得到了我们需要的降维后的数据矩阵 Y。</p>
<blockquote>
<p>  <strong>拉格朗日乘子法证明</strong>:<strong>方差就是协方差矩阵的特征值</strong></p>
</blockquote>
<h4><span id="25-最近重构性-思路">2.5 最近重构性-思路</span></h4><p>以上的证明思路主要是基于最大可分性的思想，<strong>通过一条直线使得样本点投影到该直线上的方差最大</strong>。除此之外，我们还可以<strong>将其转换为线型回归问题，其目标是求解一个线性函数使得对应直线能够更好地拟合样本点集合</strong>。这就<strong>使得我们的优化目标从方差最大转化为平方误差最小</strong>，因为映射距离越短，丢失的信息也会越小。区别于最大可分性，这是从最近重构性的角度进行论证。</p>
<h3><span id="3-求解步骤">==3. 求解步骤==</span></h3><h4><span id="总结一下-pca-的算法步骤设有-m-条-n-维数据">总结一下 PCA 的算法步骤：<strong>设有 m 条 n 维数据。</strong></span></h4><ol>
<li><strong>将原始数据按列组成 n 行 m 列矩阵 X；</strong></li>
<li><strong>将 X 的每一行进行==零均值化==，即减去这一行的均值</strong>；【<strong>零均值化</strong>】【<strong>方差、协方差好计算</strong>】</li>
<li><strong>==求出协方差矩阵==</strong> <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D" alt="[公式]"> ；</li>
<li><strong>求出协方差矩阵的特征值及对应的特征向量</strong>；</li>
<li><strong>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P</strong>；</li>
<li><img src="https://www.zhihu.com/equation?tex=Y%3DPX" alt="[公式]"> <strong>即为降维到 k 维后的数据</strong>。</li>
</ol>
<h4><span id="4-性质维度灾难-降噪-过拟合-特征独立">4. 性质【维度灾难、降噪、过拟合、特征独立】</span></h4><ol>
<li><strong>==缓解维度灾难==</strong>：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；</li>
<li><strong>==降噪==</strong>：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；</li>
<li><strong>==过拟合==</strong>：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；</li>
<li><strong>==特征独立==</strong>：PCA 不仅将数据压缩到低维，它也使得<strong>降维之后的数据各特征相互独立</strong>；</li>
</ol>
<h3><span id="5-细节">5. 细节</span></h3><h4><span id="51-零均值化">5.1 零均值化</span></h4><p>当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。==而<strong>对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来</strong>，不能使用验证集或者测试集的中心向量。==</p>
<p>其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。</p>
<p>另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现 Variance Shift 的问题。</p>
<h4><span id="52-svd-的对比">==5.2 SVD 的对比==</span></h4><p>这是两个不同的数学定义。我们先给结论：<strong>特征值和特征向量是针对方阵</strong>才有的，而<strong>对任意形状的矩阵都可以做奇异值分解</strong>。</p>
<p><strong>PCA</strong>：<strong>方阵的特征值分解</strong>，对于一个方阵 A。其中，Q 是这个矩阵 A 的特征向量组成的矩阵， <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列)。也就是说矩阵 A 的信息可以由其特征值和特征向量表示。</p>
<p><strong>SVD</strong>：<strong>矩阵的奇异值分解其实就是对于矩阵 A 的协方差矩阵 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 做特征值分解推导出来的</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=A_%7Bm%2Cn%7D%3DU_%7Bm%2Cm%7D%5CLambda_%7Bm%2Cn%7DV%5ET_%7Bn%2Cn%7D+%5Capprox+U_%7Bm%2Ck%7D%5CLambda_%7Bk%2Ck%7DV%5ET_%7Bk%2Cn%7D+%5C%5C" alt="[公式]"></p>
<p>其中：U V 都是正交矩阵，有 <img src="https://www.zhihu.com/equation?tex=U%5ETU%3DI_m%2C+V%5ETV%3DI_n" alt="[公式]"> 。这里的约等于是因为 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 中有 n 个奇异值，但是由于排在后面的很多接近 0，所以我们可以仅保留比较大的 k 个奇异值。</p>
<p><img src="https://www.zhihu.com/equation?tex=A%5ETA%3D%28U+%5CLambda+V%5ET%29%5ETU+%5CLambda+V%5ET+%3DV+%5CLambda%5ET+U%5ETU+%5CLambda+V%5ET++%3D+V%5CLambda%5E2+V%5ET+%5C%5C+AA%5ET%3DU+%5CLambda+V%5ET%28U+%5CLambda+V%5ET%29%5ET+%3DU+%5CLambda+V%5ETV+%5CLambda%5ET+U%5ET+%3D+U%5CLambda%5E2+U%5ET++%5C%5C" alt="[公式]"></p>
<p>所以，V U 两个矩阵分别是 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 的特征向量，中间的矩阵对角线的元素是 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 的特征值。我们也很容易看出 A 的奇异值和 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 的特征值之间的关系。</p>
<p>PCA 需要对协方差矩阵 <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5ET" alt="[公式]"> 。进行特征值分解； SVD 也是对 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 进行特征值分解。如果取 <img src="https://www.zhihu.com/equation?tex=A%3D%5Cfrac%7BX%5ET%7D%7B%5Csqrt%7Bm%7D%7D" alt="[公式]"> 则两者基本等价。所以 PCA 问题可以转换成 SVD 求解。</p>
<p><strong>==而实际上 Sklearn 的 PCA 就是用 SVD 进行求解的==</strong>，原因有以下几点：</p>
<ol>
<li>当样本维度很高时，协方差矩阵计算太慢；</li>
<li>方阵特征值分解计算效率不高；</li>
<li><strong>==SVD 除了特征值分解这种求解方式外，还有更高效更准确的迭代求解方式，避免了 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 的计算；==</strong></li>
<li><strong>其实 PCA 与 SVD 的右奇异向量的压缩效果相同</strong>。</li>
</ol>
<blockquote>
<ol>
<li>《机器学习》周志华</li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.codinglabs.org/articles/pca-tutorial.html">PCA 的数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm">Singular Value Decomposition (SVD) tutorial</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">机器学习中的数学（4）——线性判别分析（LDA）, 主成分分析（PCA）</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//my.oschina.net/findbill/blog/535044">从SVD到PCA——奇妙的数学游戏</a></li>
<li>scikit-learn：降维算法PCA和SVD <a target="_blank" rel="noopener" href="https://blog.csdn.net/HHG20171226/article/details/102981822">https://blog.csdn.net/HHG20171226/article/details/102981822</a></li>
</ol>
</blockquote>
<h2><span id="二-线性判别分析lda监督">二、线性判别分析（LDA）【监督】</span></h2><blockquote>
<p>  ==<strong>“投影后类内方差最小，类间方差最大”</strong>==</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/liuweiyuxiang/article/details/78874106">https://blog.csdn.net/liuweiyuxiang/article/details/78874106</a></li>
</ul>
</blockquote>
<h3><span id="21-概念">2.1 概念</span></h3><p><strong>线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。</strong></p>
<p><strong>LDA分类思想简单总结如下：</strong></p>
<ol>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。</li>
<li>对于训练数据，设法将多维数据投影到一条直线上，<strong>同类数据的投影点尽可能接近，异类数据点尽可能远离</strong>。</li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</li>
</ol>
<p><strong><font color="red"> 如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</font></strong></p>
<p>假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="image-20220526135646769.png" alt="image-20220526135646769"></p>
<p> 从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。 以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3><span id="22-原理">2.2 原理</span></h3><p> LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(<a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Linear_classifier">Linear Classifier</a>)：因为LDA是一种线性分类器。对于<strong>K-分类的一个分类问题，会有K个线性函数</strong>：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455464493.png" alt="image"></p>
<p>当满足条件：对于所有的j，都有Yk &gt; Yj,的时候，我们就说x属于类别k。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的就是所属的分类了。</p>
<p>上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455475507.gif" alt="clip_image002" style="zoom:67%;"></p>
<p>红色的方形的点为0类的原始点、蓝色的方形点为1类的原始点，经过原点的那条线就是投影的直线，从图上可以清楚的看到，红色的点和蓝色的点被<strong>原点</strong>明显的分开了，这个数据只是随便画的，如果在高维的情况下，看起来会更好一点。下面我来推导一下二分类LDA问题的公式：假设用来区分二分类的直线（投影函数)为：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455471885.png" alt="image"></p>
<p> <strong>LDA分类的一个目标是使得==不同类别==之间的距离越远越好，==同一类别==之中的距离越近越好</strong>，所以我们需要定义几个关键的值。</p>
<ul>
<li><p><strong>==类别i的原始中心点为==</strong>：（Di表示属于类别i的点)</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455478264.png" alt="image"></p>
</li>
<li><p>类别i投影后的中心点为：</p>
</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455488231.png" alt="image"></p>
<ul>
<li><strong>==衡量类别i投影后，类别点之间的分散程度（方差）为==</strong>：</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455487326.png" alt="image"></p>
<ul>
<li><strong>==最终我们可以得到一个下面的公式，表示LDA投影到w后的损失函数==</strong>：</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455484785.png" alt="image"></p>
<p>我们<strong>分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。</strong>分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。</p>
<p> 我们定义一个<strong>投影前的==各类别分散程度的矩阵==</strong>，这个矩阵看起来有一点麻烦，其实意思是，如果某一个分类的<strong>输入点集Di里面的点距离这个分类的中心店mi越近</strong>，则Si里面元素的值就越小，如果分类的点都紧紧地围绕着mi，则Si里面的元素值越更接近0.</p>
<script type="math/tex; mode=display">
S_{i}=\sum_{x \in D_{i}}\left(x-m_{i}\right)\left(x-m_{i}\right)^{T}</script><p>带入 $\mathrm{Si}$, 将 $\mathrm{J}(\mathrm{w})$ 分母化为:</p>
<p>$\tilde{s}<em>{i}=\sum</em>{x \in D<em>{i}}\left(w^{T} x-w^{T} m</em>{i}\right)^{2}=\sum<em>{x \in D</em>{i}} w^{T}\left(x-m<em>{i}\right)\left(x-m</em>{i}\right)^{T} w=w^{T} S_{i} w$</p>
<script type="math/tex; mode=display">{\tilde{S_{1}}}^{2}+{\tilde{S_{2}}}^{2}=w^{T}\left(S_{1}+S_{2}\right) w=w^{T} S_{w} w</script><p>同样的将 $\mathrm{J}(\mathrm{w})$ 分子化为:</p>
<script type="math/tex; mode=display">
\left|\widetilde{m_{1}}-\widetilde{m_{2}}\right|^{2}=w^{T}\left(m_{1}-m_{2}\right)\left(m_{1}-m_{2}\right)^{T} w=w^{T} S_{B} w</script><p>这样<strong>损失函数</strong>可以化成下面的形式:</p>
<script type="math/tex; mode=display">
J(w)=\frac{w^{T} S_{B} w}{w^{T} S_{w} w}</script><p>这样就可以用最喜欢的<strong>==拉格朗日乘子法==</strong>了, 但是还有一个问题, 如果分子、分母是都可以取任意值的, 那就会 使得有无穷解, 我们将分母限制为长度为 1, 并作为拉格朗日乘子法的限制条件, 带入得到:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&c(w)=w^{T} S_{B} w-\lambda\left(w^{T} S_{w} w-1\right) \\
&\Rightarrow \frac{d c}{d w}=2 S_{B} w-2 \lambda S_{w} w=0 \\
&\Rightarrow S_{B} w=\lambda S_{w} w
\end{aligned}</script><p><strong>==这样的式子就是一个求特征值的问题了。==</strong><br>对于 $N(N&gt;2)$ 分类的问题, 我就直接写出下面的结论了:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&S_{W}=\sum_{i=1}^{c} S_{i} \\
&S_{B}=\sum_{i=1}^{c} n_{i}\left(m_{i}-m\right)\left(m_{i}-m\right)^{T} \\
&S_{B} w_{i}=\lambda S_{w} w_{i}
\end{aligned}</script><p>这同样是一个求特征值的问题，我们求出的第i大的特征向量，就是对应的Wi了。</p>
<blockquote>
<p>  这里想多谈谈特征值，特征值在纯数学、量子力学、固体力学、计算机等等领域都有广泛的应用，特征值表示的是矩阵的性质，当我们取到矩阵的前N个最大的特征值的时候，我们可以说提取到的矩阵主要的成分（这个和之后的PCA相关，但是不是完全一样的概念）。在机器学习领域，不少的地方都要用到特征值的计算，比如说图像识别、pagerank、LDA、还有之后将会提到的PCA等等。</p>
</blockquote>
<p><strong>优缺点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>优缺点</th>
<th>简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>1. 可以使用类别的先验知识； 2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr>
<td>缺点</td>
<td>1. LDA不适合对非高斯分布样本进行降维； 2. <strong>LDA降维最多降到分类数k-1维</strong>； 3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好； 4. LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="三-t-sne-高维数据可视化">三、t-SNE 高维数据可视化</span></h2><blockquote>
<p>  高维数据可视化之t-SNE算法🌈:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/57937096">https://zhuanlan.zhihu.com/p/57937096</a></p>
</blockquote>
<p><strong>T-SNE算法是用于可视化的算法中效果最好的算法之一</strong>，相信大家也对T-SNE算法略有耳闻，本文参考T-SNE作者<strong>Laurens van der Maaten</strong>给出的源代码自己实现T-SNE算法代码，以此来加深对T-SNE的理解。先简单介绍一下T-SNE算法，T-SNE将数据点变换映射到概率分布上。</p>
<h4><span id="31-t-sne数据算法的目的">3.1 t-SNE数据算法的目的</span></h4><p><strong>主要是将数据从高维数据转到低维数据，并在低维空间里也保持其在高维空间里所携带的信息（比如高维空间里有的清晰的分布特征，转到低维度时也依然存在）。</strong></p>
<p><strong>==t-SNE将欧氏距离距离转换为条件概率，来表达点与点之间的相似度，再优化两个分布之间的距离-KL散度，从而保证点与点之间的分布概率不变。==</strong></p>
<h4><span id="32-sne原理">3.2 SNE原理</span></h4><p>$S N E$ 是<strong>通过仿射变换将数据点映射到相应概率分布上</strong>, 主要包括下面两个步骤:</p>
<ol>
<li>通过在高维空间中构建数据点之间的概率分布 $P$, 使得相似的数据点有更高的概率被选择, 而 不相似的数据点有较低的概率被选择;</li>
<li>然后在低维空间里重构这些点的概率分布 $Q$, 使得这两个概率分布尽可能相似。</li>
</ol>
<p>令输入空间是 $X \in \mathbb{R}^{n}$, 输出空间为 $Y \in \mathbb{R}^{t}(t \ll n)$ 。不妨假设含有 $m$ 个样本数据 $\left{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\right}$, 其中 $x^{(i)} \in X$, 降维后的数据为 $\left{y^{(1)}, y^{(2)}, \cdots, y^{(m)}\right}, y^{(i)} \in Y$ 。 $S N E$ 是<strong>先将欧几里得距离转化为条件概率来表达点与点之间的相似度</strong>, 即首先是计算条件概 率 $p<em>{j \mid i}$, 其正比于 $x^{(i)}$ 和 $x^{(j)}$ 之间的相似度, $p</em>{j \mid i}$ 的计算公式为:</p>
<script type="math/tex; mode=display">
p_{j \mid i}=\frac{\exp \left(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^{2}}{2 \sigma_{i}^{2}}\right)}{\sum_{k \neq i} \exp \left(-\frac{\left\|x^{(i)}-x^{(k)}\right\|^{2}}{2 \sigma_{i}^{2}}\right)}</script><p>在这里引入了一个参数 $\sigma<em>{i}$, 对于不同的数据点 $x^{(i)}$ 取值亦不相同, 因为我们关注的是不同数据 点两两之间的相似度, 故可设置 $p</em>{i \mid i}=0$ 。对于低维度下的数据点 $y^{(i)}$, 通过条件概率 $q<em>{j \mid i}$ 来 刻画 $y^{(i)}$ 与 $y^{(j)}$ 之间的相似度, $q</em>{j \mid i}$ 的计算公式为:</p>
<script type="math/tex; mode=display">
q_{j \mid i}=\frac{\exp \left(-\left\|y^{(i)}-y^{(j)}\right\|^{2}\right)}{\sum_{k \neq i} \exp \left(-\left\|y^{(i)}-y^{(k)}\right\|^{2}\right)}</script><p>同理, 设置 $q<em>{i \mid i}=0$ 。<br>如果降维的效果比较好, 局部特征保留完整, 那么有 $p</em>{i \mid j}=q_{i \mid j}$ 成立, 因此通过优化两个分布之 间的 <strong>$K L$ 散度构造出的损失函数为</strong>:</p>
<script type="math/tex; mode=display">
C\left(y^{(i)}\right)=\sum_{i} K L\left(P_{i} \| Q_{i}\right)=\sum_{i} \sum_{j} p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}}</script><p>这里的 $P<em>{i}$ 表示在给定高维数据点 $x^{(i)}$ 时, 其他所有数据点的条件概率分布; $Q</em>{i}$ 则表示在给定 低维数据点 $y^{(i)}$ 时, 其他所有数据点的条件概率分布。从损失函数可以看出, 当 $p<em>{j \mid i}$ 较大 $q</em>{j \mid i}$ 较小时, 惩罚较高; 而 $p<em>{j \mid i}$ 较小 $q</em>{j \mid i}$ 较大时, 惩罚较低。换句话说就是高维空间中两个数据点距 离较近时, 若映射到低维空间后距离较远, 那么将得到一个很高的惩罚; 反之, 高维空间中两个数 据点距离较远时, 若映射到低维空间距离较近, 将得到一个很低的惩罚值。也就是说, <strong>$S N E$ 的 损失函数更关注于局部特征, 而忽视了全局结构</strong>。</p>
<h4><span id="33-目标函数求解">3.3 目标函数求解</span></h4><h4><span id="34-对称性-sne">3.4 对称性-SNE</span></h4><p><strong>优化 <img src="https://www.zhihu.com/equation?tex=KL%28P%5CVert+Q%29" alt="[公式]"> 的一种替换思路是使用联合概率分布来替换条件概率分布</strong>，即 <img src="https://www.zhihu.com/equation?tex=P" alt="[公式]"> 是高维空间里数据点的联合概率分布， <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 是低维空间里数据点的联合概率分布，此时的损失函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=C%28y%5E%7B%28i%29%7D%29%3DKL%28P%5CVert+Q%29%3D%5Csum%5Climits_i%5Csum%5Climits_jp_%7Bij%7D%5Clog%5Cdfrac%7Bp_%7Bij%7D%7D%7Bq_%7Bij%7D%7D%5C%5C" alt="[公式]"></p>
<p>同样的 <img src="https://www.zhihu.com/equation?tex=p_%7Bii%7D%3Dq_%7Bii%7D%3D0" alt="[公式]"> ，这种改进下的 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 称为对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> ，因为它的先验假设为对 <img src="https://www.zhihu.com/equation?tex=%5Cforall+i" alt="[公式]"> 有 <img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3Dp_%7Bji%7D%2Cq_%7Bij%7D%3Dq_%7Bji%7D" alt="[公式]"> 成立，故概率分布可以改写成：</p>
<p><img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3D%5Cdfrac%7Bexp%28-%5Cfrac%7B%5CVert+x%5E%7B%28i%29%7D-x%5E%7B%28j%29%7D%5CVert%5E2%7D%7B2%5Csigma%5E2%7D%29%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7Dexp%28-%5Cfrac%7B%5CVert+x%5E%7B%28k%29%7D-x%5E%7B%28l%29%7D+%5CVert%5E2%7D%7B2%5Csigma%5E2%7D%29%7D%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+q_%7Bij%7D%3D%5Cdfrac%7Bexp%28-%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7Dexp%28-%5CVert+y%5E%7B%28k%29%7D-y%5E%7B%28l%29%7D+%5CVert%5E2%29%7D%5C%5C" alt="[公式]"></p>
<p>这种改进方法使得表达式简洁很多，但是容易受到异常点数据的影响，为了解决这个问题通过对联合概率分布定义修正为： <img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3D%5Cfrac%7Bp_%7Bj%7Ci%7D%2Bp_%7Bi%7Cj%7D%7D%7B2%7D" alt="[公式]"> ，这保证了 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_jp_%7Bij%7D+%5Cgt+%5Cfrac%7B1%7D%7B2m%7D" alt="[公式]"> ，使得每个点对于损失函数都会有贡献。对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 最大的优点是简化了梯度计算，梯度公式改写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpartial+C%28y%5E%7B%28i%29%7D%29%7D%7B%5Cpartial+y%5E%7B%28i%29%7D%7D%3D4%5Csum%5Climits_j%28p_%7Bij%7D-q_%7Bij%7D%29%28y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D%29%5C%5C" alt="[公式]"></p>
<p>研究表明，对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 的效果差不多，有时甚至更好一点。</p>
<h4><span id="35-t-sne">3.5 t-SNE</span></h4><p><img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7DSNE" alt="[公式]"> 在对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 的改进是，首先<strong>通过在高维空间中使用高斯分布将距离转换为概率分布，然后在低维空间中，使用更加偏重长尾分布的方式来将距离转换为概率分布</strong>，使得高维度空间中的中低等距离在映射后能够有一个较大的距离。</p>
<p><img src="https://pic4.zhimg.com/80/v2-928a3ada308128f26b719d510a728fbb_1440w.jpg" alt="img"></p>
<p>从图中可以看到，在没有异常点时， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布与高斯分布的拟合结果基本一致。而在第二张图中，出现了部分异常点，由于高斯分布的尾部较低，对异常点比较敏感，为了照顾这些异常点，高斯分布的拟合结果偏离了大多数样本所在位置，方差也较大。<strong>相比之下， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布的尾部较高，对异常点不敏感，保证了其鲁棒性，因此拟合结果更为合理，较好的捕获了数据的全局特征。</strong></p>
<p>使用 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布替换高斯分布之后 <img src="https://www.zhihu.com/equation?tex=q_%7Bij+%7D" alt="[公式]"> 的变化如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=q_%7Bij%7D%3D%5Cdfrac%7B%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7D%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%7D%5C%5C" alt="[公式]"></p>
<p>此外，随着自由度的逐渐增大， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布的密度函数逐渐接近标准正态分布，因此在计算梯度方面会简单很多，优化后的梯度公式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpartial+C%28y%5E%7B%28i%29%7D%29%7D%7B%5Cpartial+y%5E%7B%28i%29%7D%7D%3D4%5Csum%5Climits_%7Bj%7D%28p_%7Bij%7D-q_%7Bij%7D%29%28y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D%29%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%5C%5C" alt="[公式]"></p>
<p>总的来说， <img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7DSNE" alt="[公式]"> 的梯度更新具有以下两个优势：</p>
<ul>
<li><strong>对于低维空间中不相似的数据点，用一个较小的距离会产生较大的梯度让这些数据点排斥开来</strong>；</li>
<li><strong>这种排斥又不会无限大，因此避免了不相似的数据点距离太远</strong>。</li>
</ul>
<h4><span id="总结">总结：</span></h4><p><img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7D+SNE" alt="[公式]"> 算法其实就是在 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 算法的基础上增加了两个改进：</p>
<ul>
<li>把 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 修正为对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> ，提高了计算效率，效果稍有提升；</li>
<li>在低维空间中采用了 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布替换原来的高斯分布，解决了高维空间映射到低维空间所产生的拥挤问题，优化了 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 过于关注局部特征而忽略全局特征的问题 。</li>
</ul>
<h2><span id="四-autoencoder">四、AutoEncoder</span></h2><blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80377698">【全】一文带你了解自编码器（<em>AutoEncoder</em>）</a></li>
</ul>
</blockquote>
<p>理解为：（下图）高维数据（左测蓝色）通过某种网络变成低位数据（中间红色）后，又经过某种网络变回高维数据（右侧蓝色）。数据经过该模型前后没有变化，而中间的低维数据完全具有输入输出的高维数据的全部信息，所以可以用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=低维数据&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;157482881&quot;}">低维数据</a>代表高维数据。</p>
<p>之所以叫AutoEncoder，而不叫AutoEncoderDecoder，是因为训练好之后只有encoder部分有用，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=decoder&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;157482881&quot;}">decoder</a>部分就不用了。</p>
<p><img src="https://pic2.zhimg.com/v2-47f6429e5ffb379205ba0bcb0db399d1_b.jpg" alt="img"></p>
<p>进入深度学习的思路之后，编码的网络是开放的，可以自由设计的。一个思路是端到端，将网络的输出设为你任务要的结果（如类别、序列等），<strong>过程中的某层嵌入都可以作为降维的低维结果</strong>。当然，这种低维结果其实是模型的副产品，因为任务已经解决。比如bert模型得到（中文的）字嵌入。</p>
<h4><span id="优点">优点：</span></h4><ul>
<li>能够学习到非线性特性</li>
<li>降低数据维度</li>
</ul>
<h4><span id="缺点">缺点：</span></h4><ul>
<li>训练的<strong>计算成本高</strong></li>
<li><strong>可解释性较差</strong></li>
<li>背后的数学知识复杂</li>
<li>容易产生<strong>过度拟合</strong>的问题，尽管可以通过引入正则化策略缓解</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/25V46VQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/25V46VQ/" class="post-title-link" itemprop="url">机器学习（7）朴素贝叶斯</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:43:32" itemprop="dateCreated datePublished" datetime="2022-03-15T22:43:32+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-29 16:20:32" itemprop="dateModified" datetime="2023-01-29T16:20:32+08:00">2023-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" itemprop="url" rel="index"><span itemprop="name">贝叶斯分类器</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="朴素贝叶斯">朴素贝叶斯</span></h2><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/dev/modules/naive_bayes.html#naive-bayes">https://scikit-learn.org/dev/modules/naive_bayes.html#naive-bayes</a></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://plushunter.github.io/2017/02/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">FREE WILL 机器学习算法系列（10）：朴素贝叶斯</a></p>
</li>
<li><h5><span id="最大似然估计-最大后验估计-贝叶斯估计的对比"></span></h5></li>
</ul>
<h3><span id="一-朴素贝叶斯的学习与分类">一、朴素贝叶斯的学习与分类</span></h3><blockquote>
<p>  朴素贝叶斯（Naive Bayes）是基于<strong>贝叶斯定理</strong>与<strong>特征条件假设</strong>的<strong>分类</strong>方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯是<strong>选出各个分类类别后验概率最大</strong>的作为最终分类。</p>
<ul>
<li><strong>优点</strong>：对小规模的数据表现很好，适合多分类任务，<strong>适合增量式训练</strong>。</li>
<li><strong>缺点</strong>：对输入数据的表达形式很敏感<strong>（离散、连续，值极大极小之类的）</strong>。</li>
</ul>
</blockquote>
<h4><span id="11贝叶斯定理">1.1贝叶斯定理</span></h4><p><strong>条件概率:</strong></p>
<p>$P(A|B)$  表示事件B已经发生的前提下，事件B已经发生的前提下，事件A发生的概率，叫做事件 $B$<br>发生下事件 $A$ 的条件概率。其基本求解公式为</p>
<p><img src="image-20220325172121116.png" alt="image-20220325172121116" style="zoom:50%;"></p>
<p><strong>贝叶斯定理</strong>便是基于<strong>条件概率</strong>，通过$P(A|B)$来求$P(B|A)$：【通过<strong>先验概率</strong>计算<strong>后验概率</strong>】</p>
<p><img src="image-20220325172351101.png" alt="image-20220325172351101" style="zoom:50%;"></p>
<p>顺便提一下，上式中的分母，可以根据<strong>全概率公式</strong>分解为：</p>
<p><img src="image-20220325172306652.png" alt="image-20220325172306652" style="zoom:50%;"></p>
<h4><span id="12-特征条件独立假设">1.2 特征条件独立假设</span></h4><p>这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件<strong>独立假设</strong>。给定训练数据集$(X,Y)$，其中每个样本$X$都包括 $n$ 维特征，即$x=(x1,x2,···,xn)$，类标记集合含有$K$种类别，即$y=(y1,y2,···,yk)$.</p>
<p>如果现在来了一个新样本 $x$ 我们要怎么判断它的类别?从概率的角度来看，这个问题就是给定$x$，它属于哪个类别的概率更大。那么问题就转化为求解 $P(y1|x),P(y2|x),P(yk|x)P(y1|x),P(y2|x),P(yk|x)$ 中最大的那个，即求<strong>后验概率最大</strong>的输出：==$arg max_{y_k}P(y_k|x)$==</p>
<p><img src="image-20220325203314162.png" alt="image-20220325203314162" style="zoom:50%;"></p>
<p>根据<strong>全概率公式</strong>，可以进一步分解上式中的分母：</p>
<p><img src="image-20220325203335825.png" alt="image-20220325203335825" style="zoom:50%;"></p>
<ul>
<li><p><img src="image-20220325205350669.png" alt="image-20220325205350669" style="zoom:50%;">：<strong>先验概率</strong> 【训练集计算】</p>
</li>
<li><p><img src="image-20220325205444531.png" alt="image-20220325205444531" style="zoom:50%;">：<strong>条件概率</strong>，它的参数规模是<strong>指数</strong>数量级别的。假设第i维特征xi可取值的个数有Si个，类别取值个数为k个，那么参数个数为$k∏S_j$。</p>
</li>
<li><p><strong>独立性的假设</strong>：通俗地讲就是说假设各个维度的特征互相独立，这样<strong>参数规模</strong>就降到了$∑S_ik$, 【积-&gt;和】</p>
<p><img src="image-20220325211846297.png" alt="image-20220325211846297" style="zoom:50%;"></p>
</li>
<li><p>代入公式1得出：</p>
<p><img src="image-20220325213643901.png" alt="image-20220325213643901" style="zoom:50%;"></p>
</li>
<li><p>于是朴素贝叶斯分类器可表示为：</p>
<p><img src="image-20220325213705861.png" alt="image-20220325213705861" style="zoom:50%;"></p>
</li>
<li><p>由于分母值都是一样的：<strong>==极大后验概率估计==</strong></p>
</li>
</ul>
<p><img src="image-20220325213802190.png" alt="image-20220325213802190" style="zoom:50%;"></p>
<h4><span id="13-朴素贝叶斯法的参数估计求解">1.3 朴素贝叶斯法的参数估计【求解】</span></h4><p>朴素贝叶斯要学习的东西就是：<img src="image-20220325215221520.png" alt="image-20220325215221520" style="zoom:50%;"> 和 <img src="image-20220325215238733.png" alt="image-20220325215238733" style="zoom:50%;">【极大似然函数 + 拉格朗日乘数法】</p>
<ul>
<li><p><strong>先验概率</strong>$P(Y=ck)$的极大似然估计是, <strong>样本在$c_k$出现的次数除以样本容量</strong>：</p>
<p><img src="image-20220325215849572.png" alt="image-20220325215849572" style="zoom:50%;"></p>
</li>
<li><p>$设第 j 个特征x(j)可能取值的集合为a<em>{j1},a</em>{j2},···,a<em>{jl}, 条件概率P(X_j=a</em>{jl} |Y=ck)的极大似然估计是$：</p>
<p><img src="image-20220325221320810.png" alt="image-20220325221320810" style="zoom:50%;"></p>
</li>
</ul>
<h4><span id="14-贝叶斯估计缺失值处理拉普拉斯平滑">1.4 贝叶斯估计【缺失值处理】【拉普拉斯平滑】</span></h4><p><strong>先验概率</strong>的贝叶斯估计：</p>
<p><img src="image-20220325222259483.png" alt="image-20220325222259483" style="zoom:50%;"></p>
<p><strong>条件概率</strong>的贝叶斯估计：【<strong>离散型</strong>】</p>
<p><img src="image-20220325222226643.png" alt="image-20220325222226643" style="zoom:50%;"></p>
<h4><span id="15-朴素贝叶斯有什么优缺点"><strong><font color="red"> 1.5 朴素贝叶斯有什么优缺点？</font></strong></span></h4><h5><span id="优点数学理论-缺失异常不敏感-快-增量式训练">优点：【数学理论、缺失异常不敏感、快、增量式训练】</span></h5><ul>
<li>朴素贝叶斯模型<strong>发源于古典数学理论</strong>，有稳定的分类效率。</li>
<li><strong>对缺失数据和异常数据不太敏感</strong>，算法也比较简单，常用于文本分类。</li>
<li><strong>分类准确度高，速度快</strong>。</li>
<li><strong>对小规模的数据表现很好，能处理多分类任务，适合增量式训练，当数据量超出内存时，我们可以一批批的去增量训练</strong>(朴素贝叶斯在训练过程中只需要计算各个类的概率和各个属性的类条件概率，这些概率值可以快速地根据增量数据进行更新，无需重新全量计算)。</li>
</ul>
<h5><span id="缺点">缺点：</span></h5><ul>
<li><strong>对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）</strong>。</li>
<li><strong>对训练数据的依赖性很强</strong>，如果训练数据误差较大，那么预测出来的效果就会不佳。</li>
<li>理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。 但是在实际中，因为朴素贝叶斯“朴素，”的特点，<strong>导致在属性个数比较多或者属性之间相关性较大时，分类效果不好。</strong>而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。</li>
<li>需要知道<strong>先验概率</strong>，且先验概率很多时候是基于假设或者已有的训练数据所得的，这在某些时候可能会因为假设先验概率的原因出现分类决策上的错误。</li>
</ul>
<h2><span id="二-高斯贝叶斯模型">二、高斯贝叶斯模型</span></h2><blockquote>
<p>  classifier = naive_bayes.MultinomialNB()</p>
</blockquote>
<h4><span id="21-朴素贝叶斯连续型数据处理">2.1 朴素贝叶斯(连续型数据处理)</span></h4><ul>
<li>每一个连续的<strong>数据离散化</strong>，然后用相应的离散区间替换连续数值。这种方法对于划分离散区间的粒度要求较高，不能太细，也不能太粗。</li>
<li>假设<strong>连续数据服从某个概率分布</strong>，<strong>使用训练数据估计分布参数</strong>，通常我们用<strong>高斯分布</strong>来表示<strong>连续数据的类条件概率分布</strong>。</li>
</ul>
<p><strong>==GaussianNB 的条件概率密度计算：其中均值和方差可以通过极大似然估计得出。==</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
&p\left(X^{(j)}=a_{j l} \mid y=c_{k}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{j k}} e^{-\frac{\left(a_{j l}-\mu_{j k}\right)^{2}}{2 \sigma_{j k}^{2}}}
\end{aligned}</script><h2><span id="三-贝叶斯网络">三、贝叶斯网络</span></h2><h4><span id="31-概率图模型">3.1 概率图模型</span></h4><p>概率图模型分为<strong>贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）</strong>两大类。贝叶斯网络可以用一个有向图结构表示，马尔可夫网络可以表示成一个无向图的网络结构。更详细地说，<strong>概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型</strong>等，在机器学习的诸多场景中都有着广泛的应用。</p>
<ul>
<li><strong>贝叶斯网络</strong> — 结点与结点之间是以有向箭头相连接，代表是这个结点会影响下一个结点</li>
<li><strong>马尔可夫网络</strong> — 结点与结点之间是以无向箭头相连接，代表是结点与结点之间会相互影响</li>
</ul>
<h2><span id="四-最大似然估计-最大后验估计-贝叶斯估计的对比">四、最大似然估计、最大后验估计、贝叶斯估计的对比</span></h2><h4><span id="41-贝叶斯公式">4.1 <strong>贝叶斯公式</strong></span></h4><p>这三种方法都和贝叶斯公式有关，所以我们先来了解下贝叶斯公式：</p>
<script type="math/tex; mode=display">
p(\theta \mid X)=\frac{p(X \mid \theta) p(\theta)}{p(X)}</script><p>每一项的表示如下:</p>
<script type="math/tex; mode=display">
\text { posterior }=\frac{\text { likehood } * \text { prior }}{\text { evidence }}</script><ul>
<li>posterior: 通过样本X得到参数 $\theta$ 的概率, 也就是后验概率。</li>
<li>likehood: 通过参数 $\theta$ 得到样本X的概率, 似然函数, 通常就是我们的数据集的表现。</li>
<li>prior: 参数 $\theta$ 的先验概率, 一般是根据人的先验知识来得出的。</li>
</ul>
<h4><span id="42-极大似然估计-mle">4.2 极大似然估计 (MLE)</span></h4><p>极大似然估计的核心思想是: 认为当前发生的事件是概率最大的事件。<strong>因此就可以给定的数据集, 使得该数据集发生的概率最大来求得模型中的参数</strong>。似然函数如下:</p>
<script type="math/tex; mode=display">
p(X \mid \theta)=\prod_{x 1}^{x n} p(x i \mid \theta)</script><p>为了便于计算, 我们对似然函数两边取对数, 生成新的对数似然函数（因为对数函数是单调增函数, 因此求似然函数最大化就可 以转换成对数似然函数最大化）：</p>
<script type="math/tex; mode=display">
p(X \mid \theta)=\prod_{x 1}^{x n} p(x i \mid \theta)=\sum_{x 1}^{x n} \log p(x i \mid \theta)</script><p>求对数似然函数最大化, 可以通过导数为 0 来求解。<strong><font color="red"> 极大似然估计只关注当前的样本, 也就是只关注当前发生的事情, 不考虑事情的先验情况</font></strong>。由于计算简单, 而且不需要关注先验 知识, 因此在机器学习中的应用非常广, 最常见的就是逻辑回归。</p>
<h4><span id="43-最大后验估计-map">4.3 最大后验估计 (MAP)</span></h4><p>和最大似然估计不同的是, 最大后验估计中引入了<strong>先验概率</strong>（先验分布属于贝叶斯学派引入的, 像L1, L2正则化就是对参数引入 了拉普拉斯先验分布和高斯先验分布）, 而且最大后验估计要求的是 $p(\theta \mid X)$<br>最大后验估计可以写成下面的形式:</p>
<script type="math/tex; mode=display">
\operatorname{argmaxp}(\theta \mid X)=\operatorname{argmax} \frac{p(X \mid \theta) p(\theta)}{p(X)}=\operatorname{argmaxp}(X \mid \theta) p(\theta)=\operatorname{argmax}\left(\prod_{x 1}^{x n} p(x i \mid \theta)\right) p(\theta)</script><p>在求最大后验概率时, 可以忽略分母 $p(x)$, 因为该值不影响对 $\theta$ 的估计。同样为了便于计算, 对两边取对数, 后验概率最大化就变成了:</p>
<script type="math/tex; mode=display">
\operatorname{argmax}\left(\sum_{x 1}^{x n} \operatorname{logp}(x i \mid \theta)+\log p(\theta)\right)</script><p><strong><font color="red"> 最大后验估计不只是关注当前的样本的情况，还关注已经发生过的先验知识。在朴素贝叶斯中会有最大后验概率的应用，但并没有用上最大后验估计来求参数（因为朴素贝叶斯中的θ其实就是分类的类别）。</font></strong></p>
<p><strong>最大后验估计和最大似然估计的区别：</strong>最大后验估计允许我们把先验知识加入到估计模型中，<strong>这在样本很少的时候是很有用的（因此朴素贝叶斯在较少的样本下就能有很好的表现）</strong>，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如beta分布的α，β，我们还可以调节把估计的结果“拉”向先验的幅度，α，β越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。</p>
<h2><span id="朴素贝叶斯-qampa">朴素贝叶斯 Q&amp;A</span></h2><blockquote>
<ul>
<li>朴素贝叶斯分类器原理以及公式，出现估计概率值为 0 怎么处理（拉普拉斯平滑），缺点；</li>
<li>解释贝叶斯公式和朴素贝叶斯分类。</li>
<li>贝叶斯分类，这是一类分类方法，主要代表是朴素贝叶斯，朴素贝叶斯的原理，重点在假设各个属性类条件独立。然后能根据贝叶斯公式具体推导。考察给你一个问题，如何利用朴素贝叶斯分类去分类，比如：给你一个人的特征，判断是男是女，比如身高，体重，头发长度等特征的的数据，那么你要能推到这个过程。给出最后的分类器公式。</li>
<li>那你说说贝叶斯怎么分类啊？<strong>比如说看看今天天气怎么样？</strong>我：blabla，，，利用天气的历史数据，可以知道天气类型的先验分布，以及每种类型下特征数据（比如天气数据的特征：温度啊，湿度啊）的条件分布，这样我们根据贝叶斯公式就能求得天气类型的后验分布了。。。。面试官：en（估计也比较满意吧）<strong>那你了解关于求解模型的优化方法吗？一般用什么优化方法来解？</strong></li>
<li>贝叶斯分类器的优化和特殊情况的处理</li>
</ul>
</blockquote>
<h3><span id="1-朴素贝叶斯-svm和lr的区别"><strong><font color="red"> 1、朴素贝叶斯、SVM和LR的区别？</font></strong></span></h3><p><strong>朴素贝叶斯是生成模型</strong>，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解P(Y|X)。</p>
<p><strong>LR是判别模型</strong>，根据极大化对数似然函数直接求出条件概率P(Y|X)；朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求；<strong>朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>SVM</th>
<th>LR</th>
<th>朴素贝叶斯</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong></td>
<td><strong>想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面</strong>。</td>
<td>使用线性回归模型的预测值逼近分类任务真实标记的对数几率。</td>
<td>基于<strong>贝叶斯定理</strong>与<strong>特征条件假设</strong>的<strong>分类</strong>方法。选出各个分类类别后验概率最大的作为最终分类。</td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td>判别模型、<strong>非概率方法</strong>；</td>
<td><strong>概率方法</strong>；需要对$p(y</td>
<td>x)$进行假设，具有概率意义。</td>
<td>生成模型</td>
</tr>
<tr>
<td><strong>经验损失函数</strong></td>
<td><strong>合页损失函数</strong>；有一段平的零区域、使得SVM的对偶性有稀疏性。</td>
<td><strong>交叉熵损失函数</strong></td>
<td><strong>后验概率最大</strong></td>
</tr>
<tr>
<td><strong>训练样本</strong></td>
<td><strong>支持向量</strong>（少数样本），SVM的参数和假设函数只和支持向量有关。</td>
<td>全样本</td>
<td>全样本</td>
</tr>
<tr>
<td><strong>优化方法</strong></td>
<td>次梯度下降和坐标梯度下降 【<strong>SMO算法</strong>】</td>
<td><strong>梯度下降</strong></td>
<td>无</td>
</tr>
<tr>
<td>多分类</td>
<td><strong>多分类SVM</strong></td>
<td><strong>Softmax回归</strong></td>
<td>后验概率最大</td>
</tr>
<tr>
<td><strong>敏感程度</strong></td>
<td><strong>SVM考虑分类边界线附近的样本</strong>（决定分类超平面的样本）。在支持向量外添加或减少任何样本点对分类决策面没有任何影响；【不敏感】</td>
<td><strong>LR受所有数据点的影响</strong>。直接依赖数据分布，每个样本点都会影响决策面的结果。如果训练数据不同类别严重不平衡。【敏感】</td>
<td><strong>特征值是基于频数进行统计的。</strong>一个值的异常（变成了别的数），<strong>只是贝叶斯公式里的计算概率的分子或者分母发生微小的变化，整体结果影响不大</strong>，不敏感【概率排序】</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="2-朴素贝叶斯朴素在哪里">2、<strong>朴素贝叶斯“朴素”在哪里？</strong></span></h3><p>简单来说：它假定<strong>所有的特征在数据集中的作用是同样重要和独立的</strong>，正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。</p>
<p>利用贝叶斯定理求解联合概率P(XY)时，需要计算条件概率P(X|Y)。在计算P(X|Y)时，朴素贝叶斯做了一个很强的条件独立假设（当Y确定时，X的各个分量取值之间相互独立），即P(X1=x1,X2=x2,…Xj=xj|Y=yk) = P(X1=x1|Y=yk)P(X2=x2|Y=yk)…*P(Xj=xj|Y=yk)。 多个特征全是独立的，需要分别相乘。</p>
<h3><span id="3-在估计条件概率pxy时出现概率为0的情况怎么办">3、<strong>在估计条件概率P(X|Y)时出现概率为0的情况怎么办？</strong></span></h3><p><strong>拉普拉斯平滑法</strong>是朴素贝叶斯中处理零概率问题的一种修正方式。在进行分类的时候，可能会出现某个属性在训练集中没有与某个类同时出现过的情况，如果直接基于朴素贝叶斯分类器的表达式进行计算的话就会出现<strong>零概率现象</strong>。</p>
<p>为了避免其他属性所携带的信息被训练集中未出现过的属性值“抹去”，所以才使用拉普拉斯估计器进行修正。具体的方法是：<strong>在分子上加1,对于先验概率，在分母上加上训练集中label的类别数；对于特征i 在label下的条件概率，则在分母上加上第i个属性可能的取值数（特征 i 的unique()）</strong></p>
<p><strong>先验概率</strong>的贝叶斯估计：</p>
<p><img src="image-20230129161943432.png" alt="image-20230129161943432" style="zoom:50%;"></p>
<p><strong>条件概率</strong>的贝叶斯估计：【<strong>离散型</strong>】</p>
<p><img src="image-20230129162002522.png" alt="image-20230129162002522" style="zoom:50%;"></p>
<h3><span id="4-先验概率和后验概率都是">4、<strong>先验概率和后验概率都是？</strong></span></h3><p><strong>先验概率是指根据以往经验和分析得到的概率</strong>,如全概率公式,它往往作为”由因求果”问题中的”因”出现.<strong>后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</strong></p>
<p><strong>先验概率和后验概率是相对的。</strong>如果以后还有新的信息引入，更新了现在所谓的后验概率，得到了新的概率值，那么这个新的概率值被称为后验概率。</p>
<h3><span id="5-朴素贝叶斯算法的前提假设是什么">5、<strong>朴素贝叶斯算法的前提假设是什么？</strong></span></h3><ol>
<li>特征之间相互独立</li>
<li>每个特征同等重要</li>
</ol>
<h3><span id="6-面试的时候怎么标准回答朴素贝叶斯呢">6、<strong>面试的时候怎么标准回答朴素贝叶斯呢？</strong></span></h3><p>首先朴素贝斯是一个<strong>生成模型（很重要）</strong>，其次它通过学习已知样本，计算出联合概率，再求条件概率。</p>
<h4><span id="生成模式和判别模式的区别常见"><strong>生成模式和判别模式的区别(常见)：</strong></span></h4><p><strong>生成模式</strong>：由数据学得<strong>联合概率分布，求出条件概率分布P(Y|X)的预测模型</strong>；<strong>比较在乎数据是怎么生成的</strong>；常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机。</p>
<p><strong>判别模式</strong>：由数据学得<strong>决策函数或条件概率分布作为预测模型</strong>，<strong>要关注在数据的差异分布上，而不是生成</strong>；常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场。</p>
<h3><span id="7-为什么属性独立性假设在实际情况中很难成立但朴素贝叶斯仍能取得较好的效果排序能力">7、<strong>为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?</strong>【排序能力】</span></h3><p>首先独立性假设在实际中不存在，确实会导致朴素贝叶斯不如一些其他算法，但是就算法本身而言，朴素贝叶斯也会有不错的分类效果，原因是：</p>
<ul>
<li><strong>分类问题看中的是类别的条件概率的排序</strong>，而不是具体的概率值，所以这里面对精准概率值的计算是有一定的容错的。</li>
<li>如果特征属性之间的依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。</li>
</ul>
<h3><span id="8-朴素贝叶斯中概率计算的下溢问题如何解决"><strong><font color="red"> 8、朴素贝叶斯中概率计算的下溢问题如何解决？</font></strong></span></h3><p><strong>在朴素贝叶斯的计算过程中，需要对特定分类中各个特征出现的概率进行连乘</strong>，小数相乘，越乘越小，这样就造成下溢出。在程序中，在相应小数位置进行四舍五入，计算结果可能就变成0了。</p>
<p>为了解决这个问题，<strong>对乘积结果取自然对数</strong>。将小数的乘法操作转化为取对数后的加法操作，规避了变为0的风险同时并不影响分类结果。</p>
<h3><span id="9-朴素贝叶斯分类器对异常值和缺失值敏感吗">9、<strong>朴素贝叶斯分类器对异常值和缺失值敏感吗？</strong></span></h3><p>回想朴素贝叶斯的计算过程，它在推理的时候，输入的某个特征组合，<strong>他们的特征值在训练的时候在贝叶斯公式中都是基于频数进行统计的。</strong>所以一个值的异常（变成了别的数），<strong>只是贝叶斯公式里的计算概率的分子或者分母发生微小的变化，整体结果影响不大</strong>，就算微微影响最终概率值的获得，由于<strong>分类问题只关注概率的排序而不关注概率的值，所以影响不大</strong>，保留异常值还可以提高模型的泛化性能。</p>
<p>缺失值也是一样，如果一个数据实例缺失了一个属性的数值，在建模的时将被忽略，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。</p>
<h3><span id="10-朴素贝叶斯中有没有超参数可以调">10、<strong>朴素贝叶斯中有没有超参数可以调？</strong></span></h3><p><strong>朴素贝叶斯是没有超参数可以调的，所以它不需要调参</strong>，朴素贝叶斯是根据训练集进行分类，分类出来的结果基本上就是确定了的，拉普拉斯估计器不是朴素贝叶斯中的参数，不能通过拉普拉斯估计器来对朴素贝叶斯调参。</p>
<h3><span id="11-朴素贝叶斯有哪三个模型">11、<strong>朴素贝叶斯有哪三个模型？</strong></span></h3><ul>
<li><strong>多项式模型对应于离散变量</strong>，其中离散变量指的是category型变量，也就是类别变量，比如性别；连续变量一般是数字型变量，比如年龄，身高，体重。</li>
<li><strong>高斯模型 对应于连续变量</strong>（每一维服从正态分布）</li>
<li><strong>伯努利模型</strong> <strong>对应于文本分类</strong> （特征只能是0或者1）</li>
</ul>
<h3><span id="12-朴素贝叶斯为什么适合增量计算"><strong><font color="red"> 12、朴素贝叶斯为什么适合增量计算？</font></strong></span></h3><p>朴素贝叶斯在训练过程中实际上需要<strong>计算出各个类别的概率和各个特征的条件概率</strong>，这些概率以频数统计比值（对于多项式模型而言）的形式产生概率值，<strong>可以快速根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算。</strong></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/12XTPRK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/12XTPRK/" class="post-title-link" itemprop="url">机器学习（5）SVM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:38:05" itemprop="dateCreated datePublished" datetime="2022-03-15T22:38:05+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-19 15:22:10" itemprop="dateModified" datetime="2023-04-19T15:22:10+08:00">2023-04-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">支持向量机</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机 SVM"></a>支持向量机 SVM</h2><p><strong><font color="red"> SVM 是一个非常优雅的算法，具有完善的数学理论，虽然如今工业界用到的不多，但还是决定花点时间去写篇文章整理一下。</font></strong></p>
<p><strong>本质：SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</strong>为了对数据中的噪声有一定的容忍能力。<strong>以几何的角度，在丰富的数据理论的基础上，简化了通常的分类和回归问题。</strong></p>
<p><strong>几何意义</strong>：找到一个超平面将特征空间的正负样本分开，最大分隔（对噪音有一定的容忍能力）；</p>
<p><strong>间隔表示</strong>：划分超平面到属于不同标记的最近样本的距离之和；</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191157912.jpg" alt="【机器学习】支持向量机 SVM（非常详细）" style="zoom: 33%;"></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/posts/12XTPRK/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/26FYF9Q/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/26FYF9Q/" class="post-title-link" itemprop="url">机器学习（14）聚类*-Kmeans</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-10 22:41:00" itemprop="dateModified" datetime="2022-07-10T22:41:00+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">聚类</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="聚类算法-无监督">聚类算法 【无监督】</span></h2><blockquote>
<p>  常用聚类算法 - 小胡子的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104355127">https://zhuanlan.zhihu.com/p/104355127</a></p>
<p>  <strong>K-means, K-medians, K-mediods and K-centers</strong> - 仲基的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/398600714">https://zhuanlan.zhihu.com/p/398600714</a></p>
</blockquote>
<p>什么是聚类算法？聚类是一种机器学习技术，它涉及到数据点的分组。给定一组数据点，我们可以使用聚类算法将每个数据点划分为一个特定的组。理论上，同一组中的数据点应该具有相似的属性和/或特征，而不同组中的数据点应该具有高度不同的属性和/或特征。<strong>聚类是一种无监督学习的方法</strong>，是许多领域中常用的统计数据分析技术。</p>
<p><strong>聚类算法主要包括以下五类：</strong></p>
<ul>
<li><strong>基于分层的聚类（hierarchical methods）</strong></li>
</ul>
<p>这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。<strong>代表算法有：<em>BIRCH算法</em>（1996）、<em>CURE算法</em>、CHAMELEON算法等。</strong></p>
<blockquote>
<p>  层次聚类通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层，树的顶层是一个聚类的根节点。</p>
<p>  <strong>最小距离的层次聚类算法</strong>通过自下而上合并创建聚类树，合并算法通过计算两类数据点间的欧式距离来计算不同类别数据点间的相似度，对所有数据点中最为相似的两个数据点进行组合，组合后，最小距离（Single Linkage）的计算方法是将两个组合数据点中距离最近的两个数据点间的距离作为这两个组合数据点的距离。并反复迭代这一过程。</p>
</blockquote>
<ul>
<li><strong>基于划分的聚类（partitioning methods）</strong></li>
</ul>
<p>给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：<strong><em>==K-means算法==</em>、<em>K-medoids算法</em>、<em>CLARANS算法</em></strong></p>
<ul>
<li><strong>基于密度的聚类（density-based methods）</strong></li>
</ul>
<p>基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有<strong>：<em>==DBSCAN（Density-Based Spatial Clustering of Applic with Noise）==算法（1996）</em>、<em>OPTICS（Ordering Points to Identify Clustering Structure）算法（1999）</em>、<em>DENCLUE算法（1998）</em>、<em>WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据）</em></strong></p>
<ul>
<li><strong>基于网格的聚类（grid-based methods）</strong></li>
</ul>
<p>这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：<strong><em>STING（Statistical Information Grid）</em>、<em>CLIQUE（Clustering In Quest）算法（1998）</em>、<em>WaveCluster算法</em>。</strong>其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。</p>
<ul>
<li><strong>基于模型的聚类（model-based methods）</strong></li>
</ul>
<p>基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。</p>
<h2><span id="一-k-means-基于划分k值的选择">一、K-means 【基于划分】[==K值的选择？==]</span></h2><p><img src="https://pic1.zhimg.com/v2-e7195b6620e2e6ec743fb77702b1d3ff_1440w.jpg?source=172ae18b" alt="【机器学习】K-means（非常详细）" style="zoom:51%;"></p>
<blockquote>
<p>  K-means 聚类的迭代算法实际上是 EM 算法。EM 算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。在 K-means 中的隐变量是每个类别所属类别。</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/20463356">K-means 笔记（三）数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//sofasofa.io/forum_main_post.php%3Fpostid%3D1000282">K-means 怎么选 K?</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161733843">K-Means：隐变量、聚类、EM</a></li>
</ol>
</blockquote>
<p>k近邻法中，当<strong>训练集</strong>、<strong>距离度量</strong>、<strong>K值</strong>以及<strong>分类决策规则</strong>确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。</p>
<p><strong>K-均值是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为:</strong></p>
<ul>
<li>首先选择𝐾个<strong>随机</strong>的点，称为<strong>聚类中心</strong>（cluster centroids）；</li>
<li>对于数据集中的每一个数据，按照<strong>距离𝐾个中心点的距离</strong>，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</li>
<li>计算每一个组的平均值，将该组所<strong>关联的中心点移动到平均值</strong>的位置。</li>
<li>重复步骤，直至中心点不再变化。</li>
</ul>
<p>K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用 K-均值算法将数据分为三类，用于帮助确定将要生产的 T-恤衫的三种尺寸。</p>
<p><img src="https://camo.githubusercontent.com/86b1cfa2d801f27862bcc8cab59f04401e01defd5248311eb77ec75a02286c5f/687474703a2f2f7778332e73696e61696d672e636e2f6d773639302f30303633304465666c79316735623734367a776a676a33306668306337676e6a2e6a7067" alt="img" style="zoom: 50%;"></p>
<h3><span id="11-损失函数">1.1 损失函数</span></h3><p><strong>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和</strong>，因此 <strong>K-均值的代价函数（又称==畸变函数 Distortion function）==为</strong>：</p>
<p><img src="/Users/apple/Library/Application Support/typora-user-images/image-20220707191400653.png" alt="image-20220707191400653" style="zoom:50%;"></p>
<p>其中 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/c0d78bacde143a412a432d0f2c8d1a746fc34802fec6dfd079a07708e30a98f4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f25374263253545253742286929253744253744"><img src="https://camo.githubusercontent.com/c0d78bacde143a412a432d0f2c8d1a746fc34802fec6dfd079a07708e30a98f4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f25374263253545253742286929253744253744" alt="img"></a>代表与 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b14a6424262c05d4fb45a600d8fb5b4881cee4801e8699dcc0c58ef503164f94/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253545253742286929253744"><img src="https://camo.githubusercontent.com/b14a6424262c05d4fb45a600d8fb5b4881cee4801e8699dcc0c58ef503164f94/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253545253742286929253744" alt="img"></a>最近的聚类中心点。 我们的的优化目标便是找出使得代价函数最小的 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/c48ec5ec6d35ce0993ee193bc86c0d4eddb3dbac31e30fa1f38fbf56b1401083/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f632535452537422831292537442c632535452537422832292537442c2e2e2e2c63253545253742286d29253744"><img src="https://camo.githubusercontent.com/c48ec5ec6d35ce0993ee193bc86c0d4eddb3dbac31e30fa1f38fbf56b1401083/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f632535452537422831292537442c632535452537422832292537442c2e2e2e2c63253545253742286d29253744" alt="img"></a>和 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d287d301a22d8bb905fe7e82874a282adff45c972a158b7a2cb91f0354a4ae84/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f312c755f322c2e2e2e2c755f6b"><img src="https://camo.githubusercontent.com/d287d301a22d8bb905fe7e82874a282adff45c972a158b7a2cb91f0354a4ae84/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f312c755f322c2e2e2e2c755f6b" alt="img"></a>。</p>
<h3><span id="12-k值的选择-肘部法则">1.2 k值的选择 【肘部法则】</span></h3><p>在运行 K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：</p>
<ol>
<li>我们应该选择𝐾 &lt; 𝑚，即聚类中心点的个数要小于所有训练集实例的数量。</li>
<li>随机选择𝐾个训练实例，然后令𝐾个聚类中心分别与这𝐾个训练实例相等K-均值的一个问题在于，它有可能会<strong>停留在一个局部最小值</strong>处，而这取决于初始化的情况。</li>
</ol>
<p>为了解决这个问题，我们通常需要多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行 K-均值的结果，选择代价函数最小的结果。这种方法在𝐾较小的时候（2—10）还是可行的，<strong>但是如果𝐾较大，这么做也可能不会有明显地改善。</strong></p>
<p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么。有一个可能会谈及的方法叫作<strong>“肘部法则”</strong>。关 于“肘部法则”，我们所需要做的是改变𝐾值，也就是聚类类别数目的总数。我们用一个聚类来运行 K 均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后<strong>计算成本函数或者计算畸变函数</strong>𝐽。𝐾代表聚类数字。</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8888198abee1b3069d27a6bff19f1830a94ac4141ebe7dd300f8ee4000262c6d/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562377062616561766a3330716f3063777463392e6a7067"><img src="https://camo.githubusercontent.com/8888198abee1b3069d27a6bff19f1830a94ac4141ebe7dd300f8ee4000262c6d/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562377062616561766a3330716f3063777463392e6a7067" alt="img"></a></p>
<p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。你会发现这种模式，它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用 3 个聚类来进行聚类是正确的，<strong>这是因为那个点是曲线的肘点，畸变值下降得很快，𝐾 = 3之后就下降得很慢，那么我们就选𝐾 = 3。</strong>当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p>
<h3><span id="13-knn与k-means区别">1.3 KNN与K-means区别？</span></h3><p>K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。</p>
<h4><span id="区别">区别：</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>KNN</th>
<th>K-Means</th>
</tr>
</thead>
<tbody>
<tr>
<td>类别</td>
<td>1.KNN是<strong>分类</strong>算法 2.属于<strong>监督学习</strong> 3.训练数据集是带label的数据</td>
<td>1.K-Means是<strong>聚类</strong>算法 2.属于<strong>非监督学习</strong> 3.训练数据集是无label的数据，是杂乱无章的，经过聚类后变得有序，先无序，后有序。</td>
</tr>
<tr>
<td></td>
<td>没有明显的前期训练过程，属于memory based learning</td>
<td>有明显的前期训练过程</td>
</tr>
<tr>
<td>k值的含义</td>
<td>K的含义：一个样本x，对它进行分类，就从训练数据集中，<strong>在x附近找离它最近的K个数据点</strong>，这K个数据点，类别c占的个数最多，就把x的label设为c。</td>
<td>K的含义：<strong>K是人工固定好的数字，假设数据集合可以分为K个蔟</strong>，那么就利用训练数据来训练出这K个分类。</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="相似点"><strong>相似点</strong>：</span></h4><p>都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法思想。</p>
<h3><span id="14-k-means优缺点及改进">1.4 K-Means优缺点及改进</span></h3><p>k-means：在大数据的条件下，<strong>会耗费大量的时间和内存</strong>。 优化k-means的建议：</p>
<ol>
<li><p>减少聚类的数目K。因为，每个样本都要跟类中心计算距离。</p>
</li>
<li><p>减少样本的特征维度。比如说，<strong>通过PCA等进行降维</strong>。</p>
</li>
<li><p>考察其他的聚类算法，通过选取toy数据，去测试不同聚类算法的性能。</p>
</li>
<li><p><strong>hadoop集群</strong>，K-means算法是很容易进行并行计算的。</p>
</li>
<li><p>算法可能找到局部最优的聚类，而不是全局最优的聚类。使用改进的二分k-means算法。</p>
<p>二分k-means算法：首先将整个数据集看成一个簇，然后进行一次k-means（k=2）算法将该簇一分为二，并计算每个簇的误差平方和，选择平方和最大的簇迭代上述过程再次一分为二，直至簇数达到用户指定的k为止，此时可以达到的全局最优。</p>
</li>
</ol>
<h3><span id="二-k-means的调优与改进">二、K - means的调优与改进</span></h3><p>针对 K-means 算法的缺点，我们可以有很多种调优方式：如<strong>数据预处理</strong>（去除异常点），<strong>合理选择 K 值</strong>，<strong>高维映射</strong>等。以下将简单介绍：</p>
<h3><span id="21-数据预处理">2.1 数据预处理</span></h3><p>K-means 的本质是基于欧式距离的数据划分算法，均值和方差大的维度将对数据的聚类产生决定性影响。所以<strong>未做归一化处理和统一单位的数据是无法直接参与运算和比较</strong>的。常见的数据预处理方式有：<strong>数据归一化，数据标准化</strong>。</p>
<p>此外，离群点或者噪声数据会对均值产生较大的影响，导致中心偏移，因此我们还需要对数据进行异常点检测。</p>
<h3><span id="22-合理选择-k-值">2.2 合理选择 K 值</span></h3><p>K 值的选取对 K-means 影响很大，这也是 K-means 最大的缺点，常见的选取 K 值的方法有：<strong>手肘法、Gap statistic 方法</strong>。</p>
<p><strong>【手肘法】</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-5ca4a5fe0b06b25a2b97262abb401a16_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>当 K &lt; 3 时，曲线急速下降；当 K &gt; 3 时，曲线趋于平稳，通过手肘法我们认为拐点 3 为 K 的最佳值。</p>
<p>==【<strong>Gap statistic</strong>】==</p>
<p><img src="https://www.zhihu.com/equation?tex=Gap%28K%29%3D%5Ctext%7BE%7D%28%5Clog+D_k%29-%5Clog+D_k+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=D_k" alt="[公式]"> 为损失函数，这里 <img src="https://www.zhihu.com/equation?tex=E%28logD_k%29" alt="[公式]"> 指的是 <img src="https://www.zhihu.com/equation?tex=logD_k" alt="[公式]"> 的期望。这个数值通常通过<strong>蒙特卡洛模拟</strong>产生，我们在样本里所在的区域中按照<strong>均匀分布随机产生和原始样本数一样多的随机样本</strong>，并对这个<strong>随机样本做 K-Means</strong>，从而得到一个 <img src="https://www.zhihu.com/equation?tex=D_k+" alt="[公式]"> 。如此往复多次，通常 20 次，我们可以得到 20 个 <img src="https://www.zhihu.com/equation?tex=logD_k" alt="[公式]"> 。对这 20 个数值求平均值，就得到了 <img src="https://www.zhihu.com/equation?tex=E%28logD_k%29" alt="[公式]">  的近似值。最终可以计算 Gap Statisitc。而 Gap statistic 取得最大值所对应的 K 就是最佳的 K。</p>
<p><img src="https://pic3.zhimg.com/80/v2-9a39a8dad143e5dd52a506d83c2cbb36_1440w.jpg" alt="img"></p>
<p>由图可见，当 K=3 时，Gap(K) 取值最大，所以最佳的簇数是 K=3。</p>
<p>Github 上一个项目叫 <a href="https://link.zhihu.com/?target=https%3A//github.com/milesgranger/gap_statistic">gap_statistic</a> ，可以更方便的获取建议的类簇个数。</p>
<h3><span id="23-采用核函数">2.3 采用核函数</span></h3><p><strong>基于欧式距离的 K-means 假设了了各个数据簇的数据具有一样的的先验概率并呈现球形分布</strong>，但这种分布在实际生活中并不常见。面对非凸的数据分布形状时我们可以引入核函数来优化，这时算法又称为核 K-means 算法，是核聚类方法的一种。<strong>核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。</strong>非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。</p>
<h3><span id="24-k-means">==2.4 K-means++==</span></h3><blockquote>
<p>  <strong>K-means++ 就是选择离已选中心点最远的点</strong>。这也比较符合常理，聚类中心当然是互相离得越远越好。</p>
</blockquote>
<p>我们知道初始值的选取对结果的影响很大，对初始值选择的改进是很重要的一部分。在所有的改进算法中，K-means++ 最有名。</p>
<p>K-means++ 算法步骤如下所示：</p>
<ol>
<li>随机选取一个中心点 <img src="https://www.zhihu.com/equation?tex=a_1" alt="[公式]"> ；</li>
<li>计算数据到之前 n 个聚类中心最远的距离 <img src="https://www.zhihu.com/equation?tex=D%28x%29" alt="[公式]"> ，并以一定概率 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BD%28x%29%5E2%7D%7B%5Csum%7BD%28x%29%5E2%7D%7D" alt="[公式]"> 选择新中心点 <img src="https://www.zhihu.com/equation?tex=a_i" alt="[公式]"> ；</li>
<li>重复第二步。</li>
</ol>
<p>简单的来说，就是 <strong>K-means++ 就是选择离已选中心点最远的点</strong>。这也比较符合常理，聚类中心当然是互相离得越远越好。</p>
<p>但是这个算法的缺点在于，难以并行化。所以 k-means II 改变取样策略，并非按照 k-means++ 那样每次遍历只取样一个样本，而是每次遍历取样 k 个，重复该取样过程 <img src="https://www.zhihu.com/equation?tex=log%28n+%29" alt="[公式]"> 次，则得到 <img src="https://www.zhihu.com/equation?tex=klog%28n%29" alt="[公式]"> 个样本点组成的集合，然后从这些点中选取 k 个。当然一般也不需要 <img src="https://www.zhihu.com/equation?tex=log%28n%29" alt="[公式]"> 次取样，5 次即可。</p>
<h3><span id="25-isodata">2.5 ISODATA</span></h3><p>ISODATA 的全称是<strong>迭代自组织数据分析法</strong>。它解决了 K 的值需要预先人为的确定这一缺点。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出 K 的大小。ISODATA 就是针对这个问题进行了改进，它的思想也很直观：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别。</p>
<h3><span id="三-收敛证明em算法">==三、 收敛证明【EM算法】==</span></h3><p>我们先来看一下 K-means 算法的步骤：先随机选择初始节点，然后计算每个样本所属类别，然后通过类别再跟新初始化节点。这个过程有没有想到之前介绍的 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/78311644">EM 算法</a> 。</p>
<p>我们需要知道的是 K-means 聚类的迭代算法实际上是 EM 算法。EM 算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。在 <strong>==K-means 中的隐变量是每个样本所属类别==</strong>。</p>
<p>K-means 算法迭代步骤中的每次确认中心点以后重新进行标记对应 EM 算法中的 <strong>E 步</strong>：<strong>求当前参数条件下的 Expectation</strong>。而根据标记重新求中心点 对应 EM 算法中的 <strong>M 步</strong>：<strong>求似然函数最大化时（损失函数最小时）对应的参数 。</strong></p>
<p>首先我们看一下损失函数的形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=J%3D%5Csum_%7Bi%3D1%7D%5E%7BC%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BN%7D%7Br_%7Bij%7D%5Ccdot+%7B%5Cnu%28x_j%2C%7B%5Cmu%7D_i%29%7D%7D%7D+%5C%5C" alt="[公式]"></p>
<p>其中：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cnu%28%7Bx_j%7D%2C%5Cmu_i%29%3D%7B%7C%7Cx_j-%7B%5Cmu%7D_i%7C%7C%7D%5E%7B2%7D+%2C%5Cquad++%7Br%7D_%7Bnk%7D%3D%5Cleft+%5C%7B+%5Cbegin%7Baligned%7D+%261+%5Cquad+if+%5C%3B+x_n+%5Cin+k+%5C%5C+%260+%5Cquad+else+%5Cend%7Baligned%7D+%5Cright.+%5C%5C" alt="[公式]"></p>
<p>为了求极值，我们令损失函数求偏导数且等于 0：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%7BJ%7D%7D%7B%5Cpartial+%5Cmu_k%7D%3D2+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7D%28x_i-%7B%5Cmu%7D_%7Bk%7D%29%7D%3D0+%5C%5C" alt="[公式]"></p>
<p>k 是指第 k 个中心点，于是我们有：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu_k%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7Dx_i%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7D%7D%7D+%5C%5C" alt="[公式]"></p>
<p>可以看出，新的中心点就是所有该类的<strong>质心</strong>。</p>
<p><strong>EM 算法的缺点就是，容易陷入局部极小值，这也是 K-means 有时会得到局部最优解的原因。</strong></p>
<h3><span id="四-高斯混合模型gmm">四、高斯混合模型(GMM)</span></h3><h3><span id="41-gmm的思想">4.1 GMM的思想</span></h3><p>高斯混合模型（Gaussian Mixed Model，GMM）也是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。<strong>高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的</strong>，当前<strong>数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。</strong></p>
<p>第一张图是一个数据分布的样例，如果只用一个高斯分布来拟合图中的数据，图 中所示的椭圆即为高斯分布的二倍标准差所对应的椭圆。直观来说，图中的数据 明显分为两簇，因此只用一个高斯分布来拟和是不太合理的，需要推广到用多个 高斯分布的叠加来对数据进行拟合。第二张图是用两个高斯分布的叠加来拟合得到的结果。<strong>这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。</strong>理论上，高斯混合模型可以拟合出任意类型的分布。</p>
<p>高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来 的。在该假设下，每个单独的分模型都是标准高斯模型，其均值 $u_i$ 和方差 $\sum_i$ 是待估计的参数。此外，每个分模型都还有一个参数 $\pi_i$，可以理解为权重或生成数据的概 率。高斯混合模型的公式为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/4055f35cd39fe2e095b0fc70f8cccb1e3cab924a2ebd6bc56ca7bdcf026c79ec/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f702878293d25354373756d5f253742693d312537442535452537426b25374425354370695f694e2878253743755f692c25354373756d5f6929"><img src="https://camo.githubusercontent.com/4055f35cd39fe2e095b0fc70f8cccb1e3cab924a2ebd6bc56ca7bdcf026c79ec/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f702878293d25354373756d5f253742693d312537442535452537426b25374425354370695f694e2878253743755f692c25354373756d5f6929" alt="img"></a></p>
<p>通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列 数据点，给出一个类别的数量K后，希望求得最佳的K个高斯分模型。因此，<strong>高斯混合模型的计算，便成了最佳的均值μ，方差Σ、权重π的寻找</strong>，这类问题通常通过最大似然估计来求解。遗憾的是，此问题中直接使用最大似然估计，得到的是一个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。</p>
<p><strong>在这种情况下，可以用EM算法。 </strong>EM算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。具体到高 斯混合模型的求解，EM算法的迭代过程如下。</p>
<p>首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。</p>
<ul>
<li>E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。</li>
<li>M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。</li>
</ul>
<blockquote>
<p>  高斯混合模型是一个生成式模型。可以这样理解数据的生成过程，假设一个最简单的情况，即只有两个一维标准高斯分布的分模型<em>N</em>(0,1)和<em>N</em>(5,1)，其权重分别为0.7和0.3。那么，在生成第一个数据点时，先按照权重的比例，随机选择一个分布，比如选择第一个高斯分布，接着从<em>N</em>(0,1)中生成一个点，如−0.5，便是第一个数据点。在生成第二个数据点时，随机选择到第二个高斯分布<em>N</em>(5,1)，生成了第二个点4.7。如此循环执行，便生成出了所有的数据点。</p>
</blockquote>
<p>也就是说，我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个 数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不 变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。</p>
<h3><span id="42-gmm与k-means相比">4.2 GMM与K-Means相比</span></h3><p>高斯混合模型与K均值算法的相同点是：</p>
<ul>
<li><strong>都需要指定K值</strong>；</li>
<li><strong>都是使用EM算法来求解</strong>；</li>
<li>都往往只能收敛于局部最优。</li>
</ul>
<p>而它相比于K 均值算法的优点是，可以给出一个样本属于某类的<strong>概率</strong>是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；<strong>并且可以用于生成新的样本点</strong>。</p>
<h3><span id="五-聚类算法如何评估">五、聚类算法如何评估</span></h3><p>由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。例 如，K均值聚类可以用<strong>误差平方</strong>和来评估，但是基于密度的数据簇可能不是球形， 误差平方和则会失效。在许多情况下，判断聚类算法结果的好坏强烈依赖于主观解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。</p>
<p>聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结 果的质量。这一过程又分为三个子任务。</p>
<ol>
<li><p><strong>估计聚类趋势。</strong></p>
<p>这一步骤是检测数据分布中是否存在非随机的簇结构。如果数据是基本随机 的，那么聚类的结果也是毫无意义的。我们可以观察聚类误差是否随聚类类别数 量的增加而单调变化，如果数据是基本随机的，即不存在非随机簇结构，那么聚 类误差随聚类类别数量增加而变化的幅度应该较不显著，并且也找不到一个合适 的K对应数据的真实簇数。</p>
</li>
<li><p><strong>判定数据簇数。</strong></p>
<p>确定聚类趋势之后，我们需要找到与真实数据分布最为吻合的簇数，据此判定聚类结果的质量。数据簇数的判定方法有很多，例如<strong>手肘法</strong>和<strong>Gap Statistic</strong>方 法。需要说明的是，用于评估的最佳数据簇数可能与程序输出的簇数是不同的。 例如，有些聚类算法可以自动地确定数据的簇数，但可能与我们通过其他方法确定的最优数据簇数有所差别。</p>
</li>
<li><p><strong>测定聚类质量。</strong></p>
<p>在无监督的情况下，我们可以通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。定义评估指标可以展现面试者实际解决和分析问题的能力。事实上测量指标可以有很多种，以下列出了几种常用的度量指标，更多的指标可以阅读相关文献。</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1TSFG2S/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1TSFG2S/" class="post-title-link" itemprop="url">机器学习（15）聚类*-DBSCAN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-14 22:05:42" itemprop="dateModified" datetime="2023-03-14T22:05:42+08:00">2023-03-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">聚类</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="二-dbscan算法基于密度">二、DBSCAN算法【基于密度】</span></h2><blockquote>
<p>  （3）聚类算法之DBSCAN算法 - GISer.Wang的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77043965">https://zhuanlan.zhihu.com/p/77043965</a></p>
</blockquote>
<p>密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。其代表算法为<strong>DBSCAN算法</strong>和<strong>密度最大值</strong>算法。</p>
<h3><span id="21-dbscan算法原理">2.1 DBSCAN算法原理</span></h3><p><strong><font color="red"> DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。</font></strong>与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并<strong>可在有“噪声”的数据中发现任意形状的聚类</strong>。</p>
<h3><span id="22-若干概念">2.2 若干概念</span></h3><p><strong>DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数 <img src="https://www.zhihu.com/equation?tex=%28%CF%B5%2C+MinPts%29" alt="[公式]"> 用来描述邻域的样本分布紧密程度</strong>。其中， <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 描述了某一数据点的<strong>邻域距离阈值（半径）</strong>， <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 描述了数据点<strong>半径为</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> <strong>的邻域</strong>中数据点个数的最小个数。下面是与密度聚类相关的定义（假设我的样本集是 <img src="https://www.zhihu.com/equation?tex=D%3D%5C%7Bx_1%2Cx_2%2C...%2Cx_m%5C%7D" alt="[公式]"> )：</p>
<ul>
<li><p><strong>对象的ε领域</strong>：给定对象在半径<strong>ε</strong>内的区域；对于 <img src="https://www.zhihu.com/equation?tex=x_j%E2%88%88D" alt="[公式]"> ，其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域包含样本集 <img src="https://www.zhihu.com/equation?tex=D" alt="[公式]"> 中与 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 的距离不大于 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 的子样本集。即 <img src="https://www.zhihu.com/equation?tex=N_%CF%B5%28x_j%29%3D%5C%7Bx_i%E2%88%88D%7Cdistance%28x_i%2Cx_j%29%E2%89%A4%CF%B5%5C%7D" alt="[公式]"> , 这个子样本集的个数记为 <img src="https://www.zhihu.com/equation?tex=%7CN_%CF%B5%28x_j%29%7C" alt="[公式]"> 。 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域是一个集合</p>
</li>
<li><p><strong>核心对象</strong>：对于任一样本 <img src="https://www.zhihu.com/equation?tex=x_j%E2%88%88D" alt="[公式]"> ，如果其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域对应的 <img src="https://www.zhihu.com/equation?tex=N_%CF%B5%28x_j%29" alt="[公式]"> 至少包含 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 个样本，即如果 <img src="https://www.zhihu.com/equation?tex=+%7CN_%CF%B5%28x_j%29%7C%E2%89%A5MinPts" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 是核心对象。</p>
</li>
<li><p><strong>直接密度可达</strong>：如果 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 位于 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域中，且 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 是核心对象，则称 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 密度直达。反之不一定成立，即此时不能说 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 密度直达, 除非 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 也是核心对象，<strong>即密度直达不满足对称性</strong>。如图ε=1,m=5，q是一个核心对象，从对象q出发到对象p是<strong>直接密度可达</strong>的。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afb2323b988730.jpg" alt="2019-05-18-061126.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>密度可达</strong>：对于 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> ,如果存在样本样本序列 <img src="https://www.zhihu.com/equation?tex=p_1%2Cp_2%2C...%2Cp_T" alt="[公式]"> ,满足 <img src="https://www.zhihu.com/equation?tex=p1%3Dx_i%2Cp_T%3Dx_j" alt="[公式]"> , 且 <img src="https://www.zhihu.com/equation?tex=p_%7Bt%2B1%7D" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=p_t" alt="[公式]"> 密度直达，则称 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本 <img src="https://www.zhihu.com/equation?tex=p_1%2Cp_2%2C...%2Cp_%7BT%E2%88%921%7D" alt="[公式]"><strong>均为核心对象</strong>，因为只有核心对象才能使其他样本密度直达。<strong>密度可达也不满足对称性</strong>，这个可以由密度直达的不对称性得出。</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afbfa514735525.jpg" alt="2019-05-18-061154.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>密度相连</strong>：对于 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> ,如果存在核心对象样本 <img src="https://www.zhihu.com/equation?tex=x_k" alt="[公式]"> ，使<strong><img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 均由 <img src="https://www.zhihu.com/equation?tex=x_k" alt="[公式]"> 密度可达</strong>，则称 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 密度相连。<strong>密度相连关系满足对称性</strong>。</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afd32fc7340149.jpg" alt="2019-05-18-061202.jpg" style="zoom:50%;"></p>
<ul>
<li><p><strong>==簇：一个基于密度的簇是最大的密度相连对象的集合。==</strong></p>
</li>
<li><p><strong>噪声</strong>：不包含在任何簇中的对象称为噪声。</p>
</li>
</ul>
<p>从下图可以很容易看出理解上述定义，图中 <img src="https://www.zhihu.com/equation?tex=MinPts%3D5" alt="[公式]"> ，红色的点都是核心对象，因为其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域至少有 <img src="https://www.zhihu.com/equation?tex=5" alt="[公式]"> 个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的圆内，如果不在圆内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列，此序列是一个簇集。在这些密度可达的样本序列的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域内所有的样本相互都是密度相连的 <strong>(注意，此图中有两个簇集)</strong>。</p>
<p><img src="https://pic2.zhimg.com/80/v2-7d15fc871942e0287be42a12d6d615dd_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h3><span id="23-dbscan密度聚类思想">2.3 DBSCAN密度聚类思想</span></h3><p><strong>DBSCAN的聚类定义很简单</strong>： <strong>由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。（注意是密度相连的集合）</strong>，簇里面可以有一个或者多个核心对象。<strong>如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> <strong>-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -<strong>邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达</strong>。这些核心对象的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域里所有的样本的集合组成的一个DBSCAN聚类簇。</p>
<p>那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够<strong>密度可达</strong>的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找<strong>密度可达</strong>的样本集合，这样就得到另一个聚类簇 <strong>（这样的得到都肯定是密度相连的）</strong>。一直运行到<strong>所有核心对象都有类别为止。</strong></p>
<p>基本上这就是DBSCAN算法的主要内容了，是不是很简单？<strong>但是我们还是有三个问题没有考虑。</strong></p>
<ul>
<li><strong>异常点问题：</strong>一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。</li>
<li><strong>距离度量问题</strong>：<strong><font color="red"> 即如何计算某样本和核心对象样本的距离</font></strong>。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量<strong>样本距离，比如欧式距离、曼哈顿距离</strong>等。</li>
<li><strong>数据点优先级分配问题</strong>：例如某些样本可能到两个核心对象的距离都小于 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，<strong>此时 DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说DBSCAN的算法不是完全稳定的算法。</strong></li>
</ul>
<h3><span id="24-算法步骤">2.4 算法步骤</span></h3><p><strong>输入：样本集 <img src="https://www.zhihu.com/equation?tex=D%3D%5C%7Bx_1%2Cx_2%2C...%2Cx_m%5C%7D" alt="[公式]"> ，邻域参数 <img src="https://www.zhihu.com/equation?tex=%28%CF%B5%2CMinPts%29" alt="[公式]"></strong></p>
<ol>
<li>初始化核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9%3D%E2%88%85%2C" alt="[公式]"> 初始化类别 <img src="https://www.zhihu.com/equation?tex=k%3D0" alt="[公式]"></li>
<li>遍历 <img src="https://www.zhihu.com/equation?tex=D" alt="[公式]"> 的元素，如果是核心对象，则将其加入到核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中</li>
<li>如果核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中元素都已经被<strong>访问</strong>，<strong>则算法结束</strong>，<strong>否则转入步骤4</strong>.</li>
<li>在核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中，随机选择一个<strong>未访问</strong>的核心对象 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> ，首先将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 标记为<strong>已访问</strong>，然后将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 标记类别 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> ，最后将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域中<strong>未访问</strong>的数据，存放到种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds" alt="[公式]"> 中。</li>
<li>如果种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds%3D%E2%88%85" alt="[公式]"> ，则当前聚类簇 <img src="https://www.zhihu.com/equation?tex=C_k" alt="[公式]"> 生成完毕, 且 <img src="https://www.zhihu.com/equation?tex=k%3Dk%2B1" alt="[公式]"> ，<strong>跳转到3</strong>。否则，从种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds" alt="[公式]"> 中挑选一个种子点 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> ，首先将其标记为已访问、标记类别 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> ，然后判断 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> 是否为核心对象，如果是将 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> 中<strong>未访问</strong>的种子点加入到种子集合中，<strong>跳转到5</strong>。</li>
</ol>
<p><strong>从上述算法可知：</strong></p>
<ul>
<li><strong>每个簇至少包含一个核心对象</strong>；</li>
<li>非核心对象可以是簇的一部分，构成了簇的边缘（edge）；</li>
<li>包含过少对象的簇被认为是噪声；</li>
</ul>
<h3><span id="25-总结">2.5 总结</span></h3><h4><span id="优点">优点</span></h4><ol>
<li><strong>可以对任意形状的稠密数据集进行聚类</strong>，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li><strong>可以在聚类的同时发现异常点</strong>，对数据集中的异常点不敏感。</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>
<h4><span id="缺点">缺点</span></h4><ol>
<li><strong>不能处理密度差异过大（密度不均匀）的聚类</strong>：如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
<li>如果样本集较大时，聚类收敛时间较长;<strong>此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进；</strong></li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂，<strong>主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响</strong>。【OPTICS算法】</li>
<li><strong>边界点不完全确定性</strong></li>
</ol>
<h3><span id="26-optics算法">2.6 OPTICS算法:</span></h3><p><strong>OPTICS主要针对输入参数$ϵ$过敏感做的改进</strong>，OPTICS和DBSCNA的输入参数一样（ <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> ），虽然OPTICS算法中也需要两个输入参数，但该算法对 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 输入不敏感（一般将 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 固定为无穷大），同时该算法中并不显式的生成数据聚类，只是对数据集合中的对象进行排序，得到一个有序的对象列表，通过该有序列表，可以得到一个决策图，通过决策图可以不同 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 参数的数据集中检测簇集，即：<strong>先通过固定的 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 和无穷大的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 得到有序列表，然后得到决策图，通过决策图可以知道当 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 取特定值时（比如 <img src="https://www.zhihu.com/equation?tex=%CF%B5%3D3" alt="[公式]"> )数据的聚类情况。</strong></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1N8XMT6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1N8XMT6/" class="post-title-link" itemprop="url">机器学习（16）聚类*-HDBSCAN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 22:23:33" itemprop="dateCreated datePublished" datetime="2022-03-15T22:23:33+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-07 22:10:55" itemprop="dateModified" datetime="2022-07-07T22:10:55+08:00">2022-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">聚类</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="一-hdbscan聚类">一、HDBSCAN聚类</span></h2><blockquote>
<p>  <strong>图解HDBSCANS - Mr.g的文章</strong> - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/412918565">https://zhuanlan.zhihu.com/p/412918565</a></p>
<p>  原文: <a href="https://link.zhihu.com/?target=https%3A//nbviewer.jupyter.org/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/How%20HDBSCAN%20Works.ipynb">How HDBSCAN works</a></p>
<p>  聚类算法(Clustering Algorithms)之层次聚类(Hierarchical Clustering) - 小玉的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363879425">https://zhuanlan.zhihu.com/p/363879425</a></p>
</blockquote>
<p><strong>HDBSCAN 是由 Campello、Moulavi 和 Sander 开发的聚类算法。它通过将 DBSCAN 转换为层次聚类算法，然后用一种稳定的聚类技术提取出一个扁平的聚类来扩展 DBSCAN</strong>。这篇文章的目标是让你大致了解这个算法的工作原理及其背后的动机。与 HDBSCAN 的原论文不一样，我们这里将不将 DBSCAN 进行对照分析。作者这里更倾向将这算法类比成一种扁平聚类提取方法（ Robust Single Linkage ）。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns
<span class="token keyword">import</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> data
<span class="token operator">%</span>matplotlib inline
sns<span class="token punctuation">.</span>set_context<span class="token punctuation">(</span><span class="token string">'poster'</span><span class="token punctuation">)</span>
sns<span class="token punctuation">.</span>set_style<span class="token punctuation">(</span><span class="token string">'white'</span><span class="token punctuation">)</span>
sns<span class="token punctuation">.</span>set_color_codes<span class="token punctuation">(</span><span class="token punctuation">)</span>
plot_kwds <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">'alpha'</span> <span class="token punctuation">:</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token string">'s'</span> <span class="token punctuation">:</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token string">'linewidths'</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">&#125;</span>

moons<span class="token punctuation">,</span> _ <span class="token operator">=</span> data<span class="token punctuation">.</span>make_moons<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> noise<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span>
blobs<span class="token punctuation">,</span> _ <span class="token operator">=</span> data<span class="token punctuation">.</span>make_blobs<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> centers<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">2.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cluster_std<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span>
test_data <span class="token operator">=</span> np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">[</span>moons<span class="token punctuation">,</span> blobs<span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>test_data<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_data<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token operator">**</span>plot_kwds<span class="token punctuation">)</span>

<span class="token keyword">import</span> hdbscan
clusterer <span class="token operator">=</span> hdbscan<span class="token punctuation">.</span>HDBSCAN<span class="token punctuation">(</span>min_cluster_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> gen_min_span_tree<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
clusterer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>现在我们已经对数据聚类完了——但实际发生了什么我们还不知道。我们将拆解成下面5个步骤来进行分析：</strong></p>
<ol>
<li>根据 密度/稀疏度 进行<strong>空间转换</strong>。</li>
<li>构建基于加权距离图的最小生成树。</li>
<li>构建组件之间的层次簇结构。</li>
<li>用最小簇大小压缩层次聚类。</li>
<li>利用压缩好的生成树进行分类。</li>
</ol>
<h3><span id="11-空间转换">1.1 空间转换</span></h3><p><strong>聚类时我们希望在稀疏带噪声的数据中找到密度更高的族类——噪声的假设很重要</strong>：因为在真实情况下，数据都比较复杂，会有异常值的、缺失的数据和噪声等情况。算法的核心是单链聚类，它对噪声非常敏感：如果噪声数据点放在两个岛屿之间，可能就会将它们连在一起（也就是本来是两个族类的被分成一个）。显然，我们希望我们的算法对噪声具有鲁棒性，因此我们需要在运行单链算法之前找到一种方法来减少噪声。（作者用岛屿来比喻族类，海洋来表示噪声，下面“海洋”和“岛屿”代表这个意思。）</p>
<p><strong>我们要如何在不进行聚类的情况下找出“海洋”和“岛屿”？</strong>只要我们能够估算出样本集的密度，我们就可以认为密度较低的点都是“海洋”。要注意的是这里的目标不是完全区分出“海洋”和“岛屿”——现在只是聚类的初始步骤，并不是最终的输出——现在只是为了使我们的聚类中心对噪声更加鲁棒。因此，要识别出“海洋”的话，我们可以降低海平面（也就是加大容错范围）。出于实际目的，这意味着使每个“海洋”之间以及“海洋”与“岛屿”之间的距离会增加。</p>
<p>当然这只是直觉。它在实际中是如何工作的？我们需要一个计算量少的密度估计方式，简单到只要计算 k 个最近邻点的距离就可以。<strong><font color="red"> 我们可以直接从一个距离矩阵（不管怎样后面都要生成的）中地读取到这个距离；或者，如果我们的指标支持（并且维度较低），用 <a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/neighbors.html%23k-d-tree">kd-trees</a> 来做这种检索就很适合。</font></strong>下面正式将点 x 的参数 k 定义为<strong>核心距离</strong>，并表示为 <img src="https://www.zhihu.com/equation?tex=core_k%28x%29" alt="[公式]"> （与DBSCAN、LOF 和 HDBSCAN 文献一样）。现在我们需要一种降维方法来拉开点之间的距离（相对高维距离）。简单的方法是定义一种新距离公式，我们将其称为（与论文一样)<strong>相互可达距离（mutual reachability distance)。相互可达距离的定义如下：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=d_%7B%5Cmathrm%7Bmreach-%7Dk%7D%28a%2Cb%29+%3D+%5Cmax+%5C%7B%5Cmathrm%7Bcore%7D_k%28a%29%2C+%5Cmathrm%7Bcore%7D_k%28b%29%2C+d%28a%2Cb%29+%5C%7D" alt="[公式]" style="zoom: 150%;"></p>
<p><strong>其中 d(a,b) 是 a 和 b 之间的原始距离</strong>。在这个度量下，密集点（具有低核心距离）之间的距离保持不变，但稀疏的点与其他点的距离被拉远到用core距离来计算。这有效地“降低了海平面”，减少稀疏的“海”点，同时使“陆地”保持原状。需要注意的是，显然 k 取值很关键；较大的 k 值将更多的点圈到“海”中。下面用图片来解析更容易理解，先让k=5。然后对于给定的点，我们可以为核心距离绘制一个圆刚好圈到第六个临近点（包括点本身），如下所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-dbf4559853b0d81f1c5ae2205a7b97a9_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p><strong>再选择另一个点</strong>，进行同样的操作，这次选到另一组临近点集合（其中一些点可能是上一组的临近点）。</p>
<p><img src="https://pic3.zhimg.com/80/v2-308c1bb09e8f779232aac217bee2507e_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>我们再选一个点再做一遍，得到第三组临近点。</p>
<p><img src="https://pic4.zhimg.com/80/v2-0e0e83ecf594b202cea6d3c208e45f0f_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>如果我们现在想知道蓝绿两组之间的相互可达距离，我们可以像下图，先画一个箭头连接蓝绿两个圆心：它穿过蓝色圆心，但不穿过绿色圆圈——绿色的核心距离大于蓝色和绿色之间的距离。<strong>因此，我们认为蓝色和绿色之间的相互可达距离更大——也就是绿色圆圈的半径（如果我们将一端设在绿色点上，则最容易想象）。</strong></p>
<p>实际上，有<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1506.06422v2.pdf">基础理论</a>可以证明<strong>相互可达距离</strong>作为一种变换，在允许单链接聚类的情况下，更接近层次水平上的真实密度分布。</p>
<h3><span id="12-构建最小生成树"><strong>1.2 构建最小生成树</strong></span></h3><p><strong>为了从密集的数据集上找到“岛屿”，现在我们有了新的指标：相互可达性</strong>。当然密集区是相对的，不同的“岛屿”可能会有不同的密度。<strong><font color="red"> 理论上，我们要做的是：将数据当成是一个加权图，以数据点作为顶点，任意两点之间的边的权重等于这些点的相互可达距离。</font></strong></p>
<p>现在考虑一个阈值，一开始很高，然后逐渐变小。丢弃权重高于该阈值的任何边。我们删除边的同时，连接的组件从图里断开。最终，我们将拥有不同阈值级别的连接元件（从完全连接到完全断开）的层次结构。</p>
<p>实际当中，这样操纵非常耗时：有 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/412918565/edit#">n^2</a> 条边，我们不想多次计算连通组件算法。正确的做法是找到最小的边集，从这个集合中删除任何边都会导致组件的连接断开。但是我们还需要找到更多这样的边，使得找不到更小的边来连接组件。幸运的是，图论为我们提供了这样的东西：<strong>图的最小生成树</strong>。</p>
<p>我们可以通过 Prim 的算法非常有效地构建最小生成树——我们一次构建一条边，每次都把当前最小权重的边去连接一个尚未加入到树中的节点。可以看到下面HDBSCAN构造的树 ；请注意，这是相互可达距离的最小生成树，与图中的纯距离不同。在这种情况下，我们的 k 值为 5。 在这个例子中，存在一种更快的方法，例如用 <strong>Dual Tree Boruvka 来构建最小生成树。</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">clusterer<span class="token punctuation">.</span>minimum_spanning_tree_<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>edge_cmap<span class="token operator">=</span><span class="token string">'viridis'</span><span class="token punctuation">,</span> 
                                      edge_alpha<span class="token operator">=</span><span class="token number">0.6</span><span class="token punctuation">,</span> 
                                      node_size<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span> 
                                      edge_linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="https://pic2.zhimg.com/80/v2-a4b387ac9c43b1f681589529c82fe68d_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h3><span id="13-构建层次聚类">1.3 <strong>构建层次聚类</strong></span></h3><p><strong>给定最小生成树，下一步是将其转换为层次结构的组件</strong>。这最容易以相反的顺序完成：按距离（按递增顺序）对树的边进行排序，然后迭代，为每条边新建一个合并后的簇。这里唯一困难是识别每条边将连接在哪两个簇上，但这通过联合查找数据结构很容易。我们可以将结果视为树状图，如下所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-33a8cb27ae3ea0009e1ad77b438cf458_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>这图可以告诉我们这个鲁棒的单一链接会在哪挺下来。我们想知道；层次结构结构的聚类虽好，但我们想要的是一个扁平的聚类。我们可以通过在上图中画一条水平线并选择它穿过的聚类，来做到这一点。这实际上是 DBSCAN 里用到的操作（将只能切割成单一集群的作为噪声）。<strong>问题是，我们怎么知道在哪里画这条线？ DBSCAN 只是把它作为一个（非常不直观的）参数</strong>。更糟糕的是，我们真的要用来处理可变密度的聚类，并希望任何切割线都是通过相互可达距离选出来的，并且今后固定在一个密度水平上。理想情况下，我们希望能够在不同的地方切割树，来选择我们的聚类。这是 HDBSCAN 下一步开始的地方，并与鲁棒的单一链接产生差异。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/12GKN6G/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/12GKN6G/" class="post-title-link" itemprop="url">机器学习（10）集成学习*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:14:35" itemprop="dateCreated datePublished" datetime="2022-03-11T21:14:35+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-09 16:58:25" itemprop="dateModified" datetime="2023-03-09T16:58:25+08:00">2023-03-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>23 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="机器学习决策树中random-forest-adaboost-gbdt">【机器学习】决策树（中）——Random Forest、Adaboost、GBDT</span></h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble</a></p>
<p>  机器学习算法中GBDT与Adaboost的区别与联系是什么？ - Frankenstein的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/54626685/answer/140610056">https://www.zhihu.com/question/54626685/answer/140610056</a></p>
<p>  GBDT学习笔记 - 许辙的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/169382376">https://zhuanlan.zhihu.com/p/169382376</a></p>
<p>  GBDT - 王多鱼的文章 - 知乎 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38057220">https://zhuanlan.zhihu.com/p/38057220</a></p>
</blockquote>
<p><img src="https://pic2.zhimg.com/v2-1ac553d300784e8d158bcc686e7cf66d_1440w.jpg?source=172ae18b" alt="【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细）"></p>
<p>本文主要介绍基于集成学习的决策树，其主要通过不同学习框架生产基学习器，并综合所有基学习器的预测结果来改善单个基学习器的识别率和泛化性。</p>
<p>==模型的准确度可由偏差和方差共同决定：==</p>
<p>$\text { Error }=\text { bias }^{2}+\operatorname{var}+\xi$</p>
<p><strong>模型总体期望：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
E(F) &=E\left(\sum_{i}^{m} r_{i} f_{i}\right) \\
&=\sum_{i}^{m} r_{i} E\left(f_{i}\right)
\end{aligned}</script><p><strong>模型总体方差</strong>:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{Var}(F) &=\operatorname{Var}\left(\sum_{i}^{m} r_{i} f_{i}\right) \\
&=\sum_{i}^{m} \operatorname{Var}\left(r_{i} f_{i}\right)+\sum_{i \neq j}^{m} \operatorname{Cov}\left(r_{i} f_{i}, r_{j} f_{j}\right) \\
&=\sum_{i}^{m} r_{i}{ }^{2} \operatorname{Var}\left(f_{i}\right)+\sum_{i \neq j}^{m} \rho r_{i} r_{j} \sqrt{\operatorname{Var}\left(f_{i}\right)} \sqrt{\operatorname{Var}\left(f_{j}\right)} \\
&=m r^{2} \sigma^{2}+m(m-1) \rho r^{2} \sigma^{2} \\
&=m r^{2} \sigma^{2}(1-\rho)+m^{2} r^{2} \sigma^{2} \rho
\end{aligned}</script><div class="table-container">
<table>
<thead>
<tr>
<th>集成学习</th>
<th>Bagging</th>
<th>Boosting</th>
<th>Stacking</th>
</tr>
</thead>
<tbody>
<tr>
<td>思想</td>
<td>对训练集进行<strong>有放回抽样</strong>得到子训练集</td>
<td>基模型的训练是有<strong>顺序</strong>的，每个基模型都会在前一个基模型学习的基础上进行学习；基于贪心策略的前向加法</td>
<td><strong>预测值</strong>将作为训练样本的特征值，进行训练得到最终预测结果。</td>
</tr>
<tr>
<td>样本抽样</td>
<td>有放回地抽取数据集</td>
<td>训练集不变</td>
<td></td>
</tr>
<tr>
<td>样本权重</td>
<td>样本权重相等</td>
<td>不断调整样本的权重</td>
<td></td>
</tr>
<tr>
<td>优化目标</td>
<td>减小的是方差</td>
<td>减小的是偏差</td>
<td></td>
</tr>
<tr>
<td>基模型</td>
<td><strong>强模型（偏差低，方差高）</strong></td>
<td><strong>弱模型（偏差高，方差低）</strong>而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</td>
<td><strong>强模型（偏差低，方差高）</strong></td>
</tr>
<tr>
<td>相关性</td>
<td></td>
<td>对于 Boosting 来说，由于基模型共用同一套训练集，所以基模型间具有强相关性，故模型间的相关系数近似等于 1</td>
<td></td>
</tr>
<tr>
<td>模型偏差</td>
<td><strong>整体模型的偏差与基模型近似</strong>。(<script type="math/tex">\mu</script>)</td>
<td>基于贪心策略的前向加法，随着基模型数的增多，偏差减少。</td>
<td></td>
</tr>
<tr>
<td>模型方差</td>
<td>随着<strong>模型的增加可以降低整体模型的方差</strong>，故其基模型需要为强模型；(<script type="math/tex">\frac{\sigma^{2}(1-\rho)}{m}+\sigma^{2} \rho</script>)</td>
<td><strong>整体模型的方差与基模型近似</strong>（<script type="math/tex">\sigma^{2}</script>）</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="1-集成学习">1. 集成学习</span></h2><p>常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。三种集成学习框架在基学习器的产生和综合结果的方式上会有些区别，我们先做些简单的介绍。</p>
<h3><span id="11-bagging">1.1 Bagging</span></h3><p>Bagging 全称叫 <strong>Bootstrap aggregating</strong>，，==每个基学习器都会对训练集进行<strong>有放回抽样</strong>得到子训练集==，比较著名的采样法为 0.632 自助法（<strong>Bootstrap</strong>）。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是<strong>投票法</strong>，票数最多的类别为预测类别。</p>
<p><img src="https://pic1.zhimg.com/80/v2-a0a3cb02f629f3db360fc68b4c2153c0_1440w.jpg" alt="img"></p>
<h3><span id="12-boosting">1.2 Boosting</span></h3><p><strong>Boosting 训练过程为阶梯状，基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果</strong>，用的比较多的综合方式为加权法。</p>
<p><img src="https://pic3.zhimg.com/80/v2-3aab53d50ab65e11ad3c9e3decf895c2_1440w.jpg" alt="img"></p>
<h3><span id="13-stacking">1.3 Stacking</span></h3><p><strong>Stacking 是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行的预测，==其预测值将作为训练样本的特征值==，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终预测结果。</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-f6787a16c23950d129a7927269d5352a_1440w.jpg" alt="img"></p>
<p>==那么，为什么集成学习会好于单个学习器呢？原因可能有三：==</p>
<ul>
<li><p>训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；</p>
</li>
<li><p>假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；</p>
</li>
<li><p>可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。</p>
</li>
</ul>
<h3><span id="14-stacking-vs-神经网络">==<strong>1.4 Stacking</strong> vs <strong>神经网络</strong>==</span></h3><blockquote>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32896968">https://zhuanlan.zhihu.com/p/32896968</a></p>
<p><strong>本文的核心观点是提供一种对于stacking的理解，即与神经网络对照来看。</strong>当然，在<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/59769987/answer/269367049">阿萨姆：为什么做stacking之后，准确率反而降低了？</a>中我已经说过stacking不是万能药，但往往很有效。通过与神经网络的对比，读者可以从另一个角度加深对stacking的理解。</p>
</li>
</ul>
</blockquote>
<h4><span id="141-stacking是一种表示学习representation-learning">1.4.1 Stacking是一种表示学习(representation learning)</span></h4><p><strong>表示学习指的是模型从原始数据中自动抽取有效特征的过程</strong>，比如深度学习就是一种表示学习的方法。关于表示学习的理解可以参考：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/264417928/answer/283087276">阿萨姆：人工智能（AI）是如何处理数据的？</a></p>
<p>原始数据可能是杂乱无规律的。在stacking中，通过第一层的多个学习器后，有效的特征被学习出来了。从这个角度来看，stacking的第一层就是特征抽取的过程。在[1]的研究中，上排是未经stacking的数据，下排是经过stacking(多个无监督学习算法)处理后的数据，我们显著的发现红色和蓝色的数据在下排中分界更为明显。<strong>==数据经过了压缩处理。这个小例子说明了，有效的stacking可以对原始数据中的特征有效的抽取==</strong>。</p>
<p><img src="https://pic1.zhimg.com/80/v2-92ff83c7c6acc0dea6bc53ffe815e8bc_1440w.jpg" alt="img" style="zoom:80%;"></p>
<h4><span id="142-stacking和神经网络从某种角度看有异曲同工之妙神经网络也可以被看作是集成学习">1.4.2  <strong>Stacking和神经网络从某种角度看有异曲同工之妙，神经网络也可以被看作是集成学习</strong></span></h4><p>承接上一点，stacking的学习能力主要来自于对于特征的表示学习，这和神经网络的思路是一致的。这也是为什么我说“第一层”，“最后一层”。</p>
<p>而且神经网络也可以被看做是一种集成学习，主要取决于不同神经元、层对于不同特征的理解不同。从浅层到深层可以理解为一种从具体到抽象的过程。</p>
<p><strong>Stacking中的第一层可以等价于神经网络中的前 n-1层，而stacking中的最终分类层可以类比于神经网络中最后的输出层。</strong>不同点在于，<strong>stacking中不同的分类器通过异质来体现对于不同特征的表示</strong>，神经网络是从同质到异质的过程且有分布式表示的特点(distributed representation)。Stacking中应该也有分布式的特点，主要表现在多个分类器的结果并非完全不同，而有很大程度的相同之处。</p>
<p>但同时这也提出了一个挑战，多个分类器应该尽量在保证效果好的同时尽量不同，stacking集成学习框架的对于基分类器的两个要求：</p>
<ul>
<li>差异化(diversity)要大</li>
<li>准确性(accuracy)要高</li>
</ul>
<h4><span id="143-stacking-的输出层为什么用逻辑回归"><strong>1.4.3 Stacking 的输出层为什么用逻辑回归？</strong></span></h4><blockquote>
<p>  <strong>表示学习的过拟合问题</strong>：</p>
<ul>
<li>仅包含学习到的特征</li>
<li>交叉验证</li>
<li>简单模型：<strong>逻辑回归</strong></li>
</ul>
</blockquote>
<p>如果你看懂了上面的两点，你应该可以理解stacking的有效性主要来自于特征抽取。<strong>而表示学习中，如影随形的问题就是过拟合，试回想深度学习中的过拟合问题。</strong></p>
<p>在[3]中，周志华教授也重申了stacking在使用中的过拟合问题。因为第二层的特征来自于对于第一层数据的学习，那么第二层数据中的特征中不该包括原始特征，<strong>以降低过拟合的风险</strong>。举例：</p>
<ul>
<li>第二层数据特征：仅包含学习到的特征</li>
<li>第二层数据特征：包含学习到的特征 + 原始特征</li>
</ul>
<p>另一个例子是，stacking中一般都用交叉验证来避免过拟合，足可见这个问题的严重性。</p>
<p>为了降低过拟合的问题，第二层分类器应该是较为简单的分类器，广义线性如逻辑回归是一个不错的选择。<strong>在特征提取的过程中，我们已经使用了复杂的非线性变换，因此在输出层不需要复杂的分类器</strong>。这一点可以对比神经网络的激活函数或者输出层，都是很简单的函数，一点原因就是不需要复杂函数并能控制复杂度。</p>
<h4><span id="144-stacking是否需要多层第一层的分类器是否越多越好"><strong>1.4.4 Stacking是否需要多层？第一层的分类器是否越多越好？</strong></span></h4><p>通过以上分析，stacking的表示学习不是来自于多层堆叠的效果，而是<strong>来自于不同学习器对于不同特征的学习能力</strong>，并有效的结合起来。一般来看，2层对于stacking足够了。多层的stacking会面临更加复杂的过拟合问题，且收益有限。</p>
<p>第一层分类器的数量对于特征学习应该有所帮助，<strong>经验角度看越多的基分类器越好。即使有所重复和高依赖性，我们依然可以通过特征选择来处理</strong>，问题不大。</p>
<h2><span id="2-偏差与方差">2. 偏差与方差</span></h2><p>上节介绍了集成学习的基本概念，这节我们主要介绍下如何从偏差和方差的角度来理解集成学习。</p>
<h4><span id="21-集成学习的偏差与方差">2.1 集成学习的偏差与方差</span></h4><p><strong>==偏差（Bias）描述的是预测值和真实值之差==</strong>；<strong>==方差（Variance）描述的是预测值作为随机变量的离散程==度</strong>。放一场很经典的图：</p>
<p><img src="https://pic2.zhimg.com/80/v2-60c942f91d33d9dedf9dd2c7d482af5d_1440w.jpg" alt="img"></p>
<p><strong>模型</strong>的<strong>偏差</strong>与<strong>方差</strong></p>
<ul>
<li><strong>偏差：</strong>描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的 High Variance，点会很分散。低偏差对应的点都打在靶心附近，所以喵的很准，但不一定很稳；</li>
<li><strong>方差：</strong>描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图 High Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。</li>
</ul>
<p>我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型，<strong>但并不是所有集成学习框架中的基模型都是弱模型</strong>。<strong>Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting 中的基模型为弱模型（偏差高，方差低）</strong>。</p>
<h4><span id="22-bagging-的偏差与方差">2.2 Bagging 的偏差与方差</span></h4><ul>
<li><strong>整体模型的期望等于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。</strong></li>
<li><strong>整体模型的方差小于等于基模型的方差，当且仅当相关性为 1 时取等号，随着基模型数量增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。</strong>但是，模型的准确度一定会无限逼近于 1 吗？并不一定，当基模型数增加到一定程度时，方差公式第一项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。</li>
</ul>
<h4><span id="23-boosting-的偏差与方差">2.3 Boosting 的偏差与方差</span></h4><ul>
<li>整体模型的方差等于基模型的方差，如果基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting 框架中的基模型必须为弱模型。</li>
<li>此外 Boosting 框架中采用基于贪心策略的前向加法，整体模型的期望由基模型的期望累加而成，所以随着基模型数的增多，整体模型的期望值增加，整体模型的准确度提高。</li>
</ul>
<h4><span id="24-小结">2.4 小结</span></h4><ul>
<li>我们可以使用<strong>==模型的偏差和方差来近似描述模型的准确度==</strong>；</li>
<li>对于 Bagging 来说，整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型；</li>
<li>对于 Boosting 来说，整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</li>
</ul>
<h2><span id="3-random-forestbagging">3. Random Forest（Bagging）</span></h2><blockquote>
<ol>
<li>随机森林具有<strong>防止过拟合能力</strong>，精度比大多数单个算法要好；<ol>
<li>随机森林分类器可以<strong>处理缺失值</strong>；</li>
</ol>
</li>
<li><strong>==于有袋外数据(OOB)==，可以在模型生成过程中取得真实误差的无偏估计，且不损失训练数据量在训练过程中，能够检测到feature间的互相影响，且可以得出feature的重要性，具有一定参考意义；</strong><ol>
<li>每棵树可以独立、同时生成，容易做成<strong>并行化方法</strong>；</li>
</ol>
</li>
<li>具有一定的特征选择能力。</li>
</ol>
</blockquote>
<p><strong>Random Forest（随机森林），用随机的方式建立一个森林。RF 算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。</strong></p>
<p>对于分类问题，其输出的类别是由个别树输出的众数所决定的。在回归问题中，把每一棵决策树的输出进行平均得到最终的回归结果。</p>
<h3><span id="31-思想">3.1 思想</span></h3><p>Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：</p>
<ul>
<li><strong>样本随机：</strong>假设训练数据集共有 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 个对象的数据，从样本数据中采取有放回（<strong>Boostrap</strong>）随机抽取 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 个样本（因为是有放回抽取，有些数据可能被选中多次，有些数据可能不被选上)，每一次取出的样本不完全相同，这些样本组成了决策树的训练数据集；</li>
<li><strong>特征随机：</strong>假设每个样本数据都有 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 个特征，从所有特征中随机地选取 <img src="https://www.zhihu.com/equation?tex=k%28k%3C%3DK%29" alt="[公式]"> 个特征，选择最佳分割属性作为节点建立CART决策树，决策树成长期间 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 的大小始终不变（<strong>在Python中构造随机森林模型的时候，默认取特征的个数 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 的平方根，即 <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BK%7D" alt="[公式]"></strong> )；</li>
<li>重复前面的步骤，建立 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 棵CART树，这些树都要完全的成长且不被修剪，这些树形成了森林；</li>
<li>根据这些树的预测结果进行投票，决定样本的最后预测类别。（针对回归模型，是根据这些决策树模型的平均值来获取最终的结果）</li>
</ul>
<p>随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；<strong>==随机选择特征是指在每个节点在分裂过程中都是随机选择特征的==</strong>（区别与每棵树随机选择一批特征）。</p>
<blockquote>
<p>  这种随机性导致随机森林的==偏差会有稍微的增加==（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的==方差减小==，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。</p>
</blockquote>
<p>随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算==不剪枝也不会出现过拟合==。</p>
<h3><span id="32-处理缺失值的方法">3.2 处理缺失值的方法</span></h3><ul>
<li>方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是<strong>分类变量(categorical var)缺失，用众数补上</strong>，如果是<strong>连续型变量(numerical var)缺失，用中位数补</strong>。</li>
<li>方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用<strong>proximity矩阵进行加权平均的方法补缺失值</strong>。然后迭代4-6次，这个补缺失值的思想和KNN有些类似。</li>
</ul>
<h3><span id="33-优缺点">3.3 优缺点</span></h3><h4><span id="优点"><strong>优点</strong>：</span></h4><ol>
<li><strong>模型准确率高</strong>：随机森林既可以处理分类问题，也可以处理回归问题，即使存在部分数据缺失的情况，随机森林也能保持很高的分类精度。</li>
<li><strong>能够处理数量庞大的高维度的特征</strong>，且不需要进行降维（因为特征子集是随机选择的）；</li>
<li><strong>易于并行化</strong>，在大数据集上有很大的优势；</li>
<li><p><strong>可解释性</strong>：可以生成树状结构，判断各个特征的重要性；</p>
<blockquote>
<p>  在sklearn中，随机森林<strong>基于每棵树分裂时的GINI指数下降量</strong>来判断各个特征的重要性。但是这种方法存在一个问题：当特征是连续的时候或者是类别很多的离散特征时，该方法会将这些特征的重要性增加。</p>
<p>  解决方法：对特征编码，使得特征的取值数量相近。</p>
</blockquote>
</li>
<li><strong>对异常值、缺失值不敏感；</strong></li>
<li><strong>随机森林有袋外数据（OOB），因此不需要单独划分交叉验证集</strong>。</li>
</ol>
<h4><span id="缺点">缺点：</span></h4><ul>
<li>随机森林解决回归问题的效果不如分类问题；（因为它的预测不是天生连续的，在解决回归问题时，随机森林并不能为训练数据以外的对象给出答案）</li>
<li><strong>树之间的相关性越大，错误率就越大</strong>；</li>
<li><strong>当训练数据噪声较大时，容易产生过拟合现象。</strong></li>
</ul>
<h3><span id="34-基学习期的选择">==3.4 基学习期的选择？==</span></h3><h4><span id="为什么集成学习的基分类器通常是决策树还有什么">为什么集成学习的基分类器通常是决策树？还有什么？</span></h4><p>基分类器通常是决策树：样本权重、方便调节、随机性；</p>
<ul>
<li><strong>==决策树可以较方便地将样本权重整合到训练过程中，而不需要通过过采样来调整样本权重。==</strong></li>
<li>树的表达能力和泛化能力，<strong>方便调节</strong>（可以通过树的层数来调节）</li>
<li>样本的扰动对决策树的影响较大，<strong><font color="red"> 因此不同子样本集合生成的决策树基分类器随机性较大。这样的不稳定的分类器更适合作为基分类器。</font></strong>此外树节点分类时随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li>
</ul>
<h4><span id="可以将随机森林的基分类器由决策树替换成线性分类器或k-nn吗">可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></h4><p>Bagging主要好处是集成后的方差，比基分类器小。bagging采用的基分类，最好是本身对样本分布较为敏感。而线性分类器和K-NN都是较为稳定的分类器（参数模型？）甚至<strong>可能因为采样，而导致他们再训练中更难收敛，从而增大了集成分类器的偏差。</strong></p>
<h2><span id="4-adaboost-boosting-样本权重更新">4 Adaboost (Boosting) 样本权重更新?</span></h2><p>AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：<strong>前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的==足够小的错误率或达到预先指定的最大迭代次数==。</strong></p>
<h3><span id="41-思想">4.1 思想</span></h3><p><strong>==Adaboost 迭代算法有三步：==</strong></p>
<ul>
<li>初始化训练样本的权值分布，每个样本具有相同权重；</li>
<li>训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；</li>
<li>将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，<strong>加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重</strong>。</li>
</ul>
<h3><span id="42-细节">4.2 细节</span></h3><h5><span id="421-损失函数">==<strong>4.2.1 损失函数 ???</strong>==</span></h5><p>Adaboost 模型是<strong>加法模型</strong>，学习算法为<strong>前向分步学习算法</strong>，损失函数为<strong>指数函数的分类问题</strong>。</p>
<p><strong>加法模型</strong>：最终的强分类器是由若干个弱分类器<strong>加权平均</strong>得到的。</p>
<p><strong>前向分布学习算法</strong>：算法是通过一轮轮的弱学习器学习，<strong>利用前一个弱学习器的结果来更新后一个弱学习器的训练集权重</strong>。第 k 轮的强学习器为：</p>
<script type="math/tex; mode=display">
F_{k}(x)=\sum_{i=1}^{k} \alpha_{i} f_{i}(x)=F_{k-1}(x)+\alpha_{k} f_{k}(x)</script><p><strong>定义损失函数为 n 个样本的指数损失函数</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28y%2CF%29+%3D+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bn%7Dexp%28-y_iF_%7Bk%7D%28x_i%29%29++%5C%5C" alt="[公式]"></p>
<p>利用前向分布学习算法的关系可以得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++L%28y%2C+F%29+%26%3D+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B%28-y_i%29+%28F_%7Bk-1%7D%28x_i%29+%2B+%5Calpha_k+f_k%28x_i%29%29%5D++%5C%5C+%26%3D++%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B-y_i+F_%7Bk-1%7D%28x_i%29+-y_i++%5Calpha_k+f_k%28x_i%29%5D+%5C%5C+%26%3D++%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B-y_i+F_%7Bk-1%7D%28x_i%29+%5D+exp%5B-y_i++%5Calpha_k+f_k%28x_i%29%5D+++%5Cend%7Balign%7D++%5C%5C" alt="[公式]"></p>
<p><strong>因为 <img src="https://www.zhihu.com/equation?tex=F_%7Bk-1%7D%28x%29" alt="[公式]"> 已知==，所以令 <img src="https://www.zhihu.com/equation?tex=w_%7Bk%2Ci%7D+%3D+exp%28-y_iF_%7Bk-1%7D%28x_i%29%29" alt="[公式]"> ，==随着每一轮迭代而将这个式子带入损失函数，损失函数转化为：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=L%28y%2C+F%28x%29%29+%3D%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bk%2Ci%7Dexp%5B-y_i%5Calpha_k+f_k%28x_i%29%5D+%5C%5C" alt="[公式]"></p>
<p>我们求 <img src="https://www.zhihu.com/equation?tex=f_k%28x%29" alt="[公式]"> ，可以得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=f_k%28x%29+%3Dargmin%5C%3B+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bk%2Ci%7DI%28y_i+%5Cneq+f_k%28x_i%29%29+%5C%5C" alt="[公式]"></p>
<p>将 <img src="https://www.zhihu.com/equation?tex=f_k%28x%29" alt="[公式]"> 带入损失函数，并对 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 求导，使其等于 0，则就得到了：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Calpha_k+%3D+%5Cfrac%7B1%7D%7B2%7Dlog%5Cfrac%7B1-e_k%7D%7Be_k%7D++%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=e_k" alt="[公式]"> 即为我们前面的<strong>分类误差率</strong>。</p>
<p><img src="https://www.zhihu.com/equation?tex=+e_k+%3D+%5Cfrac%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7D%5E%7B%E2%80%99%7DI%28y_i+%5Cneq+f_k%28x_i%29%29%7D%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7D%5E%7B%E2%80%99%7D%7D+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7DI%28y_i+%5Cneq+f_k%28x_i%29%29+%5C%5C" alt="[公式]"></p>
<p><strong>最后看样本权重的更新</strong>。利用 <img src="https://www.zhihu.com/equation?tex=F_%7Bk%7D%28x%29+%3D+F_%7Bk-1%7D%28x%29+%2B+%5Calpha_kf_k%28x%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=w_%7Bk%2B1%2Ci%7D%3Dw_%7Bk%2Ci%7Dexp%5B-y_i%5Calpha_kf_k%28x%2Ci%29%5D" alt="[公式]"> ，即可得：</p>
<p><img src="https://www.zhihu.com/equation?tex=w_%7Bk%2B1%2Ci%7D+%3D+w_%7Bki%7Dexp%5B-y_i%5Calpha_kf_k%28x_i%29%5D+%5C%5C" alt="[公式]"></p>
<p>这样就得到了样本权重更新公式。</p>
<h4><span id="422-正则化"><strong>4.2.2 正则化</strong></span></h4><p><strong>为了防止 Adaboost 过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长</strong>（learning rate）。对于前面的弱学习器的迭代,加上正则化项 <img src="https://www.zhihu.com/equation?tex=%5Cmu+" alt="[公式]"> 我们有：</p>
<script type="math/tex; mode=display">
F_{k}(x)=F_{k-1}(x)+\mu \alpha_{k} f_{k}(x)</script><p><img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 的取值范围为 <img src="https://www.zhihu.com/equation?tex=0%3C%5Cmu%5Cleq1" alt="[公式]"> 。对于同样的训练集学习效果，较小的 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>
<h3><span id="43-优缺点">4.3 优缺点</span></h3><p><strong>4.3.1 优点</strong></p>
<ol>
<li>分类精度高；</li>
<li>可以<strong>用各种回归分类模型来构建弱学习器，非常灵活</strong>；</li>
<li>不容易发生过拟合。</li>
</ol>
<p><strong>4.3.2 缺点</strong></p>
<ol>
<li>对异常点敏感，异常点会获得较高权重。</li>
</ol>
<h2><span id="5-gbdt">5. GBDT</span></h2><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://www.cnblogs.com/modifyrong/p/7744987.html">https://www.cnblogs.com/modifyrong/p/7744987.html</a></p>
</blockquote>
<p><strong>GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，该算法由多棵决策树组成，从名字中我们可以看出来它是属于 Boosting 策略。GBDT 是被公认的泛化能力较强的算法。</strong></p>
<h3><span id="51-思想">5.1 思想</span></h3><p><strong>GBDT是boosting算法的一种，按照boosting的思想，在GBDT算法的每一步，用一棵决策树去拟合当前学习器的残差，获得一个新的弱学习器。将这每一步的决策树组合起来，就得到了一个强学习器</strong>。GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和 Shrinkage（一个重要演变）</p>
<h4><span id="511-回归树regression-decision-tree"><strong>5.1.1 回归树（Regression Decision Tree）</strong></span></h4><p>如果认为 GBDT 由很多分类树那就大错特错了（虽然调整后也可以分类）。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。GBDT 的核心在于累加所有树的结果作为最终结果，所以 GBDT 中的树都是<strong>回归树</strong>，不是分类树，这一点相当重要。</p>
<p><strong><font color="red"> 回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。</font></strong></p>
<h4><span id="512-梯度迭代gradient-boosting"><strong>5.1.2 梯度迭代（Gradient Boosting）</strong></span></h4><p>上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，GBDT 的每一棵树都是以之前树得到的<strong>残差【负梯度】</strong>来更新目标值，这样每一棵树的<strong>值加起来</strong>即为 GBDT 的预测值。</p>
<p>模型的预测值可以表示为：</p>
<script type="math/tex; mode=display">
F_{k}(x)=\sum_{i=1}^{k} f_{i}(x)</script><p><img src="https://www.zhihu.com/equation?tex=f_%7Bi%7D%28x%29+" alt="[公式]"> 为<strong>基模型与其权重的乘积</strong>，模型的训练目标是使预测值 <img src="https://www.zhihu.com/equation?tex=F_k%28x%29" alt="[公式]"> 逼近真实值 y，也就是说要让每个基模型的预测值逼近各自要预测的部分真实值。<strong>==贪心==</strong>的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：</p>
<script type="math/tex; mode=display">
F_{k}(x)=F_{k-1}(x)+f_{k}(x)</script><p>其实很简单，其<strong>==残差其实是最小均方损失函数关于预测值的反向梯度(划重点)==</strong>：<strong>用负梯度的解作为样本新的真实值</strong>。基于残差 GBDT 容易对异常值敏感。</p>
<script type="math/tex; mode=display">
-\frac{\partial\left(\frac{1}{2}\left(y-F_{k}(x)\right)^{2}\right)}{\partial F_{k}(x)}=y-F_{k}(x)</script><p>很明显后续的模型会对第 4 个值关注过多，这不是一种好的现象，所以一般回归类的损失函数会用<strong>绝对损失或者 Huber 损失函数</strong>来代替平方损失函数。</p>
<script type="math/tex; mode=display">
L(y, F)=|y-F|</script><script type="math/tex; mode=display">
L(y, F)= \begin{cases}\frac{1}{2}(y-F)^{2} & |y-F| \leq \delta \\ \delta(|y-F|-\delta / 2) & |y-F|>\delta\end{cases}</script><p>GBDT 的 Boosting 不同于 Adaboost 的 Boosting，<strong>==GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0==</strong>，这样后面的树就能专注于那些被分错的样本。</p>
<blockquote>
<p>  <strong><font color="red"> 最后补充一点拟合残差的问题，无论损失函数是什么形式，每个决策树拟合的都是负梯度。只有当损失函数是均方损失时，负梯度刚好是残差</font></strong>，也就是说<strong>拟合残差只是针对均方损失的特例</strong>，并不能说GBDT的迭代的过程是拟合残差。</p>
</blockquote>
<h4><span id="513-缩减shrinkage添加权重-基数增大"><strong>5.1.3 缩减（Shrinkage）</strong>添加权重、基数增大</span></h4><blockquote>
<p>  <strong><font color="red"> gbdt中的步长和参数中的学习率作用是什么？详细讲一讲？</font></strong></p>
<ul>
<li>参数中的学习率用于梯度下降</li>
</ul>
</blockquote>
<p>Shrinkage 的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。</p>
<p>Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。<strong>本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大</strong>。</p>
<h3><span id="52-优缺点">5.2 优缺点</span></h3><h4><span id="优点"><strong>优点</strong></span></h4><ol>
<li>可以自动进行特征组合，拟合非线性数据；在稠密数据集上泛化能力和表达能力很好。</li>
<li>可以灵活处理各种类型的数据，不需要对数据预处理和归一化。</li>
<li>预测可以并行，计算数据很快。</li>
</ol>
<h4><span id="缺点"><strong>缺点</strong></span></h4><ol>
<li>对异常点敏感。</li>
</ol>
<h3><span id="53-gbdt-与-adaboost-的对比">5.3 GBDT 与 Adaboost 的对比</span></h3><h4><span id="相同"><strong>相同：</strong></span></h4><ol>
<li>都是 Boosting 家族成员，使用弱分类器；</li>
<li>都使用前向分布算法；</li>
</ol>
<h4><span id="不同"><strong>不同：</strong></span></h4><ol>
<li><strong>迭代思路不同</strong>：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；</li>
<li><strong>损失函数不同</strong>：AdaBoost 采用的是<strong>指数损失</strong>，GBDT 使用的是<strong>绝对损失</strong>或者 <strong>Huber 损失函数</strong>；</li>
</ol>
<h3><span id="54-gbdt算法用于分类问题-二分类1维-多分类k维one-vs-all"><strong><font color="red"> 5.4 GBDT算法用于分类问题、二分类1维、多分类k维（one vs all）</font></strong></span></h3><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46445201">https://zhuanlan.zhihu.com/p/46445201</a></p>
</blockquote>
<p>将GBDT应用于回归问题，相对来说比较容易理解。因为<strong>回归问题的损失函数一般为平方差损失函数</strong>，这时的残差，恰好等于预测值与实际值之间的差值。每次拿一棵决策树去拟合这个差值，使得残差越来越小，这个过程还是比较intuitive的。</p>
<p><strong><font color="red"> 将GBDT用于分类问题，类似于逻辑回归、FM模型用于分类问题，其实是在用一个线性模型或者包含交叉项的非线性模型，去拟合所谓的对数几率 <img src="https://www.zhihu.com/equation?tex=%5Cln+%5Cfrac%7Bp%7D%7B1-p%7D" alt="[公式]"> 。</font></strong>而GBDT也是一样，只是用一系列的梯度提升树去拟合这个对数几率，实际上最终得到的是一系列CART回归树。其分类模型可以表达为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-+%5Csum_%7Bm%3D0%7D%5EM+h_m%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p>其中<img src="https://www.zhihu.com/equation?tex=h_m%28x%29" alt="[公式]"> 就是学习到的决策树。清楚了这一点之后，我们便可以参考逻辑回归，单样本 <img src="https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29" alt="[公式]"> 的损失函数可以表达为<strong>交叉熵</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss%28x_i%2C+y_i%29+%3D+-y_i+%5Clog+%5Chat%7By_i%7D+-+%281-y_i%29+%5Clog%281-%5Chat%7By_i%7D%29%5C%5C" alt="[公式]"></p>
<p>假设第k步迭代之后当前学习器为 <img src="https://www.zhihu.com/equation?tex=F%28x%29+%3D+%5Csum_%7Bm%3D0%7D%5Ek+h_m%28x%29" alt="[公式]"> ，将 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]"> 的表达式带入之后， 可将损失函数写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss%28x_i%2C+y_i%7CF%28x%29%29+%3D+y_i+%5Clog+%5Cleft%28+1%2Be%5E%7B-F%28x_i%29%7D+%5Cright%29+%2B+%281-y_i%29+%5Cleft%5B+F%28x_i%29+%2B+%5Clog+%5Cleft%28+1%2Be%5E%7B-F%28x_i%29%7D+%5Cright%29+%5Cright%5D%5C%5C" alt="[公式]"></p>
<p><strong>可以求得损失函数相对于当前学习器的负梯度为：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+F%28x%29%7D%7C_%7Bx_i%2Cy_i%7D+%3D+y_i+-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-F%28x_i%29%7D%7D%3D+y_i+-+%5Chat%7By_i%7D%5C%5C" alt="[公式]"></p>
<p>可以看到，同回归问题很类似，下一棵决策树的训练样本为： <img src="https://www.zhihu.com/equation?tex=%5C%7B+x_i%2C+y_i+-+%5Chat%7By_i%7D+%5C%7D_%7Bi%3D1%7D%5En" alt="[公式]"> ，其所需要拟合的残差为真实标签与预测概率之差。于是便有下面GBDT应用于二分类的算法：</p>
<ul>
<li><p><img src="https://www.zhihu.com/equation?tex=F_0%28x%29+%3D+h_0%28x%29+%3D+%5Clog+%5Cfrac%7Bp_1%7D%7B1-p_1%7D" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=p_1" alt="[公式]"> 是训练样本中y=1的比例，利用先验信息来初始化学习器</p>
</li>
<li><p>For <img src="https://www.zhihu.com/equation?tex=m+%3D+1%2C2%2C...%2CM" alt="[公式]"> ：</p>
<ul>
<li>计算 <img src="https://www.zhihu.com/equation?tex=g_i+%3D+%5Chat%7By_i%7D+-+y_i" alt="[公式]"> ，并使用训练集 <img src="https://www.zhihu.com/equation?tex=%5C%7B%28x_i%2C+-g_i%29%5C%7D_%7Bi%3D1%7D%5En" alt="[公式]"> <strong>训练一棵回归树</strong> <img src="https://www.zhihu.com/equation?tex=t_m%28x%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-F_%7Bm-1%7D%28x%29%7D%7D" alt="[公式]"></li>
<li>通过一维最小化损失函数找到树的最优权重： <img src="https://www.zhihu.com/equation?tex=%5Crho_m+%3D+%5Cmathop%7B%5Cmathrm%7Bargmin%7D%7D%5Climits_%7B%5Crho%7D+%5Csum_i+loss%28x_i%2C+y_i%7CF_%7Bm-1%7D%28x%29%2B%5Crho+t_m%28x%29%29" alt="[公式]"></li>
<li><strong>考虑shrinkage</strong>，可得这一轮迭代之后的学习器 <img src="https://www.zhihu.com/equation?tex=F_m%28x%29+%3D+F_%7Bm-1%7D%28x%29+%2B+%5Calpha+%5Crho_m+t_m%28x%29" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 为学习率</li>
</ul>
</li>
<li><p>得到最终学习器为： <img src="https://www.zhihu.com/equation?tex=F_M%28x%29" alt="[公式]"></p>
</li>
</ul>
<p>以上就是将GBDT应用于二分类问题的算法流程。类似地，对于多分类问题，则需要考虑以下softmax模型：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_1%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D2%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_2%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=...+...%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3Dk%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_k%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><strong>其中 <img src="https://www.zhihu.com/equation?tex=F_1" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=F_k" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 个不同的tree ensemble。每一轮的训练实际上是训练了 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 棵树去拟合softmax的每一个分支模型的负梯度</strong>【one-hot中的一维】。softmax模型的单样本损失函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss+%3D+-%5Csum_%7Bi%3D1%7D%5Ek+y_i+%5Clog+P%28y_i%7Cx%29+%3D+-%5Csum_%7Bi%3D1%7D%5Ek+y_i+%5Clog+%5Cfrac%7Be%5E%7BF_i%28x%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5Ek+e%5E%7BF_j%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><strong>这里的 <img src="https://www.zhihu.com/equation?tex=y_i%5C+%28i%3D1...k%29" alt="[公式]"> 是样本label在k个类别上作one-hot编码之后的取值，只有一维为1，其余都是0。由以上表达式不难推导：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+F_q%7D+%3D+y_q+-+%5Cfrac%7Be%5E%7BF_q%28x%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5Ek+e%5E%7BF_j%28x%29%7D%7D+%3D+y_q+-+%5Chat%7By_q%7D%5C%5C" alt="[公式]"></p>
<p>可见，这k棵树同样是拟合了样本的真实标签与预测概率之差，与二分类的过程非常类似。</p>
<h2><span id="6-集成学习qampa">6 集成学习Q&amp;A</span></h2><h4><span id="61-为什么gbdt和随机森林稍好点都不太适用直接用高维稀疏特征训练集"><strong><font color="red"> 6.1 为什么gbdt和随机森林(稍好点)都不太适用直接用高维稀疏特征训练集？</font></strong></span></h4><h5><span id="原因">原因：</span></h5><p>gbdt这类boosting或者rf这些bagging集成分类器模型的算法，是典型的贪心算法，在当前节点总是选择对当前数据集来说最好的选择</p>
<p>一个6层100树的模型，要迭代2^(5 4 3 2 1 0)<em>100次<em>*,每次都根据当前节点最大熵或者最小误差分割来选择变量</em></em></p>
<p><strong>那么，高维稀疏数据集里很多“小而美”的数据就被丢弃了</strong>，因为它对当前节点来说不是最佳分割方案(比如，关联分析里，支持度很低置信度很高的特征)</p>
<p>但是高维数据集里面，对特定的样本数据是有很强预测能力的，比如你买叶酸，买某些小的孕妇用品品类，对应这些人6个月后买奶粉概率高达40%，但叶酸和孕妇用品销量太小了，用户量全网万分之一都不到，这种特征肯定是被树算法舍弃的，哪怕这些特征很多很多。。它仍是被冷落的份。。。</p>
<h5><span id="方法lightgbm-互斥捆绑算法">方法：【LightGBM 互斥捆绑算法】</span></h5><h5><span id="选择svm和lr这种能提供最佳分割平面的算法可能会更好">选择svm和lr这种能提供最佳分割平面的算法可能会更好；</span></h5><p>但如果top.特征已经能够贡献很大的信息量了，比如刚才孕妇的案例，你用了一个孕妇用品一级类目的浏览次数购买金额购买次数这样的更大更强的特征包含了这些高维特征的信息量，那可能gbdt会更好</p>
<p>实际情况的数据集是，在数据仓库里的清洗阶段，你可以选择把它做成高维的特征，也可以选择用算法把它做成低维的特征，一般有</p>
<p>1-在数据清洗阶段，或用类目升级(三级类目升级到二三级类目)范围升级的方式来做特征，避免直接清洗出来高维特征</p>
<p>2-在特征生成后，<strong>利用数据分析结论简单直接的用多个高维特征合并</strong>(<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加减乘除&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A574865175}">加减乘除</a>逻辑判断都行，随你合并打分)的方式来做特征，前提你hold得住工作量判断量，但这个如果业务洞察力强效果有可能特别好</p>
<p>3-在特征工程的特征处理阶段，我们可以用<strong>PCA因子构建等降维算法做特征整合</strong>，对应训练集，也这么搞，到时候回归或预测的时候，就用这个因子或者主成分的值来做特征</p>
<h4><span id="61-为什么集成学习的基分类器通常是决策树还有什么">6.1 为什么集成学习的基分类器通常是决策树？还有什么？</span></h4><p>基分类器通常是决策树：样本权重、方便调节、随机性；</p>
<ul>
<li><strong>==决策树可以较方便地将样本权重整合到训练过程中，而不需要通过过采样来调整样本权重。==</strong></li>
<li>树的表达能力和泛化能力，<strong>方便调节</strong>（可以通过树的层数来调节）</li>
<li>样本的扰动对决策树的影响较大，<strong><font color="red"> 因此不同子样本集合生成的决策树基分类器随机性较大。这样的不稳定的分类器更适合作为基分类器。</font></strong>此外树节点分类时随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li>
</ul>
<h4><span id="62-可以将随机森林的基分类器由决策树替换成线性分类器或k-nn吗">6.2 可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></h4><p>Bagging主要好处是集成后的方差，比基分类器小。bagging采用的基分类，最好是本身对样本分布较为敏感。而线性分类器和K-NN都是较为稳定的分类器（参数模型？）甚至可能因为采样，而导致他们再训练中更难收敛，从而增大了集成分类器的偏差。</p>
<h4><span id="63-为什么可以利用gbdt算法实现特征组合和筛选gbdtlr"><strong><font color="red"> 6.3 为什么可以利用GBDT算法实现特征组合和筛选？【GBDT+LR】</font></strong></span></h4><p>GBDT模型是有一组有序的树模型组合起来的，前面的树是由对大多数样本有明显区分度的特征分裂构建而成，经过前面的树，仍然存在少数残差较大的样本，后面的树主要由能对这些少数样本有区分度的特征分裂构建。优先选择对整体有区分度的特征，然后再选择对少数样本有区分度的特征，这样才更加合理，所以<strong>GBDT子树节点分裂是一个特征选择的过程，而子树的多层结构则对特征组合的过程，最终实现特征的组合和筛选。</strong></p>
<p><strong>GBDT+LR融合方案：</strong></p>
<p>（1）利用GBDT模型训练数据，最终得到一系列弱分类器的cart树。</p>
<p>（2）<strong>生成新的训练数据。将原训练数据重新输入GBDT模型，对于每一个样本，都会经过模型的一系列树，对于每棵树，将样本落到的叶子节点置为1，其他叶子为0，然后将叶子节点的数字从左至右的拼接起来，形成该棵树的特征向量，最后将所有树的特征向量拼接起来，形成新的数据特征，之后保留原样本标签形成新的训练数据。</strong></p>
<p>（3）将上一步得到的训练数据作为输入数据输入到LR模型中进行训练</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/YFRZTY/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/YFRZTY/" class="post-title-link" itemprop="url">机器学习（11）LGB*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:13:30" itemprop="dateCreated datePublished" datetime="2022-03-11T21:13:30+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-29 16:27:31" itemprop="dateModified" datetime="2023-01-29T16:27:31+08:00">2023-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="参考链接">参考链接</span></h2><ul>
<li><p><strong>XGBoost官方文档</strong>：<a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/index.html">https://xgboost.readthedocs.io/en/latest/index.html</a></p>
</li>
<li><p>LightGBM算法梳理：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/78293497">https://zhuanlan.zhihu.com/p/78293497</a></p>
</li>
<li><p>详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/366234433">https://zhuanlan.zhihu.com/p/366234433</a></p>
</li>
<li><p>【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/87885678">https://zhuanlan.zhihu.com/p/87885678</a></p>
</li>
<li><p>xgboost面试题整理: <a target="_blank" rel="noopener" href="https://xiaomindog.github.io/2021/06/22/xgb-qa/">https://xiaomindog.github.io/2021/06/22/xgb-qa/</a></p>
</li>
</ul>
<h2><span id="机器学习决策树下xgboost-lightgbm">【机器学习】决策树（下）——XGBoost、LightGBM</span></h2><p><img src="https://pic2.zhimg.com/80/v2-358e4bfce928d0460bd5e8b4cab8f715_1440w.jpg" alt="img"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Boosting 算法</th>
<th>GBDT</th>
<th>XGBoost</th>
<th>LightGBM</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong><img src="image-20220315210756470.png" alt="image-20220315210756470" style="zoom:25%;"></td>
<td>回归树、梯度迭代、缩减（Shrinkage）;<strong>GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0</strong></td>
<td><strong>二阶导数、线性分类器、正则化</strong>、缩减、<strong>列抽样、并行化</strong></td>
<td><strong>更快的训练速度和更低的内存使用</strong></td>
</tr>
<tr>
<td>目标函数</td>
<td><img src="image-20220315213233284.png" alt="image-20220315213233284" style="zoom: 25%;"></td>
<td><img src="image-20220315213503054.png" alt="image-20220315213503054" style="zoom: 67%;"><img src="image-20220315213608526.png" alt="image-20220315213608526" style="zoom: 33%;"></td>
<td>同上</td>
</tr>
<tr>
<td>损失函数</td>
<td>最小均方损失函数、<strong>绝对损失或者 Huber 损失函数</strong></td>
<td>【线性】最小均方损失函数、==sigmod和softmax==</td>
<td><strong>复杂度模型</strong>：<img src="image-20220315215849417.png" alt="image-20220315215849417" style="zoom: 25%;"></td>
</tr>
<tr>
<td>基模型</td>
<td>CART模型</td>
<td>CART模型/ ==回归模型==</td>
<td>CART模型/ ==回归模型==</td>
</tr>
<tr>
<td>抽样算法</td>
<td>无</td>
<td><strong>列抽样</strong>：借鉴了<strong>随机森林</strong>的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</td>
<td><strong>单边梯度抽样算法；</strong>根据样本梯度来对梯度小的这边样本进行采样，一部分大梯度和随机分布</td>
</tr>
<tr>
<td><strong>切分点算法</strong></td>
<td>CART模型</td>
<td><strong>预排序</strong>、<strong>贪心算法</strong>、<strong>近似算法（</strong>加权分位数缩略图<strong>）</strong></td>
<td><strong>直方图算法</strong>：内存消耗降低，计算代价减少；（不需要记录特征到样本的索引）</td>
</tr>
<tr>
<td><strong>缺失值算法</strong></td>
<td>CART模型</td>
<td><strong>稀疏感知算法</strong>：选择增益最大的枚举项即为最优<strong>缺省方向</strong>。【<strong><font color="red"> 稀疏数据优化不足</font></strong>】【<strong>gblinear 补0</strong>】</td>
<td><strong>互斥特征捆绑算法</strong>：<strong>互斥</strong>指的是一些特征很少同时出现非0值。<strong>稀疏感知算法</strong>；【<strong>gblinear 补0</strong>】</td>
</tr>
<tr>
<td><strong>建树策略</strong></td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Leaf-wise</strong>：每次分裂增益最大的叶子节点，直到达到停止条件。</td>
</tr>
<tr>
<td><strong>正则化</strong></td>
<td>无</td>
<td>L1 和 L2 正则化项</td>
<td>L1 和 L2 正则化项</td>
</tr>
<tr>
<td><strong>Shrinkage（缩减）</strong></td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>类别特征优化</td>
<td>无</td>
<td>无</td>
<td><strong>类别特征最优分割</strong>：<strong>many-vs-many</strong></td>
</tr>
<tr>
<td>并行化设计</td>
<td>无</td>
<td><strong>块结构设计</strong>、</td>
<td><strong>特征并行</strong>、 <strong>数据并行</strong>、<strong>投票并行</strong></td>
</tr>
<tr>
<td>==缓存优化==</td>
<td>无</td>
<td>为每个线程分配一个连续的缓存区、<strong>“核外”块计算</strong></td>
<td>1、所有的特征都采用相同的方法获得梯度；2、其次，因为不需要存储特征到样本的索引，降低了存储消耗</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>对异常点敏感；</td>
<td><strong>预排序</strong>：仍需要遍历数据集；==不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。==</td>
<td><strong>内存更小</strong>： 索引值、特征值边bin、互斥特征捆绑; <strong>速度更快</strong>：遍历直方图；单边梯度算法过滤掉梯度小的样本；基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；特征并行、数据并行方法加速计算</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="一-lightgbm">一、LightGBM</span></h2><blockquote>
<ul>
<li>《<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=Lightgbm%3A+A+highly+efficient+gradient+boosting+decision+tree&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">Lightgbm: A highly efficient gradient boosting decision tree</a>》</li>
<li>《A communication-efficient <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=parallel+algorithm+for+decision+tree&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">parallel algorithm for decision tree</a>》</li>
</ul>
</blockquote>
<p>LightGBM 由微软提出，主要用于解决 GDBT 在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中。从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有<strong>训练速度快、内存占用低</strong>的特点。下图分别显示了 XGBoost、XGBoost_hist（利用梯度直方图的 XGBoost） 和 LightGBM 三者之间针对不同数据集情况下的内存和训练时间的对比：</p>
<p><img src="https://pic1.zhimg.com/80/v2-e015e3c4018f44787d74a47c9e0cd040_1440w.jpg" alt="img"></p>
<p>那么 LightGBM 到底如何做到<strong>更快的训练速度和更低的内存</strong>使用的呢？</p>
<h4><span id="lightgbm-为了解决这些问题提出了以下几点解决方案"><strong><font color="red"> LightGBM 为了解决这些问题提出了以下几点解决方案：</font></strong></span></h4><ol>
<li><p><strong>【减小内存、最优分类点】直方图算法</strong>；【特征离散化 + 内存占用 + 方差减少】</p>
</li>
<li><p><strong>【样本维度】 单边梯度抽样算法</strong>；【<strong>根据样本梯度来对梯度小的这边样本进行采样</strong>，一部分大梯度和随机分布】</p>
<blockquote>
<p>  <strong>一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。</strong></p>
</blockquote>
</li>
<li><p><strong>【特征维度】互斥特征捆绑算法</strong>；【特征稀疏行优化 +分箱 】</p>
</li>
<li><p><strong>【分裂算法】基于最大深度的 Leaf-wise 的垂直生长算法</strong>；【深度限制的最大分裂收益的叶子】</p>
</li>
<li><p><strong>类别特征最优分割</strong>；</p>
</li>
<li><p><strong>特征并行和数据并行</strong>；</p>
</li>
<li><p><strong>缓存优化。</strong></p>
</li>
</ol>
<h3><span id="11-数学原理">1.1 数学原理</span></h3><h3><span id="111-直方图算法"><strong>1.1.1 直方图算法</strong></span></h3><h4><span id="1-直方图算法"><strong>(1) 直方图算法</strong></span></h4><p><strong><font color="red"> 直方图算法的基本思想是将连续的特征离散化为 k （默认256 1字节）个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。</font></strong></p>
<p>我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）：</p>
<ul>
<li><strong>内存占用更小：</strong>XGBoost 需要用 <strong>32 位的浮点数去存储特征值，并用 32 位的整形去存储排序索引</strong>，而 LightGBM 只需要用 8 位去存储直方图，<strong>相当于减少了 1/8</strong>；</li>
<li><strong>计算代价更小：</strong>计算特征分裂增益时，XGBoost 需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k 次，直接将时间复杂度从代价是O( feature <em> <strong>distinct_values_of_the_feature</strong>); 而 histogram 只需要计算 bins次, 代价是( feature </em> <strong>bins</strong>)。<strong>distinct_values_of_the_feature &gt;&gt; bins</strong></li>
</ul>
<p><img src="image-20220625171413733.png" alt="image-20220625171413733"></p>
<ol>
<li><strong>直方图优化算法需要在训练前预先把特征值转化为bin value</strong>，也就是对每个特征的取值做个分段函数，将所有样本在该特征上的取值划分到某一段（bin）中。最终把特征取值从连续值转化成了离散值。需要注意得是：feature value对应的bin value在整个训练过程中是不会改变的。</li>
<li><strong>最外面的 for 循环表示的意思是对当前模型下所有的叶子节点处理</strong>，需要遍历所有的特征，来找到增益最大的特征及其划分值，以此来分裂该叶子节点。</li>
<li>在某个叶子上，第二个 for 循环就开始遍历所有的特征了。<strong>对于每个特征，首先为其创建一个直方图 (new Histogram() )</strong>。这个直方图存储了两类信息，分别是<strong><font color="red"> 每个bin中样本的梯度之和 $H[ f.bins[i] ].g$ </font></strong>，还有就是<strong>每个bin中样本数量</strong>$（H[f.bins[i]].n）$</li>
<li>第三个 for 循环遍历所有样本，累积上述的两类统计值到样本所属的bin中。即直方图的每个 bin 中包含了一定的样本，在此计算每个 bin 中的样本的梯度之和并对 bin 中的样本记数。</li>
<li>最后一个for循环, 遍历所有bin, 分别以当前bin作为分割点, 累加其左边的bin至当前bin的梯度和（ $\left.S<em>{L}\right)$ 以及样本数量 $\left(n</em>{L}\right)$, 并与父节点上的总梯度和 $\left(S<em>{p}\right)$ 以及总样本数量 $\left(n</em>{p}\right)$ 相减, 得到右边 所有bin的梯度和 $\left(S<em>{R}\right)$ 以及样本数量 $\left(n</em>{R}\right)$, 带入公式, 计算出增益, 在遍历过程中取最大的增 益, 以此时的特征和bin的特征值作为分裂节点的特征和分裂特征取值。</li>
</ol>
<h4><span id="2-源码分析">(2) 源码分析</span></h4><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/anshuai_aw1/article/details/83040541">https://blog.csdn.net/anshuai_aw1/article/details/83040541</a></p>
<p>  <strong><font color="red"> 『我爱机器学习』集成学习（四）LightGBM</font></strong>：<a target="_blank" rel="noopener" href="https://www.hrwhisper.me/machine-learning-lightgbm/">https://www.hrwhisper.me/machine-learning-lightgbm/</a></p>
</blockquote>
<p>问题一：<strong>如何将特征映射到bin呢？即如何分桶？对于连续特征和类别特征分别怎么样处理？</strong></p>
<p>问题二：<strong>如何构建直方图？直方图算法累加的g是什么？难道没有二阶导数h吗？</strong></p>
<h4><span id="特征分桶">特征分桶：</span></h4><blockquote>
<p>  <strong>特征分桶的源码</strong>在<strong>bin.cpp</strong>文件和<strong>bin.h</strong>文件中。由于LGBM可以处理类别特征，因此对连续特征和类别特征的处理方式是不一样的。</p>
</blockquote>
<h4><span id="连续特征">连续特征:</span></h4><p>在<strong>bin.cpp</strong>中，我们可以看到<strong>GreedyFindBin</strong>函数和<strong>FindBinWithZeroAsOneBin</strong>函数，这两个函数得到了数值型特征取值（负数，0，正数）的各个bin的切分点，即bin_upper_bound。</p>
<h4><span id="greedyfindbin-数值型根据特征不同取值的个数划分类别型">GreedyFindBin: 数值型根据特征不同取值的个数划分，类别型？？</span></h4><ul>
<li><em>特征取值计数的数组</em>、<em>特征的不同的取值的数组</em>、<em>特征有多少个不同的取值</em></li>
<li><strong>bin_upper_bound就是记录桶分界的数组</strong></li>
<li>特征取值数比max_bin数量少，直接取distinct_values的中点放置</li>
<li>特征取值数比max_bin来得大，说明几个特征值要共用一个bin<ul>
<li>如果一个特征值的数目比mean_bin_size大，那么这些特征需要单独一个bin</li>
<li>剩下的特征取值的样本数平均每个剩下的bin：mean size for one bin</li>
</ul>
</li>
</ul>
<h4><span id="构建直方图">构建直方图：</span></h4><p>给定一个特征的值，我们现在已经可以转化为对应的bin了。现在我们就可以构建直方图了。</p>
<h4><span id="constructhistogram"><strong>ConstructHistogram</strong>：</span></h4><ul>
<li><strong>累加了一阶、二阶梯度和还有==个数==</strong></li>
<li>当然还有其它的版本，当is_constant_hessianis_constant_hessian为true的时候是不用二阶梯度的</li>
</ul>
<h4><span id="寻找最优切分点-缺失值处理-gain和xgb一样">寻找最优切分点 : 缺失值处理 + Gain和XGB一样</span></h4><h4><span id="3直方图算法优点"><strong><font color="red"> （3）直方图算法优点：</font></strong></span></h4><ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于 直方图算法，则只需要(1x样本数x维 度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的 bin 值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p>
</li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为$k$的树的时间复杂度：对特征所有取值的排序为$O(NlogN)$，$N$为样本点数目，若有$D$维特征，则$O(kDNlogN)$，而直方图算法需要$O(kD \times bin)$ (bin是histogram 的横轴的数量，一般远小于样本数量$N$)。</p>
</li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>==两个维度==</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的$k$个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
<p><img src="https://pic1.zhimg.com/80/v2-86919e4fc187a11fe3fdb72780709c98_1440w.jpg" alt="img" style="zoom:67%;"></p>
</li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM 所使用直方图算法对 Cache 天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</p>
</li>
<li><p><strong>数据并行优化</strong>，用 histgoram 可以大幅降低通信代价。用 pre-sorted 算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst 在并行的时候也使用 histogram 进行通信。</p>
</li>
</ul>
<h4><span id="4直方图算法缺点">（4）直方图算法缺点：</span></h4><p><strong>当然，直方图算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。</strong>但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；<strong>较粗的分割点也有正则化的效果，可以有效地防止过拟合</strong>；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（GradientBoosting）的框架下没有太大的影响。</p>
<h3><span id="112-单边梯度抽样算法"><strong>1.1.2 单边梯度抽样算法</strong></span></h3><font color="red"> **直方图算法仍有优化的空间**，建立直方图的复杂度为O(**feature × data**)，如果能**降低特征数**或者**降低样本数**，训练的时间会大大减少。</font>

<p><strong>GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法</strong>（Gradient-based One-Side Sampling, GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算锅中只需关注梯度高的样本，极大的减少了计算量。</p>
<ol>
<li>根据<strong>梯度的绝对值</strong>将样本进行<strong>降序</strong>排序</li>
<li>选择前a×100%的样本，这些样本称为A</li>
<li>剩下的数据(1−a)×100的数据中，随机抽取b×100%的数据，这些样本称为B</li>
<li>在计算增益的时候，放大样本B中的梯度 (1−a)/b 倍</li>
<li>关于g，在具体的实现中是一阶梯度和二阶梯度的乘积，见Github的实现（LightGBM/src/boosting/goss.hpp）</li>
</ol>
<blockquote>
<p>  a%（大梯度）+ (1-a)/ b * b % 的大梯度</p>
</blockquote>
<p><strong>使用GOSS进行采样, 使得训练算法更加的关注没有充分训练(under-trained)的样本, 并且只会稍微的改变原有的数据分布</strong>。原有的在特征值为 $\mathrm{d}$ 处分数据带来的增益可以定义为：</p>
<script type="math/tex; mode=display">
V_{j \mid O}(d)=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in O: x_{i j} \leq d} g_{i}\right)^{2}}{n_{l \mid O}^{j}(d)}+\frac{\left(\sum_{x_{i} \in O: x_{i j}>d} g_{i}\right)^{2}}{n_{r \mid O}^{j}(d)}\right)</script><p>其中:</p>
<ul>
<li>O为在决策树待分裂节点的训练集</li>
<li>$n<em>{o}=\sum I\left(x</em>{i} \in O\right)$</li>
<li>$n<em>{l \mid O}^{j}(d)=\sum I\left[x</em>{i} \in O: x<em>{i j} \leq d\right]$ and $n</em>{r \mid O}^{j}(d)=\sum I\left[x<em>{i} \in O: x</em>{i j}&gt;d\right]$</li>
</ul>
<p><strong>而使用GOSS后, 增益定义为：</strong></p>
<script type="math/tex; mode=display">
V_{j \mid O}(d)=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in A_{l}} g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}} g_{i}\right)^{2}}{n_{l}^{j}(d)}+\frac{\left(\sum_{x_{i} \in A_{r}} g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}} g_{r}\right)^{2}}{n_{r}^{j}(d)}\right)</script><p>其中:</p>
<ul>
<li>$A<em>{l}=\left{x</em>{i} \in A: x<em>{i j} \leq d\right}, A</em>{r}=\left{x<em>{i} \in A: x</em>{i j}&gt;d\right}$</li>
<li>$B<em>{l}=\left{x</em>{i} \in B: x<em>{i j} \leq d\right}, B</em>{r}=\left{x<em>{i} \in B: x</em>{i j}&gt;d\right}$</li>
</ul>
<p>实验表明，该做法并没有降低模型性能，反而还有一定提升。究其原因，应该是采样也会增加弱学习器的多样性，从而潜在地提升了模型的泛化能力，稍微有点像深度学习的dropout。</p>
<h3><span id="113-互斥特征捆绑算法冲突小的特征可能与多个特征包组合特征集合"><strong>1.1.3 互斥特征捆绑算法</strong>【冲突小的特征可能与多个特征包组合】[特征集合]</span></h3><blockquote>
<p>  <strong>==互斥==指的是一些特征很少同时出现非0值</strong>【<strong>类似one-hot特征</strong>】</p>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/366234433">详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）</a></p>
</blockquote>
<p><strong><font color="red"> 互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行合并，则可以降低特征数量。</font></strong>高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。</p>
<p><strong>1）首先介绍如何判定哪些特征应该捆绑在一起？</strong></p>
<p>EFB算法采用<strong>构图（build graph）</strong>的思想，将特征作为节点，不互斥的特征之间进行连边，然后从图中找出所有的捆绑特征集合。其实学过数据结构里的图算法就了解过，这个问题基本就是<a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%9B%BE%E7%9D%80%E8%89%B2%E9%97%AE%E9%A2%98/8928655%3Ffr%3Daladdin">图着色问题</a>。但是图着色问题是一个<strong>NP-hard问题</strong>，不可能在多项式时间里找到最优解。</p>
<p>因此<strong>EFB采用了一种近似的贪心策略解决办法。它允许特征之间存在少数的样本点并不互斥</strong>（比如某些对应的样本点之间并不同时为非0），并设置一个最大冲突阈值 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 。我们选择合适的 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 值，可以在准确度和训练效率上获得很好的trade-off（均衡)。</p>
<p>==<strong>下面给出EFB的特征捆绑的贪心策略流程：</strong>==</p>
<blockquote>
<p>  （1）将特征作为图的顶点，对于<strong>不互斥的特征进行相连</strong>（存在同时不为0的样本），特征同时不为0的样本个数作为边的权重；<br>  （2）根据顶点的度对特征进行降序排序，度越大表明特征与其他特征的冲突越大（越不太可能与其他特征进行捆绑）；【<strong>入度排序，转化为非零值个数排序</strong>】<br>  （3）设置<strong>最大冲突阈值K</strong>，外层循环先对每一个上述排序好的特征，遍历已有的特征捆绑簇，如果发现该特征加入到该特征簇中的冲突数不会超过最大阈值K，则将该特征加入到该簇中。否则新建一个特征簇，将该特征加入到新建的簇中。</p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-743681d9fd6cebee11f0dcc607f2f687_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>上面时间的复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28n%5E2%29" alt="[公式]"> ，n为特征的数量，时间其实主要花费在建图上面，两两特征计算互斥程度的时间较长（2层for循环）。对于百万级别的特征数量来说，该复杂度仍是<strong>不可行的</strong>。==为了提高效率，可以不再构建图，将特征直接按照非零值个数排序，将特征<strong>非零值个数</strong>类比为节点的度（即冲突程度)，因为更多的非零值更容易引起冲突。只是改进了排序策略，不再构建图，下面的for循环是一样的。==</p>
<p><strong>2）如何将特征捆绑簇里面的所有特征捆绑（合并）为一个特征？</strong>【<strong>直方图偏移</strong>】</p>
<p>如何进行合并，最关键的是如何能将原始特征从合并好的特征进行分离出来。EFB采用的是加入一个<strong>偏移常量</strong>（offset）来解决。</p>
<blockquote>
<p>  举个例子，我们绑定两个特征A和B，A取值范围为[0, 10)，B取值范围为[0, 20)。则我们可以加入一个偏移常量10，即将B的取值范围变为[10,30），然后合并后的特征范围就是[0, 30)，并且能很好的分离出原始特征~</p>
</blockquote>
<p>因为lgb中<strong>直方图算法</strong>对特征值进行了<strong>分桶</strong>（bin）操作，导致合并互斥特征变得更为简单。从上面伪码看到偏移常量offset直接对每个特征桶的数量累加就行，然后放入偏移常数数组（binRanges）中。</p>
<h3><span id="114-带深度限制的-leaf-wise-算法"><strong>1.1.4 带深度限制的 Leaf-wise 算法</strong></span></h3><h4><span id="level-wise">Level-wise</span></h4><p>大多数GBDT框架使用的按层生长 (level-wise) 的决策树生长策略，Level-wise遍历一次数据可以同时分裂同一层的叶子，容易进行<strong>多线程优化</strong>，也好<strong>控制模型复杂度，不容易过拟合</strong>。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
<h4><span id="leaf-wise">Leaf-wise</span></h4><p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<p><img src="https://pic2.zhimg.com/80/v2-76f2f27dd24fc452a9a65003e5cdd305_1440w.jpg" alt="img"></p>
<h3><span id="115-lightgbm类别特征最优分割">==<strong>1.1.5 LightGBM类别特征最优分割</strong>==</span></h3><blockquote>
<p>  LightGBM中只需要提前将类别映射到非负整数即可(<code>integer-encoded categorical features</code>)</p>
</blockquote>
<p><strong>我们知道，LightGBM可以直接处理类别特征，而不需要对类别特征做额外的one-hot encoding。那么LGB是如何实现的呢？</strong></p>
<p>类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足, LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。<strong>LightGBM 采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分</strong>。假设某维 特征有 k 个类别，则有 <img src="https://www.zhihu.com/equation?tex=2%5E%7B%28k-1%29%7D-1" alt="[公式]"> 种可能, 时间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%5Cleft%282%5E%7Bk%7D%5Cright%29%2C" alt="[公式]"> LightGBM 基于 Fisher的 《<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=On+Grouping+For+Maximum+Homogeneity&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">On Grouping For Maximum Homogeneity</a>》论文实现了 O(klogk) 的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=时间复杂度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">时间复杂度</a>。</p>
<p><strong>算法流程如下图所示</strong>，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序; 然后按照排序的结果依次枚举最优分割点。从下图可以看到, <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BS+u+m%28y%29%7D%7B%5Coperatorname%7BCount%7D%28y%29%7D" alt="[公式]"> 为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。</p>
<p><img src="https://pic1.zhimg.com/v2-0f1b7024e9da8f09c75b7f8e436a5d24_b.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>在Expo数据集上的实验结果表明，相比0/1展开的方法，使用LightGBM支持的类别特征可以使训练速度加速8倍，并且精度一致。</strong>更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。</p>
<h3><span id="12-工程实现-并行计算">1.2 工程实现 - 并行计算</span></h3><h3><span id="121-特征并行优化-最优划分点"><strong>1.2.1 特征并行</strong>【优化 最优划分点】</span></h3><p>传统的特征并行算法在于对数据进行垂直划分，然后使用<strong>不同机器找到不同特征的最优分裂点</strong>，<strong>基于通信整合得到最佳划分点</strong>，然后基于通信告知其他机器划分结果。==在本小节中，<strong>工作的节点称为worker</strong>==</p>
<h4><span id="传统">==<strong>传统：</strong>==</span></h4><ul>
<li>垂直划分数据<strong>（对特征划分）</strong>，<strong>不同的worker有不同的特征集</strong></li>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li><strong>具有全局最佳切分点的worker进行节点分裂，然后广播切分后的结果</strong>（<strong>左右子树的instance indices</strong>）</li>
<li>其它worker根据收到的instance indices也进行划分</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-b0d10c5cd832402e4503e2c1220f7376_r.jpg" alt="preview" style="zoom: 67%;"></p>
<p><strong>传统的特征并行方法有个很大的缺点</strong>：</p>
<ul>
<li><strong>需要告知每台机器最终划分结果，增加了额外的复杂度</strong>（因为对数据进行垂直划分，每台机器所含数据不同，划分结果需要通过通信告知）；</li>
<li>无法加速split的过程，该过程复杂度为O(#data)O(#data)，当数据量大的时候效率不高；</li>
</ul>
<h4><span id="lightgbm"><strong>==LightGBM==</strong></span></h4><p><strong>LightGBM 则不进行数据垂直划分，每台机器都有训练集完整数据</strong>，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。</p>
<ul>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li>每个worker根据全局最佳切分点进行节点分裂</li>
</ul>
<p>缺点：</p>
<ul>
<li>split过程的复杂度仍是O(#data)，当数据量大的时候效率不高</li>
<li><strong>每个worker保存所有数据，存储代价高</strong></li>
</ul>
<h3><span id="122-数据并行"><strong>1.2.2 数据并行</strong></span></h3><h4><span id="传统方法">传统方法：</span></h4><p>数据并行目标是并行化整个决策学习的过程：</p>
<ul>
<li>水平切分数据，<strong>不同的worker拥有部分数据</strong></li>
<li>每个worker根据本地数据构建局部直方图</li>
<li>合并所有的局部直方图得到全部直方图</li>
<li>根据全局直方图找到最优切分点并进行分裂</li>
</ul>
<p><img src="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-data-parallelization.png" alt="LightGBM-data-parallelization"></p>
<p>在第3步中，有两种合并的方式：</p>
<ul>
<li>采用点对点方式(point-to-point communication algorithm)进行通讯，每个worker通讯量为O(#machine∗#feature∗#bin)</li>
<li>采用collective communication algorithm(如“<a target="_blank" rel="noopener" href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html">All Reduce</a>”)进行通讯（相当于有一个中心节点，通讯后在返回结果），每个worker的通讯量为O(2∗#feature∗#bin)</li>
</ul>
<h4><span id="lightgbm中的数据并行">LightGBM中的数据并行</span></h4><ol>
<li><strong>使用“Reduce Scatter”将不同worker的不同特征的直方图合并，然后workers在局部合并的直方图中找到局部最优划分，最后同步全局最优划分。</strong></li>
<li>前面提到过，可以通过直方图作差法得到兄弟节点的直方图，因此只需要通信一个节点的直方图。</li>
</ol>
<p>传统的数据并行策略主要为水平划分数据，然后本地构建直方图并整合成全局直方图，最后在全局直方图中找出最佳划分点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 <img src="https://www.zhihu.com/equation?tex=O%28%5C%23machine+%2A+%5C%23feature+%2A%5C%23bin+%29" alt="[公式]"> ；如果使用集成的通信，则通讯开销为 <img src="https://www.zhihu.com/equation?tex=O%282+%2A+%5C%23feature+%2A%5C%23bin+%29" alt="[公式]"> ，</p>
<p><strong>LightGBM 采用分散规约（Reduce scatter）的方式将直方图整合的任务分摊到不同机器上，从而降低通信代价，并通过直方图做差进一步降低不同机器间的通信。</strong></p>
<h3><span id="123-投票并行"><strong>1.2.3 投票并行</strong></span></h3><p>LightGBM采用一种称为<strong>PV-Tree</strong>的算法进行投票并行(Voting Parallel)，其实这本质上也是一种<strong>数据并行</strong>。PV-Tree和普通的决策树差不多，只是在寻找最优切分点上有所不同。</p>
<p>其算法伪代码描述如下：</p>
<p><img src="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-pv-tree.png" alt="LightGBM-pv-tree"></p>
<ol>
<li>水平切分数据，不同的worker拥有部分数据。</li>
<li>Local voting: <strong>每个worker构建直方图，找到top-k个最优的本地划分特征</strong></li>
<li>Global voting: <strong>中心节点聚合得到最优的top-2k个全局划分特征（top-2k是看对各个worker选择特征的个数进行计数，取最多的2k个）</strong></li>
<li><strong>Best Attribute Identification</strong>： <strong>中心节点向worker收集这top-2k个特征的直方图，并进行合并，然后计算得到全局的最优划分</strong></li>
<li>中心节点将全局最优划分广播给所有的worker，worker进行本地划分。</li>
</ol>
<p><img src="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-voting-parallelization.png" alt="LightGBM-voting-parallelization"></p>
<p><strong>可以看出，PV-tree将原本需要#feature×#bin#feature×#bin 变为了2k×#bin2k×#bin，通信开销得到降低。此外，可以证明，当每个worker的数据足够多的时候，top-2k个中包含全局最佳切分点的概率非常高。</strong></p>
<h3><span id="124-缓存优化"><strong>1.2.4 缓存优化</strong></span></h3><p>上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。</p>
<p>而 LightGBM 所使用直方图算法对 Cache 天生友好：</p>
<ol>
<li>首先，<strong>所有的特征都采用相同的方法获得梯度</strong>（区别于不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中；</li>
<li>其次，因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3HRFFWP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3HRFFWP/" class="post-title-link" itemprop="url">机器学习（11）XGB*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-11 21:13:30" itemprop="dateCreated datePublished" datetime="2022-03-11T21:13:30+08:00">2022-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-17 13:58:04" itemprop="dateModified" datetime="2023-03-17T13:58:04+08:00">2023-03-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">集成学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>17k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>31 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="参考链接">参考链接</span></h2><ul>
<li><p><strong>XGBoost官方文档</strong>：<a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/index.html">https://xgboost.readthedocs.io/en/latest/index.html</a></p>
</li>
<li><p>LightGBM算法梳理：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/78293497">https://zhuanlan.zhihu.com/p/78293497</a></p>
</li>
<li><p>详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/366234433">https://zhuanlan.zhihu.com/p/366234433</a></p>
</li>
<li><p>【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/87885678">https://zhuanlan.zhihu.com/p/87885678</a></p>
</li>
<li><p>xgboost面试题整理: <a target="_blank" rel="noopener" href="https://xiaomindog.github.io/2021/06/22/xgb-qa/">https://xiaomindog.github.io/2021/06/22/xgb-qa/</a></p>
</li>
</ul>
<h2><span id="机器学习决策树下xgboost-lightgbm">【机器学习】决策树（下）——XGBoost、LightGBM</span></h2><p><img src="https://pic2.zhimg.com/80/v2-358e4bfce928d0460bd5e8b4cab8f715_1440w.jpg" alt="img"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Boosting 算法</th>
<th>GBDT</th>
<th>XGBoost</th>
<th>LightGBM</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong><img src="image-20220315210756470.png" alt="image-20220315210756470" style="zoom:25%;"></td>
<td>回归树、梯度迭代、缩减（Shrinkage）;<strong>GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0</strong></td>
<td><strong>二阶导数、线性分类器、正则化</strong>、缩减、<strong>列抽样、并行化</strong></td>
<td><strong>更快的训练速度和更低的内存使用</strong></td>
</tr>
<tr>
<td>目标函数</td>
<td><img src="image-20220315213233284.png" alt="image-20220315213233284" style="zoom: 25%;"></td>
<td><img src="image-20220315213503054.png" alt="image-20220315213503054" style="zoom: 67%;"><img src="image-20220315213608526.png" alt="image-20220315213608526" style="zoom: 33%;"></td>
<td>同上</td>
</tr>
<tr>
<td>损失函数</td>
<td>最小均方损失函数、<strong>绝对损失或者 Huber 损失函数</strong></td>
<td>【线性】最小均方损失函数、==sigmod和softmax==</td>
<td><strong>复杂度模型</strong>：<img src="image-20220315215849417.png" alt="image-20220315215849417" style="zoom: 25%;"></td>
</tr>
<tr>
<td>基模型</td>
<td>CART模型</td>
<td>CART模型/ ==回归模型==</td>
<td>CART模型/ ==回归模型==</td>
</tr>
<tr>
<td>抽样算法</td>
<td>无</td>
<td><strong>列抽样</strong>：借鉴了<strong>随机森林</strong>的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</td>
<td><strong>单边梯度抽样算法；</strong>根据样本梯度来对梯度小的这边样本进行采样，一部分大梯度和随机分布</td>
</tr>
<tr>
<td><strong>切分点算法</strong></td>
<td>CART模型</td>
<td><strong>预排序</strong>、<strong>贪心算法</strong>、<strong>近似算法（</strong>加权分位数缩略图<strong>）</strong></td>
<td><strong>直方图算法</strong>：内存消耗降低，计算代价减少；（不需要记录特征到样本的索引）</td>
</tr>
<tr>
<td><strong>缺失值算法</strong></td>
<td>CART模型</td>
<td><strong>稀疏感知算法</strong>：选择增益最大的枚举项即为最优<strong>缺省方向</strong>。【<strong><font color="red"> 稀疏数据优化不足</font></strong>】【<strong>gblinear 补0</strong>】</td>
<td><strong>互斥特征捆绑算法</strong>：<strong>互斥</strong>指的是一些特征很少同时出现非0值。<strong>稀疏感知算法</strong>；【<strong>gblinear 补0</strong>】</td>
</tr>
<tr>
<td><strong>建树策略</strong></td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Leaf-wise</strong>：每次分裂增益最大的叶子节点，直到达到停止条件。</td>
</tr>
<tr>
<td><strong>正则化</strong></td>
<td>无</td>
<td>L1 和 L2 正则化项</td>
<td>L1 和 L2 正则化项</td>
</tr>
<tr>
<td><strong>Shrinkage（缩减）</strong></td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>类别特征优化</td>
<td>无</td>
<td>无</td>
<td><strong>类别特征最优分割</strong>：<strong>many-vs-many</strong></td>
</tr>
<tr>
<td>并行化设计</td>
<td>无</td>
<td><strong>块结构设计</strong>、</td>
<td><strong>特征并行</strong>、 <strong>数据并行</strong>、<strong>投票并行</strong></td>
</tr>
<tr>
<td>==缓存优化==</td>
<td>无</td>
<td>为每个线程分配一个连续的缓存区、<strong>“核外”块计算</strong></td>
<td>1、所有的特征都采用相同的方法获得梯度；2、其次，因为不需要存储特征到样本的索引，降低了存储消耗</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>对异常点敏感；</td>
<td><strong>预排序</strong>：仍需要遍历数据集；==不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。==</td>
<td><strong>内存更小</strong>： 索引值、特征值边bin、互斥特征捆绑; <strong>速度更快</strong>：遍历直方图；单边梯度算法过滤掉梯度小的样本；基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；特征并行、数据并行方法加速计算</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="一-xgboost-线性模型多分类增量训练xgb_直方图分布式部署">一、XGBoost [线性模型？多分类？增量训练？XGB_直方图，分布式部署]</span></h2><blockquote>
<p>  <strong>增量学习</strong>：XGBoost提供两种增量训练的方式，一种是在当前迭代树的基础上增加新树，原树不变；另一种是当前迭代树结构不变，重新计算叶节点权重，同时也可增加新树。</p>
<p>  <strong>线性模型</strong>：xgboost通过泰勒公式的二阶展开迭代的残差是1导/2导，线性回归迭代的是标签，xgboost需要串行多个线性回归，预测结果为多个象形线性回归的累积值……，除了用到了线性回归的原理方程式，他们两的损失函数，下降梯度都不一样，几乎没有什么共同点</p>
<p>  <strong>XGBoost 用泰勒展开优势在哪？</strong>：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/61374305">https://www.zhihu.com/question/61374305</a></p>
<ul>
<li><strong>xgboost是以mse为基础推导出来的</strong>，在mse的情况下，xgboost的目标函数展开就是一阶项+二阶项的形式，而其他类似logloss这样的目标函数不能表示成这种形式。为了后续推导的统一，所以将<strong>目标函数进行二阶泰勒展开，就可以直接自定义损失函数了，只要二阶可导即可，增强了模型的扩展性</strong>。</li>
<li><p><strong>二阶信息能够让梯度收敛的更快，类似牛顿法比SGD收敛更快</strong>。一阶信息描述梯度变化方向，二阶信息可以描述梯度变化方向是如何变化的。</p>
<p><strong>==深入理解XGBoost==</strong>：<a target="_blank" rel="noopener" href="https://bailingnan.github.io/post/shen-ru-li-jie-xgboost/">https://bailingnan.github.io/post/shen-ru-li-jie-xgboost/</a></p>
</li>
</ul>
</blockquote>
<p>XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包，比常见的工具包快 10 倍以上。Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是<strong>目标函数</strong>的定义。故本文将从数学原理和工程实现上进行介绍，并在最后介绍下 Xgboost 的优点。</p>
<h3><span id="11-数学原理">1.1 数学原理</span></h3><p><strong>1.1.1 目标函数</strong></p>
<p>我们知道 XGBoost 是由$k$个基模型组成的一个加法运算式：</p>
<script type="math/tex; mode=display">
\hat{y}_{i}=\sum_{t=1}^{k} f_{t}\left(x_{i}\right)</script><p><strong>损失函数：</strong></p>
<script type="math/tex; mode=display">
L=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)</script><p>我们知道模型的预测精度由模型的<strong>偏差</strong>和<strong>方差</strong>共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的<strong>损失函数$L$</strong>与抑<strong>制模型复杂度的正则项 <script type="math/tex">\Omega</script></strong>组成。支持<strong>决策树</strong>也支持<strong>线性模型</strong>。</p>
<script type="math/tex; mode=display">
O b j=\sum_{i=1}^{n} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{t=1}^{k} \Omega\left(f_{t}\right)</script><p><strong>Boosting模型是向前加法：</strong></p>
<script type="math/tex; mode=display">
\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)</script><p>目标函数就可以写成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
O b j^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\
&=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right)
\end{aligned}</script><p>求此时最优化目标函数，就相当于求解 <script type="math/tex">f_{t}\left(x_{i}\right)</script>。根据泰勒展开式：</p>
<script type="math/tex; mode=display">
f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}</script><p><strong>我们==把<script type="math/tex">\hat{y}_{i}^{t-1}</script>,视为x， <script type="math/tex">f_{t}\left(x_{i}\right)</script>视为<script type="math/tex">\Delta x</script>==，故可以将目标函数写成</strong>：</p>
<script type="math/tex; mode=display">
O b j^{(t)}=\sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{t-1}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t} \Omega\left(f_{i}\right)</script><p>由于<strong>第一项为常数，对优化没有影响，所以我们只需要求出每一步损失函数的一阶导和二阶导的值</strong>【==前t-1的结果和标签求==】，然后最优化目标函数，就可以得到每一步的f(x),最后根据加法模型得到一个整体模型。</p>
<script type="math/tex; mode=display">
O b j^{(t)} \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t} \Omega\left(f_{i}\right)</script><blockquote>
<p>  以<strong>平方损失函数</strong>【绝对值、hubor损失】为例（GBDT 残差）：</p>
<p>  <img src="image-20220404141858337.png" alt="image-20220404141858337" style="zoom:50%;"></p>
<p>  其中 <img src="https://www.zhihu.com/equation?tex=g_%7Bi%7D" alt="[公式]"> 为损失函数的一阶导， <img src="https://www.zhihu.com/equation?tex=h_%7Bi%7D" alt="[公式]"> 为损失函数的二阶导，<strong>注意这里的导是对 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i%5E%7Bt-1%7D" alt="[公式]"> 求导</strong>。</p>
<script type="math/tex; mode=display">
  \begin{aligned}
  &g_{i}=\frac{\partial\left(\hat{y}^{t-1}-y_{i}\right)^{2}}{\partial \hat{y}^{t-1}}=2\left(\hat{y}^{t-1}-y_{i}\right) \\
  &h_{i}=\frac{\partial^{2}\left(\hat{y}^{t-1}-y_{i}\right)^{2}}{\hat{y}^{t-1}}=2
  \end{aligned}</script></blockquote>
<h4><span id="112-基于决策树的目标函数"><strong>1.1.2 基于决策树的目标函数</strong></span></h4><p>我们知道 Xgboost 的基模型<strong>不仅支持决策树，还支持线性模型</strong>，这里我们主要介绍基于决策树的目标函数。</p>
<p>我们可以将决<strong>策树定义为<script type="math/tex">f_{t}(x)=w_{q(x)}</script></strong>，x为某一样本，这里的 <img src="https://www.zhihu.com/equation?tex=q%28x%29" alt="[公式]"> 代表了该样本在哪个叶子结点上，而 <img src="https://www.zhihu.com/equation?tex=w_q" alt="[公式]"> 则代表了叶子结点取值 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> ，所以 <img src="https://www.zhihu.com/equation?tex=w_%7Bq%28x%29%7D" alt="[公式]"> 就代表了每个样本的取值 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> （即预测值)。</p>
<p><strong>决策树的复杂度</strong>可由<strong>叶子数 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"></strong> 组成，叶子节点越少模型越简单，此外<strong>叶子节点也不应该含有过高的权重</strong> <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> （类比 LR 的每个变量的权重)，所以目标函数的正则项可以定义为：</p>
<script type="math/tex; mode=display">
\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}</script><p>即<strong>决策树模型的复杂度</strong>由生成的所有<strong>决策树的叶子节点数量</strong>，和所有<strong>节点权重所组成的向量的 <img src="https://www.zhihu.com/equation?tex=L_2" alt="[公式]"> 范式</strong>共同决定。</p>
<p><img src="https://pic1.zhimg.com/80/v2-e0ab9287990a6098e4cdbc5a8cff4150_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>我们设 <img src="https://www.zhihu.com/equation?tex=I_j%3D+%5C%7B+i+%5Cvert+q%28x_i%29%3Dj+%5C%7D" alt="[公式]"> 为第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 个叶子节点的样本集合，故我们的目标函数可以写成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
O b j^{(t)} & \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\
&=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\
&=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T
\end{aligned}</script><p>第二步是遍历所有的样本后求每个样本的损失函数，但样本最终会落在叶子节点上，所以我们也可以遍历叶子节点，然后获取叶子节点上的样本集合，最后在求损失函数。即我们之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi+%5Cin+I_j%7Dg_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi+%5Cin+I_j%7Dh_i" alt="[公式]"> 这两项， <img src="https://www.zhihu.com/equation?tex=w_j" alt="[公式]"> 为第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 个叶子节点取值。</p>
<p><img src="image-20220314162051124.png" alt="image-20220314162051124" style="zoom: 25%;"></p>
<p><strong><font color="red"> 这里我们要注意 <img src="https://www.zhihu.com/equation?tex=G_j" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=H_j" alt="[公式]"> 是前 <img src="https://www.zhihu.com/equation?tex=t-1" alt="[公式]"> 步得到的结果，其值已知可视为常数，只有最后一棵树的叶子节点 <img src="https://www.zhihu.com/equation?tex=w_j" alt="[公式]"> 不确定，那么将目标函数对 <img src="https://www.zhihu.com/equation?tex=w_j" alt="[公式]"> 求一阶导，并令其等于 <img src="https://www.zhihu.com/equation?tex=0" alt="[公式]"> ，则可以求得叶子结点 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 对应的权值：</font></strong></p>
<p><img src="https://www.zhihu.com/equation?tex=w_j%5E%2A%3D-%5Cfrac%7BG_j%7D%7BH_j%2B%5Clambda%7D++%5C%5C" alt="[公式]"></p>
<p>所以<strong>目标函数可以化简为：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=Obj+%3D+-%5Cfrac12+%5Csum_%7Bj%3D1%7D%5ET+%5Cfrac%7BG_j%5E2%7D%7BH_j%2B%5Clambda%7D+%2B+%5Cgamma+T+%5C%5C" alt="[公式]"></p>
<p><img src="https://pic2.zhimg.com/80/v2-f6db7af6c1e683192cb0ccf48eafaf99_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>上图给出目标函数计算的例子，求每个节点每个样本的一阶导数 <img src="https://www.zhihu.com/equation?tex=g_i" alt="[公式]"> 和二阶导数 <img src="https://www.zhihu.com/equation?tex=h_i" alt="[公式]"> ，然后针对每个节点对所含样本求和得到的 <img src="https://www.zhihu.com/equation?tex=G_j" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=H_j" alt="[公式]"> ，最后遍历决策树的节点即可得到<strong>目标函数</strong>。</p>
<h4><span id="113-最优切分点划分算法"><strong>1.1.3 最优切分点划分算法</strong></span></h4><p><strong><font color="red"> 在决策树的生长过程中，一个非常关键的问题是如何找到叶子的节点的最优切分点，</font></strong>Xgboost 支持两种分裂节点的方法——<strong>贪心算法</strong>和<strong>近似算法</strong>。</p>
<p><strong>1）贪心算法</strong></p>
<ol>
<li><strong>从深度为 <img src="https://www.zhihu.com/equation?tex=0" alt="[公式]"> 的树开始，对每个叶节点枚举所有的可用特征</strong>；</li>
<li>针对每个特征，把属于该节点的训练样本根据该特征值进行<strong>升序排列</strong>，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；</li>
<li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点<strong>关联对应的样本集</strong>？？</li>
<li>回到第 1 步，递归执行到满足特定条件为止。（树的深度、gamma）</li>
</ol>
<h5><span id="那么如何计算每个特征的分裂收益呢">那么如何计算每个特征的分裂收益呢？</span></h5><p>假设我们在某一节点完成特征分裂，则分列前的目标函数可以写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Obj_%7B1%7D+%3D-%5Cfrac12+%5B%5Cfrac%7B%28G_L%2BG_R%29%5E2%7D%7BH_L%2BH_R%2B%5Clambda%7D%5D+%2B+%5Cgamma++%5C%5C" alt="[公式]"></p>
<p>分裂后的目标函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Obj_2+%3D++-%5Cfrac12+%5B+%5Cfrac%7BG_L%5E2%7D%7BH_L%2B%5Clambda%7D+%2B+%5Cfrac%7BG_R%5E2%7D%7BH_R%2B%5Clambda%7D%5D+%2B2%5Cgamma+%5C%5C" alt="[公式]"></p>
<p>则对于目标函数来说，分裂后的收益为：<strong>MAX</strong>【<strong>obj1 - obj2 （分裂后越小越好）</strong>】</p>
<p><img src="https://www.zhihu.com/equation?tex=Gain%3D%5Cfrac12+%5Cleft%5B+%5Cfrac%7BG_L%5E2%7D%7BH_L%2B%5Clambda%7D+%2B+%5Cfrac%7BG_R%5E2%7D%7BH_R%2B%5Clambda%7D+-+%5Cfrac%7B%28G_L%2BG_R%29%5E2%7D%7BH_L%2BH_R%2B%5Clambda%7D%5Cright%5D+-+%5Cgamma+%5C%5C" alt="[公式]"></p>
<p>注意<strong>该特征收益也可作为特征重要性输出的重要依据</strong>。</p>
<p>我们可以发现对于所有的分裂点 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> ，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 <img src="https://www.zhihu.com/equation?tex=G_L" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=G_R" alt="[公式]"> 。然后用上面的公式计算每个分割方案的分数就可以了。<font color="red">观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入<strong>新叶子的惩罚项（gamma)</strong>，也就是说引入的分割带来的<strong>增益如果小于一个阀值</strong>的时候，我们可以剪掉这个分割。 </font></p>
<p><strong>2）近似算法</strong>【<strong>加权分位划分点</strong>】</p>
<p><strong>贪婪算法可以的到最优解，但当数据量太大时则无法读入内存进行计算</strong>，近似算法主要针对贪婪算法这一缺点给出了近似最优解。</p>
<p>对于每个特征，只考察分位点可以减少计算复杂度。该算法会首先根据<strong>特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中</strong>，然后聚合统计信息找到所有区间的最佳分裂点。在提出候选切分点时有两种策略：</p>
<ul>
<li><strong>Global</strong>：<strong>学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割</strong>；</li>
<li><strong>Local</strong>：每次分裂前将重新提出候选切分点。</li>
</ul>
<p><strong>下图给出近似算法的具体例子，以三分位为例：</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-5d1dd1673419599094bf44dd4b533ba9_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的 <img src="https://www.zhihu.com/equation?tex=G%2CH" alt="[公式]"> 值，最终求解节点划分的增益。</p>
<h4><span id="114-加权分位数缩略图xgboost-直方图算法"><strong>1.1.4 加权分位数缩略图</strong>[XGBoost 直方图算法]</span></h4><ul>
<li><strong>第一个 for 循环：</strong>对特征 k <strong>根据该特征分布的分位数找到切割点的候选集合【==直方图==】</strong> <img src="https://www.zhihu.com/equation?tex=S_k%3D%5C%7Bs_%7Bk1%7D%2Cs_%7Bk2%7D%2C...%2Cs_%7Bkl%7D+%5C%7D" alt="[公式]"> 。XGBoost 支持 Global 策略和 Local 策略。</li>
<li><strong>第二个 for 循环：</strong>针对每个特征的候选集合，将样本映射到由该特征对应的候选点集构成的分桶区间中，即 <img src="https://www.zhihu.com/equation?tex=%7Bs_%7Bk%2Cv%7D%E2%89%A5x_%7Bjk%7D%3Es_%7Bk%2Cv%E2%88%921%7D%7D" alt="[公式]"> ，对每个桶统计 <img src="https://www.zhihu.com/equation?tex=G%2CH+" alt="[公式]"> 值，最后在这些统计量上寻找最佳分裂点。</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-161382c979557b8bae1563a459cd1ed4_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>事实上， <strong>XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值 <img src="https://www.zhihu.com/equation?tex=h_i+" alt="[公式]"> 作为样本的权重进行划分</strong>，如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-5f16246289eaa2a3ae72f971db198457_1440w.jpg" alt="img"></p>
<h5><span id="那么问题来了为什么要用-进行样本加权">==那么问题来了：为什么要用 <img src="https://www.zhihu.com/equation?tex=h_i" alt="[公式]"> 进行样本加权？==</span></h5><p>我们知道模型的目标函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=+Obj%5E%7B%28t%29%7D+%5Capprox+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%5B+g_if_t%28x_i%29+%2B+%5Cfrac12h_if_t%5E2%28x_i%29+%5Cright%5D+%2B+%5Csum_%7Bi%3D1%7D%5Et++%5COmega%28f_i%29+%5C%5C" alt="[公式]"></p>
<p>我们稍作整理，便可以看出 <img src="https://www.zhihu.com/equation?tex=h_i" alt="[公式]"> 有对 loss 加权的作用。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++Obj%5E%7B%28t%29%7D+%26+%5Capprox+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%5B+g_if_t%28x_i%29+%2B+%5Cfrac12h_if_t%5E2%28x_i%29+%5Cright%5D+%2B+%5Csum_%7Bi%3D1%7D%5Et++%5COmega%28f_i%29+%5C%5C+%5C%5C++++%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5B+g_i+f_t%28x_i%29+%2B+%5Cfrac%7B1%7D%7B2%7Dh_i+f_t%5E2%28x_i%29+%5Ccolor%7Bred%7D%7B%2B+%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7Bg_i%5E2%7D%7Bh_i%7D%7D%5D%2B%5COmega%28f_t%29+%5Ccolor%7Bred%7D%7B%2B+C%7D+%5C%5C++++%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Ccolor%7Bred%7D%7B%5Cfrac%7B1%7D%7B2%7Dh_i%7D+%5Cleft%5B+f_t%28x_i%29+-+%5Cleft%28+-%5Cfrac%7Bg_i%7D%7Bh_i%7D+%5Cright%29+%5Cright%5D%5E2+%2B+%5COmega%28f_t%29+%2B+C+%5Cend%7Balign%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7Bg_i%5E2%7D%7Bh_i%7D" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]"> 皆为常数。我们可以看到 <img src="https://www.zhihu.com/equation?tex=h_i" alt="[公式]"> 就是平方损失函数中样本的权重。</p>
<p>对于样本权值相同的数据集来说，找到候选分位点已经有了解决方案（GK 算法），但是当样本权值不一样时，该如何找到候选分位点呢？（作者给出了一个 Weighted Quantile Sketch 算法，这里将不做介绍。）</p>
<h4><span id="xgboost的近似直方图算法也类似于lightgbm这里的直方图算法-为什么慢"><strong><font color="red"> xgboost的近似直方图算法也类似于lightgbm这里的直方图算法? 为什么慢？</font></strong></span></h4><ul>
<li><strong>xgboost在每一层都动态构建直方图</strong>， 因为<strong>xgboost的直方图算法不是针对某个特定的feature</strong>，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而<strong>lightgbm中对每个特征都有一个直方图</strong>，所以构建一次直方图就够了。</li>
<li><strong>lightgbm有一些工程上的cache优化</strong></li>
</ul>
<h4><span id="115-稀疏感知算法缺失值的处理"><strong>1.1.5 稀疏感知算法</strong>【<strong>缺失值的处理</strong>】</span></h4><blockquote>
<ul>
<li><strong>特征值缺失的样本无需遍历只需直接分配到左右节点</strong></li>
<li><strong>如果训练中没有数据缺失，预测时出现了数据缺失，则默认被分类到右节点.</strong>？<ul>
<li>看c++源码是默认向左方向</li>
</ul>
</li>
</ul>
</blockquote>
<p>在决策树的第一篇文章中我们介绍 CART 树在应对数据缺失时的分裂策略【<strong>缺失代理</strong>】，XGBoost 也给出了其解决方案。XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。</p>
<p><strong>XGBoost提出的是在计算分割后的分数时，遇到缺失值，分别将缺失值带入左右两个分割节点，然后取最大值的方向为其默认方向。</strong>至于如何学到缺省值的分支，其实很简单，<strong>分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。</strong></p>
<p>在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而<strong>特征值缺失的样本无需遍历只需直接分配到左右节点</strong>，故算法所需遍历的样本量减少，下图可以看到稀疏感知算法比 basic 算法速度块了超过 50 倍。</p>
<p><img src="https://pic1.zhimg.com/80/v2-e065bea4b424ea2d13b25ed2e7004aa8_1440w.jpg" alt="img" style="zoom:67%;"></p>
<h4><span id="116-缩减与列采样"><strong>1.1.6 缩减与列采样</strong></span></h4><p>除了在目标函数中引入正则项，为了防止过拟合，XGBoost还引入了缩减(shrinkage)和列抽样（column subsampling），通过在每一步的boosting中引入缩减系数，降低每个树和叶子对结果的影响；列采样是借鉴随机森林中的思想，根据反馈，列采样有时甚至比行抽样效果更好，同时，通过列采样能加速计算。</p>
<h3><span id="12-工程实现">1.2 工程实现</span></h3><h4><span id="121-块结构设计"><strong>1.2.1 块结构设计</strong></span></h4><p>我们知道，决策树的学习<strong>最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序</strong>。而 <strong><font color="red"> XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</font></strong></p>
<blockquote>
<p>  预排序 + 块设计【独立】 + 稀疏矩阵存储 </p>
</blockquote>
<ul>
<li><strong>每一个块结构包括一个或多个已经排序好的特征</strong>；</li>
<li><strong>缺失特征值将不进行排序</strong>；</li>
<li>每个特征会存储指向<strong>样本梯度统计值</strong>的索引，方便计算一阶导和二阶导数值；</li>
</ul>
<p>这种块结构存储的特征之间相互独立，方便计算机进行并行计算。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个<strong>特征的增益计算可以同时进行</strong>，这也是 Xgboost 能够实现分布式或者多线程计算的原因。</p>
<h4><span id="122-缓存访问优化算法索引访问梯度统计-gt-缓存空间不连续"><strong>1.2.2 缓存访问优化算法</strong>【索引访问梯度统计 -&gt; 缓存空间不连续】</span></h4><p>块结构的设计可以减少节点分裂时的计算量，但<strong>特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续</strong>，这样会造成缓存命中率低，从而影响到算法的效率。</p>
<p>为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。此外适当调整块大小，也可以有助于缓存优化。</p>
<p>于exact greedy算法中, 使用<strong>缓存预取（cache-aware prefetching）</strong>。具体来说，<strong>对每个线程分配一个连续的buffer</strong>，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化）</p>
<h4><span id="123-核外块计算"><strong>1.2.3 “核外”块计算</strong></span></h4><p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，<strong>XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行</strong>。</p>
<p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p>
<ul>
<li><strong>块压缩：</strong>对 Block 进行按列压缩，并在读取时进行解压；</li>
<li><strong>块拆分：</strong>将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li>
</ul>
<h4><span id="124-xgboost损失函数">==1.2.4 <strong>XGBoost损失函数</strong>==</span></h4><blockquote>
<p>  <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_32103261/article/details/106664227">不平衡处理：xgboost 中scale_pos_weight、给样本设置权重weight、 自定义损失函数 和 简单复制正样本的区别</a></p>
</blockquote>
<p><strong>损失函数</strong>：损失函数描述了预测值和真实标签的差异，通过对损失函数的优化来获得对学习任务的一个近似求解方法。boosting类算法的损失函数的作用： Boosting的框架, 无论是GBDT还是Adaboost, 其在每一轮迭代中, <strong>根本没有理会损失函数具体是什么, 仅仅用到了损失函数的一阶导数通过随机梯度下降来参数更新</strong>。XGBoost是用了牛顿法进行的梯度更新。通过对损失进行分解得到一阶导数和二阶导数并通过牛顿法来迭代更新梯度。</p>
<h5><span id="1自定义损失函数">（1）==<strong>自定义损失函数</strong>==</span></h5><p><strong>XGBOOST是一个非常灵活的模型</strong>，允许使用者根据实际使用场景调整<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=损失函数&amp;spm=1001.2101.3001.7020">损失函数</a>，对于常见的二分类问题一般使用的binary：logistic损失函数，其形式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D-%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cleft%28y%5E%7B%28i%29%7D+%5Clog+h_%7B%5Ctheta%7D%5Cleft%28x%5E%7B%28i%29%7D%5Cright%29%2B%5Cleft%281-y%5E%7B%28i%29%7D%5Cright%29+%5Clog+%5Cleft%281-h_%7B%5Ctheta%7D%5Cleft%28x%5E%7B%28i%29%7D%5Cright%29%5Cright%29%5Cright%29+%EF%BC%883%EF%BC%89%5C%5C" alt="[公式]"></p>
<p>这个损失函数对于正类错分和负类错分给予的惩罚时相同的，但是<strong>对于不平衡数据集，或者某些特殊情况（两类错分代价不一样）的时候这样的损失函数就不再合理了。</strong></p>
<p>基于XGBoost的损失函数的分解求导，可以知道XGBoost的除正则项以外的核心影响因子是损失函数的1阶导和2阶导，所以对于任意的学习任务的损失函数，可以对其求一阶导数和二阶导数带入到XGBoost的自定义损失函数范式里面进行处理。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">custom_obj</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span> dtrain<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># pred 和 dtrain 的顺序不能弄反</span>
    <span class="token comment"># STEP1 获得label</span>
    label <span class="token operator">=</span> dtrain<span class="token punctuation">.</span>get_label<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># STEP2 如果是二分类任务，需要让预测值通过sigmoid函数获得0～1之间的预测值</span>
    <span class="token comment"># 如果是回归任务则下述任务不需要通过sigmoid</span>
    <span class="token comment"># 分类任务sigmoid化</span>
    <span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">1</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
    sigmoid_pred <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>原始预测值<span class="token punctuation">)</span>
    <span class="token comment">#回归任务</span>
    pred <span class="token operator">=</span> 原始预测值
    <span class="token comment"># STEP3 一阶导和二阶导</span>
    grad <span class="token operator">=</span> 一阶导
    hess <span class="token operator">=</span> 二阶导
 
    <span class="token keyword">return</span> grad<span class="token punctuation">,</span> hess<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
<p>非平衡分类学习任务，例如首笔首期30+的风险建模任务，首期30+的逾期率比例相对ever30+的逾期率为1/3左右，<strong>通过修正占比少的正样本权重来对影响正样本对损失函数的贡献度，可以进一步提升模型的效果</strong>.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">weighted_binary_cross_entropy</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span> dtrain<span class="token punctuation">,</span>imbalance_alpha<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># retrieve data from dtrain matrix</span>
    label <span class="token operator">=</span> dtrain<span class="token punctuation">.</span>get_label<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># compute the prediction with sigmoid</span>
    sigmoid_pred <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># gradient</span>
    grad <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>imbalance_alpha <span class="token operator">**</span> label<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>label <span class="token operator">-</span> sigmoid_pred<span class="token punctuation">)</span>
    hess <span class="token operator">=</span> <span class="token punctuation">(</span>imbalance_alpha <span class="token operator">**</span> label<span class="token punctuation">)</span> <span class="token operator">*</span> sigmoid_pred <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> sigmoid_pred<span class="token punctuation">)</span>
 
    <span class="token keyword">return</span> grad<span class="token punctuation">,</span> hess <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h5><span id="2focal-loss">（2）Focal Loss</span></h5><p><strong>Focal Loss for Dense Object Detection 是ICCV2017的Best student paper,文章思路很简单但非常具有开拓性意义，效果也非常令人称赞。</strong></p>
<ul>
<li>大家还可以看知乎的讨论：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/63581984">如何评价 Kaiming 的 Focal Loss for Dense Object Detection？</a></li>
<li>[机器学习] XGBoost 自定义损失函数-FocalLoss：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zwqjoy/article/details/109311133">https://blog.csdn.net/zwqjoy/article/details/109311133</a></li>
</ul>
<p>Focal Loss的引入主要是为了解决难易样本数量不平衡（注意，有区别于正负样本数量不平衡）的问题，实际可以使用的范围非常广泛，为了方便解释，拿目标检测的应用场景来说明</p>
<p><strong>==Focal Loss的主要思想就是改变损失函数.Focal loss是在交叉熵损失函数基础上进行的修改==</strong></p>
<p>单阶段的目标检测器通常会产生高达100k的候选目标，只有极少数是正样本，正负样本数量非常不平衡。我们在计算分类的时候常用的损失——交叉熵。<img src="https://private.codecogs.com/gif.latex?y%7B%7D%27" alt="y{}&#39;">是经过激活函数的输出，所以在0-1之间。可见普通的交叉熵对于正样本而言，输出概率越大损失越小。对于负样本而言，输出概率越小则损失越小。此时的损失函数在大量简单样本的迭代过程中比较缓慢且可能无法优化至最优。</p>
<p>为了解决<strong>正负样本不平衡</strong>的问题，我们通常会在交叉熵损失的前面加上一个参数<strong>平衡因子alpha</strong>，用来平衡正负样本本身的比例不均. 文中alpha取0.25，即正样本要比负样本占比小，这是因为负例易分。</p>
<h3><span id="13-优缺点">1.3 优缺点</span></h3><h4><span id="131-优点"><strong>1.3.1 优点</strong></span></h4><ol>
<li><strong>精度更高：</strong>GBDT 只用到一阶<strong>泰勒展开</strong>，而 XGBoost 对损失函数进行了二阶泰勒展开。<strong>XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数</strong>；</li>
<li><strong>灵活性更强：</strong>GBDT 以 CART 作为<strong>基分类器</strong>，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 <strong>XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）</strong>）。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li>
<li><strong>正则化：</strong>XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</li>
<li><strong>Shrinkage（缩减）：</strong>相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li>
<li><strong>列抽样：</strong>XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</li>
<li><strong>缺失值处理：</strong>XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；</li>
<li><strong>可以并行化操作：</strong>块结构可以很好的支持并行计算。</li>
</ol>
<h4><span id="132-缺点"><strong>1.3.2 缺点</strong></span></h4><ol>
<li>虽然利用<strong>预排序</strong>和<strong>近似算法</strong>可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要<strong>==遍历数据集==</strong>；</li>
<li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要<strong>==存储特征对应样本的梯度统计值的索引==</strong>，相当于消耗了两倍的内存。</li>
</ol>
<h2><span id="二-xgboost常用参数">二、XGBoost常用参数</span></h2><h4><span id="xgboost的参数一共分为三类">XGBoost的参数一共分为三类：</span></h4><p><a target="_blank" rel="noopener" href="https://xgboost.apachecn.org/#/">完整参数请戳官方文档</a></p>
<p>1、<strong>通用参数</strong>：宏观函数控制。</p>
<p>2、<strong>Booster参数</strong>：控制每一步的booster(tree/regression)。booster参数一般可以调控模型的效果和计算代价。我们所说的调参，很这是大程度上都是在调整booster参数。</p>
<p>3、<strong>学习目标参数</strong>：控制训练目标的表现。我们对于问题的划分主要体现在学习目标参数上。比如我们要做分类还是回归，做二分类还是多分类，这都是目标参数所提供的。</p>
<h4><span id="通用参数">通用参数</span></h4><ol>
<li><strong>booster</strong>：我们有两种参数选择，<code>gbtree</code>、<code>dart</code>和<code>gblinear</code>。gbtree、dart是采用树的结构来运行数据，而gblinear是基于线性模型。</li>
<li><strong>silent</strong>：静默模式，为<code>1</code>时模型运行不输出。</li>
<li><strong>nthread</strong>: 使用线程数，一般我们设置成<code>-1</code>,使用所有线程。如果有需要，我们设置成多少就是用多少线程。</li>
</ol>
<h4><span id="booster参数">Booster参数</span></h4><ol>
<li><p><strong>==n_estimator==</strong>: 也作<code>num_boosting_rounds</code>这是生成的<strong>最大树的数目</strong>，也是最大的迭代次数。</p>
</li>
<li><p><strong>==learning_rate==</strong>: 有时也叫作<code>eta</code>，系统默认值为<code>0.3</code>,。<strong>每一步迭代的步长</strong>，很重要。太大了运行准确率不高，太小了运行速度慢。我们一般使用比默认值小一点，<code>0.1</code>左右就很好。</p>
</li>
<li><p><strong>==gamma==</strong>：系统默认为<code>0</code>,我们也常用<code>0</code>。在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。<code>gamma</code>指定了节点分裂所需的<strong>最小损失函数下降值</strong>。 这个参数的值越大，算法越保守。因为<code>gamma</code>值越大的时候，损失函数下降更多才可以分裂节点。所以树生成的时候更不容易分裂节点。范围: <code>[0,∞]</code></p>
</li>
<li><p><strong>==subsample==</strong>：系统默认为<code>1</code>。这个参数控制对于每棵树，<strong>随机采样的比例</strong>。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：<code>0.5-1</code>，<code>0.5</code>代表平均采样，防止过拟合. 范围: <code>(0,1]</code>，<strong>注意不可取0</strong></p>
</li>
<li><p><strong>colsample_bytree</strong>：系统默认值为1。我们一般设置成0.8左右。用来控制每棵<strong>随机采样的列数的占比</strong>(每一列是一个特征)。 典型值：<code>0.5-1</code>范围: <code>(0,1]</code></p>
</li>
<li><p><strong>colsample_bylevel</strong>：默认为1,我们也设置为1.这个就相比于前一个更加细致了，它指的是每棵树每次节点分裂的时候列采样的比例</p>
</li>
<li><p><strong>max_depth</strong>： 系统默认值为<code>6</code>，我们常用<code>3-10</code>之间的数字。这个值为<strong>树的最大深度</strong>。这个值是用来控制过拟合的。<code>max_depth</code>越大，模型学习的更加具体。设置为<code>0</code>代表没有限制，范围: <code>[0,∞]</code></p>
</li>
<li><p><strong>==max_delta_step==</strong>：默认<code>0</code>,我们常用<code>0</code>.这个参数限制了<strong>每棵树权重改变的最大步长</strong>，如果这个参数的值为<code>0</code>,则意味着没有约束。如果他被赋予了某一个正值，则是这个算法更加保守。通常，这个参数我们不需要设置，但是<strong>==当个类别的样本极不平衡的时候，这个参数对逻辑回归优化器是很有帮助的。==</strong></p>
</li>
<li><p><strong>==lambda==</strong>:也称<code>reg_lambda</code>,默认值为<code>0</code>。<strong>权重的L2正则化项</strong>。(和Ridge regression类似)。这个参数是用来控制XGBoost的正则化部分的。这个参数在减少过拟合上很有帮助。</p>
</li>
<li><p><strong>alpha</strong>:也称<code>reg_alpha</code>默认为<code>0</code>,权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。</p>
</li>
<li><p><strong>==scale_pos_weight==</strong>：默认为<code>1</code>在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。通常可以将其设置为<strong>负样本的数目与正样本数目的比值</strong>。<strong>xgboost中==scale_pos_weight、对样本进行weight设置和简单复制正样本==得到的结果是一样的，本质上都是改变了训练的损失函数。通过自定义设置损失函数可得到验证。实际上基本思想都是通过过采样的方法处理不平衡数据。</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token punctuation">(</span>label <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">.</span>0f<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    w <span class="token operator">*=</span> scale_pos_weight<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>
<span class="token comment"># 见源码 src/objective/regression_obj.cu</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>  在DMatrix里边设置每个样本的weight 是 怎样改变训练过程的呢，其实是改变训练的损失函数，源代码里的代码如下，可以看到对不同的样本赋予不同的权重实际上是影响了该样本在训练过程中贡献的损失，进而改变了一阶导和二阶导。</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">_out_gpair<span class="token punctuation">[</span>_idx<span class="token punctuation">]</span> <span class="token operator">=</span> GradientPair<span class="token punctuation">(</span>Loss<span class="token punctuation">:</span><span class="token punctuation">:</span>FirstOrderGradient<span class="token punctuation">(</span>p<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">*</span> w<span class="token punctuation">,</span>
                   Loss<span class="token punctuation">:</span><span class="token punctuation">:</span>SecondOrderGradient<span class="token punctuation">(</span>p<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">*</span> w<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment"># 见源码 src/objective/regression_obj.cu</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</li>
</ol>
<h4><span id="学习目标参数">学习目标参数</span></h4><h5><span id="objective-缺省值reglinear">objective [缺省值=reg:linear]</span></h5><ul>
<li><code>reg:linear</code>– <strong>线性回归</strong></li>
<li><code>reg:logistic</code> – <strong>逻辑回归</strong></li>
<li><code>binary:logistic</code> – 二分类逻辑回归，输出为概率</li>
<li><code>binary:logitraw</code> – 二分类逻辑回归，输出的结果为wTx</li>
<li><code>count:poisson</code> – 计数问题的poisson回归，输出结果为poisson分布。在poisson回归中，max_delta_step的缺省值为0.7 (used to safeguard optimization)</li>
<li><code>multi:softmax</code> – 设置 XGBoost 使用softmax目标函数做多分类，需要设置参数num_class（类别个数）</li>
<li><code>multi:softprob</code> – 如同softmax，但是输出结果为ndata*nclass的向量，其中的值是每个数据分为每个类的概率。</li>
</ul>
<h5><span id="eval_metric-缺省值通过目标函数选择">eval_metric [缺省值=通过目标函数选择]</span></h5><ul>
<li><code>rmse</code>: <strong>均方根误差</strong></li>
<li><code>mae</code>: <strong>平均绝对值误差</strong></li>
<li><code>logloss</code>: negative log-likelihood</li>
<li><code>error</code>: 二分类错误率。其值通过错误分类数目与全部分类数目比值得到。对于预测，预测值大于0.5被认为是正类，其它归为负类。 error@t: 不同的划分阈值可以通过 ‘t’进行设置</li>
<li><code>merror</code>: 多分类错误率，计算公式为(wrong cases)/(all cases)</li>
<li><code>mlogloss</code>: ==多分类log损失==</li>
<li><code>auc</code>: 曲线下的面积</li>
<li><code>ndcg</code>: Normalized Discounted Cumulative Gain</li>
<li><code>map</code>: 平均正确率</li>
</ul>
<p>一般来说，我们都会使用<code>xgboost.train(params, dtrain)</code>函数来训练我们的模型。这里的<code>params</code>指的是<code>booster</code>参数。</p>
<h1><span id="xgboostqampa">XGBoostQ&amp;A</span></h1><ul>
<li>推荐收藏 | 又有10道XGBoost面试题送给你：<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1518305">https://cloud.tencent.com/developer/article/1518305</a></li>
</ul>
<h3><span id="1-xgboost模型如果过拟合了怎么解决"><strong>1、XGBoost模型如果过拟合了怎么解决?</strong></span></h3><ul>
<li><strong>正则项</strong>：叶子结点的数目和叶子结点权重的L2模的平方</li>
<li><strong>列抽样</strong>：训练的时候只用一部分特征，不仅可以降低过拟合，还可以加速</li>
<li><strong>子采样</strong>：每轮计算可以不使用全部样本</li>
<li><strong>shrinkage</strong>: 步长(学习率)，消弱训练出的每棵树的影响，让后面的训练有更大的学习空间</li>
</ul>
<p>当出现过拟合时，有两类参数可以缓解：</p>
<p>第一类参数：用于<strong>直接控制模型的复杂度</strong>。包括<code>max_depth,min_child_weight,gamma</code> 等参数</p>
<p>第二类参数：用于<strong>增加随机性</strong>，从而使得模型在训练时对于噪音不敏感。包括<code>subsample,colsample_bytree</code></p>
<p>还有就是直接减小<code>learning rate</code>，但需要同时增加<code>estimator</code> 参数。</p>
<h3><span id="2-怎么理解决策树-xgboost能处理缺失值而有的模型svm对缺失值比较敏感呢">2、怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感呢?</span></h3><blockquote>
<p>   微调的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/58230411/answer/242037063">https://www.zhihu.com/question/58230411/answer/242037063</a></p>
<p>  XGBoost是一种<strong>boosting</strong>的集成学习模型：支持的弱学习器（即单个的学习器，也称基学习器）有<strong>树模型</strong>和<strong>线性模型</strong>（<strong>gblinear</strong>），默认为<strong>gbtree</strong>。</p>
<ul>
<li><p><strong>gblinear</strong>，<strong>由于线性模型不支持缺失值，会将缺失值填充为0</strong>；</p>
</li>
<li><p><strong>gbtree</strong>或者<strong>dart</strong>，则支持缺失值；</p>
</li>
</ul>
</blockquote>
<ul>
<li>工具包自动处理数据缺失<strong>不代表</strong>具体的算法可以<strong>处理缺失项</strong></li>
<li>对于有缺失的数据：以<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=决策树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A242037063}">决策树</a>为原型的模型<strong>优于</strong>依赖距离度量的模型</li>
</ul>
<h3><span id="3-histogram-vs-pre-sorted">3、 Histogram VS Pre-sorted</span></h3><h4><span id="pre-sorted">Pre-sorted</span></h4><p><strong>预排序还是有一定优点的，如果不用预排序的话，在分裂节点的时候，选中某一个特征后，需要对A按特征值大小进行排序，然后计算每个阈值的增益，这个过程需要花费很多时间</strong>。</p>
<p>预排序算法在计算最优分裂时，各个特征的增益可以并行计算，并且能精确地找到分割点。但是<strong>预排序后需要保存特征值及排序后的索引，因此需要消耗两倍于训练数据的内存，时间消耗大</strong>。另外预排序后，<strong>特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化，时间消耗也大</strong>。最后，在每一层，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样。</p>
<h4><span id="historgram">Historgram</span></h4><p>首先需要指出的是，XGBoost在寻找树的分裂节点的也是支持直方图算法的，就是论文中提到的近视搜索算法（Approximate Algorithm）。<strong>只是，无论特征值是否为0，直方图算法都需要对特征的分箱值进行索引，因此对于大部分实际应用场景当中的稀疏数据优化不足。</strong></p>
<p>回过头来，为了能够发挥直方图算法的优化威力，LightGBM提出了另外两个新技术：<strong>单边梯度采样（Gradient-based One-Side Sampling</strong>）和<strong>互斥特征合并（Exclusive Feature Bundling）</strong>，<strong><font color="red"> 在减少维度和下采样上面做了优化以后才能够将直方图算法发挥得淋漓尽致。</font></strong></p>
<h3><span id="4-xgboost中的树如何剪枝">4、<strong>Xgboost中的树如何剪枝？</strong></span></h3><p><strong>在loss中增加了正则项</strong>：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度在每次分裂时，如果分裂后增益小于设置的阈值，则不分裂，则对应于Gain需要大于0才会分裂。(预剪枝)</p>
<p>则对于目标函数来说，分裂后的收益为：<strong>MAX</strong>【<strong>obj1 - obj2 （分裂后越小越好）</strong>】</p>
<p><img src="https://www.zhihu.com/equation?tex=Gain%3D%5Cfrac12+%5Cleft%5B+%5Cfrac%7BG_L%5E2%7D%7BH_L%2B%5Clambda%7D+%2B+%5Cfrac%7BG_R%5E2%7D%7BH_R%2B%5Clambda%7D+-+%5Cfrac%7B%28G_L%2BG_R%29%5E2%7D%7BH_L%2BH_R%2B%5Clambda%7D%5Cright%5D+-+%5Cgamma+%5C%5C" alt="[公式]"></p>
<p>注意<strong>该特征收益也可作为特征重要性输出的重要依据</strong>。</p>
<p>我们可以发现对于所有的分裂点 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> ，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 <img src="https://www.zhihu.com/equation?tex=G_L" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=G_R" alt="[公式]"> 。然后用上面的公式计算每个分割方案的分数就可以了。<font color="red">观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入<strong>新叶子的惩罚项（gamma)</strong>，也就是说引入的分割带来的<strong>增益如果小于一个阀值</strong>的时候，我们可以剪掉这个分割。 </font></p>
<ul>
<li>当一次分裂后，计算新生成的左、右叶子节点样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会收回此次分裂。</li>
<li>完成完整一棵树的分裂之后，再从底到顶反向检查是否有不满足分裂条件的结点，进行回溯剪枝。</li>
</ul>
<h3><span id="5-xgboost采样是有放回还是无放回的">5、<strong>Xgboost采样是有放回还是无放回的？</strong></span></h3><p>xgboost时基于boosting的方法，样本是不放回的 ，每轮样本不重复。</p>
<h3><span id="6-xgboost在工程上有哪些优化为什么要做这些工程化优化">6、<strong>Xgboost在工程上有哪些优化？为什么要做这些工程化优化？</strong></span></h3><h4><span id="块结构设计"><strong>块结构设计</strong></span></h4><p>我们知道，决策树的学习<strong>最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序</strong>。而 <strong><font color="red"> XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</font></strong></p>
<blockquote>
<p>  预排序 + 块设计【独立】 + 稀疏矩阵存储 </p>
</blockquote>
<ul>
<li><strong>每一个块结构包括一个或多个已经排序好的特征</strong>；</li>
<li><strong>缺失特征值将不进行排序</strong>；</li>
<li>每个特征会存储指向<strong>样本梯度统计值</strong>的索引，方便计算一阶导和二阶导数值；</li>
</ul>
<p>这种块结构存储的特征之间相互独立，方便计算机进行并行计算。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个<strong>特征的增益计算可以同时进行</strong>，这也是 Xgboost 能够实现分布式或者多线程计算的原因。</p>
<h4><span id="缓存访问优化算法索引访问梯度统计-gt-缓存空间不连续"><strong>缓存访问优化算法</strong>【索引访问梯度统计 -&gt; 缓存空间不连续】</span></h4><p>块结构的设计可以减少节点分裂时的计算量，但<strong>特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续</strong>，这样会造成缓存命中率低，从而影响到算法的效率。</p>
<p>为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。此外适当调整块大小，也可以有助于缓存优化。</p>
<p>于exact greedy算法中, 使用<strong>缓存预取（cache-aware prefetching）</strong>。具体来说，<strong>对每个线程分配一个连续的buffer</strong>，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化）</p>
<h4><span id="核外块计算"><strong>“核外”块计算</strong></span></h4><p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，<strong>XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行</strong>。</p>
<p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p>
<ul>
<li><strong>块压缩：</strong>对 Block 进行按列压缩，并在读取时进行解压；</li>
<li><strong>块拆分：</strong>将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li>
</ul>
<h3><span id="7-xgboost与gbdt有什么联系和不同基模型-算法-工程设计">7、<strong>Xgboost与GBDT有什么联系和不同？</strong>【基模型、算法、工程设计】</span></h3><ol>
<li><strong>基分类器</strong>：GBDT 以 CART 作为基分类器，而Xgboost的基分类器不仅支持CART决策树，还支持线性分类器，此时Xgboost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。</li>
<li><strong>导数信息</strong>：GBDT只用了一阶导数信息，Xgboost中对损失函数进行二阶泰勒展开，引入二阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶和二阶可导即可。</li>
<li><strong>正则项</strong>：Xgboost的目标函数加入正则项(叶子结点的数目和叶子结点权重的L2模的平方)，相当于分裂预剪枝过程，降低过拟合。</li>
<li><strong>列抽样</strong>：Xgboost支持列采样，与随机森林类似，用于防止过拟合且加速。(列采样就是训练的时候随机使用一部分特征)，也同时支持子采样，即每轮迭代计算可以不使用全部样本，对样本数据进行采样。</li>
<li><strong>缺失值处理</strong>：Xgboost可以处理缺失值(具体，查看上方问答)</li>
<li><strong>并行化</strong>：Xgboost可以在特征维度进行并行化，在训练前预先将每个特征按照特征值大小进行预排序，按块的形式存储，后续可以重复使用这个结构，减小计算量，分裂时可以用多线程并行计算每个特征的增益，最终选增益最大的那个特征去做分裂，提高训练速度。</li>
</ol>
<h3><span id="8-xgboost特征重要性">8、<strong><font color="red"> XGBoost特征重要性</font></strong></span></h3><blockquote>
<p>  <strong>何时使用shap value分析特征重要性？</strong> -  知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/527570173/answer/2472253431">https://www.zhihu.com/question/527570173/answer/2472253431</a></p>
</blockquote>
<p>这一思路，通常被用来做<strong>特征筛选</strong>。剔除贡献度不高的尾部特征，增强模型的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=鲁棒性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;355884348&quot;}">鲁棒性</a>的同时，起到特征降维的作用。另一个方面，则是用来做<strong>模型的可解释性</strong>。我们期望的结果是：重要的特征是符合业务直觉的；符合业务直觉的特征排名靠前。</p>
<h4><span id="xgb内置的三种特征重要性计算方法">XGB内置的三种特征重要性计算方法</span></h4><ul>
<li><strong>weight</strong>：<code>xgb.plot_importance</code>,<strong>子树模型分裂时，用到的特征次数。这里计算的是所有的树。</strong></li>
<li><strong>gain</strong>:<code>model.feature_importances_</code>,信息增益的泛化概念。这里是指，<strong>节点分裂时，该特征带来信息增益（目标函数）优化的平均值。</strong></li>
<li><strong>cover</strong>:<code>model = XGBRFClassifier(importance_type = &#39;cover&#39;)</code> 这个计算方法，需要在定义模型时定义。之后再调用<code>model.feature_importances_</code> 得到的便是基于<code>cover</code>得到的贡献度。<strong>树模型在分裂时，特征下的叶子结点涵盖的样本数除以特征用来分裂的次数。分裂越靠近根部，cover 值越大。</strong></li>
</ul>
<h4><span id="其他重要性计算方法">其他重要性计算方法</span></h4><ul>
<li><strong>permutation</strong>:<strong>如果这个特征很重要，那么我们打散所有样本中的该特征，则最后的优化目标将折损。这里的折损程度，就是特征的重要程度。</strong></li>
<li><strong>shap</strong>:<strong>轮流去掉每一个特征，算出剩下特征的贡献情况，以此来推导出被去除特征的边际贡献。该方法是目前唯一的逻辑严密的特征解释方法</strong></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/10/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/12/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'your_domain' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
