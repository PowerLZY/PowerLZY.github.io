<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"powerlzy.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="相比到达的地方，同行的人更重要！">
<meta property="og:type" content="website">
<meta property="og:title" content="PowerLZY&#39;s Blog">
<meta property="og:url" content="https://powerlzy.github.io/page/21/index.html">
<meta property="og:site_name" content="PowerLZY&#39;s Blog">
<meta property="og:description" content="相比到达的地方，同行的人更重要！">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://powerlzy.github.io/page/21/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/21/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PowerLZY's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">PowerLZY's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本博客主要用于记录个人学习笔记（测试阶段）</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lzy"
      src="/images/cat_mac.jpg">
  <p class="site-author-name" itemprop="name">lzy</p>
  <div class="site-description" itemprop="description">相比到达的地方，同行的人更重要！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">251</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PowerLZY" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PowerLZY" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3289218653@qq.com" title="E-Mail → mailto:3289218653@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/510TZM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/510TZM/" class="post-title-link" itemprop="url">理论基础（4）回归评价指标</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 12:03:27" itemprop="dateModified" datetime="2023-04-26T12:03:27+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-回归问题评价指标">一、回归问题评价指标</span></h3>
<blockquote>
<p><strong>均方差损失 Mean Squared Loss、平均绝对误差损失 Mean Absolute
Error Loss、Huber Loss、分位数损失 Quantile Loss</strong></p>
</blockquote>
<p>机器学习中的监督学习本质上是给定一系列训练样本 <span class="math inline">\(\left(x_i, y_i\right)\)</span>, 尝试学习 <span class="math inline">\(x \rightarrow y\)</span> 的映射关系, 使得给定一个
<span class="math inline">\(x\)</span>, 即便这个 <span class="math inline">\(x\)</span> 不在训练样本中, 也能够得到尽量接近真实
<span class="math inline">\(y\)</span> 的输出 <span class="math inline">\(\hat{y}\)</span> 。而损失函数 (Loss Function)
则是这个过 程中关键的一个组成部分, 用来<strong>衡量模型的输出 <span class="math inline">\(\hat{y}\)</span> 与真实的 <span class="math inline">\(y\)</span> 之间的差距</strong>,
给模型的优化指明方向。</p>
<h4><span id="11-均方差损失-mse-l2-loss">1.1 均方差损失 MSE、L2 loss</span></h4>
<h5><span id="111-基本形式与原理">1.1.1 <strong>基本形式与原理</strong></span></h5>
<p><strong>均方差Mean Squared Error
(MSE)损失是机器学习、深度学习回归任务中最常用的一种损失函数, 也称为 L2
Loss</strong>。其基本形式如下: <span class="math display">\[
J_{M S E}=\frac{1}{N} \sum_{i=1}^N\left(y_i-\hat{y_i}\right)^2
\]</span> 从直觉上理解均方差损失，这个损失函数的最小值为 0
（当预测等于真实值时），最大值为无穷大。下图是对于真 实值 <span class="math inline">\(y=0\)</span>, 不同的预测值 <span class="math inline">\([-1.5,1.5]\)</span>
的均方差损失的变化图。横轴是不同的预测值, 纵轴是均方差损失, 可以
看到随着预测与真实值绝对误差 <span class="math inline">\(|y-\hat{y}|\)</span> 的增加,
均方差损失呈二次方地增加。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232145431.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h5><span id="112-背后的假设">1.1.2 背后的假设</span></h5>
<p><strong>【独立同分布-中心极限定理】</strong>： 如果 <span class="math inline">\(\left\{X_n\right\}\)</span> 独立同分布, 且 <span class="math inline">\(\mathbb{E} X=\mu, \mathbb{D}
X=\sigma^2&gt;0\)</span> ，则 <span class="math inline">\(\mathrm{n}\)</span> 足够大时 <span class="math inline">\(\bar{X}_n\)</span> 近似服从正态分布 <span class="math inline">\(N\left(\mu, \frac{\sigma^2}{n}\right)\)</span> 即
<span class="math display">\[
\lim _{n \rightarrow \infty} P\left(\frac{\bar{X}_n-\mu}{\sigma /
\sqrt{n}}&lt;a\right)=\Phi(a)=\int_{-\infty}^a \frac{1}{\sqrt{2 \pi}}
e^{-t^2 / 2} d t
\]</span> 实际上在一定的假设下,
我们可以使用最大化似然得到均方差损失的形式。假设<strong>模型预测与真实值之间的误差服从标准高斯分布</strong>
<span class="math inline">\((\mu=0, \sigma=1)\)</span> ，则给定一个
<span class="math inline">\(x_i\)</span> 模型输出真实值 <span class="math inline">\(y_i\)</span> 的概率为 <span class="math display">\[
p\left(y_i \mid x_i\right)=\frac{1}{\sqrt{2 \pi}} \exp
\left(-\frac{\left(y_i-\hat{y}_i\right)^2}{2}\right)
\]</span> <strong>进一步我们假设数据集中 <span class="math inline">\(\mathrm{N}\)</span> 个样本点之间相互独立,
则给定所有 <span class="math inline">\(x\)</span> 输出所有真实值 <span class="math inline">\(y\)</span> 的概率, 即似然 Likelihood</strong>,
为所有 <span class="math inline">\(p\left(y_i \mid x_i\right)\)</span>
的累乘 <span class="math display">\[
L(x, y)=\prod_{i=1}^N \frac{1}{\sqrt{2 \pi}} \exp
\left(-\frac{\left(y_i-\hat{y}_i\right)^2}{2}\right)
\]</span> 通常为了计算方便，我们通常最大化对数似然Log-Likelihood <span class="math display">\[
L L(x, y)=\log (L(x, y))=-\frac{N}{2} \log 2 \pi-\frac{1}{2}
\sum_{i=1}^N\left(y_i-\hat{y_i}\right)^2
\]</span> 去掉与 <span class="math inline">\(\hat{y_i}\)</span>
无关的第一项, 然后转化为最小化负对数似然 Negative Log-Likelihood <span class="math display">\[
N L L(x, y)=\frac{1}{2} \sum_{i=1}^N\left(y_i-\hat{y}_i\right)^2
\]</span>
可以看到这个实际上就是均方差损失的形式。<strong>也就是说在模型输出与真实值的误差服从高斯分布的假设下,
最小化均方差损失函数与极大似然估计本质上是一致的</strong>,
因此在这个假设能被满足的场景中（比如回归）,
均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一
个好的选择。</p>
<h5><span id="hulu-百面机器学习-平方根误差的意外"><strong><font color="red">
hulu 百面机器学习 —— 平方根误差的”意外“</font></strong></span></h5>
<p><strong>95%的时间区间效果很好，RMSE指标居高不下的原因？</strong>
<span class="math display">\[
J_{M S E}=\frac{1}{N} \sum_{i=1}^N\left(y_i-\hat{y_i}\right)^2
\]</span>
一般情况下RSME能反应预测值与真实值的偏离程度，但是<strong>易受离群点</strong>的影响；</p>
<p><strong>解决方案</strong>：</p>
<ul>
<li>数据预处理将噪音去掉</li>
<li>将离群点的产生机制建模进去</li>
<li>更鲁棒的模型评估指标：<strong>平均绝对百分比误差</strong>（MAPE），<strong>分位数损失</strong></li>
</ul>
<h4><span id="12-平均绝对误差-mae">1.2 <strong>平均绝对误差 MAE</strong></span></h4>
<h5><span id="121-基本形式与原理">1.2.1 <strong>基本形式与原理</strong></span></h5>
<p><strong>平均绝对误差 Mean Absolute Error (MAE)
是另一类常用的损失函数, 也称为 L1 Loss</strong>。其基本形式如下 <span class="math display">\[
J_{M A E}=\frac{1}{N} \sum_{i=1}^N\left|y_i-\hat{y_i}\right|
\]</span> 同样的我们可以对这个损失函数进行可视化如下图, MAE
损失的最小值为 0 (当预测等于真实值时），最大值为
无穷大。可以看到随着预测与真实值绝对误差 <span class="math inline">\(|y-\hat{y}|\)</span> 的增加, MAE
损失呈线性增长。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232148454.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h5><span id="122-背后的假设">1.2.2 背后的假设</span></h5>
<p>同样的我们可以在一定的假设下通过最大化似然得到 MAE 损失的形式,
假设模型预测与真实值之间的误差服 从拉普拉斯分布 Laplace distribution
<span class="math inline">\((\mu=0, b=1)\)</span>, 则给定一个 <span class="math inline">\(x_i\)</span> 模型输出真实值 <span class="math inline">\(y_i\)</span> 的概率为 <span class="math display">\[
p\left(y_i \mid x_i\right)=\frac{1}{2} \exp
\left(-\left|y_i-\hat{y_i}\right|\right)
\]</span> 与上面推导 MSE 时类似, 我们可以得到的负对数似然实际上就是 MAE
损失的形式 <span class="math display">\[
\begin{gathered}
L(x, y)=\prod_{i=1}^N \frac{1}{2} \exp
\left(-\left|y_i-\hat{y}_i\right|\right) \\
L L(x, y)=N \ln \frac{1}{2}-\sum_{i=1}^N\left|y_i-\hat{y}_i\right| \\
N L L(x, y)=\sum_{i=1}^N\left|y_i-\hat{y}_i\right|
\end{gathered}
\]</span></p>
<h4><span id="13-mae-与-mse-区别">1.3 MAE 与 MSE 区别</span></h4>
<p>MAE 和 MSE 作为损失函数的主要区别是：<strong>MSE 损失相比 MAE
通常可以更快地收敛，但 MAE 损失对于 outlier
更加健壮</strong>，即更加不易受到 outlier 影响。</p>
<ul>
<li><p><strong>MSE 通常比 MAE
可以更快地收敛</strong>。当使用梯度下降算法时, MSE 损失的梯度为 <span class="math inline">\(-\hat{y}_i\)</span>, 而 MAE 损失的梯度为 <span class="math inline">\(\pm 1\)</span> , 即 MSE 的梯度的 scale
会随误差大小变化, 而 MAE 的梯度的 scale 则一直保持为 1 , 即便在绝对误 差
<span class="math inline">\(\left|y_i-\hat{y}_i\right|\)</span>
很小的时候 MAE 的梯度 scale 也同样为 1 ,
这实际上是非常不利于模型的训练的。当然你可以通
过在训练过程中动态调整学习率缓解这个问题, 但是总的来说,
损失函数梯度之间的差异导致了 MSE 在大部 分时候比 MAE
收敛地更快。这个也是 MSE 更为流行的原因。</p></li>
<li><p><strong>MAE 对于异常值（outlier） 更加
robust</strong>。我们可以从两个角度来理解这一点：</p>
<ul>
<li><p>第一个角度是直观地理解，下图是 MAE 和 MSE
损失画到同一张图里面，由于MAE 损失与绝对误差之间是线性关系，MSE
损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE
损失。<strong>因此当数据中出现一个误差非常大的 outlier 时，MSE
会产生一个非常大的损失，对模型的训练会产生较大的影响</strong>。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232151110.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure></li>
<li><p>第二个角度是从两个损失函数的假设出发，MSE
假设了误差服从高斯分布，MAE
假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于 outlier 更加
robust。参考下图（来源：<a href="https://link.zhihu.com/?target=https%3A//www.cs.ubc.ca/~murphyk/MLbook/">Machine
Learning: A Probabilistic Perspective</a> 2.4.3 The Laplace distribution
Figure 2.8），当右图右侧出现了 outliers
时，拉普拉斯分布相比高斯分布受到的影响要小很多。因此以拉普拉斯分布为假设的
MAE 对 outlier 比高斯分布为假设的 MSE 更加
robust。<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232151431.jpg" alt="img" style="zoom: 67%;"></p></li>
</ul></li>
</ul>
<h4><span id="14-huber-loss">1.4 Huber Loss</span></h4>
<blockquote>
<ul>
<li>在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定</li>
<li>在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier
更加健壮。</li>
</ul>
</blockquote>
<p>上文我们分别介绍了 MSE 和 MAE 损失以及各自的优缺点, MSE
损失收玫快但容易受 outlier 影响, MAE 对 outlier 更加健壮但是收玫慢,
Huber LosS 则是一种将 MSE 与 MAE 结合起来, 取两者优点的损失函数,
也被称作 Smooth Mean Absolute Error Loss 。其原理很简单, 就是在误差接近
0 时使用 MSE, 误差较大时使用 MAE, 公 式为 <span class="math display">\[
J_{\text {huber }}=\frac{1}{N} \sum_{i=1}^N
\mathbb{I}_{\left|y_i-\hat{y_i}\right| \leq \delta}
\frac{\left(y_i-\hat{y_i}\right)^2}{2}+\mathbb{I}_{\left|y_i-\hat{y}_i\right|&gt;\delta}\left(\delta\left|y_i-\hat{y_i}\right|-\frac{1}{2}
\delta^2\right)
\]</span> 上式中 <span class="math inline">\(\delta\)</span> 是 Huber
Loss 的一个超参数, <span class="math inline">\(\delta\)</span> 的值是
MSE 和 MAE 两个损失连接的位置。上式等号右边第一项是 MSE 的部分, 第二项是
MAE 部分, 在 MAE 的部分公式为 <span class="math inline">\(\delta\left|y_i-\hat{y_i}\right|-\frac{1}{2}
\delta^2\)</span> 是为了保证误差 <span class="math inline">\(|y-\hat{y}|= \pm \delta\)</span> 时 MAE 和 MSE
的取值一致，进而保证 Huber Loss 损失连续可导。</p>
<p>下图是 <span class="math inline">\(\delta=1.0\)</span> 时的 Huber
Loss, 可以看到在 <span class="math inline">\([-\delta, \delta]\)</span>
的区间内实际上就是 MSE 损失, 在 <span class="math inline">\((-\infty,
\delta)\)</span> 和 <span class="math inline">\((\delta,
\infty)\)</span> 区 间内为 MAE损失。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232151667.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4><span id="15-分位数损失-quantile-loss">1.5 分位数损失 Quantile Loss</span></h4>
<blockquote>
<p><strong>MAE
中分别用不同的系数控制高估和低估的损失，进而实现分位数回归</strong></p>
</blockquote>
<p><strong>分位数回归 Quantile Regression
是一类在实际应用中非常有用的回归算法</strong>，通常的回归算法是拟合目标值的期望或者中位数，而分位数回归可以通过给定不同的分位点，<strong>拟合目标值的不同分位数</strong>。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232151641.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>分位数回归是通过使用分位数损失 Quantile Loss 来实现这一点的,
分位数损失形式如下, 式中的 r 分位数系数</strong>。 <span class="math display">\[
J_{\text {quant }}=\frac{1}{N} \sum_{i=1}^N \mathbb{I}_{\hat{y}_i \geq
y_i}(1-r)\left|y_i-\hat{y_i}\right|+\mathbb{I}_{\hat{y}_i&lt;y_i}
r\left|y_i-\hat{y_i}\right|
\]</span> 我们如何理解这个损失函数呢? 这个损失函数是一个分段的函数, 将
<span class="math inline">\(\hat{y}_i \geq y_i \quad\)</span> (高估) 和
<span class="math inline">\(\hat{y}_i&lt;y_i \quad\)</span> (低估) 两种
情况分开来, 并分别给予不同的系数。当 <span class="math inline">\(r&gt;0.5\)</span> 时,
低估的损失要比高估的损失更大, 反过来当 <span class="math inline">\(r&lt;0.5\)</span> 时, 高估的损失比低估的损失大;
分位数损失实现了<strong>分别用不同的系数控制高估和低估的损失,
进而实现分位数回归</strong>。 特别地, 当 <span class="math inline">\(r=0.5\)</span> 时, 分位数损失退化为 MAE 损失,
从这里可以看出 MAE 损失实际上是分位数损失的一个特 例 一 中位数回归。</p>
<p>下图是取不同的分位点 <span class="math inline">\(0.2 、 0.5 、
0.6\)</span> 得到的三个不同的分位损失函数的可视化，可以看到 0.2 和 0.6
在高估和低 估两种情况下损失是不同的, 而 0.5 实际上就是 MAE。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232152365.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4><span id="16-平均绝对百分误差-mape">1.6 平均绝对百分误差 MAPE</span></h4>
<p>虽然平均绝对误差能够获得一个评价值,
但是你并不知道这个值代表模型拟合是优还是劣, 只有通过对比才能达到
效果。当需要以相对的观点来衡量误差时, 则使用MAPE。
平均绝对百分误差（Mean Absolute Percentage Error, MAPE）是对 MAE
的一种改进, 考虑了绝对误差相对 真实值的比例。 -
优点：考虑了预测值与真实值的误差。考虑了误差与真实值之间的比例。 <span class="math display">\[
M A P E=\frac{100}{m}
\sum_{i=1}^m\left|\frac{y_i-f\left(x_i\right)}{y_i}\right|
\]</span> 在某些场景下, 如房价从 <span class="math inline">\(5
K\)</span> 到 <span class="math inline">\(50 K\)</span> 之间, <span class="math inline">\(5 K\)</span> 预测成 <span class="math inline">\(10
K\)</span> 与 <span class="math inline">\(50 K\)</span> 预测成 <span class="math inline">\(45 K\)</span> 的差别是非常大的, 而平均
绝对百分误差考虑到了这点。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1PQETBX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1PQETBX/" class="post-title-link" itemprop="url">理论基础（4）分类评价指标</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 12:01:48" itemprop="dateModified" datetime="2023-04-26T12:01:48+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-二分类问题">一、二分类问题</span></h3>
<blockquote>
<p><strong>阈值调节问题？</strong></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232135565.png" alt="image-20220421165422230" style="zoom:50%;"></p>
<ul>
<li><strong>准确率 (Accuracy)</strong>：<strong>预测正确的概率</strong>
【<strong>(TP+TN)/(TP+TN+FP+FN)</strong>】</li>
<li><strong>精确率（查准率 Precision
)：预测为正的样本中实际为正的样本的概率</strong>
【<strong>TP/(TP+FP)</strong>】</li>
<li>错误发现率（FDR）= 1 - 精确率 = 预测为正的样本中实际为负的样本的概率
【<strong>FP/(TP+FP)</strong>】</li>
<li><strong>召回率（查全率）-
Recall</strong>：<strong>实际为正的样本中被预测为正样本的概率</strong>【<strong>TP/(TP+FN)</strong>】</li>
<li><strong>真正率（TPR） = 灵敏度（召回率） =</strong>
<strong>TP/(TP+FN)</strong></li>
<li><strong>假正率（FPR） = 1- 特异度 =</strong>
<strong>FP/(FP+TN)</strong></li>
<li><strong>F1=是准确率和召回率的调和平均值
(2×Precision×Recall)/（Precision+Recall）</strong></li>
<li>G-mean <span class="math inline">\((\mathrm{GM})=\)</span>
是准确率和召回率的几何平均值 <span class="math inline">\(G-\)</span>
mean <span class="math inline">\(=\sqrt{\text { Recall } \cdot \text {
Precision }}\)</span></li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232135401.png" alt="image-20220421165436795" style="zoom:50%;"></p>
<h4><span id="11-f1">1.1 <strong>F1</strong></span></h4>
<p>精确率（Precision）和召回率（Recall）之间的关系用图来表达，就是下面的PR曲线。可以发现他们俩的关系是「两难全」的关系。为了综合两者的表现，在两者之间找一个平衡点，就出现了一个
F1分数。</p>
<p><strong>F1=(2×Precision×Recall) /（Precision+Recall）</strong></p>
<p>P意义类似于每通过准确预测得到TP个正例需要TP+FP个预测类别为正例的样本。</p>
<p>R意义类似于每通过成功召回得到TP个正例需要TP+FN个真实类别为正例的样本。</p>
<p>F1度量了给定一批样本，对这一批样本进行预测与召回，最终得到的正例的多少。<strong>其中一半的正例是通过预测得到的，一半的正例是通过召回得到的。</strong></p>
<p>有一种把预测所需的预测类别为正例的样本和召回所需的真实类别为正例的样本看作原料，而我们的目标正例样本看作产品的感觉。<strong>所以也能解释为什么P跟R其中一者比较低的时候，F1会偏低。因为跟算术平均数不一样，两者不能互相替代，两部分各负责一半。那么加权调和平均Fbeta也可以很好的理解了。</strong></p>
<p><span class="math display">\[
\frac{1}{F_\beta}=\frac{1}{1+\beta^2}
\cdot\left(\frac{1}{P}+\frac{\beta^2}{R}\right)
\]</span></p>
<p>各自负责的比例不一样了。因此beta越大，Fbeta越着重考虑召回能力。</p>
<h4><span id="12-rocauc的概念">1.2 ROC/AUC的概念</span></h4>
<h5><span id="1灵敏度特异度真正率假正率">（1）<strong>灵敏度，特异度，真正率，假正率</strong></span></h5>
<p>在正式介绍ROC/AUC之前，我们还要再介绍两个指标，<strong>这两个指标的选择也正是ROC和AUC可以无视样本不平衡的原因。</strong>
这两个指标分别是：<strong>灵敏度和（1-特异度），也叫做真正率（TPR）和假正率（FPR）</strong>。其实我们可以发现<strong>灵敏度和召回率是一模一样的，只是名字换了而已</strong>。由于我们比较关心正样本，所以需要查看有多少负样本被错误地预测为正样本，所以使用（1-特异度），而不是特异度。</p>
<p><strong>真正率（TPR） = 灵敏度（召回率） =</strong>
<strong>TP/(TP+FN)</strong></p>
<p><strong>假正率（FPR） = 1- 特异度 =</strong>
<strong>FP/(FP+TN)</strong></p>
<p>下面是真正率和假正率的示意，我们发现<strong>TPR和FPR分别是基于实际表现1和0出发的，也就是说它们分别在实际的正样本和负样本中来观察相关概率问题。</strong></p>
<blockquote>
<p>正因为如此，所以无论样本是否平衡，都不会被影响。还是拿之前的例子，总样本中，90%是正样本，10%是负样本。我们知道用准确率是有水分的，但是用TPR和FPR不一样。这里，TPR只关注90%正样本中有多少是被真正覆盖的，而与那10%毫无关系，同理，FPR只关注10%负样本中有多少是被错误覆盖的，也与那90%毫无关系，</p>
</blockquote>
<p><strong>如果我们从实际表现的各个结果角度出发，就可以避免样本不平衡的问题了，这也是为什么选用TPR和FPR作为ROC/AUC的指标的原因。</strong></p>
<h5><span id="2roc接受者操作特征曲线">（2）ROC（接受者操作特征曲线）</span></h5>
<blockquote>
<p>ROC（Receiver Operating
Characteristic）曲线，又称接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力，ROC曲线是基于<strong>混淆矩阵</strong>得出的。</p>
</blockquote>
<p>ROC曲线中的主要两个指标就是<strong>真正率</strong>和<strong>假正率，</strong>
上面也解释了这么选择的好处所在。其中<strong>横坐标为假正率（FPR），纵坐标为真正率（TPR）</strong>，下面就是一个标准的ROC曲线图。</p>
<h5><span id="3auc的缺陷">（3）AUC的缺陷？</span></h5>
<p><strong>优点</strong>：目前普遍认为接收器工作特性曲线（ROC）曲线下的面积—AUC是评估分类模型准确性的标准方法。<strong>它避免了在阈值选择过程中假定的主观性</strong>，当连续的概率得到的分数被转换为二分类标签时，通过总结整体模型表现，其衡量模型区分正负样本的性能优于通过阈值来判断的其他方法（比如准确率、召回率等）。</p>
<ul>
<li><strong>忽略了预测的概率值和模型的拟合优度</strong></li>
<li><strong>AUC反应了太过笼统的信息。无法反应召回率、精确率等在实际业务中经常关心的指标</strong></li>
<li><font color="red">
<strong>对FPR和TPR两种错误的代价同等看待</strong></font></li>
<li>它没有给出模型误差的空间分布信息</li>
<li>最重要的一点，AUC的misleading的问题</li>
</ul>
<p><strong>auc仅反应模型的排序能力，无法反应模型的拟合优度；auc很多时候无法直接反应细粒度的和业务目标更相关的metric信息，例如
top
k的准确率，召回率等等（例如同auc的模型在不同的区间的预测能力是存在差别的）；</strong></p>
<h4><span id="13-k-s曲线">1.3、K-S曲线</span></h4>
<p><strong>K-S曲线, 又称作洛伦兹曲线</strong>。实际上,
K-S曲线的数据来源以及本质和ROC曲线是一致的, 只是ROC曲线是把真 正率 <span class="math inline">\((T P R)\)</span> 和假正率 <span class="math inline">\((F P R)\)</span> 当作横纵轴,
<strong>而K-S曲线是把真正率 <span class="math inline">\((T P R)\)</span>
和假正率 <span class="math inline">\((F P R\)</span> ) 都当作是
纵轴，横轴则由选定的阈值来充当。从 K-S 曲线就能衍生出 <span class="math inline">\(K S\)</span> 值, <span class="math inline">\(K
S=\max (T P R-F P R)\)</span> ，即是两条曲线
之间的最大间隔距离。</strong></p>
<p><strong>K-S曲线的画法:</strong></p>
<ol type="1">
<li><strong>排序</strong>: 对于二元分类器来说,
模型训练完成之后每个样本都会得到一个类概率值, 把样本按这个类概率值从大
到小进行排序；</li>
<li><strong>找阈值</strong>: 取排序后前 <span class="math inline">\(10
\% \times k(k=1,2,3, \ldots, 9)\)</span> 处的值（概率值）作为阈值,
分别计算出不同的 <span class="math inline">\(T P R\)</span> 和 <span class="math inline">\(F P R\)</span> 值, 以 <span class="math inline">\(10 \% \times k(k=1,2,3, \ldots, 9)\)</span>
为横坐标, 分别以 <span class="math inline">\(T P R\)</span> 和 <span class="math inline">\(F P R\)</span> 值为纵坐标, 就可以画出两个曲
线，这就是K-S曲线，类似于下图。</li>
<li><strong>KS值</strong>:从 K-S 曲线就能衍生出 <span class="math inline">\(K S\)</span> 值, <span class="math inline">\(K
S=\max (T P R-F P R)\)</span> ，即是两条曲线之间的最大间隔距离。KS值越大
表示模型 的区分能力越强。</li>
</ol>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232137558.jpg" alt="img" style="zoom: 50%;"></p>
<h4><span id="14-lift曲线">1.4 Lift曲线</span></h4>
<p><strong>Lift曲线它衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。实质上它强调的是投入与产出比</strong>。</p>
<p><strong>tip:</strong>理解<strong>Lift</strong>可以先看一下Quora上的一篇文章：<strong><a href="https://link.zhihu.com/?target=https%3A//www.quora.com/Whats-Lift-curve">What's
Lift curve?</a></strong></p>
<p><strong>Lift计算公式：</strong>先介绍几个相关的指标，以免混淆：</p>
<ul>
<li>准确率 (accuracy, ACC) :</li>
</ul>
<p><span class="math display">\[
A C C=\frac{T P+T N}{F P+F N+T P+T N}
\]</span></p>
<ul>
<li>正确率(Precision, PRE), 查准率:</li>
</ul>
<p><span class="math display">\[
P R E=\frac{T P}{T P+F P}
\]</span></p>
<ul>
<li>真阳性率(True Positive Rate,
TPR)，灵敏度(Sensitivity)，召回率(Recall):</li>
</ul>
<p><span class="math display">\[
T P R=\frac{T P}{T P+F N}
\]</span></p>
<ul>
<li>假阳性率(False Positice Rate, FPR), 误诊率( = 1 - 特异度):</li>
</ul>
<p><span class="math display">\[
F P R=\frac{F P}{F P+T N}
\]</span></p>
<p><strong>Lift计算公式：</strong> <span class="math display">\[
L i f t=\frac{\frac{T P}{T P+F P}}{\frac{T P+F N}{T P+F P+T N+F
N}}=\frac{P R E}{\text { 正例占比 }}
\]</span> 根据以上公式可知, <font color="red">Lift指标可以这样理解:
在不使用模型的情况下, 我们用先验概率估计正例的比例, 即上式子分母部分,
以此作为正例的命中率;</font> 利用模型后,
我们不需要从整个样本中来挑选正例, 只需要从我们预测为正例
的那个样本的子集 <span class="math inline">\(T P+F P\)</span>
中挑选正例, 这时正例的命中率为 <span class="math inline">\(P R
E\)</span>, 后者除以前者即可得提升值Lift 。</p>
<h5><span id="lift曲线">Lift曲线:</span></h5>
<p>为了作出LIft曲线，首先引入 depth 的概念: <span class="math display">\[
\operatorname{depth}=\frac{T P+F P}{T P+F P+T N+F N}
\]</span> 从公式可以看出, depth
代表的是预测为正例的样本占整个样本的比例。</p>
<p>当阈值为 0 时, 所有的样本都被预测为正例, 因此 <span class="math inline">\(\operatorname{depth}=1\)</span>, 于是 <span class="math inline">\(L i f t=1\)</span>, 模型末起提升作用。随着阈值逐
渐增大, 被预测为正例的样本数逐渐减少, depth 减小,
而较少的预测正例样本中的真实正例比例逐渐增大。当阈 值增大至1时,
没有样本被预测为正例, 此时 depth <span class="math inline">\(=0\)</span>, 而 Lift <span class="math inline">\(=0\)</span> 。由此可见, Lift 与 depth 存在相反方
向变化的关系。在此基础上作出 Lift 图:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232138374.jpg" alt="img" style="zoom: 50%;"></p>
<p>一般要求, 在尽量大的 depth 下得到尽量大的 Lift, 所以 Lift
曲线的右半部分应该尽量陡峭。</p>
<h4><span id="15-p-r曲线">1.5 <strong>P-R曲线</strong></span></h4>
<ul>
<li><p><strong>精确率（查准率）- Precision
：预测为正的样本中实际为正的样本的概率</strong>
【<strong>TP/(TP+FP)</strong>】</p></li>
<li><p><strong>召回率（查全率）-
Recall</strong>：<strong>实际为正的样本中被预测为正样本的概率</strong>【<strong>TP/(TP+FN)</strong>】</p></li>
</ul>
<p>P-R曲线刻画<strong>查准率</strong>和<strong>查全率（召回率）</strong>之间的关系，查准率指的是在所有预测为正例的数据中，真正例所占的比例，查全率是指预测为真正例的数据占所有正例数据的比例。查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，查全率高时，查准率往往偏低。</p>
<p>在很多情况下，我们可以根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在后面的是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可计算当前的查全率和查准率，以查准率为y轴，以查全率为x轴，可以画出下面的P-R曲线。</p>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/v2-dc6abbb24e2dfbfefe4777408d2a8e5c_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p>如果一个学习器的P-R曲线被另一个学习器的P-R曲线完全包住，则可断言后者的性能优于前者，当然我们可以根据曲线下方的面积大小来进行比较，但更常用的是<strong>平衡点</strong>或者是F1值。</p>
<ul>
<li><strong>平衡点（BEP）</strong>是查准率=查全率时的取值，如果这个值较大，则说明学习器的性能较好。F1值越大，我们可以认为该学习器的性能较好。</li>
<li><font color="red">
<strong>F1度量</strong>：<strong>BEP过于简单，这个平衡点是建立在”查准率=查全率“的前提下，无法满足实际不同场景的应用。</strong></font></li>
</ul>
<p>我们先来引入加权调和平均: <span class="math inline">\(F_\beta\)</span> : <span class="math display">\[
\frac{1}{F_\beta}=\frac{1}{1+\beta^2}\left(\frac{1}{P}+\frac{\beta^2}{R}\right)
\text { 公式 }(1)
\]</span> <strong>加权调和平均与算术平均 <span class="math inline">\(\frac{P+R}{2}\)</span> 和几何平均 <span class="math inline">\(\sqrt{P+R}\)</span> 相比,
调和平均更重视较小值（这可以从倒数上看出 来）</strong>。当 <span class="math inline">\(\beta=1\)</span>,
即F1是基于查准率和查全率的调和平均定义的, <span class="math inline">\(\mathrm{F} 1\)</span> 的公式如下: <span class="math display">\[
\frac{1}{F_1}=\frac{1}{2}\left(\frac{1}{P}+\frac{1}{R}\right)
\]</span> 我们把公式求倒数，即可得： <span class="math display">\[
F 1=\frac{2 * P * R}{P+R}
\]</span> 在一些应用中,
对查准率和查全率的重视程度不同。例如在商品推荐中, 为了尽可能少打扰用户,
更希望推荐的内 容确实是用户感兴趣的, 此时查准率更重要;
而在罪犯信息检索或者病人检查系统中, 更希望尽可能少的漏判, 此
时查全率更重要。F1度量的一般形式是 <span class="math inline">\(F_\beta\)</span>
，能让我们自定义对查准率/查全率的不同偏好: <span class="math display">\[
F_\beta=\frac{\left(1+\beta^2\right) * P * R}{\left(\beta^2 *
P\right)+R}
\]</span> <strong>其中, <span class="math inline">\(\beta&gt;0\)</span>
度量了查全率对查准率的相对重要性 (不明白的同学可以回看公式1）， <span class="math inline">\(\beta=1\)</span> 时退化为标准F1, <span class="math inline">\(\beta&gt;1\)</span> 时查全率有更大影响; <span class="math inline">\(\beta&lt;1\)</span> 时,
查准率有更大影响。</strong></p>
<h4><span id="16-对数损失log-loss">1.6 <strong>对数损失(Log Loss)</strong></span></h4>
<p><strong>AUC
ROC考虑用于确定模型性能的预测概率</strong>。<font color="red">然而, AUC
ROC存在问题, 它只考虑概率的顺序,
因此没有考虑模型预测更可能为正样本的更高概率的能力(即考虑了大小,
但没有考虑更高精度)。</font>在这种情况下, 我们可以使用对数损失,
即每个实例的正例预测概率的对数的负平均值。</p>
<p>对数损失 (Logistic Loss, logloss)
是对预测概率的似然估计，其标准形式为： <span class="math display">\[
\log \operatorname{loss}=\log P(Y \mid X)
\]</span> 对数损失最小化本质是上利用样本中的已知分布,
求解拟合这种分布的最佳模型参数, 使这种分布出现概率最大。</p>
<p>对数损失对应的二分类的计算公式为： <span class="math display">\[
\log \operatorname{loss}=-\frac{1}{N} \sum_{i=1}^N\left(y_i \log
\hat{y}_i+\left(1-y_i\right) \log \left(1-\hat{y}_i\right)\right), \quad
y \in[0,1]
\]</span> 其中 <span class="math inline">\(\mathrm{N}\)</span> 为样本数,
<span class="math inline">\(\hat{y}_i\)</span> 为预测为 1
的概率。对数损失在多分类问题中也可以使用，其计算公式为: <span class="math display">\[
\log \operatorname{loss}=-\frac{1}{N} \frac{1}{C} \sum_{i=1}^N
\sum_{j=1}^C\left(y_{i j} \log \hat{y_{i j}}\right), \quad y \in[0,1]
\]</span> 其中, <span class="math inline">\(\mathrm{N}\)</span>
为样本数, C为类别数, logloss衡量的是预测概率分布和真实概率分布的差异性,
取值越小越好。</p>
<h4><span id="17-多分类">1.7 多分类</span></h4>
<p>很多时候我们有多个<strong>二分类混洧矩阵</strong>，例如进行多次训练测试，每次得到一个混淆矩阵；或是在多个数据集上进
行训练测试，希望估计算法的全局性能; 或者是执行分类任务,
每两两类别的组合都对应一个混淆矩阵; 总之是在 <strong><span class="math inline">\(\mathrm{n}\)</span>
个分类混淆矩阵上综合考察查准率和查全率</strong>。</p>
<ul>
<li><strong>宏观:</strong> 在各个混淆军阵上分别计算出查准率和查全率,
记为 <span class="math inline">\((P 1, R 1),(P 2, R 2),
\ldots(\mathrm{Pn}, \mathrm{Rn})\)</span>,
在<strong>计算平均值</strong>, 这样 就得到“宏观查准率"(macro-P),
“宏观查全率”(macro-R)、“宏观F1"(macro-F1):</li>
</ul>
<p><span class="math display">\[
\begin{gathered}
\text { macro }-P=\frac{1}{n} \sum_{i=1}^n P_i \\
\text { macro }-R=\frac{1}{n} \sum_{i=1}^n R_i \\
\text { macro }-F 1=\frac{2 * \text { macro }-P * \text { macro
}-R}{\text { macro }-P+\text { macro }-R}
\end{gathered}
\]</span></p>
<ul>
<li><strong>微观：</strong>将个混淆矩阵对应的元素进行平均,
得到TP、FP、TN、FN的平均值, 分别记为 <span class="math inline">\(\overline{T P} 、 \overline{F P} 、 \overline{F
N}\)</span> 、 <span class="math inline">\(\overline{T N}\)</span>,
再基于这些平均值计算出“微观查准率"(micro-P),
“微观查全率”(micro-R)、“微观F1"(micro-F1):</li>
</ul>
<p><span class="math display">\[
\begin{gathered}
\text { micro }-P=\frac{\overline{T P}}{\overline{T P}+\overline{F P}}
\\
\text { micro }-R=\frac{\overline{T P}}{\overline{T P}+\overline{F N}}
\\
\text { micro }-F 1=\frac{2 * \text { micro }-P * \text { micro
}-R}{\text { micro }-P+\text { micro }-R}
\end{gathered}
\]</span></p>
<h3><span id="二-评分总结sklearn">二、评分总结（sklearn）</span></h3>
<blockquote>
<p>sklearn.metrics -
回归/分类模型的评估方法:https://zhuanlan.zhihu.com/p/408078074</p>
</blockquote>
<h4><span id="21-分类模型">2.1 分类模型</span></h4>
<h5><span id="accuracy_score"><strong>accuracy_score</strong></span></h5>
<p><strong>分类准确率分数是指所有分类正确的百分比</strong>。分类准确率这一衡量分类器的标准比较容易理解，但是它不能告诉你响应值的潜在分布，并且它也不能告诉你分类器犯错的类型。所以在使用的时候，一般需要搭配matplotlib等数据可视化工具来观察预测的分类情况，与实际的结果做更加直观的比较。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score  </span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]  </span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]  </span><br><span class="line">accuracy_score(y_true, y_pred)  <span class="comment"># 默认normalization = True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.5</span></span><br><span class="line">accuracy_score(y_true, y_pred, normalize=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span></span><br></pre></td></tr></table></figure>
<h5><span id="recall_score"><strong>recall_score</strong></span></h5>
<p>召回率 =<strong>提取出的正确信息条数
/样本中的信息条数</strong>。通俗地说，就是所有准确的条目有多少被检索出来了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">recall_score(y_true, y_pred, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>,average=<span class="string">&#x27;binary&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line">参数average : string, [<span class="literal">None</span>, ‘micro’, ‘macro’(default), ‘samples’, ‘weighted’]</span><br></pre></td></tr></table></figure>
<p>将一个二分类matrics拓展到多分类或多标签问题时，我们可以将数据看成多个二分类问题的集合，每个类都是一个二分类。接着，我们可以通过跨多个分类计算每个二分类metrics得分的均值，这在一些情况下很有用。你可以使用<strong>average参数</strong>来指定。</p>
<ul>
<li>macro：计算二分类metrics的均值，为每个类给出相同权重的分值。</li>
<li>weighted:对于不均衡数量的类来说，计算二分类metrics的平均，通过在每个类的score上进行加权实现。</li>
<li>micro：给出了每个样本类以及它对整个metrics的贡献的pair（sample-weight），而非对整个类的metrics求和，它会每个类的metrics上的权重及因子进行求和，来计算整个份额。</li>
<li>samples：应用在multilabel问题上。它不会计算每个类，相反，它会在评估数据中，通过计算真实类和预测类的差异的metrics，来求平均（sample_weight-weighted）</li>
<li>average：average=None将返回一个数组，它包含了每个类的得分.</li>
</ul>
<h5><span id="roc_curve"><strong>roc_curve</strong></span></h5>
<p>ROC曲线指受试者工作特征曲线/接收器操作特性(receiver operating
characteristic，ROC)曲线,是<strong>反映灵敏性和特效性连续变量的综合指标</strong>,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界值，从而计算出一系列敏感性和特异性。ROC曲线是根据一系列不同的二分类方式（分界值或决定阈），<strong>以真正例率（也就是灵敏度）（True
Positive Rate,TPR）为纵坐标，假正例率（1-特效性）（False Positive
Rate,FPR）为横坐标</strong>绘制的曲线。</p>
<p>通过ROC我们可以观察到模型正确识别的正例的比例与模型错误地把负例数据识别成正例的比例之间的权衡。TPR的增加以FPR的增加为代价。ROC曲线下的面积是模型准确率的度量，<strong>AUC</strong>（Area
under roc curve）。</p>
<p><strong>TPR</strong> = TP /（TP + FN）
（正样本<strong>预测数</strong> / 正样本<strong>实际数</strong>）</p>
<p><strong>FPR</strong> = FP /（FP + TN）
（负样本<strong>预测数</strong> /负样本<strong>实际数</strong>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics  </span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])  </span><br><span class="line">scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])  </span><br><span class="line">fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=<span class="number">2</span>)  </span><br><span class="line">fpr  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.</span> ,  <span class="number">0.5</span>,  <span class="number">0.5</span>, <span class="number">1.</span> ])  </span><br><span class="line">tpr  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.5</span>,  <span class="number">0.5</span>,  <span class="number">1.</span> , <span class="number">1.</span> ])  </span><br><span class="line">thresholds  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.8</span> ,  <span class="number">0.4</span> ,  <span class="number">0.35</span>, <span class="number">0.1</span> ])  </span><br><span class="line"></span><br><span class="line"><span class="comment"># check auc score</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc   </span><br><span class="line">metrics.auc(fpr, tpr)   </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.75</span>   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以直接根据预测值+真实值来计算出auc值，略过roc的计算过程</span></span><br><span class="line">‘’‘</span><br><span class="line">sklearn.metrics.roc_auc_score(y_true, y_score, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line">average : string, [<span class="literal">None</span>, ‘micro’, ‘macro’(default), ‘samples’, ‘weighted’]</span><br><span class="line">’‘’</span><br><span class="line"><span class="comment"># 真实值（必须是二值）、预测值（可以是0/1,也可以是proba值）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score  </span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])  </span><br><span class="line">y_scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])  </span><br><span class="line">roc_auc_score(y_true, y_scores)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.75</span>  </span><br></pre></td></tr></table></figure>
<h5><span id="confusion-metric"><strong>confusion metric</strong></span></h5>
<p>混淆矩阵（confusion
matrix），又称为可能性表格或是错误矩阵。它是一种特定的矩阵用来呈现算法性能的可视化效果。其每一列代表预测值，每一行代表的是实际的类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matric(y_true, y_pred, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">&#x27;binary&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h5><span id="precision_score"><strong>precision_score</strong></span></h5>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">precision_score(y_true, y_pred, labels=None, pos_label=1, average=&#x27;binary&#x27;)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232157475.jpg" alt="preview">
<figcaption aria-hidden="true">preview</figcaption>
</figure>
<h3><span id="三-评价指标qampa">三、评价指标Q&amp;A</span></h3>
<h5><span id="精度指标存在的问题"><strong>精度指标存在的问题</strong>？</span></h5>
<ul>
<li>有倾向性的问题。比如，判断空中的飞行物是导弹还是其他飞行物，很显然为了减少损失，我们更倾向于相信是导弹而采用相应的防护措施。此时判断为导弹实际上是其他飞行物与判断为其他飞行物实际上是导弹这两种情况的重要性是不一样的；</li>
<li>样本类别数量严重不均衡的情况。比如银行客户样本中好客户990个，坏客户10个。如果一个模型直接把所有客户都判断为好客户，得到精度为99%，但这显然是没有意义的。</li>
</ul>
<h5><span id="为什么-roc和-auc-都能应用于非均衡的分类问题"><strong>为什么 ROC
和 AUC 都能应用于非均衡的分类问题？</strong></span></h5>
<p><strong>ROC曲线只与横坐标 (FPR) 和 纵坐标 (TPR) 有关系</strong>
。我们可以发现TPR只是正样本中预测正确的概率，而FPR只是负样本中预测错误的概率，和正负样本的比例没有关系。因此
ROC
的值与实际的正负样本比例无关，因此既可以用于均衡问题，也可以用于非均衡问题。而
AUC 的几何意义为ROC曲线下的面积，因此也和实际的正负样本比例无关。</p>
<h4><span id="参考文献">参考文献</span></h4>
<ul>
<li>一文看懂机器学习指标：准确率、精准率、召回率、F1、ROC曲线、AUC曲线:https://zhuanlan.zhihu.com/p/93107394</li>
<li><strong>机器学习-最全面的评价指标体系:
https://zhuanlan.zhihu.com/p/359997979</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/HaoMood/homepage/blob/master/files/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-03-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.pdf">机器学习工程师面试宝典-03-模型评估</a></li>
<li><strong><a target="_blank" rel="noopener" href="http://www.china-nb.cn/gongsidongtai/17-85.html">分类模型评估指标——准确率、精准率、召回率、F1、ROC曲线、AUC曲线</a></strong></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/2624Z0W/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2624Z0W/" class="post-title-link" itemprop="url">理论基础（5）模型评估</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-24 14:08:32" itemprop="dateCreated datePublished" datetime="2022-03-24T14:08:32+08:00">2022-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 12:10:55" itemprop="dateModified" datetime="2023-04-26T12:10:55+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">理论基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-ab-测试"><font color="red">一、A/B 测试</font></span></h3>
<blockquote>
<p>【AB测试最全干货】史上最全知识点及常见面试题（上篇） -
数据分析狗一枚的文章 - 知乎 https://zhuanlan.zhihu.com/p/375902281</p>
</blockquote>
<h4><span id="引言">引言</span></h4>
<p>科学家门捷列夫说「没有测量，就没有科学」，在AI场景下我们同样需要定量的数值化指标来指导我们更好地应用模型对数据进行学习和建模。</p>
<p>事实上，在机器学习领域，对模型的测量和评估至关重要。选择与问题相匹配的评估方法，能帮助我们快速准确地发现在模型选择和训练过程中出现的问题，进而对模型进行优化和迭代。本文我们系统地讲解一下机器学习模型评估相关知识。</p>
<h4><span id="11-模型评估的目标">1.1 模型评估的目标</span></h4>
<p><strong>模型评估的目标是选出泛化能力强的模型完成机器学习任务</strong>。实际的机器学习任务往往需要进行大量的实验，经过反复调参、使用多种模型算法（甚至多模型融合策略）来完成自己的机器学习问题，并观察哪种模型算法在什么样的参数下能够最好地完成任务。</p>
<p>但是我们无法提前获取「未知的样本」，因此我们会基于已有的数据进行切分来完成模型训练和评估，借助于切分出的数据进行评估，可以很好地判定模型状态（过拟合
or 欠拟合），进而迭代优化。</p>
<p>在建模过程中，为了获得泛化能力强的模型，我们需要一整套方法及评价指标。</p>
<ul>
<li><strong>评估方法</strong>：为保证客观地评估模型，对数据集进行的有效划分实验方法。</li>
<li><strong>性能指标</strong>：量化地度量模型效果的指标。</li>
</ul>
<h4><span id="12-离线与在线实验方法">1.2 离线与在线实验方法</span></h4>
<p>进行评估的实验方法可以分为「离线」和「在线」两种。</p>
<h5><span id="离线实验方法">离线实验方法：</span></h5>
<blockquote>
<p>在<strong>离线评估</strong>中，经常使用<strong>准确率（Accuracy）、查准率（Precision）、召回率（Recall）、ROC、AUC、PRC</strong>等指标来评估模型。</p>
</blockquote>
<p><strong>模型评估通常指离线试验</strong>。原型设计（Prototyping）阶段及离线试验方法，包含以下几个过程：</p>
<ul>
<li>使用历史数据训练一个适合解决目标任务的一个或多个机器学习模型。</li>
<li>对模型进行验证（Validation）与离线评估（Offline Evaluation）。</li>
<li>通过评估指标选择一个较好的模型。</li>
</ul>
<h5><span id="在线实验方法">在线实验方法：</span></h5>
<blockquote>
<p><strong>在线评估</strong>与离线评估所用的评价指标不同，一般使用一些商业评价指标，如<strong>用户生命周期值（Customer
Lifetime value）、广告点击率（Click Through
Rate）、用户流失率</strong>（Customer Churn Rate）等标。</p>
</blockquote>
<p>除了离线评估之外，其实还有一种在线评估的实验方法。由于模型是在老的模型产生的数据上学习和验证的，而线上的数据与之前是不同的，因此离线评估并不完全代表线上的模型结果。因此我们需要在线评估，来验证模型的有效性。</p>
<p><span class="math inline">\(A/B Test\)</span>
是目前在线测试中最主要的方法。 <span class="math inline">\(A/B
Test\)</span> 是为同一个目标制定两个方案让一部分用户使用 <span class="math inline">\(A\)</span> 方案, 另一部分用户使用 <span class="math inline">\(B\)</span> 方案, 记录下用户的使用情况,
看哪个方案更符合设计目标。如果不做AB实验直接上
线新方案，新方案甚至可能会毁掉你的产品。</p>
<h4><span id="13模型离线评估后为什么要进行ab测试"><strong><font color="red"> 1.3
模型离线评估后，为什么要进行ab测试？</font></strong></span></h4>
<ul>
<li><strong>离线评估无法消除过拟合的影响</strong>，因此离线评估结果无法代替线上的评估效果</li>
<li><strong>离线评估过程中无法模拟线上的真实环境，例如数据丢失、样本反馈延迟</strong></li>
<li>线上的<strong>某些商业指标例如收益、留存等无法通过离线计算</strong></li>
</ul>
<h4><span id="14如何进行线上ab测试">1.4
<strong>如何进行线上ab测试？</strong></span></h4>
<p>进行ab测试的主要手段时对用户进行分桶，即将<strong>用户分成实验组和对照组</strong>。实验组使用新模型，对照组使用base模型。<strong>分桶过程中需要保证样本的独立性和采样的无偏性</strong>，确保每个用户只划分到一个桶中，分桶过程中需要保证user
id是一个<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=随机数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22440144351%22%7D">随机数</a>，才能保证数据无偏的。</p>
<h3><span id="二-模型评估">二、模型评估</span></h3>
<h4><span id="21-holdout">2.1 holdout</span></h4>
<p><strong>留出法是机器学习中最常见的评估方法之一，它会从训练数据中保留出验证样本集，这部分数据不用于训练，而用于模型评估</strong>。</p>
<h4><span id="22-交叉验证">2.2 交叉验证</span></h4>
<p><strong>留出法的数据划分，可能会带来偏差</strong>。在机器学习中，另外一种比较常见的评估方法是交叉验证法——
<span class="math inline">\(K\)</span> <strong>折交叉验证对<span class="math inline">\(K\)</span>
个不同分组训练的结果进行平均来减少方差</strong>。</p>
<h4><span id="23-自助法">2.3 自助法</span></h4>
<p>Bootstrap
是一种用小样本估计总体值的一种非参数方法，在进化和生态学研究中应用十分广泛。<strong>Bootstrap通过有放回抽样生成大量的伪样本，通过对伪样本进行计算，获得统计量的分布，从而估计数据的整体分布</strong>。</p>
<h3><span id="三-超参数调优">三、超参数调优</span></h3>
<p>神经网咯是有许多超参数决定的，例如网络深度，学习率，正则等等。如何寻找最好的超参数组合，是一个老人靠经验，新人靠运气的任务。</p>
<h4><span id="31-网格搜索">3.1 网格搜索</span></h4>
<h4><span id="32-随机搜索">3.2 随机搜索</span></h4>
<h4><span id="33-贝叶斯优化">3.3 贝叶斯优化</span></h4>
<h5><span id="贝叶斯优化什么黑盒优化">贝叶斯优化什么?【黑盒优化】</span></h5>
<p>求助 gradient-free
的优化算法了，这类算法也很多了，<strong>贝叶斯优化就属于无梯度优化算法</strong>中的一种，它希望在尽可能少的试验情况下去尽可能获得优化命题的全局最优解。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232211987.jpg" alt="img" style="zoom: 33%;"></p>
<ul>
<li>目标函数 <span class="math inline">\(f(x)\)</span> 及其导数末知,
否则就可以用梯度下降等方法求解。</li>
<li>计算目标函数时间成本大, 意味着像蚁群算法、遗传算法这种方法也失效了,
因为计算一次要花费很多时间。</li>
</ul>
<h5><span id="概述">概述</span></h5>
<p>贝叶斯优化,
是一种使用<strong>贝叶斯定理来指导搜索以找到目标函数的最小值或最大值的方法</strong>,
就是在每次迭代的时 候, 利用之前观测到的历史信息 (先验知识)
来进行下一次优化, 通俗点讲, <strong>就是在进行一次迭代的时候,
先回顾下之前的迭代结果, 结果太差的 <span class="math inline">\(x\)</span> 附近就不去找了, 尽量往结果好一点的
<span class="math inline">\(x\)</span> 附近去找最优解</strong>,
这样一来搜索的效率就大大提高了, 这其实和人的思维方式也有点像,
每次在学习中试错, 并且在下次的时候根据这些经验来找到最 优的策略。</p>
<h5><span id="贝叶斯优化过程">贝叶斯优化过程</span></h5>
<p>首先，假设有一个这样的函数<span class="math inline">\(c(x)\)</span>，我们需要找到他的最小值，如下图所示，这也是我们所需要优化的目标函数，但是我们并不能够知道他的具体形状以及表达形式是怎么样的。</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232211373.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>贝叶斯优化是通过一种叫做代理优化的方式来进行的，就是不知道真实的目标函数长什么样，我们就用一个<strong>代理函数（surrogate
function）来代替目标函数</strong>，<strong>而这个代理函数就可以通过先采样几个点，再通过这几个点来给他拟合出来</strong>，如下图虚线所示：</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232211592.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>基于构造的代理函数,
<strong>我们就可以在可能是最小值的点附近采集更多的点</strong>,
或者在还没有采样过的区域来采集更多
的点，有了更多点，就可以<font color="red">更新代理函数</font>，使之更逼近真实的目标函数的形状，这样的话也更容易找到目标函数的
最小值, 这个采样的过程同样可以通过构建一个采集函数来表示,
也就是知道了当前代理函数的形状, 如何选择下 一个 <span class="math inline">\(x\)</span> 使得收益最大。</p>
<p><strong>然后重复以上过程，最终就可以找到函数的最小值点了，这大致就是贝叶斯优化的一个过程:</strong></p>
<ol type="1">
<li><strong>初始化一个代理函数的先验分布</strong></li>
<li><strong>选择数据点 <span class="math inline">\(x\)</span>,
使得采集函数 <span class="math inline">\(a(x)\)</span>
取最大值</strong></li>
<li><strong>在目标函数 <span class="math inline">\(c(x)\)</span>
中评估数据点 <span class="math inline">\(x\)</span> 并获取其结果 <span class="math inline">\(y\)</span></strong></li>
<li><strong>使用新数据 <span class="math inline">\((x, y)\)</span>
更新代理函数，得到一个后验分布 (作为下一步的先验分布)</strong></li>
<li>重复2-4步，直到达到最大迭代次数</li>
</ol>
<p>举个例子, 如图所示, 一开始只有两个点 <span class="math inline">\((\mathrm{t}=2)\)</span>,
代理函数的分布是紫色的区域那块, 然后根据代理函数算出一
个采集函数（绿色线), 取采集函数的最大值所在的 <span class="math inline">\(x\)</span> (红色三角处), 算出 <span class="math inline">\(y\)</span>, 然后根据新的点 <span class="math inline">\((x, y)\)</span> 更新 代理函数和采集函数 <span class="math inline">\((\mathrm{t}=3)\)</span>
，继续重复上面步骤，选择新的采集函数最大值所在的 <span class="math inline">\(x\)</span>, 算出 <span class="math inline">\(y\)</span>, 再更新代理函 数和采集函数,
然后继续迭代。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304232211302.jpg" alt="img" style="zoom: 67%;"></p>
<p>问题的核心就在于代理函数和采集函数如何构建，常用的代理函数有：</p>
<ol type="1">
<li><strong>高斯过程（Gaussian processes）</strong></li>
<li><strong>Tree Parzer Estimator</strong></li>
<li><strong>概率随机森林：针对类别型变量</strong></li>
</ol>
<p>采集函数则需要兼顾两方面的性质：</p>
<ol type="1">
<li>利用当前已开发的区域（Exploitation）：即在当前最小值附近继续搜索</li>
<li>探索尚未开发的区域（Exploration）：即在还没有搜索过的区域里面搜索，可能那里才是全局最优解</li>
</ol>
<p><strong>常用的采集函数有：</strong></p>
<ol type="1">
<li>Probability of improvement（PI）</li>
<li>Expected improvement（EI）</li>
<li>Confidence bound criteria，包括LCB和UCB</li>
</ol>
<h4><span id="34-hyperopt">3.4 Hyperopt</span></h4>
<p>Hyperopt 是一个强大的 Python 库，用于超参数优化，由 jamesbergstra
开发。Hyperopt
使用贝叶斯优化的形式进行参数调整，允许你为给定模型获得最佳参数。它可以在大范围内优化具有数百个参数的模型。</p>
<h3><span id="参考文献">参考文献</span></h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/390373572"><em>贝叶斯优化</em>(原理+代码解读)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27916208">LightGBM调参指南(带贝叶斯优化代码)</a></p>
<ul>
<li>贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息</li>
<li>贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸</li>
<li>贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部最优</li>
</ul>
<h4><span id="可用的贝叶斯优化框架">可用的贝叶斯优化框架</span></h4>
<ol type="1">
<li>BayesianOptimization：<a href="https://link.zhihu.com/?target=https%3A//github.com/fmfn/BayesianOptimization">https://github.com/fmfn/BayesianOptimization</a></li>
<li>清华开源的openbox：<a href="https://link.zhihu.com/?target=https%3A//open-box.readthedocs.io/zh_CN/latest/index.html">https://open-box.readthedocs.io/zh_CN/latest/index.html</a></li>
<li>华为开源的HEBO：<a href="https://link.zhihu.com/?target=https%3A//github.com/huawei-noah/HEBO">https://github.com/huawei-noah/HEBO</a></li>
<li><strong>Hyperopt</strong>：<a href="https://link.zhihu.com/?target=http%3A//hyperopt.github.io/hyperopt/">http://hyperopt.github.io/hype</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/31DSKYD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/31DSKYD/" class="post-title-link" itemprop="url">Python常见问题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-18 15:16:22" itemprop="dateCreated datePublished" datetime="2022-03-18T15:16:22+08:00">2022-03-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 16:36:24" itemprop="dateModified" datetime="2023-04-26T16:36:24+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">【draft】工程</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E3%80%90draft%E3%80%91%E5%B7%A5%E7%A8%8B/%E6%B5%81%E7%A8%8B%E7%9A%84Python/" itemprop="url" rel="index"><span itemprop="name">流程的Python</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="python-面试考点">Python-面试考点</span></h2>
<ul>
<li>运行可视化网站： https://pythontutor.com/</li>
<li>吐血总结！40道Python面试题集锦（附答案） - 幽默的程序猿日常的文章 -
知乎 https://zhuanlan.zhihu.com/p/366679675</li>
</ul>
<h3><span id="1基础知识">1.基础知识</span></h3>
<h4><span id="a基础要求">a.基础要求</span></h4>
<ul>
<li><p><strong>参数中的 * 和 </strong> 的作用（要求：基础知识）**</p>
<blockquote>
<p>python
从参数定位到<strong>仅限关键字参数</strong>：https://blog.csdn.net/littleRpl/article/details/89457557</p>
<p>参数中*以及**的作用：https://blog.csdn.net/weixin_41978699/article/details/121008512</p>
</blockquote>
<p>*参数：将任何剩余的参数都以元组的方式传入这个可变参数。允许省略可变参数的函数名</p>
<p>**参数：会将所有参数放入一个<code>dict</code>供函数使用</p></li>
<li><p><strong>选代器（iterator）和生成器（generator)
(要求：基础知识）</strong></p>
<ul>
<li></li>
</ul></li>
<li><p><strong>str和unicode的区别（python2下）（要求：基础知识）</strong></p></li>
<li><p><strong>装饰器用途（要求：基础知识）</strong></p></li>
<li><p><strong>is和＝＝的区别（要求：基础知识）</strong></p>
<ul>
<li>== 符号比较的是两个对象的值，is 比较的是两个的对象的标识【id()函数 ,
Cpython中内存地址】</li>
<li>x is None</li>
<li>is 符号比 == 运算快【不能重载】，== 是语法糖，【a.__ eq __(b)】</li>
</ul></li>
<li><p>**双下划线开头的魔法方法，比如＿add__（至少知道一个，以及其作用）**</p></li>
<li><p><strong>字符串编码：ascii/unicode/utf-8
分别是什么，以及三者的区别（要求：要求至少能回答一个）</strong></p></li>
<li><p><strong>内置数据结构</strong></p>
<ul>
<li><p>dict (要求：至少知道底层实现是基于hash表，可以扩展提问）</p></li>
<li><p>list (要求：至少知道底层实现类似链表，可以扩展提问）</p></li>
<li><p>tuple(要求：知道其与list(异同）</p>
<ul>
<li><strong>元组的相对不可变形</strong>：标识不变，值可变</li>
<li></li>
</ul></li>
<li><p>set(要求：知道使用场景）</p></li>
</ul></li>
</ul>
<h4><span id="b进阶要求">b.进阶要求</span></h4>
<ul>
<li>yield / yield from的区别（要求：可选）</li>
</ul>
<p>yield只是将普通函数变成生成器，yield一个值，迭代时可以得到一个值；而yield
from是将后面的值变成一个可迭代对象。</p>
<ul>
<li><p>python 性能优化（要求：基础知识）</p>
<ul>
<li><p>python为什么慢？</p>
<ul>
<li><p>python是动态语言</p></li>
<li><p>python是解释执行</p></li>
<li><p>python中一切都是对象</p></li>
<li><p>python GIL</p></li>
<li><p><strong>垃圾回收</strong></p></li>
</ul></li>
</ul></li>
<li><p>python
模块查找顺序（要求：发挥空间比较大，junior岗位至少知道基本知识）</p></li>
<li><p>python
字符串编码问题为什么这么复杂（要求：非必须，发挥空间较大，可以引导被面者深入牌答，以及与其他语言的对比）</p></li>
</ul>
<h3><span id="2-正则表达式">2 正则表达式</span></h3>
<h4><span id="a基础要求">a.基础要求</span></h4>
<p>能够根据实际需求，实现中等难度的正则表达式</p>
<h4><span id="b进阶要求">b.进阶要求</span></h4>
<p>贪婪与非贫要模式（要求：非junior岗位需要掌握）
捕获与命名捕获（要求：非junior岗位需要掌握）</p>
<h3><span id="3-常见模块使用待补充完善">3 常见模块使用（待补充完善）</span></h3>
<h4><span id="a基础要求">a.基础要求：</span></h4>
<p>能够熟练应用基础python模块，如string,logging,json,collections(熟悉其中一个）</p>
<h4><span id="b进阶要求">b.进阶要求：</span></h4>
<p>1requests,itertools,multiprocessing等</p>
<h3><span id="4垃圾回收">==4.垃圾回收==</span></h3>
<h4><span id="a基础要求">a.基础要求：</span></h4>
<p>垃圾回收过程（要求：必须目产出<strong>引用计数</strong>，其他非必须，发挥里间级大，可以引导面试者深
入回答） 引用计数的缺点（要求：必須掌握）</p>
<h4><span id="b进阶要求">b.进阶要求：</span></h4>
<p>分线回收</p>
<hr>
<h2><span id="pythonqampa">PythonQ&amp;A</span></h2>
<blockquote>
<p>https://blog.csdn.net/u013486414/article/details/119701505</p>
</blockquote>
<h4><span id="代码效率优化">代码效率优化？</span></h4>
<ul>
<li><h5><span id="尽量使用python内置函数">尽量使用python内置函数</span></h5></li>
<li><h5><span id="字符串拼接使用python的标准式">字符串拼接使用python的标准式</span></h5></li>
<li><h5><span id="需要单次遍历的迭代的数组采用生成器替代惰性计算">需要单次遍历的迭代的数组采用生成器替代【惰性计算】</span></h5></li>
<li><h5><span id="if-x代替if-xtrue">if x代替if x==True</span></h5></li>
</ul>
<h4><span id="什么是duck-type">什么是duck type?</span></h4>
<p>鸭子类型更关注对象的行为，只要实现了某种接口方法就行，而不在乎是什么类型（比如说定义了
__iter__魔法方法的类实例对象都可以用for来迭代）</p>
<h4><span id="py3和py2的区别">py3和py2的区别</span></h4>
<ul>
<li>print在py3里是一个函数，在py2里只是一个关键字</li>
<li>py3文件的默认编码是utf8，py2文件的默认编码是ascii</li>
<li>py3的str是unicode字符串，而py2的str是bytes</li>
<li>py3的range()返回一个可迭代对象，py2的
range()返回一个列表，xrange()返回一个可迭代对象,</li>
<li>py3的除法返回float，py2的除法返回int</li>
</ul>
<h4><span id="可变对象与不可变对象">可变对象与不可变对象</span></h4>
<ul>
<li><strong>可变对象</strong>: list，dict，set</li>
<li><strong>不可变对象</strong>: bool，int，float，tuple，str,
frozenset</li>
</ul>
<h4><span id="可哈希和不可哈希对象">可哈希和不可哈希对象</span></h4>
<h4><span id="什么时候需要捕获异常">什么时候需要捕获异常?</span></h4>
<ul>
<li>Django的ORM框架操作数据库时，获取数据，更新数据等都有可能会异常</li>
<li>socket通信时，recv()方法可能会因为对方突然中断连接导致异常</li>
</ul>
<h4><span id="什么是cpython-gil">什么是CPython GIL?</span></h4>
<p>GIL，Global Interpreter
Lock，即<strong>全局解释器锁</strong>，引入GIL是因为CPython的<strong>内存管理并不是线程安全的,</strong>为了保护多线程下对python对象的访问，每个线程在执行过程中都需要先获取GIL，保证同一时刻只有一个线程在执行代码，GIL使得python的多线程不能充分发挥多核CPU的性能，对CPU密集型程序的影响较大。</p>
<h4><span id="什么是生成器">什么是生成器?</span></h4>
<p>生成器是一种可迭代对象，可以挂起并保持当前的状态</p>
<p>生成器遇到yield处会停止执行，调用next()或send()才会继续执行</p>
<p>定义一个生成器有两种方式，一种是<strong>生成器推导式</strong>，一种是在普通函数中添加<strong>yield语句并实例化</strong></p>
<h4><span id="浅拷贝和深拷贝">浅拷贝和深拷贝</span></h4>
<p>浅拷贝出来的是一个独立的对象，但<strong>它的子对象还是原对象中的子对象</strong></p>
<p><strong>深拷贝会递归地拷贝原对象中的每一个子对象，因此拷贝后的对象和原对象互不相关。</strong></p>
<h4><span id="迭代器与可迭代对象的区别">迭代器与可迭代对象的区别</span></h4>
<p>可迭代对象类，必须自定义__iter__()魔法方法，range，list类的实例化对象都是可迭代对象</p>
<p>迭代器类，必须自定义__iter__()和__next__()魔法方法，用iter()函数可以创建可迭代对象的迭代器</p>
<h4><span id="闭包">闭包</span></h4>
<p>闭包就是一个嵌套函数，它的内部函数 使用
外部函数的变量或参数,它的外部函数返回了内部函数</p>
<p>可以保存外部函数内的变量，不会随着外部函数调用完而销毁。</p>
<h4><span id="python垃圾回收机制">python垃圾回收机制</span></h4>
<p><strong>引用计数</strong>为主，<strong>标记清除</strong>和<strong>分代回收</strong>为辅</p>
<p>引用计数机制是这样的：</p>
<ul>
<li><p>当对象被创建，被引用，作为参数传递，存储到容器中，引用计数+1</p></li>
<li><p>当对象离开作用域，引用指向别的对象，del，从容器中移除，引用计数-1</p></li>
<li><p>当引用计数降为0，python就会自动回收该对象所在的内存空间，</p></li>
<li><p>但是引用计数无法<strong>解决循环引用</strong>的问题，所以引入了<strong>标记清除</strong>和<strong>分代回收机制</strong></p></li>
</ul>
<h4><span id="async和await的作用">async和await的作用</span></h4>
<p>async: 声明一个函数为异步函数，函数内只要有await就要声明为async</p>
<p>await:
搭配asyncio.sleep()时会切换协程，当切换回来后再继续执行下面的语句</p>
<h4><span id="内置的数据结构和算法">内置的数据结构和算法</span></h4>
<ul>
<li>内置数据结构: list，dict，tuple，set</li>
<li>内置算法: sorted，max</li>
</ul>
<h4><span id="collections模块">collections模块</span></h4>
<p>collections模块提供了一些好用的容器数据类型，其中常用的有:
namedtuple，<strong>deque</strong>，<strong>Counter</strong>，<strong>OrderedDict，defaultdict</strong></p>
<h4><span id="为什么dict查找的时间复杂度是o1">为什么dict查找的时间复杂度是O(1)?</span></h4>
<p>dict底层是哈希表，哈希表类似于C语言的数组，可以实现按索引随机访问</p>
<p>但dict的key不一定是整数，需要先通过哈希函数，再经过取余操作转换为索引</p>
<h4><span id="list-tuple的底层结构">list tuple的底层结构</span></h4>
<p>list和tuple底层都是顺序表结构</p>
<p>list底层是可变数组，数组里存放的是元素对象的指针</p>
<h4><span id="set的底层结构">set的底层结构</span></h4>
<p>哈希表，key就是元素，value都是空</p>
<h4><span id="class方法-和static方法的区别">class方法 和
static方法的区别</span></h4>
<p>class方法的第一个参数是cls，可以访问类属性，类方法</p>
<p>static方法和普通函数一样，只不过是放在类里，要通过类或实例来调用，但是它不能访问类和实例的属性和方法</p>
<h4><span id="什么是装饰器">什么是装饰器?</span></h4>
<p>装饰器是一个接收函数作为参数的闭包函数</p>
<p>它可以在不修改函数内部源代码的情况下，给函数添加额外的功能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_time</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>():</span><br><span class="line">        t1 = time.time()</span><br><span class="line">        func()</span><br><span class="line">        t2 = time.time()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;cost time: &#123;&#125;s&#x27;</span>.<span class="built_in">format</span>(t2-t1))</span><br><span class="line">    <span class="keyword">return</span> inner</span><br></pre></td></tr></table></figure>
<h4><span id="什么是元类-使用场景">什么是元类? 使用场景</span></h4>
<p><strong>元类是创建类的类</strong>，<strong>type还有继承自type的类都是元类</strong></p>
<p><strong>作用</strong>: 在类定义时（new, init）和 类实例化时(call)
可以添加自定义的功能</p>
<p><strong>使用场景</strong>:
ORM框架中创建一个类就代表数据库中的一个表，但是定义这个类时为了统一需要把里面的类属性全部改为小写，这个时候就要用元类重写new方法，把attrs字典里的key转为小写</p>
<h4><span id="python局部变量">Python局部变量</span></h4>
<p>python中list作为全局变量无需global声明的原因，则不会有歧义。它是“明确的”，因为如果把b当作是局部变量的话，它会报KeyError，所以它只能是引用全局的b,故不需要多此一举显式声明global。</p>
<h4><span id="python-读取大文件">Python 读取大文件？</span></h4>
<p>最近无论是面试还是笔试，有一个高频问题始终阴魂不散，那就是给一个大文件，至少超过10g,在内存有限的情况下（低于2g），该以什么姿势读它？</p>
<p><strong>一般：</strong></p>
<ul>
<li>with 上下文管理器会自动关闭打开的文件描述符</li>
<li>在迭代文件对象时，内容是一行一行返回的，不会占用太多内存</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">retrun_count</span>(<span class="params">fname</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算文件有多少行</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname) <span class="keyword">as</span> file: </span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure>
<p><strong>更底层：</strong> fp.read()
<code>iter(partial(file.read, block_size), '')</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">chunked_file_reader</span>(<span class="params">fp, block_size=<span class="number">1024</span> * <span class="number">8</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成器函数：分块读取文件内容</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        chunk = fp.read(block_size)</span><br><span class="line">        <span class="comment"># 当文件没有更多内容时，read 调用将会返回空字符串 &#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> chunk:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">yield</span> chunk</span><br><span class="line"><span class="comment"># iter()优化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chunked_file_reader</span>(<span class="params">file, block_size=<span class="number">1024</span> * <span class="number">8</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成器函数：分块读取文件内容，使用 iter 函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 首先使用 partial(fp.read, block_size) 构造一个新的无需参数的函数</span></span><br><span class="line">    <span class="comment"># 循环将不断返回 fp.read(block_size) 调用结果，直到其为 &#x27;&#x27; 时终止</span></span><br><span class="line">    <span class="keyword">for</span> chunk <span class="keyword">in</span> <span class="built_in">iter</span>(partial(file.read, block_size), <span class="string">&#x27;&#x27;</span>):</span><br><span class="line">        <span class="keyword">yield</span> chunk</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">return_count_v3</span>(<span class="params">fname</span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> chunk <span class="keyword">in</span> chunked_file_reader(fp):</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure>
<h4><span id="pandas分批读取大数据集">pandas分批读取大数据集?</span></h4>
<p>pandas 的 chunksize 读取</p>
<h4><span id="python可变数据类型-不可变数据类型">Python可变数据类型 不可变数据类型？</span></h4>
<p><strong>不可变类型</strong>：数值型、字符串型string和元组tuple；不允许变量的值发生变化，如果改变了变量的值，相当于是新建了一个对象。</p>
<p><strong>可变数据类型</strong>：列表list和字典dict，Set集合</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1M5VH4X/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1M5VH4X/" class="post-title-link" itemprop="url">深度学习（6）LSTM*</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:45:12" itemprop="dateCreated datePublished" datetime="2022-03-16T21:45:12+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-13 22:55:53" itemprop="dateModified" datetime="2022-07-13T22:55:53+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN/" itemprop="url" rel="index"><span itemprop="name">RNN</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="lstm和gru算法简单梳理">LSTM和GRU算法简单梳理🍭</span></h2>
<h3><span id="前言-从反向传播推导到梯度消失and爆炸的原因及解决方案"><font color="red">
前言 - 从反向传播推导到梯度消失and爆炸的原因及解决方案？</font></span></h3>
<blockquote>
<ul>
<li>从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导）
- 韦伟的文章 - 知乎 https://zhuanlan.zhihu.com/p/76772734</li>
</ul>
<p><strong>本质上</strong>是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。其实<strong>梯度爆炸和梯度消失问题都是因为网络太深</strong>，网络权值更新不稳定造成的，本质上是因为<strong>梯度反向传播中的连乘效应</strong>。</p>
</blockquote>
<h3><span id="一-反向传播推导到梯度消失and爆炸的原因及解决方案">一、反向传播推导到梯度消失and爆炸的原因及解决方案</span></h3>
<h4><span id="11-反向传播推导">1.1 ==反向传播推导：==</span></h4>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>以上图为例开始推起来，先说明几点，i1，i2是输入节点，h1，h2为隐藏层节点，o1，o2为输出层节点，除了输入层，其他两层的节点结构为下图所示：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>举例说明，<img src="https://www.zhihu.com/equation?tex=NET_%7Bo1%7D" alt="[公式]"> 为输出层的输入，也就是隐藏层的输出经过线性变换后的值，
<img src="https://www.zhihu.com/equation?tex=OUT_%7Bo1%7D" alt="[公式]"> 为经过激活函数sigmoid后的值；同理 <img src="https://www.zhihu.com/equation?tex=NET_%7Bh1%7D" alt="[公式]">
为隐藏层的输入，也就是输入层经过线性变换后的值， <img src="https://www.zhihu.com/equation?tex=OUT_%7Bh1%7D" alt="[公式]">
为经过激活函数sigmoid 的值。只有这两层有激活函数，输入层没有。</p>
<blockquote>
<p><strong>定义一下sigmoid的函数：</strong> <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29+%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D" alt="[公式]"> <strong>说一下sigmoid的求导：</strong></p>
</blockquote>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csigma%5E%7B%5Cprime%7D%28z%29+%26%3D%5Cleft%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D%5Cright%29%5E%7B%5Cprime%7D+%5C%5C+%26%3D%5Cfrac%7Be%5E%7B-z%7D%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B1%2Be%5E%7B-z%7D-1%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Cfrac%7B%5Csigma%28z%29%7D%7B%5Cleft%281%2Be%5E%7B-z%7D%5Cright%29%5E%7B2%7D%7D+%5C%5C+%26%3D%5Csigma%28z%29%281-%5Csigma%28z%29%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>定义一下损失函数，这里的损失函数是均方误差函数，即：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D%5Csum+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget+-+output%7D%29%5E%7B2%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>具体到上图，就是：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss_%7Bt+o+t+a+l%7D%3D+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget1+-+out_o1%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28%5Ctext+%7Btarget2+-+out_o2%7D%29%5E%7B2%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>到这里，所有前提就交代清楚了，前向传播就不推了，默认大家都会，下面推反向传播。</p>
<ul>
<li><strong>第一个反向传播（热身）</strong></li>
</ul>
<p>先来一个简单的热热身，求一下损失函数对W5的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D" alt="[公式]"></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>首先根据链式求导法则写出对W5求偏导的总公式，再把图拿下来对照（如上），可以看出，需要计算三部分的求导【损失函数、激活函数、线性函数】，下面就一步一步来：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_%7B5%7D%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B2%7D%28%7Btarget_1+-+out_%7Bo1%7D%7D%29%5E%7B2%7D%2B+%5Cfrac%7B1%7D%7B2%7D%28+%7Btarget_2+-+out_%7Bo2%7D%7D%29%5E%7B2%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bo1%7D%7D%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+w_5%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+w_5%7D+%3Dout_%7Bh_1%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>综上三个步骤，得到总公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_5%7D+%3D+%28out_%7Bo1%7D+-+target_1%29+%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+out_%7Bh_1%7D++%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<ul>
<li><strong>第二个反向传播：</strong></li>
</ul>
<p>接下来，要求损失函数对w1的偏导，即： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D" alt="[公式]"></p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-cbcb60d90cbd259a717cbe991aa93f5c_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-7c0b41fdbd084ed875480516967857ed_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>还是把图摆在这，方便看，先写出总公式，对w1求导有个地方要注意，w1的影响不仅来自o1还来自o2，从图上可以一目了然，所以总公式为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E6%80%BB%E5%85%AC%E5%BC%8F%EF%BC%9A%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D%2B%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+out_%7Bo2%7D%7D%7B%5Cpartial+net_%7Bo2%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bo2%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D++%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>所以总共分为左右两个式子，分别又对应5个步骤，详细写一下左边，右边同理：</p>
<p><img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+out_%7Bo1%7D+-+target_1%5C%5C" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29+%5C%5C" alt="[公式]"></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+out_%7Bh1%7Dw_5%2Bout_%7Bh2%7Dw_6%7D%7B%5Cpartial+out_%7Bh1%7D%7D+%3Dw_5%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+out_%7Bh1%7D%7D%7B%5Cpartial+net_%7Bh1%7D%7D+%3D+%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A+%5Cfrac%7B%5Cpartial+net_%7Bh1%7D%7D%7B%5Cpartial+w_1%7D+%3D+%5Cfrac%7B%5Cpartial+i_1w_1%2Bi_2w_2%7D%7B%5Cpartial+w_1%7D+%3Di_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>右边也是同理，就不详细写了，写一下总的公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D++%26%3D%5Cleft%28%28out_%7Bo1%7D+-+target_1%29%5Ccdot+%28%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29%29%5Ccdot+w_5+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29+%5C%5C+%26%2B%7B%5Cleft%28%28out_%7Bo2%7D+-+target_2%29%5Ccdot+%28%5Csigma%28net_%7Bo2%7D%29%281-%5Csigma%28net_%7Bo2%7D%29%29%29%5Ccdot+w_7+%5Ccdot+%28%5Csigma%28net_%7Bh1%7D%29%281-%5Csigma%28net_%7Bh1%7D%29%29%29%5Ccdot+i_1+%5Cright%29%7D+%5C%5C+%5Cend%7Baligned%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>这个公式只是对如此简单的一个网络结构的一个节点的偏导，就这么复杂。。亲自推完才深深的意识到。。。</p>
<p>为了后面描述方便，把上面的公式化简一下， <img src="https://www.zhihu.com/equation?tex=out_%7Bo1%7D+-+target_1" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]"> ，
<img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%281-%5Csigma%28net_%7Bo1%7D%29%29" alt="[公式]"> 记为 <img src="https://www.zhihu.com/equation?tex=%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D" alt="[公式]"> ，则：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Loss_%7Bt+o+t+a+l%7D%7D%7B%5Cpartial+w_1%7D+%3D+C_%7Bo1%7D+%5Ccdot+%5Csigma%28net_%7Bo1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_5+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1+%2B+C_%7Bo2%7D+%5Ccdot+%5Csigma%28net_%7Bo2%7D%29%5E%7B%5Cprime%7D+%5Ccdot+w_7+%5Ccdot+%5Csigma%28net_%7Bh1%7D%29%5E%7B%5Cprime%7D+%5Ccdot+i_1%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="12梯度消失爆炸产生原因">1.2
<strong>==梯度消失，爆炸产生原因：==</strong></span></h4>
<p>从上式其实已经能看出来，求和操作其实不影响，主要是是看乘法操作就可以说明问题，可以看出，损失函数对w1的偏导，与
<img src="https://www.zhihu.com/equation?tex=C_%7Bo1%7D" alt="[公式]">
，权重w，sigmoid的导数有关，明明还有输入i为什么不提？因为如果是多层神经网络的中间某层的某个节点，那么就没有输入什么事了。所以产生影响的就是刚刚提的三个因素。</p>
<p>再详细点描述，如图，多层神经网络：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-0f2ded75fbecc449a25bfd58b8c58d35_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25631496">PENG：神经网络训练中的梯度消失与梯度爆炸282
赞同 · 26 评论文章</a></p>
<p>假设（假设每一层只有一个神经元且对于每一层 <img src="https://www.zhihu.com/equation?tex=y_i%3D%5Csigma%5Cleft%28z_i%5Cright%29%3D%5Csigma%5Cleft%28w_ix_i%2Bb_i%5Cright%29" alt="[公式]">，其中<img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]">为sigmoid函数），如图：</p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-ea9beb6c28c7d4e89be89dc5f4cbae2e_1440w.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>则：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+y_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+y_%7B4%7D%7D%7B%5Cpartial+z_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B4%7D%7D%7B%5Cpartial+x_%7B4%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B4%7D%7D%7B%5Cpartial+z_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B3%7D%7D%7B%5Cpartial+x_%7B3%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B3%7D%7D%7B%5Cpartial+z_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B2%7D%7D%7B%5Cpartial+x_%7B2%7D%7D+%5Cfrac%7B%5Cpartial+x_%7B2%7D%7D%7B%5Cpartial+z_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+z_%7B1%7D%7D%7B%5Cpartial+b_%7B1%7D%7D%7D+%5C%5C+%7B%3DC_%7By4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B4%7D%5Cright%29+w_%7B4%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B3%7D%5Cright%29+w_%7B3%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B2%7D%5Cright%29+w_%7B2%7D+%5Csigma%5E%7B%5Cprime%7D%5Cleft%28z_%7B1%7D%5Cright%29%7D%5Cend%7Barray%7D+%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>看一下sigmoid函数的求导之后的样子：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-208a4aa5dc657fe86919f3549d853793_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>发现sigmoid函数求导后最大最大也只能是0.25。</strong></p>
<p>再来看W，一般我们初始化权重参数W时，通常都小于1，用的最多的应该是0，1正态分布吧。</p>
<p><font color="red"><strong>所以 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq0.25" alt="[公式]">
，多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说几乎不更新，这就是梯度消失的根本原因。</strong></font></p>
<p>再来看看<strong>梯度爆炸</strong>的原因，也就是说如果 <img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cgeq1" alt="[公式]">
时，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。sigmoid的函数是不可能大于1了，上图看的很清楚，那只能是w了，这也就是经常看到别人博客里的一句话，初始权重过大，一直不理解为啥。。现在明白了。</p>
<p>但梯度爆炸的情况一般不会发生，对于sigmoid函数来说， <img src="https://www.zhihu.com/equation?tex=%5Csigma%28z%29%5E%7B%5Cprime%7D" alt="[公式]"> 的大小也与w有关，因为 <img src="https://www.zhihu.com/equation?tex=z%3Dwx%2Bb" alt="[公式]">
，除非该层的输入值<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]">在一直一个比较小的范围内。</p>
<p>其实<strong>梯度爆炸和梯度消失问题都是因为网络太深</strong>，网络权值更新不稳定造成的，本质上是因为<strong>梯度反向传播中的连乘效应</strong>。</p>
<p>==<strong>所以，总结一下，为什么会发生梯度爆炸和消失：</strong>==</p>
<blockquote>
<p>本质上是因为神经网络的更新方法，梯度消失是因为反向传播过程中对梯度的求解会产生sigmoid导数和参数的连乘，sigmoid导数的最大值为0.25，权重一般初始都在0，1之间，乘积小于1，多层的话就会有多个小于1的值连乘，导致靠近输入层的梯度几乎为0，得不到更新。梯度爆炸是也是同样的原因，只是如果初始权重大于1，或者更大一些，多个大于1的值连乘，将会很大或溢出，导致梯度更新过大，模型无法收敛。</p>
</blockquote>
<h3><span id="13-梯度消失-爆炸解决方案">1.3 梯度消失、爆炸解决方案？</span></h3>
<blockquote>
<p><strong>参考：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33006526">DoubleV：详解深度学习中的梯度消失、爆炸原因及其解决方法</a></p>
<ul>
<li>预训练加微调</li>
<li>梯度剪切、正则</li>
</ul>
</blockquote>
<h4><span id="解决方案一预训练加微调"><strong>解决方案一（预训练加微调）：</strong></span></h4>
<p>提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（<strong>fine-tunning</strong>）。</p>
<p>Hinton在训练深度信念网络（Deep Belief
Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p>
<h4><span id="解决方案二梯度剪切-正则"><strong>解决方案二（梯度剪切、正则）：</strong></span></h4>
<p><strong>梯度剪切</strong>这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p>
<p><strong>正则化</strong>是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Loss%3D%28y-W%5ETx%29%5E2%2B+%5Calpha+%7C%7CW%7C%7C%5E2%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]">
是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</p>
<p>注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些</p>
<h4><span id="解决方案三改变激活函数"><strong><font color="red">
解决方案三（改变激活函数）：</font></strong></span></h4>
<p>首先说明一点，<strong>tanh激活函数不能有效的改善这个问题</strong>，先来看tanh的形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Ctanh+%28x%29%3D%5Cfrac%7Be%5E%7Bx%7D-e%5E%7B-x%7D%7D%7Be%5E%7Bx%7D%2Be%5E%7B-x%7D%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>再来看tanh的导数图像：</p>
<p><img src="https://pic4.zhimg.com/80/v2-66a7e4fcf11a2d85c15e7bf7b88b2d1b_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p><strong>发现虽然比sigmoid的好一点，sigmoid的最大值小于0.25，tanh的最大值小于1，但仍是小于1的，所以并不能解决这个问题。</strong></p>
<p><strong>Relu</strong>:思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Coperatorname%7BRe%7D+%5Coperatorname%7Blu%7D%28%5Cmathrm%7Bx%7D%29%3D%5Cmax+%28%5Cmathrm%7Bx%7D%2C+0%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D%7B0%2C+x%3C0%7D+%5C%5C+%7Bx%2C+x%3E0%7D%5Cend%7Barray%7D%5Cright%5C%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><img src="https://pic2.zhimg.com/80/v2-55475ee2d90cd7257a39f62549a65769_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>从上图中，我们可以很容易看出，<strong>relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</strong></p>
<p><strong>relu</strong>的主要贡献在于：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练</li>
</ul>
<p>同时也存在一些<strong>缺点</strong>：</p>
<ul>
<li><strong>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</strong></li>
<li>输出不是以0为中心的</li>
</ul>
<p><strong>leakrelu</strong></p>
<p>leakrelu就是为了解决relu的0区间带来的影响，其数学表达为： <img src="https://www.zhihu.com/equation?tex=leakrelu%3D%5Cbegin%7Bequation%7D+f%28x%29%3D+%5Cbegin%7Bcases%7D+x%2C+%26+%7Bx%5Cgt+0%7D+%5C%5C%5C%5C+x%2Ak%2C+%26+%7Bx%5Cleq+0%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]">
其中k是leak系数，一般选择0.1或者0.2，或者通过学习而来解决死神经元的问题。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3ab1bd8fb85542a0c85eb907b73fa327_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>leakrelu解决了0区间带来的影响，而且包含了relu的所有优点</p>
<p><strong>elu</strong></p>
<p>elu激活函数也是为了解决relu的0区间带来的影响，其数学表达为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7Bx%2C%7D+%26+%7B%5Ctext+%7B+if+%7D+x%3E0%7D+%5C%5C+%7B%5Calpha%5Cleft%28e%5E%7Bx%7D-1%5Cright%29%2C%7D+%26+%7B%5Ctext+%7B+otherwise+%7D%7D%5Cend%7Barray%7D%5Cright.%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其函数及其导数数学形式为：</p>
<p><img src="https://pic3.zhimg.com/80/v2-ec3c80e51129bd76d49cad6e52d449c2_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>但是elu相对于leakrelu来说，计算要更耗时间一些，因为有e。</p>
<h4><span id="解决方案四batchnorm梯度消失"><strong>解决方案四（batchnorm）：</strong>【梯度消失】</span></h4>
<p><strong>Batchnorm</strong>是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch
normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。</p>
<p>具体的batchnorm原理非常复杂，在这里不做详细展开，此部分大概讲一下batchnorm解决梯度的问题上。具体来说就是反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：
正向传播中<img src="https://www.zhihu.com/equation?tex=f_3%3Df_2%28w%5ET%2Ax%2Bb%29" alt="[公式]">，那么反向传播中，<img src="https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Cpartial+f_2%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+f_1%7Dw" alt="[公式]">，反向传播式子中有w的存在，所以<img src="https://www.zhihu.com/equation?tex=w" alt="[公式]">的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出做scale和shift的方法，通过一定的规范化手段，<strong>把每层神经网络任意神经元这个输入值的分布【假设原始是正态分布】强行拉回到接近均值为0方差为1的标准正太分布，即严重偏离的分布强制拉回比较标准的分布，<font color="red">
这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生</font>，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<h4><span id="解决方案五残差结构"><strong><font color="red">
解决方案五（残差结构）：</font></strong></span></h4>
<p><img src="https://pic4.zhimg.com/80/v2-3134d24348c47ca2001d37fef1c3f8bf_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>如图，把输入加入到某层中，这样求导时，总会有个1在，这样就不会梯度消失了。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot+%5Cfrac%7B%5Cpartial+x_%7BL%7D%7D%7B%5Cpartial+x_%7Bl%7D%7D%3D%5Cfrac%7B%5Cpartial+%5Coperatorname%7Bloss%7D%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Ccdot%5Cleft%281%2B%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7BL%7D%7D+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D+F%5Cleft%28x_%7Bi%7D%2C+W_%7Bi%7D%5Cright%29%5Cright%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>式子的第一个因子 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D" alt="[公式]"> 表示的损失函数到达 L
的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p>
<p><code>注：上面的推导并不是严格的证</code>，只为帮助理解</p>
<p>==<strong>解决方案六（LSTM）：</strong>==</p>
<p>在介绍这个方案之前，有必要来推导一下RNN的反向传播，<strong>因为关于梯度消失的含义它跟DNN不一样！不一样！不一样！</strong></p>
<p>先推导再来说，从这copy的：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28687529">沉默中的思索：RNN梯度消失和爆炸的原因565
赞同</a></p>
<p>RNN结构如图：</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-ab844e07a86f910d2852198c3117ddb7_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>假设我们的时间序列只有三段， <img src="https://www.zhihu.com/equation?tex=S_%7B0%7D" alt="[公式]">
为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下： <img src="https://www.zhihu.com/equation?tex=S_%7B1%7D%3DW_%7Bx%7DX_%7B1%7D%2BW_%7Bs%7DS_%7B0%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B1%7D%3DW_%7Bo%7DS_%7B1%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B2%7D%3DW_%7Bx%7DX_%7B2%7D%2BW_%7Bs%7DS_%7B1%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B2%7D%3DW_%7Bo%7DS_%7B2%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7B3%7D%3DW_%7Bx%7DX_%7B3%7D%2BW_%7Bs%7DS_%7B2%7D%2Bb_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=O_%7B3%7D%3DW_%7Bo%7DS_%7B3%7D%2Bb_%7B2%7D" alt="[公式]"></p>
<p>假设在t=3时刻，损失函数为 <img src="https://www.zhihu.com/equation?tex=L_%7B3%7D%3D%5Cfrac%7B1%7D%7B2%7D%28Y_%7B3%7D-O_%7B3%7D%29%5E%7B2%7D" alt="[公式]"> 。</p>
<p>则对于一次训练任务的损失函数为 <img src="https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bt%3D0%7D%5E%7BT%7D%7BL_%7Bt%7D%7D" alt="[公式]"> ，即每一时刻损失值的累加。</p>
<p>使用随机梯度下降法训练RNN其实就是对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D+" alt="[公式]"> 、
<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]">
、 <img src="https://www.zhihu.com/equation?tex=W_%7Bo%7D" alt="[公式]"> 以及 <img src="https://www.zhihu.com/equation?tex=b_%7B1%7D" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=b_%7B2%7D" alt="[公式]">
求偏导，并不断调整它们以使L尽可能达到最小的过程。</p>
<p>现在假设我们我们的时间序列只有三段，t1，t2，t3。</p>
<p><strong>我们只对t3时刻的 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D%E3%80%81W_%7B0%7D" alt="[公式]"> 求偏导（其他时刻类似）：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7B0%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bo%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D%2B%5Cfrac%7B%5Cpartial%7BL_%7B3%7D%7D%7D%7B%5Cpartial%7BO_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B3%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B3%7D%7D%7D%7B%5Cpartial%7BS_%7B2%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B2%7D%7D%7D%7B%5Cpartial%7BS_%7B1%7D%7D%7D%5Cfrac%7B%5Cpartial%7BS_%7B1%7D%7D%7D%7B%5Cpartial%7BW_%7Bs%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>可以看出对于 <img src="https://www.zhihu.com/equation?tex=W_%7B0%7D" alt="[公式]">
求偏导并没有长期依赖，但是对于 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导，会随着时间序列产生长期依赖</strong>。因为 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]">
随着时间序列向前传播，而 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" alt="[公式]"> 又是
<img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]">的函数。</p>
<p>根据上述求偏导的过程，我们可以得出任意时刻对 <img src="https://www.zhihu.com/equation?tex=W_%7Bx%7D%E3%80%81W_%7Bs%7D" alt="[公式]"> 求偏导的公式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Csum_%7Bk%3D0%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%7B%5Cpartial%7BS_%7Bt%7D%7D%7D%7D%28%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D%29%5Cfrac%7B%5Cpartial%7BS_%7Bk%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>任意时刻对<img src="https://www.zhihu.com/equation?tex=W_%7Bs%7D" alt="[公式]"> 求偏导的公式同上。</p>
<p><font color="red"> 如果加上激活函数， <img src="https://www.zhihu.com/equation?tex=S_%7Bj%7D%3Dtanh%28W_%7Bx%7DX_%7Bj%7D%2BW_%7Bs%7DS_%7Bj-1%7D%2Bb_%7B1%7D%29" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D" alt="[公式]"> = <img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7Btanh%5E%7B%27%7D%7DW_%7Bs%7D" alt="[公式]">激活函数tanh和它的导数图像在上面已经说过了，所以原因在这就不赘述了，还是一样的，激活函数导数小于1。</font></p>
<blockquote>
<p>==<strong>现在来解释一下，为什么说RNN和DNN的梯度消失问题含义不一样？</strong>==</p>
<ol type="1">
<li><strong>先来说DNN中的反向传播：</strong>在上文的DNN反向传播中，我推导了两个权重的梯度，第一个梯度是直接连接着输出层的梯度，求解起来并没有梯度消失或爆炸的问题，因为它没有连乘，只需要计算一步。第二个梯度出现了连乘，也就是说越靠近输入层的权重，梯度消失或爆炸的问题越严重，可能就会消失会爆炸。<strong>一句话总结一下，DNN中各个权重的梯度是独立的，该消失的就会消失，不会消失的就不会消失。</strong></li>
<li><strong>再来说RNN：</strong>RNN的特殊性在于，它的权重是共享的。抛开W_o不谈，因为它在某时刻的梯度不会出现问题（某时刻并不依赖于前面的时刻），但是W_s和W_x就不一样了，每一时刻都由前面所有时刻共同决定，是一个相加的过程，这样的话就有个问题，当距离长了，计算最前面的导数时，最前面的导数就会消失或爆炸，但当前时刻整体的梯度并不会消失，因为它是求和的过程，当下的梯度总会在，只是前面的梯度没了，但是更新时，由于权值共享，所以整体的梯度还是会更新，<strong>通常人们所说的梯度消失就是指的这个，指的是当下梯度更新时，用不到前面的信息了，因为距离长了，前面的梯度就会消失，也就是没有前面的信息了，但要知道，整体的梯度并不会消失，因为当下的梯度还在，并没有消失。</strong></li>
<li><strong>一句话概括：</strong>RNN的梯度不会消失，RNN的梯度消失指的是当下梯度用不到前面的梯度了，但DNN靠近输入的权重的梯度是真的会消失。</li>
</ol>
</blockquote>
<p>说完了RNN的反向传播及梯度消失的含义，终于该说<strong>为什么LSTM可以解决这个问题了</strong>，这里默认大家都懂LSTM的结构，对结构不做过多的描述。<strong>见第三节</strong>。【LSTM通过它的“门控装置”有效的缓解了这个问题，这也就是为什么我们现在都在使用LSTM而非普通RNN。】</p>
<h3><span id="二-lstm-框架结构">二、LSTM 框架结构</span></h3>
<h4><span id="前言">前言：</span></h4>
<blockquote>
<p>LSTM是RNN的一种变体，更高级的RNN，那么它的本质还是一样的，还记得RNN的特点吗，<strong>可以有效的处理序列数据，</strong>当然LSTM也可以，还记得RNN是如何处理有效数据的吗，是不是<strong>每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息</strong>，如图，我们把存每一时刻信息的地方叫做Memory
Cell，中文就是记忆细胞，可以这么理解。</p>
<p><img src="https://pic3.zhimg.com/80/v2-a5590a65bf7a93bbaafc1a6b03cf3862_1440w.png" alt="img" style="zoom:50%;"></p>
<p><strong>RNN什么信息它都存下来，因为它没有挑选的能力，而LSTM不一样，它会选择性的存储信息，因为它能力强，它有门控装置，它可以尽情的选择。</strong>如下图，普通RNN只有中间的Memory
Cell用来存所有的信息，而从下图我们可以看到，<strong>LSTM多了三个Gate</strong>。</p>
<p><img src="https://pic4.zhimg.com/80/v2-5602237fa98e90614cea748aa6a8b6d3_1440w.jpg" alt="img" style="zoom:50%;"></p>
<ul>
<li><strong>Input
Gate</strong>：输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory
Cell。</li>
<li><strong>Output Gate</strong>：输出门，每一时刻是否有信息从Memory
Cell输出取决于这一道门。</li>
<li><strong>Forget Gate</strong>：遗忘门，每一时刻Memory
Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory
Cell里的值清除，也就是遗忘掉。</li>
</ul>
<p>在了解LSTM的内部结构之前，我们需要先回顾一下普通RNN的结构，以免在这里很多读者被搞懵，如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-acee3e085ee62fe162bcac5cd135b54c_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>我们可以看到，左边是为了简便描述RNN的工作原理而画的缩略图，右边是展开之后，每个时间点之间的流程图，<strong>注意，我们接下来看到的LSTM的结构图，是一个时间点上的内部结构，就是整个工作流程中的其中一个时间点，也就是如下图：</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-ccf7b5baee04f3f24bd04637df9bcd3a_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>注意，<strong>上图是普通RNN的一个时间点的内部结构</strong>，上面已经讲过了公式和原理，<strong>LSTM的内部结构更为复杂，不过如果这么类比来学习，我认为也没有那么难</strong>。</p>
<p><img src="https://pic2.zhimg.com/80/v2-1428c54d3ae79cf12616e7051c07799d_1440w.jpg" alt="img" style="zoom:50%;"></p>
<ul>
<li>Cell：memory cell，也就是一个记忆存储的地方，这里就类似于普通RNN的
<img src="https://www.zhihu.com/equation?tex=S_t" alt="[公式]">
，都是用来存储信息的，这里面的信息都会保存到下一时刻，其实标准的叫法应该是
<img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]">
，因为这里对应神经网络里的隐藏层，所以是hidden的缩写，无论普通RNN还是LSTM其实t时刻的记忆细胞里存的信息，都应该被称为
<img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"></li>
<li><img src="https://www.zhihu.com/equation?tex=a" alt="[公式]">
是这一时刻的输出，也就是类似于普通RNN里的 <img src="https://www.zhihu.com/equation?tex=O_t" alt="[公式]"></li>
<li>四个 <img src="https://www.zhihu.com/equation?tex=Z%EF%BC%8CZ_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]"> ，这四个相辅相成，才造就了中间的Memory
Cell里的值，你肯恩要问普通RNN里有个 <img src="https://www.zhihu.com/equation?tex=X_t+" alt="[公式]">
作为输入，那LSTM的输入在哪？别着急，其实这四个 <img src="https://www.zhihu.com/equation?tex=Z%EF%BC%8CZ_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]"> 都有输入向量 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]">
的参与。对了，在解释这四个分别是什么之前，我要先解释一下上图的所有这个符号：<img src="https://pic4.zhimg.com/80/v2-b74b5413f6897f2890b3ede634f60efb_1440w.png" alt="img" style="zoom:50%;">都代表一个激活函数，<strong>LSTM里常用的激活函数有两个，一个是tanh，一个是sigmoid</strong>。</li>
<li><figure>
<img src="https://www.zhihu.com/equation?tex=Z%3Dtanh%28W%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_i%3D%5Csigma%28W_i%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_f%3D%5Csigma%28W_f%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5CZ_o%3D%5Csigma%28W_o%5Bx_t%2Ch_%7Bt-1%7D%5D%29%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure></li>
<li><img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
<strong>是最为普通的输入</strong>，可以从上图中看到， <img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
是通过该时刻的输入 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]"> 和上一时刻存在memory cell里的隐藏层信息 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
向量拼接，再与权重参数向量 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
点积，得到的值经过激活函数tanh最终会得到一个数值。</li>
<li><img src="https://www.zhihu.com/equation?tex=Z_i" alt="[公式]">
<strong>input gate的缩写i，所以也就是输入门的门控装置</strong>， <img src="https://www.zhihu.com/equation?tex=Z_i+" alt="[公式]">
同样也是通过该时刻的输入 <img src="https://www.zhihu.com/equation?tex=X_t" alt="[公式]">
和上一时刻隐藏状态，也就是上一时刻存下来的信息 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
向量拼接，在与权重参数向量 <img src="https://www.zhihu.com/equation?tex=W_i" alt="[公式]">
点积（注意每个门的权重向量都不一样，这里的下标i代表input的意思，也就是输入门)。得到的值经过激活函数sigmoid的最终会得到一个0-1之间的一个数值，用来作为<strong>输入门的控制信号</strong>。</li>
<li>以此类推，就不详细讲解 <img src="https://www.zhihu.com/equation?tex=Z_f%EF%BC%8CZ_o" alt="[公式]">
了，分别是缩写forget和output的门控装置，原理与上述输入门的门控装置类似。上面说了，只有
<img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]">
是输入，其他的三个都是门控装置，负责把控每一阶段的信息记录与遗忘，具体是怎样的呢？我们先来看公式：<strong>首先解释一下，经过这个sigmod激活函数后，得到的
<img src="https://www.zhihu.com/equation?tex=Z_i%EF%BC%8CZ_f%EF%BC%8CZ_o" alt="[公式]">
都是在0到1之间的数值，1表示该门完全打开，0表示该门完全关闭</strong>，</li>
</ul>
</blockquote>
<h4><span id="lstm迭代过程">==LSTM迭代过程==</span></h4>
<blockquote>
<p>LSTM和GRU算法简单梳理🍭: https://zhuanlan.zhihu.com/p/72500407</p>
</blockquote>
<p><img src="https://pic3.zhimg.com/80/v2-10f9ec56794c9f89ca2b6ce86c7693ee_1440w.jpg?source=d16d100b" alt="img" style="zoom: 33%;"></p>
<p><img src="https://pic2.zhimg.com/80/v2-867b66c85751aa3bfeeb4956054e0eb8_1440w.jpg?source=d16d100b" alt="img" style="zoom:67%;"></p>
<p><img src="https://www.zhihu.com/equation?tex=h_%7Bt%7D" alt="[公式]"> ：当前序列的隐藏状态、 <img src="https://www.zhihu.com/equation?tex=x_%7Bt%7D" alt="[公式]">
：当前序列的输入数据、 <img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D" alt="[公式]">
：当前序列的细胞状态、 <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]"> ：
<img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
激活函数、 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]"> ： <img src="https://www.zhihu.com/equation?tex=+%5Ctanh" alt="[公式]">
激活函数。</p>
<p><strong>遗忘门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++f_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7Bf%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bf%7D%5Cright%29++%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>输入门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+i_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bi%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bi%7D%5Cright%29+%5C%5C+%5Ctilde%7BC%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W_%7BC%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7BC%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>细胞状态更新：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D%3DC_%7Bt-1%7D%5Codot+f_t%2Bi_t%5Codot+%5Ctilde%7BC_t%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><strong>输出门：</strong></p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D++o_t%26%3D%5Csigma%7B%28W_o%5Ccdot%5Bh_%7Bt-1%7D%2Cx_t%5D%2Bb_o%29%7D%5C%5C+h_t%26%3Do_t%5Codot+%5Ctanh%7B%28C_t%29%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="21-lstm之遗忘门">2.1 LSTM之遗忘门</span></h4>
<p><img src="https://pic3.zhimg.com/80/v2-e616cbe445d60aee532ef3d9db0fb8f6_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>遗忘门是控制是否遗忘的</strong>，在 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]">
中即以一定的概率控制是否遗忘上一层的细胞状态。图中输入的有前一序列的隐藏状态
<img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]"> 和当前序列的输入数据 <img src="https://www.zhihu.com/equation?tex=x_t" alt="[公式]"> ，通过一个
<img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
激活函数得到遗忘门的输出 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]"> 。因为 <img src="https://www.zhihu.com/equation?tex=+sigmoid" alt="[公式]">
函数的取值在 <img src="https://www.zhihu.com/equation?tex=%5B0%2C+1%5D" alt="[公式]"> 之间，所以 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]">
表示的是遗忘前一序列细胞状态的概率，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++f_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7Bf%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bf%7D%5Cright%29++%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="22-lstm之输入门">2.2 LSTM之输入门</span></h4>
<p><img src="https://pic2.zhimg.com/80/v2-1dbddd39ff6a039631b8ff1e4bc294a5_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>输入门是用来决定哪些数据是需要更新的</strong>，由 <img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
层决定；然后，一个 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]"> 层为新的候选值创建一个向量 <img src="https://www.zhihu.com/equation?tex=+%5Ctilde%7BC_t%7D" alt="[公式]"> ，这些值能够加入到当前细胞状态中，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+i_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bi%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7Bi%7D%5Cright%29+%5C%5C+%5Ctilde%7BC%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W_%7BC%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%2Bb_%7BC%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="23-lstm之细胞状态更新">2.3 LSTM之细胞状态更新</span></h4>
<p><img src="https://pic4.zhimg.com/80/v2-82f48ff07dd75ee4346ab44c573fe0b7_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>前面的遗忘门和输入门的结果都会作用于细胞状态</strong> <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
，<strong>在决定需要遗忘和需要加入的记忆之后，就可以更新前一序列的细胞状态
<img src="https://www.zhihu.com/equation?tex=+C_%7Bt-1%7D" alt="[公式]"> 到当前细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
了</strong>，前一序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_%7Bt-1%7D" alt="[公式]">
乘以遗忘门的输出 <img src="https://www.zhihu.com/equation?tex=f_t" alt="[公式]"> 表示决定遗忘的信息， <img src="https://www.zhihu.com/equation?tex=+i_t%5Codot+%5Ctilde%7BC_t%7D" alt="[公式]"> 表示新的记忆信息，数学表达式为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=C_%7Bt%7D%3DC_%7Bt-1%7D%5Codot+f_t%2Bi_t%5Codot+%5Ctilde%7BC_t%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h4><span id="24-lstm之输出门">2.4 LSTM之输出门</span></h4>
<p><img src="https://pic3.zhimg.com/80/v2-6335c8e72a056eeb80970c88988f6bd2_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>在得到当前序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]">
后，就可以计算当前序列的输出隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"></strong>
了，隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"> 的更新由两部分组成，第一部分是 <img src="https://www.zhihu.com/equation?tex=o_t" alt="[公式]">
，它由前一序列的隐藏状态 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
和当前序列的输入数据 <img src="https://www.zhihu.com/equation?tex=x_t" alt="[公式]"> 通过激活函数 <img src="https://www.zhihu.com/equation?tex=sigmoid" alt="[公式]">
得到，第二部分由当前序列的细胞状态 <img src="https://www.zhihu.com/equation?tex=C_t" alt="[公式]"> 经过 <img src="https://www.zhihu.com/equation?tex=%5Ctanh" alt="[公式]">
激活函数后的结果组成，数学表达式为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D++o_t%26%3D%5Csigma%7B%28W_o%5Ccdot%5Bh_%7Bt-1%7D%2Cx_t%5D%2Bb_o%29%7D%5C%5C+h_t%26%3Do_t%5Codot+%5Ctanh%7B%28C_t%29%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C" alt="[公式]">
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<h3><span id="三-gru-框架结构">三、GRU 框架结构</span></h3>
<p><img src="https://pic3.zhimg.com/80/v2-7547e5c91e1590bf67e0641e9e29ec66_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>循环门单元( <img src="https://www.zhihu.com/equation?tex=Gated%5C+Recurrent%5C+Unit%2C%5C+GRU" alt="[公式]">
)，它组合了遗忘门和输入门到一个单独的更新门当中，也合并了细胞状态 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]">
和隐藏状态</strong> <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]">，并且还做了一些其他的改变使得其模型比标准 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]">
模型更简单，其数学表达式式为： <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+z_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Bz%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+r_%7Bt%7D+%26%3D%5Csigma%5Cleft%28W_%7Br%7D+%5Ccdot%5Cleft%5Bh_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+%5Ctilde%7Bh%7D_%7Bt%7D+%26%3D%5Ctanh+%5Cleft%28W+%5Ccdot%5Cleft%5Br_%7Bt%7D+%5Codot+h_%7Bt-1%7D%2C+x_%7Bt%7D%5Cright%5D%5Cright%29+%5C%5C+h_%7Bt%7D+%26%3D%5Cleft%281-z_%7Bt%7D%5Cright%29+%5Codot+h_%7Bt-1%7D%2Bz_%7Bt%7D+%5Codot+%5Ctilde%7Bh%7D_%7Bt%7D+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D%5C%5C+" alt="[公式]"></p>
<p>首先介绍 <img src="https://www.zhihu.com/equation?tex=GRU" alt="[公式]"> 的两个门，它们分别是重置门 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]"> 和更新门
<img src="https://www.zhihu.com/equation?tex=z_t" alt="[公式]">
，计算方法与 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]"> 中门的计算方法是一致的；然后是计算候选隐藏层 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> ，该候选隐藏层和 <img src="https://www.zhihu.com/equation?tex=LSTM" alt="[公式]"> 中的 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BC%7D_t" alt="[公式]"> 类似，都可以看成是当前时刻的新信息，<strong>其中 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]">
用来控制需要保留多少之前的记忆，如果 <img src="https://www.zhihu.com/equation?tex=r_t" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=0" alt="[公式]"> 则表示 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> 只保留当前序列的输入信息；最后 <img src="https://www.zhihu.com/equation?tex=+z_t" alt="[公式]">
控制需要从前一序列的隐藏层 <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D" alt="[公式]">
中遗忘多少信息和需要加入多少当前序列的隐藏层信息</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D_t" alt="[公式]"> ，从而得到当前序列的输出隐藏层信息 <img src="https://www.zhihu.com/equation?tex=h_t" alt="[公式]"> ，而 <img src="https://www.zhihu.com/equation?tex=GRU" alt="[公式]">
是没有输出门的。</p>
<p>GRU和LSTM的性能差不多，但GRU参数更少，更简单，所以训练效率更高。但是，如果数据的依赖特别长且数据量很大的话，LSTM的效果可能会稍微好一点，毕竟参数量更多。所以默认推荐使用LSTM。</p>
<h4><span id="参考资料️">参考资料⬇️</span></h4>
<p><a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding
LSTM Networks</a></p>
<h2><span id="lstm-qampa">LSTM Q&amp;A</span></h2>
<h3><span id="1-为什么lstm可以解决梯度消失和梯度爆炸">1、为什么LSTM可以解决梯度消失和梯度爆炸？</span></h3>
<p><img src="https://pic3.zhimg.com/80/v2-10f9ec56794c9f89ca2b6ce86c7693ee_1440w.jpg?source=d16d100b" alt="img" style="zoom: 33%;"></p>
<p><strong>参考（这个老哥说的是最好的）：</strong><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/34878706">LSTM如何来避免梯度弥散和梯度爆炸？</a></p>
<ul>
<li>==<strong>LSTM 中梯度的传播有很多条路径</strong>，<img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+c_t+%3D+f_t%5Codot+c_%7Bt-1%7D+%2B+i_t+%5Codot+%5Chat%7Bc_t%7D" alt="[公式]">
这条路径上只有逐元素相乘和相加的操作，梯度流最稳定==；但是其他路径（例如
<img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+h_%7Bt-1%7D+%5Crightarrow+i_t+%5Crightarrow+c_t" alt="[公式]"> )上梯度流与普通 RNN
类似，照样会发生相同的权重矩阵反复连乘。</li>
<li><strong>LSTM 刚提出时没有遗忘门</strong>，或者说相当于 <img src="https://www.zhihu.com/equation?tex=f_t%3D1" alt="[公式]">
，这时候在 <img src="https://www.zhihu.com/equation?tex=c_%7Bt-1%7D+%5Crightarrow+c_t" alt="[公式]"> 直接相连的短路路径上，<img src="https://www.zhihu.com/equation?tex=dl%2Fdc_t" alt="[公式]">
可以无损地传递给 <img src="https://www.zhihu.com/equation?tex=dl%2Fdc_%7Bt-1%7D" alt="[公式]">
，从而<strong>这条路径</strong>上的梯度畅通无阻，不会消失。类似于 ResNet
中的残差连接。</li>
<li>但是在<strong>其他路径</strong>上，LSTM 的梯度流和普通 RNN
没有太大区别，依然会爆炸或者消失。由于总的远距离梯度 =
各条路径的远距离梯度之和，即便其他远距离路径梯度消失了，只要保证有一条远距离路径（就是上面说的那条高速公路）梯度不消失，总的远距离梯度就不会消失（正常梯度
+ 消失梯度 = 正常梯度）。因此 LSTM
通过改善<strong>一条路径</strong>上的梯度问题拯救了<strong>总体的远距离梯度</strong>。</li>
<li>同样，因为总的远距离梯度 =
各条路径的远距离梯度之和，高速公路上梯度流比较稳定，但其他路径上梯度有可能爆炸，此时总的远距离梯度
= 正常梯度 + 爆炸梯度 = 爆炸梯度，因此 <strong>LSTM
仍然有可能发生梯度爆炸</strong>。不过，<strong>==由于 LSTM
的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于
1），因此 LSTM
发生梯度爆炸的频率要低得多==</strong>。实践中梯度爆炸一般通过梯度裁剪来解决。</li>
<li>对于现在常用的带遗忘门的 LSTM 来说，4 中的分析依然成立，而 3
分为两种情况：其一是遗忘门接近 1（例如模型初始化时会把 forget bias
设置成较大的正数，让遗忘门饱和），这时候远距离梯度不消失；其二是<strong>遗忘门接近
0，但这时模型是故意阻断梯度流的，这不是 bug 而是
feature</strong>（例如情感分析任务中有一条样本 “A，但是
B”，模型读到“但是”后选择把遗忘门设置成 0，遗忘掉内容
A，这是合理的）。当然，常常也存在 f 介于 [0, 1]
之间的情况，在这种情况下只能说 LSTM
改善（而非解决）了梯度消失的状况。</li>
</ul>
<h3><span id="2-为什么lstm模型中既存在sigmoid又存在tanh两种激活函数">2、为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数？</span></h3>
<p>关于激活函数的选取，在LSTM中，遗忘门、输入门和输出门使用
Sigmoid函数作为激活函数;在<strong>生成候选记忆</strong>时，使用双曲正切函数<strong>tanh</strong>作为激活函数。值得注意的是，这两个激活函数都是<strong>饱和</strong>的也就是说在<strong>输入达到一定值的情况下，输出就不会发生明显变化</strong>了。如果是用非饱和的激活图数，例如ReLU，那么将<strong>难以实现门控的效果。</strong></p>
<ul>
<li><p>Sigmoid的输出在0-1之同，符合门控的物理定义，且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关，在生成候选记亿时，</p></li>
<li><p><strong>tanh函数，是因为其输出在-1-1之间，这与大多数场景下特征分布是0中心的吻合</strong>。此外，<strong>tanh函数在输入为0近相比
Sigmoid函数有更大的梯度，通常使模型收敛更快。</strong></p></li>
</ul>
<p>激活函数的选择也不是一成不变的。<strong>例如在原始的LSTM中，使用的激活函数是
Sigmoid函数的变种，h(x)=2sigmoid(x)-1,g(x)＝4
sigmoid(x)-2，这两个函数的范国分别是[-1，1]和[-2，2]</strong>。并且在原始的LSTM中，只有输入门和输出门，没有遗忘门，其中输入经过输入门后是直接与记忆相加的，所以输入门控g(x)的值是0中心的。</p>
<p><strong>后来经过大量的研究和实验，人们发现增加遗忘门对LSTM的性能有很大的提升</strong>且<strong>h(x)使用tanh比2
sigmoid(x)-1要好</strong>，所以现代的LSTM采用
Sigmoid和tanh作为激活函数。<strong>事实上在门控中，使用
Sigmoid函数是几乎所有现代神经网络模块的共同选择</strong>。例如在<strong>门控循环单元和注意力机制</strong>中，也广泛使用
Sigmoid i函数作为门控的激活函数。</p>
<h4><span id="为什么">为什么？</span></h4>
<ol type="1">
<li>门是控制开闭的，全开时值为1，全闭值为０。用于遗忘和保留信息。</li>
<li>对于求值的激活函数无特殊要求。</li>
</ol>
<h4><span id="能更换吗">能更换吗？</span></h4>
<ol type="1">
<li>门是控制开闭的，全开时值为1，全闭值为０。用于遗忘和保留信息。门的激活函数只能是值域为０到１的，最常见的就是sigmoid。</li>
<li>对于求值的激活函数无特殊要求。</li>
</ol>
<h3><span id="3-能不能把tanh换成relu">3、能不能把tanh换成relu？</span></h3>
<p><strong>不行</strong>？对于梯度爆炸的问题用梯度裁剪解决就行了。</p>
<ol type="1">
<li><strong>会造成输出值爆炸</strong>。RNN共享参数矩阵，长程的话相当于多个相乘，最后输出类似于
<img src="https://www.zhihu.com/equation?tex=f%5Cleft%5BW+%5Cldots%5Cleft%5BW+f%5Cleft%5BW+f%5Cleft%5BW+f_%7Bi%7D%5Cright%5D%5Cright%5D%5Cright%5D%5Cright%5D" alt="[公式]"> ，其中是 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 激活函数，如果 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
有一个大于1的特征值，且使用relu激活函数，那最后的输出值会爆炸。但是使用tanh激活函数，能够把输出值限制在-1和1之间。</li>
<li><strong>这里relu并不能解决梯度消失或梯度爆炸的问题</strong>。假设有t=3，最后一项输出反向传播对W求导，
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+W_%7B1%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+f_%7B2%7D%2B%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D+f_%7B1%7D%2B%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D+W+%5Cfrac%7B%5Cpartial+f_%7B1%7D%7D%7B%5Cpartial+a_%7B1%7D%7D+%5Cfrac%7B%5Cpartial+a_%7B1%7D%7D%7B%5Cpartial+W_%7B1%7D%7D" alt="[公式]"> 。我们用最后一项做分析，即使使用了<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=relu&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22350613342%22%7D">relu</a>，
<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B3%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+a_%7B2%7D%7D%3D%5Cfrac%7B%5Cpartial+f_%7B3%7D%7D%7B%5Cpartial+a_%7B1%7D%7D%3D1" alt="[公式]"> ，还是会有两个 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">
相乘，并不能解决梯度消失或梯度爆炸的问题。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3MJS4P5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3MJS4P5/" class="post-title-link" itemprop="url">深度学习（4）CNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:44:58" itemprop="dateCreated datePublished" datetime="2022-03-16T21:44:58+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-18 21:22:12" itemprop="dateModified" datetime="2023-04-18T21:22:12+08:00">2023-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">【draft】深度学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E3%80%90draft%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CNN/" itemprop="url" rel="index"><span itemprop="name">CNN</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="cnn-门控卷积网络">CNN-门控卷积网络</span></h2>
<ul>
<li>https://zhuanlan.zhihu.com/p/59064623</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/FishPotatoChen/article/details/123389289?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123389289-blog-108305491.pc_relevant_paycolumn_v3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-123389289-blog-108305491.pc_relevant_paycolumn_v3&amp;utm_relevant_index=2">参数估计</a></li>
<li>**<font color="red"> 卷积神经网络中用1*1
卷积有什么作用或者好处呢？</font>** - 初识CV的回答 - 知乎
https://www.zhihu.com/question/56024942/answer/1850649283</li>
</ul>
<h2><span id="一-参数估计">一、参数估计</span></h2>
<p>卷积操作的本质是<strong>稀疏交互</strong>和<strong>参数共享</strong>。</p>
<p><strong>稀疏交互</strong>：每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重。<strong>物理意义</strong>：通常图像，文字、语音等现实世界中的数据都具有<strong>局部的特征结构</strong>。</p>
<p><strong>参数共享</strong>：卷积核中的每一个元素将作用于每一个局部输入的特定位置上。<strong>物理意义</strong>：平移等变性</p>
<p><strong>池化操作</strong>：针对非重叠区域，均值池化、最大池化。除了能显著降低参数外，还能够保持对平移、伸缩、旋转操作的不变性。</p>
<h4><span id="11-正向传播">1.1 正向传播</span></h4>
<p>这里用输入为 <span class="math inline">\(3 * 3\)</span> 矩阵 <span class="math inline">\(A^{l-1}\)</span>, 步长为 1 , 卷积核为 <span class="math inline">\(2 * 2\)</span> 矩阵 <span class="math inline">\(W^{l}\)</span>, 输出为 <span class="math inline">\(2 * 2\)</span> 矩阵 <span class="math inline">\(Z^{l}\)</span> 的卷积层为 例。矩阵 <span class="math inline">\(A^{l-1}\)</span> 可以是整个神经网络的输入,
也可以是池化层的输出。这个模型简化为输入层 <span class="math inline">\(A^{l-1}\)</span> 经 过卷积计算得到特征图 <span class="math inline">\(Z^{l}, Z^{l}\)</span> 经过激活函数 <span class="math inline">\(\sigma(x)\)</span> 得到输出层 <span class="math inline">\(A^{l}\)</span> (实际上在现实工程中很多时候不用激
活函数)。对于第 <span class="math inline">\(l\)</span> 层, 有下列表达式:
<span class="math display">\[
\left[\begin{array}{lll}
a_{1} &amp; a_{2} &amp; a_{3} \\
a_{4} &amp; a_{5} &amp; a_{6} \\
a_{7} &amp; a_{8} &amp; a_{9}
\end{array}\right]^{l-1} \Rightarrow\left[\begin{array}{cc}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l} \Rightarrow\left[\begin{array}{ll}
z_{1} &amp; z_{2} \\
z_{3} &amp; z_{4}
\end{array}\right]^{l} \Rightarrow\left[\begin{array}{ll}
a_{1} &amp; a_{2} \\
a_{3} &amp; a_{4}
\end{array}\right]^{l}
\]</span></p>
<p><span class="math display">\[
\left\{\begin{aligned}
a_{1}^{l} &amp;=\sum\left(\left[\begin{array}{ll}
a_{1} &amp; a_{2} \\
a_{4} &amp; a_{5}
\end{array}\right]^{l-1} \cdot\left[\begin{array}{ll}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l}\right)=\sigma\left(\omega_{1}
a_{1}^{l-1}+\omega_{2} a_{2}^{l-1}+\omega_{3} a_{3}^{l-1}+\omega_{4}
a_{4}^{l-1}+b^{l}\right) \\
a_{2}^{l} &amp;=\sum\left(\left[\begin{array}{ll}
a_{2} &amp; a_{3} \\
a_{5} &amp; a_{6}
\end{array}\right]^{l-1} \cdot\left[\begin{array}{ll}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l}\right)=\sigma\left(\omega_{1}
a_{2}^{l-1}+\omega_{2} a_{3}^{l-1}+\omega_{3} a_{5}^{l-1}+\omega_{4}
a_{6}^{l-1}+b^{l}\right) \\
a_{3}^{l} &amp;=\sum\left(\left[\begin{array}{ll}
a_{4} &amp; a_{5} \\
a_{7} &amp; a_{8}
\end{array}\right]^{l-1} \cdot\left[\begin{array}{ll}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l}\right)=\sigma\left(\omega_{1}
a_{4}^{l-1}+\omega_{2} a_{5}^{l-1}+\omega_{3} a_{7}^{l-1}+\omega_{4}
a_{8}^{l-1}+b^{l}\right) \\
a_{4}^{l} &amp;=\sum\left(\left[\begin{array}{ll}
a_{5} &amp; a_{6} \\
a_{8} &amp; a_{9}
\end{array}\right]^{l-1} \cdot\left[\begin{array}{ll}
\omega_{1} &amp; \omega_{2} \\
\omega_{3} &amp; \omega_{4}
\end{array}\right]^{l}\right)=\sigma\left(\omega_{1}
a_{5}^{l-1}+\omega_{2} a_{6}^{l-1}+\omega_{3} a_{8}^{l-1}+\omega_{4}
a_{9}^{l-1}+b^{l}\right)
\end{aligned}\right.
\]</span></p>
<p>简单来说, 卷积过程就是对应的位置代入函数之后相加求和,
不同的函数有不同的参数 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span>, 我们需 要训练的是卷积核参数,
所以这个公式还可以写做 <span class="math inline">\(Z^{l}=W^{l} *
A^{l-1}+b^{l}, \sigma(x)\)</span> 是激活函数,
<strong>我们假设是ReLU函数, 求导比较好求</strong>,
所以我们接下来的计算忽略了对激活层的求导。 <span class="math display">\[
\sigma(x)=\left\{\begin{array}{lc}
0 &amp; , \quad x&lt;0 \\
x &amp; , \quad x&gt;=0
\end{array}\right.
\]</span></p>
<h3><span id="12-反向传播">1.2 反向传播</span></h3>
<h2><span id="二-cnn-李宏毅">二、CNN - 李宏毅</span></h2>
<h2><span id="三-cnn-qampa">三、CNN Q&amp;A</span></h2>
<h4><span id="31卷积神经网络的卷积核大小-个数卷积层数如何确定呢">3.1
卷积神经网络的卷积核大小、个数，卷积层数如何确定呢？</span></h4>
<ul>
<li>https://cloud.tencent.com/developer/article/1462368</li>
</ul>
<p>每一层卷积有多少channel数，以及一共有多少层卷积，这些暂时没有理论支撑，一般都是靠感觉去设置几组候选值，然后通过实验挑选出其中的最佳值。这也是现在深度卷积神经网络虽然效果拔群，但是一直为人诟病的原因之一。</p>
<h4><span id="32卷积神经网络中用11-卷积有什么作用或者好处呢">3.2
卷积神经网络中用1*1 卷积有什么作用或者好处呢？</span></h4>
<ul>
<li>卷积神经网络中用1*1 卷积有什么作用或者好处呢？ - YJango的回答 - 知乎
https://www.zhihu.com/question/56024942/answer/194997553</li>
</ul>
<p><img src="image-20220627212030158.png" alt="image-20220627212030158" style="zoom: 50%;"></p>
<p><strong>降维</strong>：Inception结构可以看出，这些1X1的卷积的作用是为了让网络根据需要能够更灵活的控制数据的depth的。</p>
<p><strong>加入非线性</strong>。卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（
non-linear activation ），提升网络的表达能力；</p>
<p><strong>1 * 1的卷积就是多个feature channels线性叠加，nothing
more!只不过这个组合系数恰好可以看成是一个1 *
1的卷积</strong>。这种表示的好处是，完全可以回到模型中其他常见N*N的框架下，不用定义新的层。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/4YNH5E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/4YNH5E/" class="post-title-link" itemprop="url">特征工程（9）TF-IDF</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 21:06:42" itemprop="dateCreated datePublished" datetime="2022-03-16T21:06:42+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-26 14:54:35" itemprop="dateModified" datetime="2023-04-26T14:54:35+08:00">2023-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">特征工程</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3><span id="一-tf-idf">一、TF-IDF</span></h3>
<blockquote>
<p>https://blog.csdn.net/u010417185/article/details/87905899</p>
</blockquote>
<p><strong>TF-IDF(Term Frequency-Inverse Document Frequency,
词频-逆文件频率)</strong>是一种用于资讯检索与资讯探勘的常用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=加权技术&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2297273457%22%7D">加权技术</a>。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p>
<p>上述引用总结就是, <strong>一个词语在一篇文章中出现次数越多,
同时在所有文档中出现次数越少,
越能够代表该文章。</strong>这也就是TF-IDF的含义。</p>
<h4><span id="11-tf"><strong>1.1 TF</strong></span></h4>
<p><strong>TF(Term
Frequency，词频)</strong>表示词条在文本中出现的频率，这个数字通常会被归一化(一般是词频除以文章总词
数),
以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频,
而不管该词语重要与否）。TF 用公式表示如下: <span class="math display">\[
T F_{i, j}=\frac{n_{i, j}}{\sum_k n_{k, j}}
\]</span> 其中, <span class="math inline">\(n_{i, j}\)</span> 表示词条
<span class="math inline">\(t_i\)</span> 在文档 <span class="math inline">\(d_j\)</span> 中出现的次数, <span class="math inline">\(T F_{i, j}\)</span> 就是表示词条 <span class="math inline">\(t_i\)</span> 在文档 <span class="math inline">\(d_j\)</span> 中出现的频率。</p>
<p><strong>但是, 需要注意, 一些通用的词语对于主题并没有太大的作用,
反倒是一些出现频率较少的词才能够表达文章的主题,
所以单纯使用是TF不合适的</strong>。权重的设计必须满足：一个词预测主题的能力越强,
权重越大, 反之, 权重 越小。所有统计的文章中,
一些词只是在其中很少几篇文章中出现, 那么这样的词对文章的主题的作用很大,
这些 词的权重应该设计的较大。IDF就是在完成这样的工作。</p>
<h4><span id="12-idf"><strong>1.2 IDF</strong></span></h4>
<p><strong>IDF(Inverse Document
Frequency，逆文件频率)</strong>表示关键词的普遍程度。如果包含词条 <span class="math inline">\(i\)</span> 的文档越少, IDF越 大,
则说明该词条具有很好的类别区分能力。某一特定词语的IDF,
可以由总文件数目除以包含该词语之文件的数 目, 再将得到的商取对数得到:
<span class="math display">\[
I D F_i=\log \frac{|D|}{1+\left|j: t_i \in d_j\right|}
\]</span> 其中, <span class="math inline">\(|D|\)</span>
表示所有<strong>文档的数量, <span class="math inline">\(\left|j: t_i \in
d_j\right|\)</span> 表示包含词条 <span class="math inline">\(t_i\)</span> 的文档数量</strong>, 为什么这里要加 1
呢? 主要是<strong>防止包含词条 <span class="math inline">\(t_i\)</span>
的数量为 0 从而导致运算出错的现象发生</strong>。</p>
<p>某一特定文件内的高词语频率, 以及该词语在整个文件集合中的低文件频率,
可以产生出高权重的TF-IDF。因此, TF-IDF倾向于<strong>过滤淖常见的词语,
保留重要的词语,</strong> 表达为 <span class="math display">\[
T F-I D F=T F \cdot I D F
\]</span> 最后在计算完文档中每个字符的tfidf之后, 对其进行归一化,
将值保留在0-1之间, 并保存成稀疏矩阵。</p>
<h3><span id="二-tf-idf-qampa">二、TF-IDF Q&amp;A</span></h3>
<h4><span id="1-究竟应该是对整个语料库进行tf-idf呢还是先对训练集进行tf-idf然后再对xtest进行tf-idf呢两者有什么区别"><strong>1、究竟应该是对整个语料库进行tf-idf呢？还是先对训练集进行tf-idf，然后再对xtest进行tf-idf呢？两者有什么区别？</strong></span></h4>
<h5><span id="fit">fit</span></h5>
<ul>
<li>学习输入的数据有多少个不同的单词，以及每个单词的idf</li>
</ul>
<h5><span id="transform-训练集">transform 训练集</span></h5>
<ul>
<li>回我们一个document-term matrix.</li>
</ul>
<h5><span id="transform-测试集">transform 测试集</span></h5>
<p>transform的过程也很让人好奇。要知道，他是将测试集的数据中的文档数量纳入进来，重新计算每个词的idf呢，还是<strong>直接用训练集学习到的idf去计算测试集里面每一个tf-idf</strong>呢？</p>
<p><strong>如果纳入了测试集新词，就等于预先知道测试集中有什么词，影响了idf的权重。这样预知未来的行为，会导致算法丧失了泛化性。</strong></p>
<h4><span id="2-tf-idf模型加载太慢"><font color="red">2、TF-IDF
模型加载太慢</font></span></h4>
<blockquote>
<p>https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> idfs <span class="keyword">import</span> idfs <span class="comment"># numpy array with our pre-computed idfs</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># subclass TfidfVectorizer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyVectorizer</span>(<span class="title class_ inherited__">TfidfVectorizer</span>):</span><br><span class="line">    <span class="comment"># plug our pre-computed IDFs</span></span><br><span class="line">    TfidfVectorizer.idf_ = idfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiate vectorizer</span></span><br><span class="line">vectorizer = MyVectorizer(lowercase = <span class="literal">False</span>,</span><br><span class="line">                          min_df = <span class="number">2</span>,</span><br><span class="line">                          norm = <span class="string">&#x27;l2&#x27;</span>,</span><br><span class="line">                          smooth_idf = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plug _tfidf._idf_diag</span></span><br><span class="line">vectorizer._tfidf._idf_diag = sp.spdiags(idfs,</span><br><span class="line">                                         diags = <span class="number">0</span>,</span><br><span class="line">                                         m = <span class="built_in">len</span>(idfs),</span><br><span class="line">                                         n = <span class="built_in">len</span>(idfs))</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/1FJMY6C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/1FJMY6C/" class="post-title-link" itemprop="url">贝叶斯分类器（4）EM算法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 17:16:27" itemprop="dateCreated datePublished" datetime="2022-03-16T17:16:27+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:32:53" itemprop="dateModified" datetime="2023-04-22T16:32:53+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" itemprop="url" rel="index"><span itemprop="name">贝叶斯分类器</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="em期望最大-概率模型">EM——期望最大 [概率模型]</span></h2>
<p><strong>EM 算法通过引入隐含变量，使用
MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM
算法首先会固定其中的第一个参数，然后使用 MLE
计算第二个变量值；接着通过固定第二个变量，再使用 MLE
估测第一个变量值，依次迭代，直至收敛到局部最优解。</strong></p>
<p><strong><font color="red"> EM 算法，全称 Expectation Maximization
Algorithm。期望最大算法是一种迭代算法，用于含有隐变量（Hidden
Variable）的概率参数模型的最大似然估计或极大后验概率估计。</font></strong></p>
<p>本文思路大致如下：先简要介绍其思想，然后举两个例子帮助大家理解，有了感性的认识后再进行严格的数学公式推导。</p>
<h3><span id="1-思想">1. 思想</span></h3>
<p>EM 算法的核心思想非常简单，分为两步：<strong>Expection-Step</strong>
和 <strong>Maximization-Step</strong>。<strong>E-Step
主要通过观察数据和现有模型来估计参数</strong>，然后用这个估计的参数值来计算似然函数的期望值；而
M-Step
是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后<strong>似然函数都会增加</strong>，所以函数最终会收敛。</p>
<p><font color="red"> <span class="math inline">\(E M\)</span>
算法一句话总结就是: <span class="math inline">\(E\)</span> 步固定 <span class="math inline">\(\theta\)</span> 优化 <span class="math inline">\(Q, M\)</span> 步固定 <span class="math inline">\(Q\)</span> 优化 <span class="math inline">\(\theta\)</span> 。</font></p>
<h3><span id="2-例子">2 例子</span></h3>
<h4><span id="21-例子-a">2.1 例子 A</span></h4>
<p>假设有两枚硬币 <span class="math inline">\(\mathrm{A}\)</span> 和
<span class="math inline">\(B\)</span>,
他们的随机抛郑的结果如下图所示:</p>
<p><img src="https://pic4.zhimg.com/80/v2-4e19d89b47e21cf284644b0576e9af0f_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>我们很容易估计出两枚硬币抛出正面的概率： <span class="math display">\[
\begin{aligned}
&amp; \theta_A=24 / 30=0.8 \\
&amp; \theta_B=9 / 20=0.45
\end{aligned}
\]</span> 现在我们加入隐变量, 抺去每轮投郑的硬币标记:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221558453.jpg" alt="img" style="zoom:50%;"></p>
<p>碰到这种情况, 我们该如何估计 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> 的值?</p>
<p>我们多了一个隐变量 <span class="math inline">\(Z=\left(z_1, z_2, z_3,
z_4, z_5\right)\)</span>, 代表每一轮所使用的硬币,
我们需要知道每一轮抛郑所使用的硬币这样才能估计 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> 的值, 但是估计隐变量 <span class="math inline">\(\mathrm{Z}\)</span> 我们又需要知道 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> 的值,
才能用极大似然估计法去估计出 Z。这就陷入了一个鸡生蛋和蛋生鸡的问题。</p>
<p>其解决方法就是先随机初始化 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span>, 然后用去估计 <span class="math inline">\(Z\)</span>, 然后基于 <span class="math inline">\(Z\)</span> 按照最大似然概率去估计新的 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> , 循环至收敛。</p>
<h5><span id="212-计算"><strong>2.1.2 计算</strong></span></h5>
<p>随机初始化 <span class="math inline">\(\theta_A=0.6\)</span> 和 <span class="math inline">\(\theta_B=0.5\)</span></p>
<p>对于第一轮来说, 如果是硬币 <span class="math inline">\(A\)</span>,
得出的 5 正 5 反的概率为: <span class="math inline">\(0.6^5 *
0.4^5\)</span>; 如果是硬币 <span class="math inline">\(B\)</span>,
得出的 5 正 5 反的概率为: <span class="math inline">\(0.5^5 *
0.5^5\)</span> 。我们可以算出使用是硬币 <span class="math inline">\(A\)</span> 和硬币 <span class="math inline">\(B\)</span> 的概率 分别为: <span class="math display">\[
\begin{aligned}
&amp; P_A=\frac{0.6^5 * 0.4^5}{\left(0.6^5 * 0.4^5\right)+\left(0.5^5 *
0.5^5\right)}=0.45 \\
&amp; P_B=\frac{0.5^5 * 0.5^5}{\left(0.6^5 * 0.4^5\right)+\left(0.5^5 *
0.5^5\right)}=0.55
\end{aligned}
\]</span> <img src="https://pic4.zhimg.com/80/v2-b325de65a5bcac196fc0939f346410d7_1440w.jpg" alt="img"></p>
<p>从期望的角度来看, 对于第一轮抛郑, 使用硬币 <span class="math inline">\(A\)</span> 的概率是 0.45 , 使用硬币 <span class="math inline">\(B\)</span> 的概率是 0.55。同理其他轮。这一
步我们实际上是估计出了 Z 的概率分布，这部就是 E-Step。</p>
<p>结合硬币 <span class="math inline">\(A\)</span> 的概率和上一张结果,
我们利用期望可以求出硬币 <span class="math inline">\(A\)</span> 和硬币
<span class="math inline">\(B\)</span> 的贡献。以第二轮硬币 <span class="math inline">\(A\)</span> 为例子, 计算方式为: <span class="math display">\[
\begin{aligned}
&amp; H: 0.80 * 9=7.2 \\
&amp; T: 0.80 * 1=0.8
\end{aligned}
\]</span> 于是我们可以得到：</p>
<figure>
<img src="https://pic1.zhimg.com/80/v2-9b6e8c50c0761c6ac19909c26e0a71d4_1440w.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>然后用极大似然估计来估计新的 <span class="math inline">\(\theta_A\)</span> 和 <span class="math inline">\(\theta_B\)</span> 。 <span class="math display">\[
\begin{aligned}
\theta_A &amp; =\frac{21.3}{21.3+8.6}=0.71 \\
\theta_B &amp; =\frac{11.7}{11.7+8.4}=0.58
\end{aligned}
\]</span></p>
<p>这步就对应了
M-Step，重新估计出了参数值。如此反复迭代，我们就可以算出最终的参数值。</p>
<p>上述讲解对应下图：</p>
<p><img src="https://pic3.zhimg.com/v2-6cac968d6500cbca58fc90347c288466_r.jpg" alt="preview" style="zoom:50%;"></p>
<h4><span id="22-例子-b">2.2 例子 B</span></h4>
<p>如果说例子 A 需要计算你可能没那么直观, 那就举更一个简单的例子:</p>
<p>现在一个班里有 50 个男生和 50
个女生，且男女生分开。我们假定男生的身高服从正态分布： <span class="math inline">\(N\left(\mu_1, \sigma_1^2\right)\)</span> ,
女生的身高则服从另一个正态分布： <span class="math inline">\(N\left(\mu_2, \sigma_2^2\right)\)</span>
。这时候我们可以用 极大似然法 (MLE) , 分别通过这 50 个男生和 50
个女生的样本来估计这两个正态分布的参数。</p>
<p>但现在我们让情况复杂一点, 就是这 50 个男生和 50
个女生混在一起了。我们拥有 100 个人的身高数据, 却不知 道这 100
个人每一个是男生还是女生。</p>
<p>这时候情况就有点箩尬, 因为通常来说,
我们只有知道了精确的男女身高的正态分布参数我们才能知道每一个人更
有可能是男生还是女生。但从另一方面去考量,
我们只有知道了每个人是男生还是女生才能尽可能准确地估计男女
各自身高的正态分布的参数。</p>
<p>这个时候有人就想到我们必须从某一点开始,
并用迭代的办法去解决这个问题：<strong>我们先设定男生身高和女生身高分
布的几个参数（初始值）, 然后根据这些参数去判断每一个样本
(人）是男生还是女生, 之后根据标注后的样本再
反过来重新估计参数。之后再多次重复这个过程，直至稳定。这个算法也就是 EM
算法。</strong></p>
<h3><span id="3-推导">3. 推导</span></h3>
<p>给定数据集, 假设样本间相互独立, 我们想要拟合模型 <span class="math inline">\(p(x ; \theta)\)</span>
到数据的参数。根据分布我们可以 得到如下似然函数: <span class="math display">\[
\begin{aligned}
L(\theta) &amp; =\sum_{i=1}^n \log p\left(x_i ; \theta\right) \\
&amp; =\sum_{i=1}^n \log \sum_z p\left(x_i, z ; \theta\right)
\end{aligned}
\]</span></p>
<p>第一步是<strong>对极大似然函数取对数</strong>，第二步是对每个样本的每个可能的类别
z 求<strong>联合概率分布之和</strong>。如果这个 z
是已知的数，那么使用极大似然法会很容易。但如果 z 是隐变量，我们就需要用
EM
算法来求。<strong>事实上，隐变量估计问题也可以通过梯度下降等优化算法，但事实由于求和项将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而
EM 算法则可看作一种非梯度优化方法。</strong></p>
<h4><span id="31-求解含有隐变量的概率模型">3.1 求解含有隐变量的概率模型</span></h4>
<p>为了求解含有隐变量 <span class="math inline">\(z\)</span> 的概率模型
<span class="math inline">\(\hat{\theta}=\underset{\theta}{\arg \max }
\sum_{i=1}^m \log \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ;
\theta\right)\)</span> 需要一些特殊的技巧, 通过引入隐变量<span class="math inline">\(z^{(i)}\)</span> 的概率分布为 <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span>, 因为 <span class="math inline">\(\log (x)\)</span>
是凹函数故结合凹函数形式下的詹森不等式进行放缩处理 <span class="math display">\[
\begin{aligned}
\sum_{i=1}^m \log \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ; \theta\right)
&amp; =\sum_{i=1}^m \log \sum_{z^{(i)}} Q_i\left(z^{(i)}\right)
\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_i\left(z^{(i)}\right)}
\\
&amp; =\sum_{i=1}^m \log \mathbb{E}\left(\frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{Q_i\left(z^{(i)}\right)}\right) \\
&amp; \left.\geq \sum_{i=1}^m \mathbb{E}\left[\log \frac{p\left(x^{(i)},
z^{(i)} ; \theta\right)}{Q_i\left(z^{(i)}\right)}\right)\right] \\
&amp; =\sum_{i=1}^m \sum_{z^{(i)}} Q_i\left(z^{(i)}\right) \log
\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_i\left(z^{(i)}\right)}
\end{aligned}
\]</span> 其中由概率分布的充要条件 <span class="math inline">\(\sum_{z^{(i)}} Q_i\left(z^{(i)}\right)=1 、
Q_i\left(z^{(i)}\right) \geq 0\)</span> 可看成下述关于 <span class="math inline">\(z\)</span> 函数分布列的形式:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221618376.jpg" alt="img" style="zoom:50%;"></p>
<p>这个过程可以看作是对 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 求了下界, 假设 <span class="math inline">\(\theta\)</span> 已经给定那么 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 的值就取决于 <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span> 和 <span class="math inline">\(p\left(x^{(i)}, z^{(i)}\right)\)</span> 了, 因
此可以通过调整这两个概率使下界不断上升, 以逼近 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 的真实值,
当不等式变成等式时说明调整后的概率能够 等价于 <span class="math inline">\(\mathcal{L}(\theta)\)</span>
，所以必须找到使得等式成立的条件, 即寻找 <span class="math display">\[
\left.\mathbb{E}\left[\log \frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{Q_i\left(z^{(i)}\right)}\right)\right]=\log
\mathbb{E}\left[\frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{Q_i\left(z^{(i)}\right)}\right]
\]</span> 由期望得性质可知当 <span class="math display">\[
\frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{Q_i\left(z^{(i)}\right)}=C, \quad C \in \mathbb{R}
\quad(*)
\]</span> 等式成立，对上述等式进行变形处理可得 <span class="math display">\[
\begin{aligned}
&amp; p\left(x^{(i)}, z^{(i)} ; \theta\right)=C Q_i\left(z^{(i)}\right)
\\
&amp; \Leftrightarrow \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ;
\theta\right)=C \sum_{z^{(i)}} Q_i\left(z^{(i)}\right)=C \\
&amp; \Leftrightarrow \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ;
\theta\right)=C
\end{aligned}
\]</span> 把 <span class="math inline">\((* *)\)</span> 式带入 <span class="math inline">\((*)\)</span> 化简可知 <span class="math display">\[
\begin{aligned}
Q_i\left(z^{(i)}\right) &amp; =\frac{p\left(x^{(i)}, z^{(i)} ;
\theta\right)}{\sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ; \theta\right)}
\\
&amp; =\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{p\left(x^{(i)} ;
\theta\right)} \\
&amp; =p\left(z^{(i)} \mid x^{(i)} ; \theta\right)
\end{aligned}
\]</span> 至此, 可以推出在固定参数 <span class="math inline">\(\theta\)</span> 后, <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span>
的计算公式就是后验概率, 解决了 <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span>
如何选择得问题。这一步 称为 <span class="math inline">\(E\)</span> 步,
建立 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 得下界;
接下来得 <span class="math inline">\(M\)</span> 步, 就是在给定 <span class="math inline">\(Q_i\left(z^{(i)}\right)\)</span> 后, 调整 <span class="math inline">\(\theta\)</span> 去极大化 <span class="math inline">\(\mathcal{L}(\theta)\)</span> 的下界即 <span class="math display">\[
\begin{aligned}
&amp; \underset{\theta}{\arg \max } \sum_{i=1}^m \log p\left(x^{(i)} ;
\theta\right) \\
&amp; \Leftrightarrow \underset{\theta}{\arg \max } \sum_{i=1}^m
\sum_{z^{(i)}} Q_i\left(z^{(i)}\right) \log \frac{p\left(x^{(i)},
z^{(i)} ; \theta\right)}{Q_i\left(z^{(i)}\right)} \\
&amp; \Leftrightarrow \underset{\theta}{\arg \max } \sum_{i=1}^m
\sum_{z^{(i)}} Q_i\left(z^{(i)}\right)\left[\log p\left(x^{(i)}, z^{(i)}
; \theta\right)-\log Q_i\left(z^{(i)}\right)\right] \\
&amp; \Leftrightarrow \underset{\theta}{\arg \max } \sum_{i=1}^m
\sum_{z^{(i)}} Q_i\left(z^{(i)}\right) \log p\left(x^{(i)}, z^{(i)} ;
\theta\right)
\end{aligned}
\]</span> 因此EM算法的迭代形式为：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221619542.jpg" alt="img" style="zoom:50%;"></p>
<p><img src="https://pic3.zhimg.com/80/v2-2f7fc5ca144d2f85f14d46e88055dd86_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>这张图的意思就是: 首先我们固定 <span class="math inline">\(\theta\)</span>, 调整 <span class="math inline">\(Q(z)\)</span> 使下界 <span class="math inline">\(J(z, Q)\)</span> 上升至与 <span class="math inline">\(L(\theta)\)</span> 在此点 <span class="math inline">\(\theta\)</span> 处相等 (绿色曲线到 蓝色曲线) ,
然后固定 <span class="math inline">\(Q(z)\)</span>, 调整 <span class="math inline">\(\theta\)</span> 使下界 <span class="math inline">\(J(z, Q)\)</span> 达到最大值 <span class="math inline">\(\left(\theta_t\right.\)</span> 到 <span class="math inline">\(\left.\theta_{t+1}\right)\)</span> ，然后再固定
<span class="math inline">\(\theta\)</span>, 调整 <span class="math inline">\(Q(z)\)</span> , 一直到收敛到似然函数 <span class="math inline">\(L(\theta)\)</span> 的最大值处的 <span class="math inline">\(\theta\)</span> 。</p>
<p><strong><font color="red"> EM 算法通过引入隐含变量，使用
MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM
算法首先会固定其中的第一个参数，然后使用 MLE
计算第二个变量值；接着通过固定第二个变量，再使用 MLE
估测第一个变量值，依次迭代，直至收敛到局部最优解。</font></strong></p>
<h4><span id="32-em算法的收敛性">3.2 EM算法的收敛性</span></h4>
<p>不妨假设 <span class="math inline">\(\theta^{(k)}\)</span> 和 <span class="math inline">\(\theta^{(k+1)}\)</span> 是EM算法第 <span class="math inline">\(k\)</span> 次迭代和第 <span class="math inline">\(k+1\)</span> 次迭代的结果, 要确保 <span class="math inline">\(E M\)</span> 算法收敛那么等价于证明 <span class="math inline">\(\mathcal{L}\left(\theta^{(k)}\right) \leq
\mathcal{L}\left(\theta^{(k+1)}\right)\)</span>
也就是说极大似然估计单调增加,
那么算法最终会迭代到极大似然估计的最大值。在选定 <span class="math inline">\(\theta^{(k)}\)</span> 后可以得到 <span class="math inline">\(E\)</span> 步 <span class="math inline">\(Q_i^{(k)}\left(z^{(i)}\right)=p\left(z^{(i)} \mid
x^{(i)} ; \theta^{(k)}\right)\)</span>, 这一步保证了在给定 <span class="math inline">\(\theta^{(k)}\)</span> 时, 詹森不等式中的等式成立即
<span class="math inline">\(\mathcal{L}\left(\theta^{(k)}\right)=\sum_{i=1}^m
\sum_{z^{(i)}} Q_i^{(k)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)},
z^{(i)} ; \theta^{(k)}\right)}{Q_i\left(z^{(i)}\right)}\)</span></p>
<p>然后再进行 <span class="math inline">\(M\)</span> 步, 固定 <span class="math inline">\(Q_i^{(k)}\left(z^{(i)}\right)\)</span> 并将 <span class="math inline">\(\theta^{(k)}\)</span> 视作变量, 对上式 <span class="math inline">\(\mathcal{L}\left(\theta^{(k)}\right)\)</span>
求导后得到 <span class="math inline">\(\theta^{(k+1)}\)</span>
因此有如下式子成立 <span class="math display">\[
\begin{aligned}
\mathcal{L}\left(\theta^{(k)}\right) &amp; =\sum_{i=1}^m \sum_{z^{(i)}}
Q_i^{(k)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ;
\theta^{(k)}\right)}{Q_i\left(z^{(i)}\right)} \\
&amp; \leq \sum_{i=1}^m \sum_{z^{(i)}} Q_i^{(k)}\left(z^{(i)}\right)
\log \frac{p\left(x^{(i)}, z^{(i)} ;
\theta^{(k)}\right)}{Q_i\left(z^{(i)}\right)} \\
&amp; \leq \mathcal{L}\left(\theta^{(k+1)}\right)
\end{aligned}
\]</span> 首先 (a) 式是前面 <span class="math inline">\(E\)</span>
步所保证詹森不等式中的等式成立的条件, (a) 到 (b) 是 <span class="math inline">\(M\)</span> 步的定义, (b) 到 (c) 对任意 参数都成立,
而其等式的条件是固定 <span class="math inline">\(\theta\)</span>
并调整好 <span class="math inline">\(Q\)</span> 时成立, <span class="math inline">\((b)\)</span> 到 <span class="math inline">\((c)\)</span> 只是固定 <span class="math inline">\(Q\)</span> 调整 <span class="math inline">\(\theta\)</span>, 在得到 <span class="math inline">\(\theta^{(k+1)}\)</span> 时, 只是最大化 <span class="math inline">\(\mathcal{L}\left(\theta^{(k)}\right)\)</span>,
也就是 <span class="math inline">\(\mathcal{L}\left(\theta^{(k+1)}\right)\)</span>
的一个下界而没有使等式成立。</p>
<h3><span id="4-另一种理解">4. 另一种理解</span></h3>
<p>坐标上升法（Coordinate ascent）：</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221621244.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>途中直线为迭代优化路径，因为每次只优化一个变量，所以可以看到它没走一步都是平行与坐标轴的。</p>
<p>EM 算法类似于坐标上升法，E 步：固定参数，优化 Q；M 步：固定
Q，优化参数。交替将极值推向最大。</p>
<h3><span id="5-应用">5. 应用</span></h3>
<p>EM 的应用有很多，比如、混合高斯模型、聚类、HMM 等等。其中 <strong>EM
在 K-means 中的用处</strong>，我将在介绍 K-means 中的给出。</p>
<h3><span id="参考文献">参考文献</span></h3>
<ol type="1">
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/27976634">怎么通俗易懂地解释 EM
算法并且举个例子?</a></p></li>
<li><p><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/zouxy09/article/details/8537620">从最大似然到
EM 算法浅解</a></p></li>
<li><h5><span id="em算法"></span></h5></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/171MAZN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/171MAZN/" class="post-title-link" itemprop="url">机器学习（17）KNN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:48" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:48+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-10 19:48:26" itemprop="dateModified" datetime="2022-07-10T19:48:26+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2><span id="1-什么是knnkd树-siftbbf算法">1. 什么是KNN【KD树 +
SIFT+BBF算法】</span></h2>
<blockquote>
<p>KNN与KD树：https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272</p>
<p>【数学】kd 树算法之详细篇 - 椰了的文章 - 知乎
https://zhuanlan.zhihu.com/p/23966698</p>
<p><strong>KNN是生成式模型还是判别式的</strong>，为什么？ -
风控算法小白的回答 - 知乎
https://www.zhihu.com/question/475072467/answer/2027766449</p>
</blockquote>
<h3><span id="11-knn的通俗解释">1.1 KNN的通俗解释</span></h3>
<p>何谓K近邻算法，即K-Nearest Neighbor
algorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。</p>
<p>用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，<strong>在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。</strong></p>
<p>​ <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067"><img src="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067" alt="img"></a></p>
<p>​ <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67"><img src="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67" alt="img"></a></p>
<p>如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），KNN就是解决这个问题的。</p>
<p>如果<strong>K=3</strong>，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>红色</strong>的三角形一类。</p>
<p>如果<strong>K=5</strong>，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>蓝色</strong>的正方形一类。</p>
<p><strong>于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</strong></p>
<h3><span id="12-近邻的距离度量">1.2 近邻的距离度量</span></h3>
<p>我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。</p>
<p><strong>有哪些距离度量的表示法</strong>(普及知识点，可以跳过)：</p>
<ol type="1">
<li><p><strong>==欧氏距离==</strong>，最常见的两点之间或多点之间的距离表示法，又称之为<strong>欧几里得度量</strong>，它定义于欧几里得空间中，如点
x = (x1,...,xn) 和 y = (y1,...,yn) 之间的距离为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0699ac91c0bb89297e4cf418ad7d46cac175652d1a1de30fccc66a16a0eef254/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6428782c79293d2535437371727425374228785f312d795f3129253545322b28785f322d795f3229253545322b2e2e2e2b28785f6e2d795f6e29253545322537443d2535437371727425374225354373756d5f253742693d312537442535452537426e25374428785f692d795f692925354532253744"><img src="https://camo.githubusercontent.com/0699ac91c0bb89297e4cf418ad7d46cac175652d1a1de30fccc66a16a0eef254/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6428782c79293d2535437371727425374228785f312d795f3129253545322b28785f322d795f3229253545322b2e2e2e2b28785f6e2d795f6e29253545322537443d2535437371727425374225354373756d5f253742693d312537442535452537426e25374428785f692d795f692925354532253744" alt="img"></a></p>
<ul>
<li><p>二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b21b48fd9bce0b9f47d16d8e5505a6b600a02e1032632df73a77d5ba43012e2a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f322925354532253744"><img src="https://camo.githubusercontent.com/b21b48fd9bce0b9f47d16d8e5505a6b600a02e1032632df73a77d5ba43012e2a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f322925354532253744" alt="img"></a></p></li>
<li><p>三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ebf7fbc4dd6e6eb0d80576e7e5cd19d38ea7a8b9cd801680fc9b39f242367515/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f3229253545322b287a5f312d7a5f322925354532253744"><img src="https://camo.githubusercontent.com/ebf7fbc4dd6e6eb0d80576e7e5cd19d38ea7a8b9cd801680fc9b39f242367515/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f3229253545322b287a5f312d7a5f322925354532253744" alt="img"></a></p></li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/62f0123f5d52ed280b23c7972c60f635d52a02d0b4958bd5bf0a7c61328f3225/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374225354373756d5f2537426b3d312537442535452537426e25374428785f253742316b2537442d785f253742326b2537442925354532253744"><img src="https://camo.githubusercontent.com/62f0123f5d52ed280b23c7972c60f635d52a02d0b4958bd5bf0a7c61328f3225/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374225354373756d5f2537426b3d312537442535452537426e25374428785f253742316b2537442d785f253742326b2537442925354532253744" alt="img"></a></p>
<p>也可以用表示成向量运算的形式：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8e7528374c22f456904f868c6619a94ba0e51180896315eb3b019f9d7b358742/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228612d622928612d622925354554253744"><img src="https://camo.githubusercontent.com/8e7528374c22f456904f868c6619a94ba0e51180896315eb3b019f9d7b358742/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228612d622928612d622925354554253744" alt="img"></a></p></li>
</ul></li>
<li><p><strong>曼哈顿距离</strong>，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在<strong>==欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和==</strong>。例如在平面上，坐标（x1,
y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为： <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/eeacd6c593ac3eadb18eebfdaf9e27626a54b8f8bdb9f53bc11a90713e8b0bf8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253743785f312d785f322537432b253743795f312d795f32253743"><img src="https://camo.githubusercontent.com/eeacd6c593ac3eadb18eebfdaf9e27626a54b8f8bdb9f53bc11a90713e8b0bf8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253743785f312d785f322537432b253743795f312d795f32253743" alt="img"></a>，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。</p>
<p>通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，此即曼哈顿距离名称的来源，
同时，曼哈顿距离也称为城市街区距离(City Block distance)。</p>
<ul>
<li><p>二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/c914feff78b9ccb3f920a67c8dba69b110d641abde5f2533f184a656780cb94d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d253743785f312d785f322537432b253743795f312d795f32253743"><img src="https://camo.githubusercontent.com/c914feff78b9ccb3f920a67c8dba69b110d641abde5f2533f184a656780cb94d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d253743785f312d785f322537432b253743795f312d795f32253743" alt="img"></a></p></li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/eb4a219d2fbe50979339a7e1a83464ec27957562f106da7a3b08da8cc3394795/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d25354373756d5f2537426b3d312537442535452537426e253744253743785f253742316b2537442d785f253742326b253744253743"><img src="https://camo.githubusercontent.com/eb4a219d2fbe50979339a7e1a83464ec27957562f106da7a3b08da8cc3394795/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d25354373756d5f2537426b3d312537442535452537426e253744253743785f253742316b2537442d785f253742326b253744253743" alt="img"></a></p></li>
</ul></li>
<li><p><strong>切比雪夫距离</strong>，若二个向量或二个点p 、and
q，其座标分别为Pi及qi，则两者之间的切比雪夫距离定义如下：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/3a305bb0cf8cdcd6a77f9e2e2cc1085542ae46f8c70c5b43c67be9728d3d99d6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686562797368657625374428702c71293d6d61785f6928253743705f692d715f6925374329"><img src="https://camo.githubusercontent.com/3a305bb0cf8cdcd6a77f9e2e2cc1085542ae46f8c70c5b43c67be9728d3d99d6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686562797368657625374428702c71293d6d61785f6928253743705f692d715f6925374329" alt="img"></a></p>
<p>这也等于以下Lp度量的极值： <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8fe6945bfb5795e12cfe28b72758c59a598e0c5a89b1ffb874f01cc8ff77a7ba/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d392d32345f32322d31392d34312e706e67"><img src="https://camo.githubusercontent.com/8fe6945bfb5795e12cfe28b72758c59a598e0c5a89b1ffb874f01cc8ff77a7ba/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d392d32345f32322d31392d34312e706e67" alt="img"></a>，因此切比雪夫距离也称为L∞度量。</p>
<p>以数学的观点来看，切比雪夫距离是由一致范数（uniform
norm）（或称为上确界范数）所衍生的度量，也是超凸度量（injective metric
space）的一种。</p>
<p>在平面几何中，若二点p及q的直角坐标系坐标为(x1,y1)及(x2,y2)，则切比雪夫距离为：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2c68232279b3d8c857a5ebf5bcb91f6792abeb47daedeb22214391611680dd97/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686573732537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329"><img src="https://camo.githubusercontent.com/2c68232279b3d8c857a5ebf5bcb91f6792abeb47daedeb22214391611680dd97/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686573732537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329" alt="img"></a></p>
<p><strong>玩过国际象棋的朋友或许知道，国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？。你会发现最少步数总是max(
| x2-x1 | , | y2-y1 | ) 步
。有一种类似的一种距离度量方法叫切比雪夫距离。</strong></p>
<ul>
<li><p>二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离 ：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/9387d181258b2a722bcabf891beb2d616a2ec297f35bade87679099641f27f09/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329"><img src="https://camo.githubusercontent.com/9387d181258b2a722bcabf891beb2d616a2ec297f35bade87679099641f27f09/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329" alt="img"></a></p></li>
<li><p>两个n维向量a(x11,x12,…,x1n)与
b(x21,x22,…,x2n)间的切比雪夫距离：</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ea4a630eab19efb4c0270f3808dde70fec7bb5f21d37f76ab016316277fbb730/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d61785f6928253743785f25374231692537442d785f253742326925374425374329"><img src="https://camo.githubusercontent.com/ea4a630eab19efb4c0270f3808dde70fec7bb5f21d37f76ab016316277fbb730/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d61785f6928253743785f25374231692537442d785f253742326925374425374329" alt="img"></a></p></li>
</ul></li>
</ol>
<p><strong>==简单说来，各种“距离”的应用场景简单概括为：==</strong></p>
<ul>
<li><strong>空间：欧氏距离</strong>，</li>
<li><strong>路径：曼哈顿距离，国际象棋国王：切比雪夫距离</strong>，</li>
<li>以上三种的统一形式:闵可夫斯基距离，</li>
<li>加权：标准化欧氏距离，</li>
<li>排除量纲和依存：马氏距离，</li>
<li>向量差距：夹角余弦，</li>
<li><strong>编码差别：汉明距离</strong>，</li>
<li>集合近似度：杰卡德类似系数与距离，</li>
<li>相关：相关系数与相关距离。</li>
</ul>
<h3><span id="13-k值选择">1.3 K值选择</span></h3>
<ol type="1">
<li>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></li>
<li>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且<strong>K值的增大就意味着整体的模型变得简单。</strong></li>
<li>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</li>
</ol>
<p>在实际应用中，K值一般取一个比较小的数值，<strong>例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</strong></p>
<h3><span id="14-knn最近邻分类算法的过程">1.4 KNN最近邻分类算法的过程</span></h3>
<ol type="1">
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前 k 个最小距离的样本；</li>
<li>根据这 k 个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<h2><span id="关于knn的一些问题">关于KNN的一些问题</span></h2>
<ol type="1">
<li><p>在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用<strong>曼哈顿距离</strong>？</p>
<p><strong>答：</strong>我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向的运动。</p></li>
<li><p>KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?</p>
<p>答：极大的节约了时间成本．点线距离如果
&gt;　最小点，无需回溯上一层，如果&lt;,则再上一层寻找。</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://powerlzy.github.io/posts/3BS4T2Z/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat_mac.jpg">
      <meta itemprop="name" content="lzy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PowerLZY's Blog">
      <meta itemprop="description" content="相比到达的地方，同行的人更重要！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | PowerLZY's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/3BS4T2Z/" class="post-title-link" itemprop="url">降维（1）PCA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-16 16:35:35" itemprop="dateCreated datePublished" datetime="2022-03-16T16:35:35+08:00">2022-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-04-22 16:27:10" itemprop="dateModified" datetime="2023-04-22T16:27:10+08:00">2023-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">降维与度量学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>数据降维算法: https://www.zhihu.com/column/c_1194552337170214912</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221528241.jpg" alt="【机器学习】降维——PCA（非常详细）" style="zoom: 50%;"></p>
<h2><span id="一-pca">一、 PCA</span></h2>
<p><strong><font color="red"> 降维问题的优化目标：将一组 <span class="math inline">\(\mathrm{N}\)</span> 维向量降为 <span class="math inline">\(\mathrm{K}\)</span> 维，其目标是选择 <span class="math inline">\(\mathrm{K}\)</span>
个单位正交基，使得原始数据变换到这组基上 后，各变量两两间协方差为 0
，而变量方差则尽可能大（在正交的约束下，取最大的 <span class="math inline">\(\mathrm{K}\)</span> 个方差）。</font></strong></p>
<p>要找的 <span class="math inline">\(\mathbf{P}\)</span>
是能让<strong>原始协方差矩阵对角化</strong>的 <span class="math inline">\(\mathbf{P}\)</span> 。换句话说,
优化目标变成了寻找一个矩阵 <span class="math inline">\(\mathbf{P}\)</span>, <strong>满足 <span class="math inline">\(P C P^T\)</span> 是一个对
角矩阵，并且对角元素按从大到小依次排列，那么 <span class="math inline">\(P\)</span> 的前 <span class="math inline">\(K\)</span> 行就是要寻找的基，用 <span class="math inline">\(P\)</span> 的前 <span class="math inline">\(K\)</span> 行组成的矩阵乘以 <span class="math inline">\(X\)</span> 就使得 X 从 N 维降到了 K
维并满足上述优化条件。</strong></p>
<p><strong>PCA (Principal Component Analysis) 是一种常见的数据分析方式,
常用于高维数据的降维，可用于提取数 据的主要特征分量。</strong>PCA
的数学推导可以从<strong>最大可分型</strong>和<strong>最近重构性</strong>两方面进行,
前者的优化条件为划分后方差 最大,
后者的优化条件为点到划分平面距离最小，这里我将从最大可分性的角度进行证明。</p>
<h3><span id="1-向量表示与基变换">1. 向量表示与基变换</span></h3>
<p>我们先来介绍些线性代数的基本知识。</p>
<h4><span id="111-内积">1.1.1 内积</span></h4>
<p><strong>两个向量的 A 和 B 内积</strong>我们知道形式是这样的： <span class="math display">\[
\left(a_1, a_2, \cdots, a_n\right) \cdot\left(b_1, b_2, \cdots,
b_n\right)^{\top}=a_1 b_1+a_2 b_2+\cdots+a_n b_n
\]</span>
内积运算将两个向量映射为实数，其计算方式非常容易理解，但我们无法看出其物理含义。接下来我们从几何角度
来分析，为了简单起见，我们假设 <span class="math inline">\(A\)</span> 和
<span class="math inline">\(B\)</span> 均为二维向量，则： <span class="math display">\[
A=\left(x_1, y_1\right), \quad B=\left(x_2, y_2\right) A \cdot B=|A||B|
\cos (\alpha)
\]</span> 其几何表示见下图：</p>
<figure>
<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304221530907.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们看出 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(B\)</span> 的内积等于 <span class="math inline">\(A\)</span> 到 <span class="math inline">\(B\)</span> 的投影长度乘以 <span class="math inline">\(B\)</span> 的模。</p>
<p>如果假设 <span class="math inline">\(\mathrm{B}\)</span> 的模为 1 ，
即让 <span class="math inline">\(|B|=1\)</span> ，那么就变成了: <span class="math display">\[
A \cdot B=|A| \cos (a)
\]</span> 也就是说, <strong>A 与 <span class="math inline">\(B\)</span>
的内积值等于 A 向 <span class="math inline">\(B\)</span>
所在直线投影的标量大小。</strong></p>
<h4><span id="12-基">1.2 基</span></h4>
<p>在我们常说的坐标系种, 向量 <span class="math inline">\((3,2)\)</span>
其实隐式引入了一个定义：以 <span class="math inline">\(x\)</span> 轴和
<span class="math inline">\(y\)</span> 轴上正方向长度为 1
的向量为标准。向 量 <span class="math inline">\((3,2)\)</span>
实际是说在 <span class="math inline">\(x\)</span> 轴投影为 3 而 <span class="math inline">\(y\)</span> 轴的投影为 2。注意投影是一个标量,
所以可以为负。</p>
<p>所以, 对于向量 <span class="math inline">\((3,2)\)</span> 来说,
如果我们想求它在 <span class="math inline">\((1,0),(0,1)\)</span>
这组基下的坐标的话, 分别内积即可。当然, 内积完 了还是 <span class="math inline">\((3,2)\)</span> 。</p>
<p>所以, 我们大致可以得到一个结论, 我们<strong>要准确描述向量,
首先要确定一组基, 然后给出在基所在的各个直线上的 投影值,
就可以了</strong>。为了方便求坐标, 我们希望这组基向量模长为
1。因为向量的内积运算, 当模长为 1 时, 内积
可以直接表示投影。然后还需要这组基是线性无关的, 我们一般用正交基,
非正交的基也是可以的, 不过正交基有 较好的性质。</p>
<h4><span id="13-基变换的矩阵表示">1.3 基变换的矩阵表示</span></h4>
<p>这里我们先做一个练习：对于向量 <span class="math inline">\((3,2)\)</span> 这个点来说, 在 <span class="math inline">\(\left(\frac{1}{\sqrt{2}},
\frac{1}{\sqrt{2}}\right)\)</span> 和 <span class="math inline">\(\left(-\frac{1}{\sqrt{2}},
\frac{1}{\sqrt{2}}\right)\)</span> 这组基下的坐标是多少? 我们拿 <span class="math inline">\((3,2)\)</span> 分别与之内积, 得到 <span class="math inline">\(\left(\frac{5}{\sqrt{2}},-\frac{1}{\sqrt{2}}\right)\)</span>
这个新坐标。</p>
<p>我们可以用矩阵相乘的形式简洁的表示这个变换： <span class="math display">\[
\left(\begin{array}{cc}
1 / \sqrt{2} &amp; 1 / \sqrt{2} \\
-1 / \sqrt{2} &amp; 1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{l}
3 \\
2
\end{array}\right)=\left(\begin{array}{c}
5 / \sqrt{2} \\
-1 / \sqrt{2}
\end{array}\right)
\]</span> 左边矩阵的两行分别为两个基, 乘以原向量,
其结果刚好为新基的坐标。推广一下, 如果我们有 <span class="math inline">\(m\)</span> 个二维向量,
只要将二维向量按列排成一个两行 <span class="math inline">\(m\)</span>
列矩阵, 然后用“基矩阵”乘以这个矩阵就可以得到了所有这些向量在新基下
的值。例如对于数据点 <span class="math inline">\((1,1),(2,2),(3,3)\)</span> 来说,
想变换到刚才那组基上, 则可以这样表示: <span class="math display">\[
\left(\begin{array}{cc}
1 / \sqrt{2} &amp; 1 / \sqrt{2} \\
-1 / \sqrt{2} &amp; 1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{lll}
1 &amp; 2 &amp; 3 \\
1 &amp; 2 &amp; 3
\end{array}\right)=\left(\begin{array}{ccc}
2 / \sqrt{2} &amp; 4 / \sqrt{2} &amp; 6 / \sqrt{2} \\
0 &amp; 0 &amp; 0
\end{array}\right)
\]</span> 我们可以把它写成通用的表示形式： <span class="math display">\[
\left(\begin{array}{c}
p_1 \\
p_2 \\
\vdots \\
p_R
\end{array}\right)\left(\begin{array}{llll}
a_1 &amp; a_2 &amp; \cdots &amp; a_M
\end{array}\right)=\left(\begin{array}{cccc}
p_1 a_1 &amp; p_1 a_2 &amp; \cdots &amp; p_1 a_M \\
p_2 a_1 &amp; p_2 a_2 &amp; \cdots &amp; p_2 a_M \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
p_R a_1 &amp; p_R a_2 &amp; \cdots &amp; p_R a_M
\end{array}\right)
\]</span> 其中 <span class="math inline">\(p_i\)</span> 是一个行向量,
表示第 <span class="math inline">\(\mathrm{i}\)</span> 个基, <span class="math inline">\(a_j\)</span> 是一个列向量, 表示第 <span class="math inline">\(\mathrm{j}\)</span>
个原始数据记录。实际上也就是做了一个向 量矩阵化的操作。</p>
<p><font color="red"> 上述分析给矩阵相乘找到了一种物理解释:
两个矩阵相乘的意义是将右边矩阵中的每一列向量 <span class="math inline">\(a_i\)</span> 变换到左边矩阵
中以每一行行向量为基所表示的空间中去。也就是说一个矩阵可以表示一种线性变换。</font></p>
<h3><span id="2-最大可分性">2. 最大可分性</span></h3>
<p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示,
<strong>如果基的数量少于向量本身的维数, 则可以达
到降维的效果。</strong></p>
<p><strong>但是我们还没回答一个最关键的问题:
如何选择基才是最优的。或者说, 如果我们有一组 <span class="math inline">\(\mathbf{N}\)</span> 维向量, 现在要将其 降到 K
维（K 小于 N），那么我们应该如何选择 <span class="math inline">\(\mathrm{K}\)</span>
个基才能最大程度保留原有的信息？</strong></p>
<p>一种直观的看法是： <font color="red"> 希望投影后的投影值尽可能分散,
因为如果重叠就会有样本消失。当然这个也可以从樀的角
度进行理解，樀越大所含信息越多。</font></p>
<h4><span id="21-方差">2.1 方差</span></h4>
<p>我们知道数值的分散程度,
可以用数学上的方差来表述。<strong>一个变量的方差可以看做是每个元素与变量均值的差的平
方和的均值</strong>, 即: <span class="math display">\[
\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^m\left(a_i-\mu\right)^2
\]</span> <strong>为了方便处理，我们将每个变量的均值都化为 0</strong> ,
因此方差可以直接用每个元素的平方和除以元素个数表示: <span class="math display">\[
\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^m a_i^2
\]</span>
于是上面的问题被形式化表述为：<strong>寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大</strong>。</p>
<h4><span id="22-协方差">2.2 协方差</span></h4>
<p>在一维空间中我们可以用方差来表示数据的分散程度。而对于高维数据，我们用协方差进行约束，<strong>协方差可以表示两个变量的相关性</strong>。<strong><font color="red">
为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性</font></strong>，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。</p>
<p>协方差公式为：</p>
<p><span class="math display">\[
\operatorname{Cov}(a, b)=\frac{1}{m-1}
\sum_{i=1}^m\left(a_i-\mu_a\right)\left(b_i-\mu_b\right)
\]</span></p>
<p>由于均值为 0，所以我们的协方差公式可以表示为：</p>
<p><span class="math display">\[
\operatorname{Cov}(a, b)=\frac{1}{m} \sum_{i=1}^m a_i b_i
\]</span> 当样本数较大时，不必在意其是 m 还是
m-1，为了方便计算，我们分母取 m。</p>
<p><strong><font color="red"> 协方差为 0
时，表示两个变量完全不相关</font></strong>。为了让协方差为
0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。</p>
<p>（<strong>补充</strong>：协方差为 0
时，两个变量只是线性不相关。完全独立是有问题的，才疏学浅，还望见谅。）</p>
<p><strong><font color="red"> 至此，我们得到了降维问题的优化目标：将一组
N 维向量降为 K 维，其目标是选择 K
个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为
0，而变量方差则尽可能大（在正交的约束下，取最大的 K
个方差）。</font></strong></p>
<h4><span id="23-协方差矩阵">2.3 协方差矩阵</span></h4>
<p>针对我们给出的优化目标，接下来我们将从数学的角度来给出优化目标。我们看到，最终要达到的目的与<strong>变量内方差及变量间协方差</strong>有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们有：</p>
<p>假设我们只有 <span class="math inline">\(\mathrm{a}\)</span> 和 <span class="math inline">\(\mathrm{b}\)</span> 两个变量,
那么我们将它们按行组成矩阵 <span class="math inline">\(\mathrm{X}\)</span> : <span class="math display">\[
X=\left(\begin{array}{cccc}
a_1 &amp; a_2 &amp; \cdots &amp; a_m \\
b_1 &amp; b_2 &amp; \cdots &amp; b_m
\end{array}\right)
\]</span> 然后: <span class="math display">\[
\frac{1}{m} X X^{\top}=\left(\begin{array}{cc}
\frac{1}{m} \sum_{i=1}^m a_i^2 &amp; \frac{1}{m} \sum_{i=1}^m a_i b_i \\
\frac{1}{m} \sum_{i=1}^m a_i b_i &amp; \frac{1}{m} \sum_{i=1}^m b_i^2
\end{array}\right)=\left(\begin{array}{cc}
\operatorname{Cov}(a, a) &amp; \operatorname{Cov}(a, b) \\
\operatorname{Cov}(b, a) &amp; \operatorname{Cov}(b, b)
\end{array}\right)
\]</span> 我们可以看到这个矩阵对角线上的分别是两个变量的方差,
而其它元素是 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span> 的协方差。两者被统一到了一个
矩阵里。</p>
<p>设我们有 <span class="math inline">\(\mathrm{m}\)</span> 个 <span class="math inline">\(\mathrm{n}\)</span> 维数据记录, 将其排列成矩阵
<span class="math inline">\(X_{n, m}\)</span>, 设 <span class="math inline">\(C=\frac{1}{m} X X^T\)</span>, 则 <span class="math inline">\(\mathrm{C}\)</span> 是一个对称矩阵, 其对角线分别
对应各个变量的方差, 而第 <span class="math inline">\(\mathrm{i}\)</span>
行 <span class="math inline">\(\mathrm{j}\)</span> 列和 <span class="math inline">\(\mathrm{j}\)</span> 行 <span class="math inline">\(\mathrm{i}\)</span> 列元素相同, 表示 <span class="math inline">\(\mathrm{i}\)</span> 和 <span class="math inline">\(\mathrm{j}\)</span> 两个变量的协方差。</p>
<h4><span id="24-矩阵对角化">2.4 矩阵对角化</span></h4>
<p>根据我们的优化条件，<strong>我们需要将除对角线外的其它元素化为
0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大）</strong>，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系。</p>
<p>设原始数据矩阵 <span class="math inline">\(X\)</span>
对应的协方差矩阵为 <span class="math inline">\(C\)</span>, 而 <span class="math inline">\(P\)</span> 是一组基按行组成的矩阵, 设 <span class="math inline">\(Y=P X\)</span>, 则 <span class="math inline">\(Y\)</span> 为 <span class="math inline">\(X\)</span> 对 <span class="math inline">\(P\)</span> 做基变换后的 数据。设 <span class="math inline">\(Y\)</span> 的协方差矩阵为 <span class="math inline">\(D\)</span>, 我们推导一下 <span class="math inline">\(D\)</span> 与 <span class="math inline">\(C\)</span> 的关系: <span class="math display">\[
\begin{aligned}
D &amp; =\frac{1}{m} Y Y^T \\
&amp; =\frac{1}{m}(P X)(P X)^T \\
&amp; =\frac{1}{m} P X X^T P^T \\
&amp; =P\left(\frac{1}{m} X X^T\right) P^T \\
&amp; =P C P^T
\end{aligned}
\]</span> 这样我们就看清楚了, 我们要找的 <span class="math inline">\(\mathrm{P}\)</span> 是能让原始协方差矩阵对角化的
<span class="math inline">\(\mathrm{P}\)</span> 。换句话说,
优化目标变成了<strong>寻找一个矩 阵 <span class="math inline">\(\mathbf{P}\)</span>, 满足 <span class="math inline">\(P C P^T\)</span>
是一个对角矩阵，并且对角元素按从大到小依次排列，那么 <span class="math inline">\(\mathbf{P}\)</span> 的前 <span class="math inline">\(\mathbf{K}\)</span> 行就是要寻找的基, 用 <span class="math inline">\(\mathbf{P}\)</span> 的前 <span class="math inline">\(\mathrm{K}\)</span> 行组成的矩阵乘以 <span class="math inline">\(\mathrm{X}\)</span> 就使得 <span class="math inline">\(\mathrm{X}\)</span> 从 <span class="math inline">\(\mathrm{N}\)</span> 维降到了 <span class="math inline">\(\mathrm{K}\)</span>
维并满足上述优化条件。</strong></p>
<p>至此, 我们离 PCA 还有仅一步之遥, 我们还需要完成对角化。</p>
<p><strong>由上文知道，协方差矩阵 C
是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质:</strong></p>
<ol type="1">
<li>实对称矩阵不同特征值对应的特征向量必然正交。</li>
<li>设特征向量 <span class="math inline">\(\lambda\)</span> 重数为 <span class="math inline">\(r\)</span>, 则必然存在 <span class="math inline">\(r\)</span> 个线性无关的特征向量对应于 <span class="math inline">\(\lambda\)</span> ，因此可以将这 <span class="math inline">\(r\)</span> 个特征向量单位正 交化。</li>
</ol>
<p>由上面两条可知, 一个 <span class="math inline">\(\mathrm{n}\)</span>
行 <span class="math inline">\(\mathrm{n}\)</span>
列的实对称矩阵一定可以找到 <span class="math inline">\(\mathrm{n}\)</span> 个单位正交特征向量, 设这 <span class="math inline">\(\mathrm{n}\)</span> 个特征向量为 <span class="math inline">\(e_1, e_2, \cdots, e_n\)</span>,
我们将其按列组成矩阵: <span class="math inline">\(E=\left(e_1, e_2,
\cdots, e_n\right)\)</span> 。</p>
<p>则对协方差矩阵 C 有如下结论: <span class="math display">\[
E^T C E=\Lambda=\left(\begin{array}{llll}
\lambda_1 &amp; &amp; &amp; \\
&amp; \lambda_2 &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \lambda_n
\end{array}\right)
\]</span> 其中 <span class="math inline">\(\Lambda\)</span> 为对角矩阵,
其对角元素为各特征向量对应的特征值（可能有重复）。到这里,
我们发现我们已经找到了 需要的矩阵 P: <span class="math inline">\(P=E^{\top}\)</span> 。</p>
<p><strong><span class="math inline">\(P\)</span>
是协方差矩阵的特征向量单位化后按行排列出的矩阵</strong>，其中每一行都是
<span class="math inline">\(C\)</span> 的一个特征向量。如果设 <span class="math inline">\(P\)</span> 按照 <span class="math inline">\(\Lambda\)</span> 中 特征值的从大到小,
将特征向量从上到下排列, 则用 <span class="math inline">\(P\)</span> 的前
<span class="math inline">\(K\)</span> 行组成的矩阵乘以原始数据矩阵
<span class="math inline">\(X\)</span>, 就得到了我们
需要的降维后的数据矩阵 <span class="math inline">\(Y\)</span> 。</p>
<blockquote>
<p><strong>拉格朗日乘子法证明</strong>:<strong>方差就是协方差矩阵的特征值</strong></p>
</blockquote>
<h4><span id="25-最近重构性-思路">2.5 最近重构性-思路</span></h4>
<p>以上的证明思路主要是基于最大可分性的思想，<strong>通过一条直线使得样本点投影到该直线上的方差最大</strong>。除此之外，我们还可以<strong>将其转换为线型回归问题，其目标是求解一个线性函数使得对应直线能够更好地拟合样本点集合</strong>。这就<strong>使得我们的优化目标从方差最大转化为平方误差最小</strong>，因为映射距离越短，丢失的信息也会越小。区别于最大可分性，这是从最近重构性的角度进行论证。</p>
<h3><span id="3-求解步骤">3. 求解步骤</span></h3>
<p><strong>总结一下 PCA 的算法步骤：设有 <span class="math inline">\(m\)</span> 条 <span class="math inline">\(n\)</span> 维数据。</strong></p>
<ol start="3" type="1">
<li>将原始数据按列组成 <span class="math inline">\(\mathbf{n}\)</span>
行 <span class="math inline">\(\mathbf{m}\)</span> 列矩阵 <span class="math inline">\(\mathbf{X}\)</span>;</li>
<li>将 <span class="math inline">\(\mathrm{X}\)</span>
的每一行进行零均值化，即减去这一行的均值；【零均值化】【方差、协方差好计算】</li>
<li>求出协方差矩阵 <span class="math inline">\(C=\frac{1}{m} X
X^{\top}\)</span>;</li>
<li>求出协方差矩阵的特征值及对应的特征向量;</li>
<li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 <span class="math inline">\(\mathbf{k}\)</span> 行组成矩阵 <span class="math inline">\(\mathbf{P}\)</span>;</li>
<li><span class="math inline">\(Y=P X\)</span> 即为降维到 <span class="math inline">\(\mathbf{k}\)</span> 维后的数据。</li>
</ol>
<h3><span id="4性质维度灾难-降噪-过拟合-特征独立">4.
性质【维度灾难、降噪、过拟合、特征独立】</span></h3>
<ol type="1">
<li><strong>缓解维度灾难</strong>：PCA
算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；</li>
<li><strong>降噪</strong>：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；</li>
<li><strong>过拟合</strong>：PCA
保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以
PCA 也可能加剧了过拟合；</li>
<li><strong>特征独立</strong>：PCA
不仅将数据压缩到低维，它也使得<strong>降维之后的数据各特征相互独立</strong>；</li>
</ol>
<h3><span id="5-细节">5. 细节</span></h3>
<h4><span id="51-零均值化">5.1 零均值化</span></h4>
<p>当对训练集进行 PCA
降维时，也需要对验证集、测试集执行同样的降维。而<strong>对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来</strong>，不能使用验证集或者测试集的中心向量。</p>
<p>其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。</p>
<p>另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现
Variance Shift 的问题。</p>
<h4><span id="52-svd-的对比">5.2 SVD 的对比</span></h4>
<p>这是两个不同的数学定义。我们先给结论：<strong>特征值和特征向量是针对方阵</strong>才有的，而<strong>对任意形状的矩阵都可以做奇异值分解</strong>。</p>
<p><strong>PCA</strong>：<strong>方阵的特征值分解</strong>，对于一个方阵
A。其中，Q 是这个矩阵 A 的特征向量组成的矩阵， <span class="math inline">\(\Lambda\)</span>
是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列)。也就是说矩阵
A 的信息可以由其特征值和特征向量表示。</p>
<p><strong>SVD：矩阵的奇异值分解其实就是对于矩阵 <span class="math inline">\(\mathrm{A}\)</span> 的协方差矩阵 <span class="math inline">\(A^T A\)</span> 和 <span class="math inline">\(A
A^T\)</span> 做特征值分解推导出来的</strong>: <span class="math display">\[
A_{m, n}=U_{m, m} \Lambda_{m, n} V_{n, n}^T \approx U_{m, k} \Lambda_{k,
k} V_{k, n}^T
\]</span> 其中: <span class="math inline">\(U，V\)</span> 都是正交矩阵,
有 <span class="math inline">\(U^T U=I_m, V^T V=I_n\)</span>
。这里的约等于是因为 <span class="math inline">\(\Lambda\)</span> 中有
<span class="math inline">\(\mathrm{n}\)</span> 个奇异值, 但是由于排在
后面的很多接近 0, 所以我们可以仅保留比较大的 <span class="math inline">\(\mathrm{k}\)</span> 个奇异值。 <span class="math display">\[
\begin{aligned}
&amp; A^T A=\left(U \Lambda V^T\right)^T U \Lambda V^T=V \Lambda^T U^T U
\Lambda V^T=V \Lambda^2 V^T \\
&amp; A A^T=U \Lambda V^T\left(U \Lambda V^T\right)^T=U \Lambda V^T V
\Lambda^T U^T=U \Lambda^2 U^T
\end{aligned}
\]</span> 所以, <span class="math inline">\(V \cup\)</span>
两个矩阵分别是 <span class="math inline">\(A^T A\)</span> 和 <span class="math inline">\(A A^T\)</span> 的特征向量,
中间的矩阵对角线的元素是 <span class="math inline">\(A^T A\)</span> 和
<span class="math inline">\(A A^T\)</span> 的特征值。我 们也很容易看出
<span class="math inline">\(\mathrm{A}\)</span> 的奇异值和 <span class="math inline">\(A^T A\)</span> 的特征值之间的关系。</p>
<p>PCA 需要对协方差矩阵 <span class="math inline">\(C=\frac{1}{m} X
X^T\)</span> 。进行特征值分解; SVD 也是对 <span class="math inline">\(A^T A\)</span> 进行特征值分解。如果取 <span class="math inline">\(A=\frac{X^T}{\sqrt{m}}\)</span>
则两者基本等价。所以 PCA 问题可以转换成 SVD 求解。</p>
<p><strong>而实际上 Sklearn 的 PCA 就是用 SVD
进行求解的</strong>，原因有以下几点：</p>
<ol type="1">
<li>当样本维度很高时，协方差矩阵计算太慢；</li>
<li>方阵特征值分解计算效率不高；</li>
<li><strong>SVD
除了特征值分解这种求解方式外，还有更高效更准确的迭代求解方式，避免了<span class="math inline">\(A^T A\)</span>的计算；</strong></li>
<li><strong>其实 PCA 与 SVD 的右奇异向量的压缩效果相同</strong>。</li>
</ol>
<h3><span id="参考链接">参考链接</span></h3>
<ol type="1">
<li>《机器学习》周志华</li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.codinglabs.org/articles/pca-tutorial.html">PCA
的数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm">Singular
Value Decomposition (SVD) tutorial</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">机器学习中的数学（4）——线性判别分析（LDA）,
主成分分析（PCA）</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//my.oschina.net/findbill/blog/535044">从SVD到PCA——奇妙的数学游戏</a></li>
<li>scikit-learn：降维算法PCA和SVD
https://blog.csdn.net/HHG20171226/article/details/102981822</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/20/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><span class="page-number current">21</span><a class="page-number" href="/page/22/">22</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/22/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzy</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

  <a href="https://github.com/PowerLZY" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.3/mermaid.min.js","integrity":"sha256-e0o3JYsdjqKajf9eOe22FhioYSz9WofRY4dLKo3F6do="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>
        <script>
        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'powerlzy.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });
        </script></body>
</html>
