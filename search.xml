<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hadoop基础</title>
    <url>/posts/3NVTGC1/</url>
    <content><![CDATA[<blockquote>
<p>  应该是指可以打通整个项目开发过程的，比如基于BS架构的话，就是熟悉前后端，知道怎么把算法模型部署到后端服务器，熟悉分布式计算，能玩转mapreduce，spark，hadoop，能熟练搭建项目框架</p>
</blockquote>
<p>三、大数据（简单了解其中一二个）</p>
<p>Hadoop基础、MapReduce、spark、hive、flink</p>
]]></content>
      <categories>
        <category>大数据处理</category>
      </categories>
  </entry>
  <entry>
    <title>hadoop&amp;spark的</title>
    <url>/posts/2T4G0D4/</url>
    <content><![CDATA[<h1><span id="hadoop和spark的区别和联系">hadoop和spark的区别和联系</span></h1><blockquote>
<p>  ==<strong>总结一句话：spark在hadoop肩膀上可以让大数据跑的更快</strong>==</p>
</blockquote>
<h3><span id="一-hadoop">一、 hadoop</span></h3><h4><span id="11-hadoop简介">1.1 hadoop简介</span></h4><p><strong>Hadoop是一个由Apache基金会所开发的分布式系统基础架构</strong>。 <strong>Hadoop实现了一个分布式文件系统HDFS</strong>。HDFS有高容错性的特点，并且设计用来部署在低廉的硬件上；而且它提供高吞吐量来访问应用程序的数据，适合那些有着超大数据集的应用程序。Hadoop的框架最核心的设计就是：HDFS和MapReduce。<strong>HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算</strong>。</p>
<h4><span id="12-hadoop优点">1.2 hadoop优点</span></h4><p><strong>Hadoop 以一种可靠、高效、可伸缩的方式进行数据处理。</strong></p>
<ul>
<li><p><strong>可靠性</strong>: Hadoop将数据存储在多个备份，Hadoop提供高吞吐量来访问应用程序的数据。</p>
</li>
<li><p><strong>高扩展性</strong>： Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。</p>
</li>
<li><p><strong>高效性</strong>： Hadoop以并行的方式工作，通过并行处理加快处理速度。</p>
</li>
<li><p><strong>高容错性</strong>： Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。</p>
</li>
<li><p><strong>低成本</strong>： Hadoop能够部署在低廉的（low-cost）硬件上。</p>
</li>
</ul>
<h3><span id="二-spark"><strong>二、spark</strong></span></h3><h4><span id="21-spark简介">2.1 spark简介</span></h4><p><strong>Spark 是专为大规模数据处理而设计的快速通用的计算引擎</strong>。Spark拥有Hadoop MapReduce所具有的优点，Spark在Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark性能以及运算速度高于MapReduce。</p>
<h4><span id="22-spark优点">2.2 spark优点</span></h4><p><strong>计算速度快</strong>: 因为spark从磁盘中读取数据，把<strong>中间数据放到内存中</strong>，，完成所有必须的分析处理，将结果写回集群，所以spark更快。</p>
<ul>
<li><p><strong>Spark 提供了大量的库</strong>: 包括Spark Core、Spark SQL、Spark Streaming、MLlib、GraphX。</p>
</li>
<li><p><strong>支持多种资源管理器</strong>: Spark 支持 Hadoop YARN，及其自带的独立集群管理器</p>
</li>
<li><p><strong>操作简单</strong>: 高级 API 剥离了对集群本身的关注，Spark 应用开发者可以专注于应用所要做的计算本身</p>
</li>
</ul>
<h3><span id="三-spark与hadoop的不同点">三、spark与hadoop的不同点</span></h3><h4><span id="31-应用场景不同">3.1 应用场景不同</span></h4><p>Hadoop和Spark两者都是大数据框架，但是各自应用场景是不同的。<strong>Hadoop是一个分布式数据存储架构，它将巨大的数据集分派到一个由普通计算机组成的集群中的多个节点进行存储，降低了硬件的成本</strong>。<strong>Spark是那么一个专门用来对那些分布式存储的大数据进行处理的工具，它要借助hdfs的数据存储</strong>。</p>
<h4><span id="32-处理速度不同">3.2 处理速度不同</span></h4><p><strong>hadoop的MapReduce是分步对数据进行处理的，从磁盘中读取数据，进行一次处理，将结果写到磁盘</strong>，然后在从磁盘中读取更新后的数据，再次进行的处理，最后再将结果存入磁盘，这存取磁盘的过程会影响处理速度。<strong>spark从磁盘中读取数据，把中间数据放到内存中</strong>，，完成所有必须的分析处理，将结果写回集群，所以spark更快。</p>
<h4><span id="33-容错性不同">3.3 容错性不同</span></h4><p><strong>Hadoop将每次处理后的数据都写入到磁盘上，基本谈不上断电或者出错数据丢失的情况</strong>。Spark的数据对象存储在弹性分布式数据集 RDD，RDD是分布在一组节点中的只读对象集合，如果数据集一部分丢失，则可以根据于数据衍生过程对它们进行重建。而且RDD 计算时可以通过 CheckPoint 来实现容错。</p>
<h3><span id="四-spark与hadoop的联系">四、spark与hadoop的联系</span></h3><p>Hadoop提供分布式数据存储功能HDFS，还提供了用于数据处理的MapReduce。 MapReduce是可以不依靠spark数据的处理的。当然spark也可以不依靠HDFS进行运作，它可以依靠其它的分布式文件系统。但是两者完全可以结合在一起，<strong>hadoop提供分布式集群和分布式文件系统</strong>，<strong>spark可以依附在hadoop的HDFS代替MapReduce弥补MapReduce计算能力不足的问题。</strong></p>
]]></content>
      <categories>
        <category>大数据处理</category>
      </categories>
  </entry>
  <entry>
    <title>大数据处理Q&amp;A</title>
    <url>/posts/10D4W42/</url>
    <content><![CDATA[<h4><span id="面试常见的大数据相关问题">面试常见的大数据相关问题</span></h4><ul>
<li><a href="https://anchorety.github.io/2019/08/14/%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/">https://anchorety.github.io/2019/08/14/%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/</a></li>
</ul>
<h4><span id="海量日志数据提取出某日访问百度次数最多的那个ip">海量日志数据，提取出某日访问百度次数最多的那个IP？</span></h4><ul>
<li><a href="http://zoeyyoung.github.io/get-most-visit-ip.html">http://zoeyyoung.github.io/get-most-visit-ip.html</a></li>
</ul>
<p>具体做法如下：</p>
<ol>
<li>按照IP地址的Hash(IP)%1024值, 把海量IP日志分别存储到1024个小文件中.</li>
<li>对于每一个小文件, 构建一个以IP为key, 出现次数为value的HashMap, 同时记录当前出现次数最多的那个IP地址;</li>
<li>得到1024个小文件中的出现次数最多的IP, 再依据常规的排序算法得到总体上出现次数最多的IP.</li>
</ol>
]]></content>
      <categories>
        <category>大数据处理</category>
      </categories>
  </entry>
  <entry>
    <title>大数据处理（1）高维向量相似度匹配</title>
    <url>/posts/DTTFVX/</url>
    <content><![CDATA[<h2><span id="文本相似度匹配">文本相似度匹配</span></h2><blockquote>
<p>  from 《Scaling Up All Pairs Similarity Search》</p>
<p>  海量文本的成对相似度的高性能计算（待续） - 马东什么的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/457947482">https://zhuanlan.zhihu.com/p/457947482</a></p>
</blockquote>
<h4><span id="摘要">摘要：</span></h4><p>对于高维空间中的大量稀疏向量数据，我们研究了寻找相似性分数(由余弦距离等函数确定)高于给定阈值的所有向量对的问题。我们提出了一个简单的算法<strong>，基于新的索引和优化策略，解决了这个问题，而不依赖于近似方法或广泛的参数调整</strong>。我们展示了该方法在广泛的相似阈值设置中有效地处理各种数据集，与以前的最先进的方法相比有很大的加速。</p>
<p>(海量成对文本相似度问题对于反欺诈而言非常重要，因为无论是电商中重要的地址信息，还是设备或用户的离散features之间的相似度计算和构图，都依赖于高性能的文本相似度计算方法，在实践中，我们不可能直接写双循环去做计算，即使是离线也往往需要耗费大量的时间)</p>
<h3><span id="一-介绍">一、介绍</span></h3><p>许多现实世界的应用程序需要解决一个相似度搜索问题，在这个问题中，人们对所有相似度高于指定阈值的对象对都感兴趣。</p>
<ul>
<li>web搜索的查询细化:搜索引擎通常会建议其他查询公式（例如你在百度中输入百，会给你推荐百度）。生成此类查询建议的一种方法是根据查询[19]的搜索结果的相似性来查找所有的相似查询对。由于目标是只提供高质量的建议，所以我们只需要找到相似度高于阈值的查询对（topk问题）。</li>
<li>协同过滤:协同过滤算法通过确定哪些用户有相似的品味来进行推荐。因此，算法需要计算相似度高于某个阈值的相似用户对。</li>
<li>接近重复的文档检测和消除:特别是在文档索引领域，检测和清除等价的文档是重要的。在许多情况下，由于简单的相等性检验不再满足要求，微小修改的存在使这种检测变得困难。通过具有很高的相似度阈值的相似度搜索，可以实现近重复检测（这方面的工作之前看过simhash，google做海量网页去重的方法）。</li>
<li>团伙检测:最近的工作已经应用算法在一个应用程序中寻找所有相似的用户，以识别点击欺诈者[13]团伙。</li>
</ul>
]]></content>
      <categories>
        <category>大数据处理</category>
      </categories>
  </entry>
  <entry>
    <title>局部敏感哈希LSH</title>
    <url>/posts/R9HZV9/</url>
    <content><![CDATA[<h2><span id="一-局部敏感哈希函数">一、局部敏感哈希函数</span></h2><blockquote>
<p>  python_mmdt:ssdeep、tlsh、vhash、mmdthash对比 : <a href="https://www.freebuf.com/sectool/321011.html">https://www.freebuf.com/sectool/321011.html</a></p>
<p>  局部敏感哈希(Locality Sensitive Hashing，LSH)总结：<a href="http://yangyi-bupt.github.io/ml/2015/08/28/lsh.html">http://yangyi-bupt.github.io/ml/2015/08/28/lsh.html</a></p>
</blockquote>
<h3><span id="11-局部敏感哈希的基本概念">1.1 局部敏感哈希的基本概念</span></h3><p>局部敏感哈希(Locality Sensitive Hashing，LSH)的基本思想类似于一种空间域转换思想，LSH算法基于一个假设，<strong>如果两个文本在原有的数据空间是相似的，那么分别经过哈希函数转换以后的它们也具有很高的相似度</strong>；相反，如果它们本身是不相似的，那么经过转换后它们应仍不具有相似性。</p>
<h3><span id="12-hash方法">1.2 hash方法</span></h3><p><strong><a href="https://ssdeep-project.github.io/ssdeep/index.html">CTPH(ssdeep)</a>：Context Triggered Piecewise Hashes(CTPH)</strong>，又叫模糊哈希，最早由Jesse Kornblum博士在2006年提出，论文地址点击<a href="https://ssdeep-project.github.io/ssdeep/index.html">这里</a>。CTPH可用于文件/数据的<strong>同源性判定</strong>。据官方文档介绍，其计算速度是<code>tlsh</code>的两倍（测试了一下，好像并没有）。</p>
<blockquote>
<p>  当使用传统的加密散列时，会为整个文件创建一个散列。单个位的变化会对输出哈希值产生雪崩效应。另一方面，CTPH 为文件的多个固定大小段计算多个传统加密哈希。它使用<em>滚动哈希</em>。</p>
</blockquote>
<p><strong><a href="https://tlsh.org/index.html">tlsh</a>：是趋势科技开源的一款模糊哈希计算工具</strong>，将50字节以上的数据计算生成一个哈希值，通过计算哈希值之间的相似度，从而得到原始文件之间的同源性关联。据官方文档介绍，<code>tlsh</code>比<code>ssdeep</code>和<code>sdhash</code>等其他模糊哈希算法更难攻击和绕过。</p>
<p><a href="https://developers.virustotal.com/reference/files">vhash</a>：（翻遍了整个virustotal的文档，就找到这么一句话）“an in-house similarity clustering algorithm value, based on a simple structural feature hash allows you to find similar files”，大概就是说是个内部相似性聚类算法，允许你通过这个简单的值，找到相似的样本。</p>
<p><a href="https://github.com/a232319779/python_mmdt">mmdthash</a>：是开源的一款模糊哈希计算工具，将任意数据计算生成一个模糊哈希值，通过计算模糊哈希值之间的相似度，从而判断两个数据之间的关联性。详情前文1-5篇。</p>
<blockquote>
<h4><span id="mmdthash">mmdthash：</span></h4><p>  通过重采样之后的数据，我们假设其满足独立同分布。同时，我们将重采样的数据，平均分成N块，每块之间的数据进行累计求和，和值分布近似服从正态分布，我们取和值高x位的一个byte做为本块数据的敏感哈希值。</p>
<p>  51030000:D6E26822530202020202020202020202：</p>
<ul>
<li><code>51030000</code>是4字节<strong>索引</strong>敏感哈希</li>
<li><code>D6E26822530202020202020202020202</code>是16字节敏感哈希</li>
</ul>
</blockquote>
<h3><span id="13-应用">1.3 应用</span></h3><p>简单应用如，索引敏感哈希可以转成一个int32的数字，当<strong>索引敏感哈希相等</strong>时，<strong>再比较敏感哈希的距离</strong>（如曼哈顿距离，将敏感哈希转成N个<code>unsigned char</code>类型计算敏感哈希，此时<code>00</code>和<code>FF</code>之间的距离可算作1，也可算作255，具体看实现）。</p>
<p>由于特征向量的维度是固定的，因此可以很方便的使用其他数学方法，进行大规模计算。</p>
<ul>
<li>如结合矩阵运算，快速得到上万特征向量（样本）的相似度矩阵，</li>
<li>如用于机器学习的分类（KNN）、聚类（Kmeans）等</li>
</ul>
<h2><span id>#</span></h2>]]></content>
      <categories>
        <category>大数据处理</category>
      </categories>
  </entry>
  <entry>
    <title>数据湖</title>
    <url>/posts/16YPX72/</url>
    <content><![CDATA[<h1><span id="数据湖data-lake-总结">数据湖（Data Lake） 总结</span></h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/91165577">https://zhuanlan.zhihu.com/p/91165577</a></li>
</ul>
<blockquote>
<p>  数据湖从本质上来讲，是一种企业数据架构方法，物理实现上则是一个数据存储平台，用来集中化存储企业内海量的、多来源，多种类的数据，并支持对数据进行快速加工和分析。</p>
</blockquote>
<p>数据湖是一种在系统或存储库中以自然格式存储数据的方法，它有助于以各种模式和结构形式配置数据，通常是对象块或文件。<strong>数据湖的主要思想是对企业中的所有数据进行统一存储，从原始数据（源系统数据的精确副本）转换为用于报告、可视化、分析和机器学习等各种任务的目标数据。数据湖中的数据包括==结构化数据==（关系数据库数据），==半结构化数据==（CSV、XML、JSON等），==非结构化数据==（电子邮件，文档，PDF）和==二进制数据==（图像、音频、视频），从而形成一个容纳所有形式数据的==集中式数据存储==。</strong></p>
<p>从实现方式来看，目前Hadoop是最常用的部署数据湖的技术，但并不意味着数据湖就是指Hadoop集群。为了应对不同业务需求的特点，MPP数据库+Hadoop集群+传统数据仓库这种“混搭”架构的数据湖也越来越多出现在企业信息化建设规划中。</p>
]]></content>
      <categories>
        <category>大数据处理</category>
      </categories>
  </entry>
  <entry>
    <title>Changelog</title>
    <url>/posts/15B7952/</url>
    <content><![CDATA[<h2><span id="blog-修改">Blog 修改</span></h2><ul>
<li>[ ] <strong><font color="red"> Fluid 主题很好看</font></strong>：<a href="https://lizhening.github.io/links/">https://lizhening.github.io/links/</a></li>
<li><p>[ ] <strong><font color="red"> NEXT 主题学习</font></strong></p>
<ul>
<li><a href="https://cs-cshi.github.io/">https://cs-cshi.github.io/</a></li>
<li><a href="https://benn314.github.io/2022/11/22/Hexo-Next%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/">https://benn314.github.io/2022/11/22/Hexo-Next%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/</a></li>
</ul>
</li>
<li><p>[ ] 整个blog目录，git私有化管理</p>
</li>
<li>[x] 图片显示：阿里云OSS or 七牛云 or sm.sm +Gopic图床管理</li>
<li>[ ] 有的短知乎链接改了；mathpix 重新绘制</li>
<li>[x] mathpix 公式编辑？淘宝33元5000次</li>
<li>[x] Post自动编号，显示章节？：<a href="https://blog.csdn.net/Passerby_Wang/article/details/121342829">https://blog.csdn.net/Passerby_Wang/article/details/121342829</a></li>
<li>[x] Hexo表格对齐：<a href="https://javahikers.github.io/2019/06/15/hexo-inserts-tables-in-a-variety-of-ways/">https://javahikers.github.io/2019/06/15/hexo-inserts-tables-in-a-variety-of-ways/</a></li>
<li>[x] Hexo功能增强插件：<a href="https://segmentfault.com/a/1190000018402194">https://segmentfault.com/a/1190000018402194</a></li>
<li>[x] 优化博客路径：npm install hexo-abbrlink —save：<a href="https://github.com/rozbo/hexo-abbrlink">https://github.com/rozbo/hexo-abbrlink</a></li>
<li>[ ] HEXO优化：<a href="https://zhuanlan.zhihu.com/p/30836436">https://zhuanlan.zhihu.com/p/30836436</a><ul>
<li>[ ] <strong><font color="red">添加 tags，<span id="more"></span>，修改章节</font></strong></li>
<li>[ ] 配图？思维导图绘制？</li>
<li>[ ] <strong>给代码添加复制、高亮功能</strong><ul>
<li><a href="https://hexo.io/zh-cn/docs/syntax-highlight.html">https://hexo.io/zh-cn/docs/syntax-highlight.html</a></li>
<li></li>
</ul>
</li>
<li>[ ] 友联。安全相关的网站:</li>
<li>[ ] 主页文章阴影加深</li>
<li>[x] 表格里不能有公式？？<a href="https://blog.csdn.net/lx_ros/article/details/124240258?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-124240258-blog-126924064.235%5Ev29%5Epc_relevant_default_base3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-124240258-blog-126924064.235%5Ev29%5Epc_relevant_default_base3&amp;utm_relevant_index=2">公式显示优化</a><ul>
<li><a href="https://www.jianshu.com/p/7ab21c7f0674">https://www.jianshu.com/p/7ab21c7f0674</a></li>
</ul>
</li>
<li>[ ] 新建404界面</li>
<li>[ ] <strong>分类界面优化，分类介绍，排序</strong><ul>
<li>分级收起</li>
<li>分类文章top设置</li>
</ul>
</li>
<li>[ ] 标签界面优化-词云页面：<a href="https://zhuanlan.zhihu.com/p/149575559">https://zhuanlan.zhihu.com/p/149575559</a></li>
<li>[ ] 图片加水印</li>
<li>[x] <strong>更改字体 和 段落间距</strong></li>
<li>[ ] 统一参考文献</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
  </entry>
  <entry>
    <title>博客分类目录设计</title>
    <url>/posts/3859BFD/</url>
    <content><![CDATA[<h2><span id="powelzys-blog">PoweLZY’s Blog</span></h2><blockquote>
<ul>
<li>比赛相关整理</li>
<li>好论文整理</li>
<li>公司整理的可开源笔记</li>
</ul>
</blockquote>
<h3><span id="分类目录">分类目录</span></h3><ul>
<li><strong><font color="red"> 大数据处理</font></strong><ul>
<li>高维向量相似度匹配</li>
<li>Spark</li>
</ul>
</li>
<li>计算机基础 <ul>
<li>计算机网络</li>
<li>数据库</li>
<li>操作系统</li>
</ul>
</li>
<li><p><strong><font color="blue"> 【开源工具】2.0</font></strong></p>
<ul>
<li><p>网络安全</p>
<ul>
<li>【沙箱】</li>
<li>【zeek】</li>
</ul>
</li>
<li><p>深度学习</p>
<ul>
<li>【Pytorch框架】</li>
</ul>
</li>
<li><p>画图</p>
<ul>
<li>【network】</li>
<li>【matplotlib】</li>
</ul>
</li>
</ul>
</li>
<li><strong><font color="red">机器学习</font></strong><ul>
<li>理论基础</li>
<li>线性模型</li>
<li>决策树（拆分 + 总结）</li>
<li>支持向量机</li>
<li>贝叶斯分类器</li>
<li>集成学习</li>
<li>聚类</li>
<li>降维</li>
<li>概率图模型</li>
<li>推荐算法</li>
<li>异常检测</li>
</ul>
</li>
<li>特征工程</li>
<li>模型部署</li>
<li><strong><font color="red">深度学习</font></strong><ul>
<li>CNN</li>
<li>RNN<ul>
<li>RNN</li>
<li>LSTM</li>
<li>GRU</li>
</ul>
</li>
<li>Seq2Seq<ul>
<li>Attention</li>
<li>Transformer</li>
<li>AutoEncoder</li>
</ul>
</li>
<li>GNN</li>
<li>NLP</li>
<li>【Pytorch框架】</li>
<li>模型训练<ul>
<li>优化算法</li>
</ul>
</li>
</ul>
</li>
<li><strong><font color="red"> 算法应用</font></strong><ul>
<li>网络安全<ul>
<li><strong><font color="red">恶意软件检测</font></strong></li>
<li><strong><font color="red">恶意加密流量检测</font></strong></li>
<li>高级威胁发现</li>
</ul>
</li>
<li>业务安全<ul>
<li>业务安全</li>
<li>【接口安全-waap】</li>
<li></li>
</ul>
</li>
</ul>
</li>
<li><strong>工业落地：企业公开发表的论文或白皮书</strong><ul>
<li>网络安全</li>
<li>业务安全</li>
</ul>
</li>
<li><strong>算法比赛：算法比赛的top方案</strong><ul>
<li>网络安全<ul>
<li>【360 BDCI】：<a href="https://github.com/PowerLZY/malware_classification_bdci">https://github.com/PowerLZY/malware_classification_bdci</a></li>
<li>【2021 Datacon】</li>
</ul>
</li>
<li>业务安全</li>
</ul>
</li>
<li><strong>学术前沿：学术论文</strong><ul>
<li>网络安全<ul>
<li>【恶意软件逃逸攻击】</li>
</ul>
</li>
<li>业务安全</li>
</ul>
</li>
<li><strong>数据结构</strong></li>
<li><strong>流畅的Python</strong></li>
</ul>
]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征-Hot100</title>
    <url>/posts/1SP3CDD/</url>
    <content><![CDATA[<h2><span id="一-前缀树">一、前缀树</span></h2><blockquote>
<p>  一种好用的树结构：Trie树:<a href="https://zhuanlan.zhihu.com/p/508575094">https://zhuanlan.zhihu.com/p/508575094</a></p>
</blockquote>
<h3><span id="11-trie树简介-有限状态自动机-文本词频统计"><strong>1.1 Trie树简介</strong> [有限状态自动机] [文本词频统计]</span></h3><p>在计算机科学中，trie，又称<strong>前缀树</strong>或<strong>字典树</strong>，是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。</p>
<p>Trie这个术语来自于retrieval。根据词源学，trie的发明者Edward Fredkin把它读作/ˈtriː/ “tree”。但是，其他作者把它读作/ˈtraɪ/ “try”。</p>
<p>在图示中，键标注在节点中，值标注在节点之下。每一个完整的英文单词对应一个特定的整数。<strong>Trie可以看作是一个确定有限状态自动机，尽管边上的符号一般是隐含在分支的顺序中的</strong>。 Eg.一个保存了8个单词的字典树的结构如下图所示，8个单词分别是：“A”，“to”，“tea”，“ted”，“ten”，“i” ，“in”，“inn”。</p>
<p><img src="https://pic1.zhimg.com/80/v2-8740aeac82cd2fc980cd1148ab1a64dc_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>另外，<strong>单词查找树，Trie树，是一种树形结构，是一种哈希树的变种</strong>。<strong>典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计</strong>。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。</p>
<h3><span id="12-trie树性质">1.2 <strong>Trie树性质</strong></span></h3><p>它有3个基本性质：</p>
<ul>
<li>根节点不包含字符，除根节点外每一个节点都只包含一个字符；</li>
<li>从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串；</li>
<li>每个节点的所有子节点包含的字符都不相同。</li>
</ul>
<h3><span id="13-基本操作">1.3 <strong>基本操作</strong></span></h3><p>其基本操作有：查找、插入和删除,当然删除操作比较少见。</p>
<h2><span id="二-lru">二、LRU</span></h2>]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征-代码随想录</title>
    <url>/posts/2WTQDCR/</url>
    <content><![CDATA[<ul>
<li><a href="https://programmercarl.com/">代码随想录</a></li>
<li><a href>labuladong</a></li>
</ul>
<h2><span id="一-树状数组">一、树状数组</span></h2><p>leecode 题目：<a href="https://leetcode-cn.com/problems/shu-zu-zhong-de-ni-xu-dui-lcof/">数组中的逆序对</a></p>
<blockquote>
<p>  在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。</p>
<p>  示例 1:</p>
<p>  输入: [7,5,6,4]<br>  输出: 5</p>
</blockquote>
<p><strong>「树状数组」</strong>是一种可以<strong>动态维护序列前缀和</strong>的数据结构，它的功能是：</p>
<ul>
<li><strong>单点更新<code>update(i, v)</code></strong>:把序列 $i$ 位置的数加上一个值$v$，这题 v = 1</li>
<li><strong>区间查询 <code>query(i)</code>：</strong> 查询序列[1⋯<em>i</em>] 区间的区间和，即 <em>i</em> 位置的前缀和</li>
</ul>
<p>修改和查询的时间代价都是 $O(\log n)$，其中 <em>n</em> 为需要维护前缀和的序列的长度。</p>
<h2><span id="二-单调栈">二、单调栈</span></h2><p>通常是一维数组，要==寻找任一个元素的右边或者左边第一个比自己大或者小的元素的位置==，此时我们就要想到可以用<strong>单调栈</strong>了。</p>
<ul>
<li><p><strong>单调栈里存放的元素是什么？</strong></p>
<p>单调栈里只需要存放元素的下标i就可以了，如果需要使用对应的元素，直接T[i]就可以获取。</p>
</li>
<li><p><strong>单调栈里元素是递增呢？ 还是递减呢？</strong></p>
<p><strong>注意一下顺序为 从栈头到栈底的顺序</strong>，因为单纯的说从左到右或者从前到后，不说栈头朝哪个方向的话，大家一定会越看越懵。</p>
</li>
<li><p><strong>目标列表遍历顺序？</strong> 倒序？正序？</p>
</li>
</ul>
<h3><span id="21-下一个更大元素">2.1 下一个更大元素</span></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nextGreaterElement</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">    <span class="comment"># 下一个更大的元素：单调栈(不增的不要) + 哈希 </span></span><br><span class="line">    res = &#123;&#125;</span><br><span class="line">    stack = []</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">reversed</span>(nums2): <span class="comment"># 倒序？</span></span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> num &gt;= stack[-<span class="number">1</span>]:</span><br><span class="line">            stack.pop()</span><br><span class="line">        res[num] = stack[-<span class="number">1</span>] <span class="keyword">if</span> stack <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        stack.append(num)</span><br><span class="line">    <span class="keyword">return</span> [res[num] <span class="keyword">for</span> num <span class="keyword">in</span> nums1]</span><br></pre></td></tr></table></figure>
<h3><span id="21-下一个更大元素2循环数组又tm是循环">2.1 下一个更大元素2（循环数组）又TM是循环</span></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">def</span> <span class="title function_">nextGreaterElements</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 三次逆转</span></span><br><span class="line">        s = <span class="built_in">list</span>(nums[::-<span class="number">1</span>])</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums[::-<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">while</span> s <span class="keyword">and</span> s[-<span class="number">1</span>] &lt;= num:</span><br><span class="line">                s.pop()</span><br><span class="line">            <span class="keyword">if</span> s:</span><br><span class="line">                res.append(s[-<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res.append(-<span class="number">1</span>)</span><br><span class="line">            s.append(num)</span><br><span class="line">        res.reverse()</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    def nextGreaterElements(self, nums: List[int]) -&gt; List[int]:</span></span><br><span class="line"><span class="string">        # 2n 遍历</span></span><br><span class="line"><span class="string">        n = len(nums)</span></span><br><span class="line"><span class="string">        ret = [-1] * n</span></span><br><span class="line"><span class="string">        stk = list()</span></span><br><span class="line"><span class="string">        for i in range(n * 2 - 1):</span></span><br><span class="line"><span class="string">            while stk and nums[stk[-1]] &lt; nums[i % n]:</span></span><br><span class="line"><span class="string">                ret[stk.pop()] = nums[i % n]</span></span><br><span class="line"><span class="string">            stk.append(i % n)</span></span><br><span class="line"><span class="string">        return ret</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    def nextGreaterElements(self, nums: List[int]) -&gt; List[int]:</span></span><br><span class="line"><span class="string">        # 栈留判断</span></span><br><span class="line"><span class="string">        n = len(nums)</span></span><br><span class="line"><span class="string">        ans = [-1 for _ in range(n)]</span></span><br><span class="line"><span class="string">        stack = [0]</span></span><br><span class="line"><span class="string">        stack2 = []</span></span><br><span class="line"><span class="string">        for i in range(1, n):</span></span><br><span class="line"><span class="string">            while stack and nums[i] &gt; nums[stack[-1]]: # ？</span></span><br><span class="line"><span class="string">                ans[stack[-1]] = nums[i]</span></span><br><span class="line"><span class="string">                stack.pop() </span></span><br><span class="line"><span class="string">            stack.append(i)</span></span><br><span class="line"><span class="string">        while len(stack) &gt; 1: # 栈顶是最大的保留</span></span><br><span class="line"><span class="string">            for num in nums: # 从头找？</span></span><br><span class="line"><span class="string">                if num &gt; nums[stack[-1]]:</span></span><br><span class="line"><span class="string">                    ans[stack[-1]] = num</span></span><br><span class="line"><span class="string">                    break</span></span><br><span class="line"><span class="string">            stack.pop()</span></span><br><span class="line"><span class="string">        return ans</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3><span id="23-每日温度">2.3 每日温度</span></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dailyTemperatures</span>(<span class="params">self, temperatures: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">    <span class="comment"># next Geater </span></span><br><span class="line">    n = <span class="built_in">len</span>(temperatures)</span><br><span class="line">    res = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)] </span><br><span class="line">    stack = [<span class="number">0</span>] <span class="comment"># 存的初始索引</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n): <span class="comment"># 正序</span></span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> temperatures[i] &gt; temperatures[stack[-<span class="number">1</span>]]:</span><br><span class="line">            res[stack[-<span class="number">1</span>]] = i - stack[-<span class="number">1</span>]  <span class="comment"># stack[-1]</span></span><br><span class="line">            stack.pop() </span><br><span class="line">        stack.append(i)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3><span id="24-接雨水">2.4 接雨水</span></h3><p>接雨水是找每个柱子<strong>左右两边第一个大于该柱子高度的柱子</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trap</span>(<span class="params">self, height: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># 单调栈 左 弹出后 s.top() 中 s.pop()  右 height[i]</span></span><br><span class="line">    n = <span class="built_in">len</span>(height)</span><br><span class="line">    stack = [<span class="number">0</span>]</span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> height[i] &gt; height[stack[-<span class="number">1</span>]]:</span><br><span class="line">            mid = stack.pop() <span class="comment"># 弹一个左面还有的话就是有坑</span></span><br><span class="line">            <span class="keyword">if</span> stack:</span><br><span class="line">                high = <span class="built_in">min</span>(height[stack[-<span class="number">1</span>]], height[i]) - height[mid] </span><br><span class="line">                <span class="comment"># 高 = 左右最小 - 低</span></span><br><span class="line">                weith = i - stack[-<span class="number">1</span>] - <span class="number">1</span> <span class="comment"># 宽</span></span><br><span class="line">                res += weith * high </span><br><span class="line">        stack.append(i)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3><span id="25-柱状图的最大矩形">2.5 柱状图的最大矩形</span></h3><p><strong>找每个柱子左右两边第一个小于该柱子的柱子。</strong>：==<strong>栈顶和栈顶的下一个元素以及要入栈的三个元素组成了我们要求最大面积的高度和宽度</strong>==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">largestRectangleArea</span>(<span class="params">self, heights: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># 单调栈 左右两边都可以用</span></span><br><span class="line">    heights.insert(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    heights.append(<span class="number">0</span>)</span><br><span class="line">    stack = [<span class="number">0</span>]</span><br><span class="line">    ans = heights[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(heights)):</span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> heights[i] &lt; heights[stack[-<span class="number">1</span>]]:</span><br><span class="line">            mid = stack.pop()</span><br><span class="line">            <span class="keyword">if</span> stack: <span class="comment"># 存在左右最小</span></span><br><span class="line">                weith = i - stack[-<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">                <span class="comment"># high = max(heights[i], heights[stack[-1]])</span></span><br><span class="line">                ans = <span class="built_in">max</span>(ans, heights[mid] * weith)</span><br><span class="line">        stack.append(i)</span><br><span class="line">    <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h2><span id="三-单调队列">三、单调队列</span></h2><h2><span id="四-贪心算法"></span></h2><p><img src="https://code-thinking-1253855093.file.myqcloud.com/pics/20210917104315.png" alt="贪心算法大纲"></p>
<h3><span id="什么是贪心">什么是贪心</span></h3><p><strong>贪心的本质是选择每一阶段的局部最优，从而达到全局最优</strong>。</p>
<h3><span id="贪心一般解题步骤">贪心一般解题步骤</span></h3><p>贪心算法一般分为如下四步：</p>
<ul>
<li>将问题分解为若干个子问题</li>
<li>找出适合的贪心策略</li>
<li>求解每一个子问题的最优解</li>
<li>将局部最优解堆叠成全局最优解</li>
</ul>
<h2><span id="五-动态规划"></span></h2><p><img src="https://images.zsxq.com/FvoG8qppuOWSNhXBbj27ShBAJw0G?imageMogr2/auto-orient/quality/100!/ignore-error/1&amp;e=1648742399&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:q36Nzw1NTcg-NvuvnNyOWSZ_mdI=" alt="img"></p>
<p><strong>首先，动态规划问题的一般形式就是==求最值==</strong>。动态规划其实是运筹学的一种最优化方法，只不过在计算机问题上应用比较多，比如说让你求<strong>最长</strong>递增子序列呀，<strong>最小</strong>编辑距离呀等等。<strong>求解动态规划的核心问题是穷举</strong>。首先，动态规划的穷举有点特别，因为这类问题<strong>存在「重叠子问题」</strong>，如果暴力穷举的话效率会极其低下，所以需要「==备忘录」或者「DP table」==来优化穷举过程，避免不必要的计算。而且，动态规划问题一定会<strong>具备「最优子结构」</strong>，才能通过子问题的最值得到原问题的最值。</p>
<p>另外，虽然动态规划的核心思想就是穷举求最值，但是问题可以千变万化，穷举所有可行解其实并不是一件容易的事，只有列出<strong>正确的「状态转移方程」</strong>，才能正确地穷举。</p>
<p>以上提到的<strong>重叠子问题、最优子结构、状态转移方程</strong>就是动态规划三要素。具体什么意思等会会举例详解，但是在实际的算法问题中，<strong>写出状态转移方程是最困难的</strong>，这也就是为什么很多朋友觉得动态规划问题困难的原因，我来提供我研究出来的一个思维框架，辅助你思考状态转移方程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化 base case</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>][...] = base</span><br><span class="line"><span class="comment"># 进行状态转移</span></span><br><span class="line"><span class="keyword">for</span> 状态<span class="number">1</span> <span class="keyword">in</span> 状态<span class="number">1</span>的所有取值：</span><br><span class="line">    <span class="keyword">for</span> 状态<span class="number">2</span> <span class="keyword">in</span> 状态<span class="number">2</span>的所有取值：</span><br><span class="line">        <span class="keyword">for</span> ...</span><br><span class="line">            dp[状态<span class="number">1</span>][状态<span class="number">2</span>][...] = 求最值(选择<span class="number">1</span>，选择<span class="number">2.</span>..)</span><br></pre></td></tr></table></figure>
<h3><span id="51-背包问题"><strong>5.1 背包问题</strong></span></h3><p><a href="https://leetcode-cn.com/problems/coin-change/solution/dai-ma-sui-xiang-lu-dai-ni-xue-tou-wan-q-80r7/">https://leetcode-cn.com/problems/coin-change/solution/dai-ma-sui-xiang-lu-dai-ni-xue-tou-wan-q-80r7/</a></p>
<p><img src="https://img-blog.csdnimg.cn/20210117171307407.png" alt="416.分割等和子集1"></p>
<blockquote>
<p>  <strong>1、确定dp数组以及下标的含义</strong></p>
<p>  <strong>2、确定递推公式</strong></p>
<p>  <strong>3、dp数组如何初始化</strong></p>
<ul>
<li><strong>最大长度</strong></li>
<li><p><strong>递推等号左侧要初始化</strong></p>
<p><strong>4、确定遍历顺序</strong></p>
</li>
<li><p><strong>滚动数组</strong></p>
<p><strong>5、举例推导dp数组</strong></p>
</li>
</ul>
</blockquote>
<h4><span id="511-背包递推公式">5.1.1 背包递推公式</span></h4><p><strong>问==能否装满背包==</strong>（或者<strong>最多装多少</strong>）：dp[j] = max(dp[j], dp[j - nums[i]] + nums[i]); ，对应题目如下：</p>
<ul>
<li><a href="https://programmercarl.com/0416.分割等和子集.html">动态规划：416.分割等和子集(opens new window)</a></li>
<li><a href="https://leetcode-cn.com/problems/partition-to-k-equal-sum-subsets/">划分为k个相等的子集</a></li>
<li><a href="https://programmercarl.com/1049.最后一块石头的重量II.html">动态规划：1049.最后一块石头的重量 II</a></li>
</ul>
<p><strong>==问装满背包有几种方法==</strong>：dp[j] += dp[j - nums[i]] ，对应题目如下：</p>
<ul>
<li><p><a href="https://programmercarl.com/0494.目标和.html">动态规划：494.目标和(opens new window)</a></p>
</li>
<li><p><a href="https://programmercarl.com/0518.零钱兑换II.html">动态规划：518. 零钱兑换 II(opens new window)</a></p>
</li>
<li><p><a href="https://programmercarl.com/0377.组合总和Ⅳ.html">动态规划：377.组合总和Ⅳ(opens new window)</a></p>
<blockquote>
<p>  <strong>如果求组合数就是外层for循环遍历物品，内层for遍历背包</strong>。</p>
<p>  <strong>如果求排列数就是外层for遍历背包，内层for循环遍历物品</strong>。</p>
</blockquote>
</li>
<li><p><a href="https://programmercarl.com/0070.爬楼梯完全背包版本.html">动态规划：70. 爬楼梯进阶版（完全背包）</a></p>
</li>
</ul>
<p><strong>问背包装满==最大价值==</strong>：dp[j] = max(dp[j], dp[j - weight[i]] + value[i]); ，对应题目如下：</p>
<ul>
<li><a href="https://programmercarl.com/0474.一和零.html">动态规划：474.一和零</a></li>
</ul>
<p><strong>问装满背包所有物品的==最小个数==</strong>：dp[j] = min(dp[j - coins[i]] + 1, dp[j]); ，对应题目如下：</p>
<ul>
<li><a href="https://programmercarl.com/0322.零钱兑换.html">动态规划：322.零钱兑换(opens new window)</a></li>
<li><a href="https://programmercarl.com/0279.完全平方数.html">动态规划：279.完全平方数(opens new window)</a></li>
</ul>
<h4><span id="512-遍历顺序">5.1.2 遍历顺序</span></h4><h5><span id="01背包">01背包</span></h5><p>在<a href="https://programmercarl.com/背包理论基础01背包-1.html">动态规划：关于01背包问题，你该了解这些！ (opens new window)</a>中我们讲解二维dp数组01背包先遍历物品还是先遍历背包都是可以的，且第二层for循环是从小到大遍历。</p>
<p>和<a href="https://programmercarl.com/背包理论基础01背包-2.html">动态规划：关于01背包问题，你该了解这些！（滚动数组） (opens new window)</a>中，我们讲解一维dp数组01背包只能先遍历物品再遍历背包容量，且第二层for循环是从大到小遍历。</p>
<p><strong>一维dp数组的背包在遍历顺序上和二维dp数组实现的01背包其实是有很大差异的，大家需要注意！</strong></p>
<h5><span id="完全背包">完全背包</span></h5><p>说完01背包，再看看完全背包。</p>
<p>在<a href="https://programmercarl.com/背包问题理论基础完全背包.html">动态规划：关于完全背包，你该了解这些！ (opens new window)</a>中，讲解了纯完全背包的一维dp数组实现，先遍历物品还是先遍历背包都是可以的，且第二层for循环是从小到大遍历。</p>
<p>但是仅仅是纯完全背包的遍历顺序是这样的，题目稍有变化，两个for循环的先后顺序就不一样了。</p>
<p><strong>如果求==组合数==就是外层for循环遍历物品，内层for遍历背包</strong>。</p>
<p><strong>如果==求排列数==就是外层for遍历背包，内层for循环遍历物品</strong>。</p>
<p>相关题目如下：</p>
<ul>
<li>求组合数：<a href="https://programmercarl.com/0518.零钱兑换II.html">动态规划：518.零钱兑换II(opens new window)</a></li>
<li>求排列数：<a href="https://mp.weixin.qq.com/s/Iixw0nahJWQgbqVNk8k6gA">动态规划：377. 组合总和 Ⅳ (opens new window)</a>、<a href="https://programmercarl.com/0070.爬楼梯完全背包版本.html">动态规划：70. 爬楼梯进阶版（完全背包）(opens new window)</a></li>
</ul>
<p>如果求最小数，那么两层for循环的先后顺序就无所谓了，相关题目如下：</p>
<ul>
<li>求最小数：<a href="https://programmercarl.com/0322.零钱兑换.html">动态规划：322. 零钱兑换 (opens new window)</a>、<a href="https://programmercarl.com/0279.完全平方数.html">动态规划：279.完全平方数(opens new window)</a></li>
</ul>
<p><strong>对于背包问题，其实递推公式算是容易的，难是难在遍历顺序上，如果把遍历顺序搞透，才算是真正理解了</strong>。</p>
<h3><span id="52-子序列问题">5.2 <strong>子序列问题</strong></span></h3><p><img src="https://code-thinking.cdn.bcebos.com/pics/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E5%AD%90%E5%BA%8F%E5%88%97%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.jpg" alt="img" style="zoom:50%;"></p>
<ul>
<li><p><a href="https://programmercarl.com/0300.最长上升子序列.html">动态规划：最长递增子序列</a> </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 子序列 </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLIS</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(nums)</span><br><span class="line">        dp = [<span class="number">1</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, i):</span><br><span class="line">                <span class="keyword">if</span> nums[i] &gt; nums[j]:</span><br><span class="line">                    dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line">            result = <span class="built_in">max</span>(result, dp[i]) <span class="comment">#取长的子序列</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><h5><span id="动态规划最长斐波那契数列"></span></h5></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lenLongestFibSubseq</span>(<span class="params">self, A: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># 基本顺序是 k，i，j 或者 A[k] = A[j] - A[i]</span></span><br><span class="line">    n = <span class="built_in">len</span>(A)</span><br><span class="line">    dic = &#123;&#125;</span><br><span class="line">    <span class="comment"># 创建索引字典，提速</span></span><br><span class="line">    <span class="keyword">for</span> ind,val <span class="keyword">in</span> <span class="built_in">enumerate</span>(A):</span><br><span class="line">        dic[val] = ind</span><br><span class="line">    <span class="comment"># 初始化，行代表的是i，不需要取到n-1，为了给j留出位置</span></span><br><span class="line">    <span class="comment"># 初始为2，只要包含了 j i 位置，则意味着已经有了2个数字。</span></span><br><span class="line">    dp = [[<span class="number">2</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>)]</span><br><span class="line">    ret = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 因此i只能取到n-2，给j留出空间</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n-<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># j从i+1开始，毕竟j在i后面</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,n):</span><br><span class="line">            diff = A[j] - A[i]</span><br><span class="line">            <span class="keyword">if</span> diff <span class="keyword">in</span> dic <span class="keyword">and</span> dic[diff] &lt; i:</span><br><span class="line">                k = dic[diff]</span><br><span class="line">                dp[i][j] = dp[k][i] + <span class="number">1</span> <span class="comment"># 这个1，代表着k位置数字</span></span><br><span class="line">                ret = <span class="built_in">max</span>(ret,dp[i][j])</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>
<ul>
<li><p><a href="https://programmercarl.com/0718.最长重复子数组.html">动态规划：最长重复子数组</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findLength</span>(<span class="params">self, A: <span class="type">List</span>[<span class="built_in">int</span>], B: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="number">0</span>] * (<span class="built_in">len</span>(B) + <span class="number">1</span>)</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(A)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(B), <span class="number">0</span>, -<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> A[i-<span class="number">1</span>] == B[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[j] = dp[j-<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[j] = <span class="number">0</span> <span class="comment">#注意这里不相等的时候要有赋0的操作</span></span><br><span class="line">                result = <span class="built_in">max</span>(result, dp[j])</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://programmercarl.com/1143.最长公共子序列.html">动态规划：最长公共子序列</a>、 <a href="https://programmercarl.com/1035.不相交的线.html">动态规划：不相交的线</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestCommonSubsequence</span>(<span class="params">self, text1: <span class="built_in">str</span>, text2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 最长子序列 </span></span><br><span class="line">        m, n = <span class="built_in">len</span>(text1), <span class="built_in">len</span>(text2)</span><br><span class="line">        dp = [[<span class="number">0</span>] * (n+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> text2[j-<span class="number">1</span>] == text1[i-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>] +<span class="number">1</span> </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>][-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://programmercarl.com/0053.最大子序和（动态规划）.html">动态规划：最大子序和</a> 【贪心】【动态规划】</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxSubArray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># 为什么dp[-1]不是最大？需要res</span></span><br><span class="line">    <span class="comment"># dp[i] 以i结尾的最大子数组和</span></span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    dp = [<span class="number">0</span>] * (n)</span><br><span class="line">    dp[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        dp[i] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>]+nums[i], nums[i])</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://programmercarl.com/0392.判断子序列.html">动态规划：判断子序列</a> 【双指针】</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isSubsequence</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        n, m = <span class="built_in">len</span>(s), <span class="built_in">len</span>(t)</span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> j &lt; m:</span><br><span class="line">            <span class="keyword">if</span> s[i] == t[j]:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> i == n</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://programmercarl.com/0115.不同的子序列.html">动态规划：不同的子序列</a></li>
</ul>
<ul>
<li><a href="https://programmercarl.com/0583.两个字符串的删除操作.html">动态规划：两个字符串的删除操作</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dp[i][j] : word1[i-1], word1[j-1] 最少步数</span></span><br><span class="line"><span class="comment"># word1[i - 1] 与 word2[j - 1]不相同的时候，有2种情况 *  min(dp[i-1][j], dp[i][j-1])+1  </span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">minDistance</span>(<span class="params">self, word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        m, n = <span class="built_in">len</span>(word1), <span class="built_in">len</span>(word2)</span><br><span class="line">        dp = [[<span class="number">0</span>] * (n+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][i] = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>):</span><br><span class="line">            dp[j][<span class="number">0</span>] = j</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i-<span class="number">1</span>] == word2[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])+<span class="number">1</span> <span class="comment"># , dp[i-1][j-1]+1</span></span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>][-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://programmercarl.com/0072.编辑距离.html">动态规划：编辑距离</a></li>
<li><a href="https://programmercarl.com/为了绝杀编辑距离，卡尔做了三步铺垫.html">为了绝杀编辑距离，我做了三步铺垫，你都知道么？</a></li>
<li><a href="https://programmercarl.com/0647.回文子串.html">动态规划：回文子串</a></li>
<li><a href="https://leetcode-cn.com/problems/longest-palindromic-substring/">最长回文子串</a> ==[Manacher 算法]==</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 中心扩展法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># 枚举+中心扩展法</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">extendCenter</span>(<span class="params">left, right</span>):</span><br><span class="line">            <span class="keyword">while</span> <span class="number">0</span> &lt;= left <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">                left -= <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 返回的为while不成立的值</span></span><br><span class="line">            <span class="keyword">return</span> left+<span class="number">1</span>, right-<span class="number">1</span> </span><br><span class="line">        start, end = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            left1, right1 = extendCenter(i,i)</span><br><span class="line">            left2, right2 = extendCenter(i,i+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> right1 - left1 &gt; end - start:</span><br><span class="line">                start, end = left1, right1</span><br><span class="line">            <span class="keyword">if</span> right2 - left2 &gt; end - start:</span><br><span class="line">                start, end = left2, right2</span><br><span class="line">        <span class="keyword">return</span> s[start:end+<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="comment"># dp[i][j]: 是否是回文 dp[i+1][j-1] -&gt; dp[i][j]</span></span><br><span class="line">    <span class="comment"># 返回的子串，不是数字 不能记录长度</span></span><br><span class="line">    n = <span class="built_in">len</span>(s)</span><br><span class="line">    dp = [[<span class="literal">False</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)] <span class="comment"># 右上为1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dp[i][i] = <span class="literal">True</span></span><br><span class="line">    begin = <span class="number">0</span></span><br><span class="line">    maxlen = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 判断是否是回文子串中，如果是则记录begin and len</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>, -<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i,n):</span><br><span class="line">            <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                <span class="keyword">if</span> j - i &lt;=<span class="number">1</span>:</span><br><span class="line">                    dp[i][j] = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">elif</span> dp[i+<span class="number">1</span>][j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> dp[i][j] <span class="keyword">and</span> j - i + <span class="number">1</span> &gt; maxlen:</span><br><span class="line">                maxlen = j - i + <span class="number">1</span></span><br><span class="line">                begin = i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s[begin:begin+maxlen]</span><br></pre></td></tr></table></figure>
<h4><span id="manacher-算法">Manacher 算法</span></h4><p><strong>Manacher 算法是在线性时间内求解最长回文子串的算法</strong>。在本题中，我们要求解回文串的个数，为什么也能使用 Manacher 算法呢？这里我们就需要理解一下 Manacher 的基本原理。</p>
<ul>
<li>奇偶长度处理： abaaabaa 会被处理成 #a#b#a#a##<em>a</em>#<em>b</em>#<em>a</em>#<em>a</em>#</li>
<li>==<strong>$f(i)$</strong> 来表示以 s 的第 i 位为回文中心，可以拓展出的<strong>最大回文半径 （包括 # ）</strong>==，那么$ f(i) - 1$就是以 i 为中心的最大回文串长度 。</li>
<li>利用已经计算出来的状态来更新 f(i）：<strong>回文右端点rm</strong> ：i + f(i) - 1</li>
</ul>
<ul>
<li><p><a href="https://programmercarl.com/0516.最长回文子序列.html">动态规划：最长回文子序列</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindromeSubseq</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 动态规划 dp[i][j]: s[i:j] 最大回文长度</span></span><br><span class="line">        <span class="comment"># dp[i+1][j-1] -&gt; dp[i][j] 遍历顺序 and j - i &gt;=2</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [[<span class="number">0</span>] * i +[<span class="number">1</span>] * (n-i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)] <span class="comment"># 从定义出发 右上三角 = 1 有意义</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,n): <span class="comment"># j - i &gt;=2</span></span><br><span class="line">                <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                    dp[i][j] = dp[i+<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">2</span> </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i+<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])                </span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][n-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><h5><span id="最少回文分割"></span></h5></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minCut</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 动态规划1:判断回文</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        g = [[<span class="literal">True</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">                g[i][j] = (s[i] == s[j]) <span class="keyword">and</span> g[i + <span class="number">1</span>][j - <span class="number">1</span>]</span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        f = [<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)] * n</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> g[<span class="number">0</span>][i]:</span><br><span class="line">                f[i] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">                    <span class="keyword">if</span> g[j + <span class="number">1</span>][i]:</span><br><span class="line">                        <span class="comment"># （0, j) + (j+1, i)</span></span><br><span class="line">                        f[i] = <span class="built_in">min</span>(f[i], f[j] + <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> f[n - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><h5><span id="最长连续序列"></span></h5></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestConsecutive</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 动态规划</span></span><br><span class="line">        dic, res = <span class="built_in">dict</span>(), <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> num <span class="keyword">not</span> <span class="keyword">in</span> dic:</span><br><span class="line">                left, right = dic.get(num - <span class="number">1</span>, <span class="number">0</span>), dic.get(num + <span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">                cur = <span class="number">1</span> + left +right</span><br><span class="line">                <span class="keyword">if</span> res &lt; cur:</span><br><span class="line">                    res = cur</span><br><span class="line">                dic[num] = cur</span><br><span class="line">                dic[num - left] = cur</span><br><span class="line">                dic[num + right] = cur</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3><span id="53-打家劫舍问题">5.3 打家劫舍问题</span></h3><ul>
<li><a href="https://programmercarl.com/0198.打家劫舍.html">动态规划：开始打家劫舍</a></li>
<li><a href="https://programmercarl.com/0213.打家劫舍II.html">动态规划：继续打家劫舍 环</a></li>
<li><a href="https://programmercarl.com/0337.打家劫舍III.html">动态规划：还要打家劫舍 树状dp</a></li>
</ul>
<h3><span id="54-股票问题">5.4 股票问题</span></h3><p><img src="https://code-thinking.cdn.bcebos.com/pics/%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.jpg" alt="股票问题总结"></p>
<ul>
<li><p><a href="https://programmercarl.com/0121.买卖股票的最佳时机.html">动态规划：121.买卖股票的最佳时机(opens new window)</a>  股票只能买卖一次，问最大利润</p>
</li>
<li><p><a href="https://programmercarl.com/0122.买卖股票的最佳时机II（动态规划）.html">动态规划：122.买卖股票的最佳时机II(opens new window)</a> 可以多次买卖股票，问最大收益。</p>
</li>
<li><a href="https://programmercarl.com/0123.买卖股票的最佳时机III.html">动态规划：123.买卖股票的最佳时机III(opens new window)</a> 最多买卖两次，问最大收益。</li>
<li><a href="https://programmercarl.com/0188.买卖股票的最佳时机IV.html">动态规划：188.买卖股票的最佳时机IV(opens new window)</a> 最多买卖k笔交易，问最大收益。</li>
<li><a href="https://programmercarl.com/0309.最佳买卖股票时机含冷冻期.html">动态规划：309.最佳买卖股票时机含冷冻期(opens new window)</a> 可以多次买卖但每次卖出有冷冻期1天。</li>
<li><a href="https://programmercarl.com/0714.买卖股票的最佳时机含手续费（动态规划）.html">动态规划：714.买卖股票的最佳时机含手续费(opens new window)</a> 可以多次买卖，但每次有手续费。</li>
</ul>
<h3><span id="55-编辑距离问题">5.5 编辑距离问题</span></h3><h4><span id="判断子序列">判断子序列</span></h4><p><a href="https://programmercarl.com/0392.判断子序列.html">动态规划：392.判断子序列 (opens new window)</a>给定字符串 s 和 t ，判断 s 是否为 t 的子序列。</p>
<p>这道题目 其实是可以用<strong>双指针</strong>或者<strong>贪心</strong>的的，但是我在开篇的时候就说了这是编辑距离的入门题目，因为从题意中我们也可以发现，只需要计算删除的情况，不用考虑增加和替换的情况。</p>
<ul>
<li>if (s[i - 1] == t[j - 1])<ul>
<li>t中找到了一个字符在s中也出现了</li>
</ul>
</li>
<li>if (s[i - 1] != t[j - 1])<ul>
<li>相当于t要删除元素，继续匹配</li>
</ul>
</li>
</ul>
<p>状态转移方程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (s[i - <span class="number">1</span>] == t[j - <span class="number">1</span>]) dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span> dp[i][j] = dp[i][j - <span class="number">1</span>];</span><br></pre></td></tr></table></figure>
<h4><span id="不同的子序列">不同的子序列</span></h4><p><a href="https://programmercarl.com/0115.不同的子序列.html">动态规划：115.不同的子序列 (opens new window)</a>给定一个字符串 s 和一个字符串 t ，计算在 s 的子序列中 t 出现的个数。</p>
<p>本题虽然也只有删除操作，不用考虑替换增加之类的，但相对于<a href="https://programmercarl.com/0392.判断子序列.html">动态规划：392.判断子序列 (opens new window)</a>就有难度了，这道题目双指针法可就做不了。</p>
<p>当s[i - 1] 与 t[j - 1]相等时，dp[i][j]可以有两部分组成。</p>
<p>一部分是用s[i - 1]来匹配，那么个数为 dp[i - 1] [j - 1]</p>
<p>一部分是不用s[i - 1]来匹配，个数为 dp[i - 1] [j]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (s[i - <span class="number">1</span>] == t[j - <span class="number">1</span>]) &#123;</span><br><span class="line">    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + dp[i - <span class="number">1</span>][j];</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    dp[i][j] = dp[i - <span class="number">1</span>][j];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="两个字符串的删除操作">两个字符串的删除操作</span></h4><p><a href="https://programmercarl.com/0583.两个字符串的删除操作.html">动态规划：583.两个字符串的删除操作 (opens new window)</a>给定两个单词 word1 和 word2，找到使得 word1 和 word2 相同所需的最小步数，每步可以删除任意一个字符串中的一个字符。</p>
<p>本题和<a href="https://programmercarl.com/0115.不同的子序列.html">动态规划：115.不同的子序列 (opens new window)</a>相比，其实就是两个字符串可以都可以删除了，情况虽说复杂一些，但整体思路是不变的。</p>
<ul>
<li>当word1[i - 1] 与 word2[j - 1]相同的时候</li>
<li>当word1[i - 1] 与 word2[j - 1]不相同的时候</li>
</ul>
<p>当word1[i - 1] 与 word2[j - 1]相同的时候，dp[i][j] = dp[i - 1] [j - 1];</p>
<p>当word1[i - 1] 与 word2[j - 1]不相同的时候，有2种情况：</p>
<p>情况一：删word1[i - 1]，最少操作次数为dp[i - 1] [j] + 1</p>
<p>情况二：删word2[j - 1]，最少操作次数为dp[i][j - 1] + 1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (word1[i - <span class="number">1</span>] == word2[j - <span class="number">1</span>]) &#123;</span><br><span class="line">    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>];</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    dp[i][j] = <span class="built_in">min</span>(&#123;dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">2</span>, dp[i - <span class="number">1</span>][j] + <span class="number">1</span>, dp[i][j - <span class="number">1</span>] + <span class="number">1</span>&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="编辑距离">编辑距离</span></h4><p><a href="https://programmercarl.com/0072.编辑距离.html">动态规划：72.编辑距离 (opens new window)</a>给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。</p>
<p>编辑距离终于来了，<strong>有了前面三道题目的铺垫，应该有思路了</strong>，本题是两个字符串可以增删改，比 <a href="https://programmercarl.com/0392.判断子序列.html">动态规划：判断子序列 (opens new window)</a>，<a href="https://programmercarl.com/0115.不同的子序列.html">动态规划：不同的子序列 (opens new window)</a>，<a href="https://programmercarl.com/0583.两个字符串的删除操作.html">动态规划：两个字符串的删除操作 (opens new window)</a>都要复杂的多。</p>
<p>在确定递推公式的时候，首先要考虑清楚编辑的几种操作，整理如下：</p>
<ul>
<li>if (word1[i - 1] == word2[j - 1])<ul>
<li>不操作</li>
</ul>
</li>
<li>if (word1[i - 1] != word2[j - 1])<ul>
<li>增</li>
<li>删</li>
<li>换</li>
</ul>
</li>
</ul>
<p>也就是如上四种情况。</p>
<p>if (word1[i - 1] == word2[j - 1]) 那么说明不用任何编辑，dp[i][j] 就应该是 dp[i - 1] [j - 1]，即dp[i][j] = dp[i - 1] [j - 1];</p>
<p><strong>在整个动规的过程中，最为关键就是正确理解dp[i] [j]的定义！</strong></p>
<p>if (word1[i - 1] != word2[j - 1])，此时就需要编辑了，如何编辑呢？</p>
<p>操作一：word1增加一个元素，使其word1[i - 1]与word2[j - 1]相同，那么就是以下标i-2为结尾的word1 与 i-1为结尾的word2的最近编辑距离 加上一个增加元素的操作。</p>
<p>即 dp[i][j] = dp[i - 1] [j] + 1;</p>
<p>操作二：word2添加一个元素，使其word1[i - 1]与word2[j - 1]相同，那么就是以下标i-1为结尾的word1 与 j-2为结尾的word2的最近编辑距离 加上一个增加元素的操作。</p>
<p>即 dp[i][j] = dp[i][j - 1] + 1;</p>
<p>这里有同学发现了，怎么都是添加元素，删除元素去哪了。</p>
<p><strong>word2添加一个元素，相当于word1删除一个元素</strong>，例如 word1 = “ad” ，word2 = “a”，word2添加一个元素d，也就是相当于word1删除一个元素d，操作数是一样！</p>
<p>操作三：替换元素，word1替换word1[i - 1]，使其与word2[j - 1]相同，此时不用增加元素，那么以下标i-2为结尾的word1 与 j-2为结尾的word2的最近编辑距离 加上一个替换元素的操作。</p>
<p>即 dp[i][j] = dp[i - 1] [j - 1] + 1;</p>
<p>综上，当 if (word1[i - 1] != word2[j - 1]) 时取最小的，即：dp[i] [j] = min({dp[i - 1] [j - 1], dp[i - 1] [j], dp[i][j - 1]}) + 1;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (word1[i - <span class="number">1</span>] == word2[j - <span class="number">1</span>]) &#123;</span><br><span class="line">    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    dp[i][j] = <span class="built_in">min</span>(&#123;dp[i - <span class="number">1</span>][j - <span class="number">1</span>], dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]&#125;) + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3><span id="让字符串成为最小回文">==让字符串成为最小回文==</span></h3><p><a href="https://leetcode-cn.com/problems/minimum-insertion-steps-to-make-a-string-palindrome/">https://leetcode-cn.com/problems/minimum-insertion-steps-to-make-a-string-palindrome/</a></p>
<h2><span id="六-回溯法">六、回溯法</span></h2><p><strong>一个决策树的遍历过程（回溯法）</strong>：一种通过探索所有可能的候选解来找出所有的解的算法。如果候选解被确认不是一个解（或者至少不是最后一个解），回溯算法会通过在上一步进行一些变化抛弃该解，即回溯并且再次尝试。</p>
<p>回溯法，一般可以解决如下几种问题：</p>
<ul>
<li>组合问题：N个数里面按一定规则找出k个数的集合</li>
<li>切割问题：一个字符串按一定规则有几种切割方式</li>
<li>子集问题：一个N个数的集合里有多少符合条件的子集</li>
<li>排列问题：N个数按一定规则全排列，有几种排列方式</li>
<li><strong>棋盘问题</strong>：N皇后，解数独等等</li>
</ul>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（10）回溯</title>
    <url>/posts/2G4BJ32/</url>
    <content><![CDATA[<h2><span id="回溯-子集-组合-排列-岛屿">回溯 - 子集、组合、排列、岛屿</span></h2><blockquote>
<p>  result = []<br>  def backtrack(路径, 选择列表):<br>      if 满足结束条件:<br>          result.add(路径)<br>          return<br>      for 选择 in 选择列表:<br>          做选择<br>          backtrack(路径, 选择列表)<br>          撤销选择</p>
</blockquote>
<h4><span id="剑指-offer-ii-079-所有子集"></span></h4><blockquote>
<p>  <a href="https://leetcode-cn.com/problems/combination-sum/">39.组合总和</a></p>
<p>  <a href="https://leetcode-cn.com/problems/combination-sum-ii/">40. 组合总和 II</a></p>
<p>  <a href="https://leetcode-cn.com/problems/permutations/">46. 全排列</a></p>
<p>  <a href="https://leetcode-cn.com/problems/permutations-ii/">47. 全排列 II</a></p>
<p>  <a href="https://leetcode-cn.com/problems/subsets/">78. 子集</a></p>
<p>  <a href="https://leetcode-cn.com/problems/subsets-ii/">90. 子集 II</a></p>
</blockquote>
<p>给定一个整数数组 <code>nums</code> ，数组中的元素 <strong>互不相同</strong> 。返回该数组所有可能的子集（幂集）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">输入：nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">输出：[[],[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">subsets</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">idx, tmp</span>):</span><br><span class="line">            ans.append(tmp[:])</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(idx, n):</span><br><span class="line">                tmp.append(nums[j])</span><br><span class="line">                backtrace(j + <span class="number">1</span>, tmp)</span><br><span class="line">                tmp.pop()</span><br><span class="line">        backtrace(<span class="number">0</span>, [])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">subsets</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># import itertools.combinations</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums) + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> tmp <span class="keyword">in</span> itertools.combinations(nums, i):</span><br><span class="line">                ans.append(tmp)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-12-矩阵中的路径"></span></h4><p>给定一个 m x n 二维字符网格 board 和一个字符串单词 word 。如果 word 存在于网格中，返回 true ；否则，返回 false 。单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。</p>
<p> 例如，在下面的 3×4 的矩阵中包含单词 “ABCCED”（单词中的字母已标出）。</p>
<p><img src="https://assets.leetcode.com/uploads/2020/11/04/word2.jpg" alt="img"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">exist</span>(<span class="params">self, board: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]], word: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">      m = <span class="built_in">len</span>(board)</span><br><span class="line">      n = <span class="built_in">len</span>(board[<span class="number">0</span>])</span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">x, y, index</span>):</span><br><span class="line">           <span class="keyword">if</span> index == <span class="built_in">len</span>(word):</span><br><span class="line">           		<span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">           <span class="keyword">for</span> dx, dy <span class="keyword">in</span> ((-<span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, -<span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)):</span><br><span class="line">               nx, ny = x + dx, y + dy</span><br><span class="line">               <span class="keyword">if</span> <span class="number">0</span> &lt;= nx &lt; m <span class="keyword">and</span> <span class="number">0</span> &lt;= ny &lt; n <span class="keyword">and</span> board[nx][ny] == word[index]:</span><br><span class="line">                   board[nx][ny] = <span class="string">&#x27;/&#x27;</span></span><br><span class="line">               		 <span class="keyword">if</span> dfs(nx, ny, index + <span class="number">1</span>):</span><br><span class="line">                       <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                   board[nx][ny] = word[index]</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">              <span class="keyword">if</span> board[i][j] == word[<span class="number">0</span>]:</span><br><span class="line">                  board[i][j] = <span class="string">&#x27;/&#x27;</span></span><br><span class="line">              		<span class="keyword">if</span> dfs(i, j, <span class="number">1</span>):</span><br><span class="line">                  		<span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">               		board[i][j] = word[<span class="number">0</span>]</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-13-机器人的运动范围"></span></h4><p>地上有一个m行n列的方格，从坐标 [0,0] 到坐标 [m-1,n-1] 。<strong>一个机器人从坐标 [0, 0] 的格子开始移动，它每次可以向左、右、上、下移动一格（不能移动到方格外），也不能进入行坐标和列坐标的数位之和大于k的格子</strong>。例如，当k为18时，机器人能够进入方格 [35, 37] ，因为3+5+3+7=18。但它不能进入方格 [35, 38]，因为3+5+3+8=19。请问该机器人能够到达多少个格子？</p>
<ul>
<li>bfs、bfs、岛屿面积 </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">movingCount</span>(<span class="params">self, m: <span class="built_in">int</span>, n: <span class="built_in">int</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># bfs</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">sum_</span>(<span class="params">x, y</span>):</span><br><span class="line">            <span class="keyword">return</span> x%<span class="number">10</span> + x//<span class="number">10</span> +  y%<span class="number">10</span> + y//<span class="number">10</span></span><br><span class="line">        que = collections.deque()</span><br><span class="line">        que.append((<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">        s = <span class="built_in">set</span>()</span><br><span class="line">        s.add((<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">        <span class="keyword">while</span> que:</span><br><span class="line">            x, y = que.popleft()</span><br><span class="line">            <span class="keyword">for</span> i, j <span class="keyword">in</span> [(x + <span class="number">1</span>, y), (x, y + <span class="number">1</span>)]:</span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt;= i &lt; m <span class="keyword">and</span> <span class="number">0</span> &lt;= j &lt; n <span class="keyword">and</span> (i, j) <span class="keyword">not</span> <span class="keyword">in</span> s <span class="keyword">and</span> sum_(i,j) &lt;= k:</span><br><span class="line">                    que.append((i, j))</span><br><span class="line">                    s.add((i, j))</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(s)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">movingCount</span>(<span class="params">self, m: <span class="built_in">int</span>, n: <span class="built_in">int</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># dfs【向下向右】</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">i: <span class="built_in">int</span>, j: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">            <span class="keyword">if</span> i &gt;= m <span class="keyword">or</span> j &gt;= n <span class="keyword">or</span> sum_(i, j) &gt; k <span class="keyword">or</span> f[i][j] == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            f[i][j] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span> + dfs(i + <span class="number">1</span>, j) + dfs(i, j + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">sum_</span>(<span class="params">x, y</span>):</span><br><span class="line">            <span class="keyword">return</span> x%<span class="number">10</span> + x//<span class="number">10</span> +  y%<span class="number">10</span> + y//<span class="number">10</span></span><br><span class="line">        </span><br><span class="line">        f = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m)]</span><br><span class="line">        <span class="keyword">return</span> dfs(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">movingCount</span>(<span class="params">self, m: <span class="built_in">int</span>, n: <span class="built_in">int</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 回溯: 岛屿面积  </span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">sumGrid</span>(<span class="params">x, y</span>):</span><br><span class="line">            <span class="keyword">return</span> x%<span class="number">10</span> + x//<span class="number">10</span> +  y%<span class="number">10</span> + y//<span class="number">10</span></span><br><span class="line">        visited = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">x, y</span>):</span><br><span class="line">            visited.add((x, y))</span><br><span class="line">            <span class="keyword">for</span> i, j <span class="keyword">in</span> [(x-<span class="number">1</span>, y), (x, y-<span class="number">1</span>), (x, y+<span class="number">1</span>), (x+<span class="number">1</span>, y)]:</span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt;= i &lt; m <span class="keyword">and</span> <span class="number">0</span>&lt;= j &lt; n <span class="keyword">and</span> (i, j) <span class="keyword">not</span> <span class="keyword">in</span> visited <span class="keyword">and</span> sumGrid(i, j) &lt;= k:</span><br><span class="line">                    backtrace(i, j)</span><br><span class="line">        backtrace(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(visited) </span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-34-二叉树中和为某一值的路径"></span></h4><p>给你二叉树的根节点 root 和一个整数目标和 targetSum ，<strong>找出==所有==从根节点到叶子节点 路径总和等于给定目标和的路径。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">self, root: TreeNode, target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">root, tmp, target</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> target == root.val:</span><br><span class="line">                ans.append(tmp[:] + [root.val])</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            tmp.append(root.val)</span><br><span class="line">            backtrace(root.left, tmp, target - root.val)</span><br><span class="line">            backtrace(root.right, tmp, target - root.val)</span><br><span class="line">            tmp.pop()</span><br><span class="line">        ans = []</span><br><span class="line">        backtrace(root, [], target)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-38-字符串的排列"></span></h4><ul>
<li><a href="https://leetcode.cn/problems/zi-fu-chuan-de-pai-lie-lcof/solution/dai-ma-sui-xiang-lu-jian-zhi-offer-38-zi-gwt6/">https://leetcode.cn/problems/zi-fu-chuan-de-pai-lie-lcof/solution/dai-ma-sui-xiang-lu-jian-zhi-offer-38-zi-gwt6/</a></li>
</ul>
<p>输入一个字符串，打印出该字符串中字符的<strong>所有排列</strong>。</p>
<ul>
<li><strong>还要强调的是==去重一定要对元素经行排序==，这样我们才方便通过相邻的节点来判断是否重复使用了</strong>。</li>
<li><strong>组合问题和排列问题是在树形结构的叶子节点上收集结果，而子集问题就是取树上所有节点的结果</strong>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">  	<span class="keyword">def</span> <span class="title function_">permutation</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        <span class="comment"># itertools.permutations(s)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">set</span>(<span class="string">&quot;&quot;</span>.join(t) <span class="keyword">for</span> t <span class="keyword">in</span> itertools.permutations(s)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">permuteUnique</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums: </span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        res = []</span><br><span class="line">        used = <span class="built_in">set</span>() <span class="comment"># [0] * len(nums)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtracking</span>(<span class="params">path</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(path) == <span class="built_in">len</span>(nums):</span><br><span class="line">                res.append(path[:])</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> used:</span><br><span class="line">                    <span class="keyword">if</span> i&gt;<span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i-<span class="number">1</span>] <span class="keyword">and</span> i-<span class="number">1</span> <span class="keyword">not</span> <span class="keyword">in</span> used:</span><br><span class="line">                      	<span class="comment"># 重复剪枝</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    used.add(i)</span><br><span class="line">                    path.append(nums[i])</span><br><span class="line">                    backtracking(path)</span><br><span class="line">                    path.pop()</span><br><span class="line">                    used.remove(i)</span><br><span class="line">        <span class="comment"># 记得给nums排序</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        backtracking([])</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-081-允许重复选择元素的组合"></span></h4><p><strong>给定一个无重复元素的正整数数组 candidates 和一个正整数 target</strong> ，找出 candidates 中所有可以使数字和为目标数 target 的唯一组合。<strong>candidates 中的数字可以无限制重复被选取。如果至少一个所选数字数量不同，则两种组合是不同的</strong>。 </p>
<blockquote>
<p>  输入: candidates = [2,3,6,7], target = 7<br>  输出: [[7],[2,2,3]]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combinationSum</span>(<span class="params">self, candidates: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># 回溯，无重复元素，根据剩余值凑成目标</span></span><br><span class="line">        ans = []</span><br><span class="line">        path = []</span><br><span class="line">        candidates.sort() <span class="comment"># 预先排序，</span></span><br><span class="line">        <span class="comment"># 收集逻辑为target == 0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtracking</span>(<span class="params">index,path,target</span>):</span><br><span class="line">            <span class="keyword">if</span> index &gt;= <span class="built_in">len</span>(candidates) <span class="keyword">or</span> target &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> </span><br><span class="line">            <span class="keyword">if</span> target == <span class="number">0</span>: <span class="comment"># 收集条件</span></span><br><span class="line">                ans.append(path[:])</span><br><span class="line">                <span class="keyword">return</span>    </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(index,<span class="built_in">len</span>(candidates)):  <span class="comment"># 注意可以重复收集          </span></span><br><span class="line">                path.append(candidates[i])  <span class="comment"># 做选择</span></span><br><span class="line">                backtracking(i,path,target-candidates[i])</span><br><span class="line">                path.pop() <span class="comment"># 取消选择</span></span><br><span class="line">         </span><br><span class="line">        backtracking(<span class="number">0</span>,[],target)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-082-含有重复元素集合的组合"></span></h4><p><strong>给定一个可能有重复数字的整数数组 <code>candidates</code> 和一个目标数 <code>target</code></strong> ，找出 <code>candidates</code> 中所有可以使数字和为 <code>target</code> 的组合。</p>
<p><code>candidates</code> 中的每个数字在每个组合中只能使用一次，解集不能包含重复的组合。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">输入: candidates = [<span class="number">10</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">6</span>,<span class="number">1</span>,<span class="number">5</span>], target = <span class="number">8</span>,</span><br><span class="line">输出:</span><br><span class="line">[</span><br><span class="line">[<span class="number">1</span>,<span class="number">1</span>,<span class="number">6</span>],</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>],</span><br><span class="line">[<span class="number">1</span>,<span class="number">7</span>],</span><br><span class="line">[<span class="number">2</span>,<span class="number">6</span>]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combinationSum2</span>(<span class="params">self, candidates: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        ans, n = [], <span class="built_in">len</span>(candidates)</span><br><span class="line">        candidates.sort()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">idx, tmp, target</span>):</span><br><span class="line">            <span class="keyword">if</span> idx &gt; n <span class="keyword">or</span> target &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">if</span> target == <span class="number">0</span>:</span><br><span class="line">                ans.append(tmp[:])</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(idx, n):</span><br><span class="line">                <span class="keyword">if</span> i &gt; idx <span class="keyword">and</span> candidates[i] == candidates[i - <span class="number">1</span>]:</span><br><span class="line">                    <span class="comment"># 去重：让递归的同一级不出现相同元素、递归的不同级可以出现相同元素</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                tmp.append(candidates[i])</span><br><span class="line">                backtrace(i + <span class="number">1</span>, tmp, target - candidates[i])</span><br><span class="line">                tmp.pop()</span><br><span class="line">        backtrace(<span class="number">0</span>, [], target)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-083-没有重复元素集合的全排列"></span></h4><p>给定一个不含重复数字的整数数组 <code>nums</code> ，返回其 <strong>所有可能的全排列</strong> 。可以 <strong>按任意顺序</strong> 返回答案。</p>
<blockquote>
<p>  输入：nums = [1,2,3]<br>  输出：[[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">permute</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># 库函数[数组交换]、回溯模版</span></span><br><span class="line">        <span class="comment"># return list(itertools.permutations(nums))</span></span><br><span class="line">        ans, n = [], <span class="built_in">len</span>(nums)</span><br><span class="line">        visited = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">idx, tmp</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(tmp) == n:</span><br><span class="line">                ans.append(tmp[:])</span><br><span class="line">                <span class="keyword">return</span> </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                    visited.add(i)</span><br><span class="line">                    tmp.append(nums[i])</span><br><span class="line">                    backtrace(i + <span class="number">1</span>, tmp)</span><br><span class="line">                    tmp.pop()</span><br><span class="line">                    visited.remove(i)</span><br><span class="line">        backtrace(<span class="number">0</span> ,[])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-084-含有重复元素集合的全排列"></span></h4><p>给定一个可包含重复数字的整数集合 <code>nums</code> ，<strong>按任意顺序</strong> 返回它所有不重复的全排列。</p>
<blockquote>
<p>  输入：nums = [1,1,2]<br>  输出：<br>  [[1,1,2],<br>   [1,2,1],<br>   [2,1,1]]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">permuteUnique</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># visited 记录访问节点的集合</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        ans = []</span><br><span class="line">        vis = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">tmp</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(tmp) == n:</span><br><span class="line">                ans.append(tmp[:])</span><br><span class="line">                <span class="keyword">return</span> </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">in</span> vis:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i - <span class="number">1</span>] <span class="keyword">and</span> i - <span class="number">1</span> <span class="keyword">not</span> <span class="keyword">in</span> vis:</span><br><span class="line">                    <span class="comment"># not in 保留顺序剪枝 </span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                vis.add(i)</span><br><span class="line">                backtrace(tmp + [nums[i]])</span><br><span class="line">                vis.remove(i)</span><br><span class="line">        nums.sort()</span><br><span class="line">        backtrace([])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="22-括号生成"></span></h4><p>数字 <code>n</code> 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 <strong>有效的</strong> 括号组合。</p>
<blockquote>
<p>  输入：n = 3<br>  输出：[“((()))”,”(()())”,”(())()”,”()(())”,”()()()”]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generateParenthesis</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        <span class="comment"># 回溯</span></span><br><span class="line">        <span class="comment"># 第一步：得到全部 2^(2n) 种组合，然后再根据我们刚才总结出的合法括号组合的性质筛选出合法的组合</span></span><br><span class="line">        <span class="comment"># 第二步：剪枝  1、可用左括号大于右括号 2、括号可用小于0</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">left, right, tmp</span>):</span><br><span class="line">            <span class="keyword">if</span> left &gt; right:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">if</span> left &lt; <span class="number">0</span>  <span class="keyword">or</span> right &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">if</span> left == <span class="number">0</span> <span class="keyword">and</span> right == <span class="number">0</span>:</span><br><span class="line">                ans.append(<span class="string">&quot;&quot;</span>.join(tmp[:]))</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            tmp.append(<span class="string">&#x27;(&#x27;</span>)</span><br><span class="line">            backtrace(left - <span class="number">1</span>, right, tmp)</span><br><span class="line">            tmp.pop()</span><br><span class="line"></span><br><span class="line">            tmp.append(<span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">            backtrace(left, right - <span class="number">1</span>, tmp)</span><br><span class="line">            tmp.pop()</span><br><span class="line">        backtrace(n, n, [])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-086-分割回文子字符串"></span></h4><p>给定一个字符串 s ，请将 s 分割成一些子串，使每个子串都是 回文串 ，返回 s 所有可能的分割方案。回文串 是正着读和反着读都一样的字符串。</p>
<blockquote>
<p>  输入：s = “google”<br>  输出：[[“g”,”o”,”o”,”g”,”l”,”e”],[“g”,”oo”,”g”,”l”,”e”],[“goog”,”l”,”e”]]</p>
</blockquote>
<ul>
<li><strong><font color="red"> 动态规划预处理 + 回溯全排列</font></strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">partition</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]:</span><br><span class="line">        <span class="comment"># 动态规划预处理 + 回溯全排列条件</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [[<span class="literal">False</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dp[i][i] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">                <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                    <span class="keyword">if</span> j - i &lt;= <span class="number">1</span> <span class="keyword">or</span> dp[i + <span class="number">1</span>][j - <span class="number">1</span>]:</span><br><span class="line">                        dp[i][j] = <span class="literal">True</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">idx, tmp</span>):</span><br><span class="line">            <span class="keyword">if</span> idx == n:</span><br><span class="line">                ans.append(tmp[:])</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(idx, n):</span><br><span class="line">                <span class="keyword">if</span> dp[idx][i]:</span><br><span class="line">                    tmp.append(s[idx: i + <span class="number">1</span>])</span><br><span class="line">                    backtrace(i + <span class="number">1</span>, tmp)</span><br><span class="line">                    tmp.pop()</span><br><span class="line">        backtrace(<span class="number">0</span>, [])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="93-复原-ip-地址-回溯剪枝"></span></h4><p><strong>有效 IP 地址</strong> 正好由四个整数（每个整数位于 <code>0</code> 到 <code>255</code> 之间组成，且不能含有前导 <code>0</code>），整数之间用 <code>&#39;.&#39;</code> 分隔。</p>
<ul>
<li>例如：”0.1.2.201” 和 “192.168.1.1” 是 有效 IP 地址，但是 “0.011.255.245”、”192.168.1.312” 和 “192.168@1.1” 是 <strong>无效 IP 地址。</strong></li>
</ul>
<p><strong>给定一个只包含数字的字符串 s ，用以表示一个 IP 地址，返回所有可能的有效 IP 地址，这些地址可以通过在 s 中插入 ‘.’ 来形成。</strong>你 不能 重新排序或删除 s 中的任何数字。你可以按 任何 顺序返回答案。</p>
<blockquote>
<p>  输入：s = “25525511135”<br>  输出：[“255.255.11.135”,”255.255.111.35”]</p>
</blockquote>
<p><img src="https://pic.leetcode-cn.com/b581bdde1cef982f0af3182af17fc3c41960c76a7445af0dcfd445c89b4c2eaa-「力扣」第 93 题：复原 IP 地址-1.png" alt="「力扣」第 93 题：复原 IP 地址-1.png" style="zoom: 33%;"></p>
<ul>
<li>如果还没有找到 4 段 IP 地址就已经遍历完了字符串，那么提前回溯;</li>
<li>由于不能有前导零，如果当前数字为 0，那么这一段 IP 地址只能为 0;</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">restoreIpAddresses</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        SEG_COUNT = <span class="number">4</span></span><br><span class="line">        ans = <span class="built_in">list</span>()</span><br><span class="line">        segments = [<span class="number">0</span>] * SEG_COUNT</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">segId: <span class="built_in">int</span>, segStart: <span class="built_in">int</span></span>):</span><br><span class="line">            <span class="comment"># 如果找到了 4 段 IP 地址并且遍历完了字符串，那么就是一种答案</span></span><br><span class="line">            <span class="keyword">if</span> segId == SEG_COUNT:</span><br><span class="line">                <span class="keyword">if</span> segStart == <span class="built_in">len</span>(s):</span><br><span class="line">                    ipAddr = <span class="string">&quot;.&quot;</span>.join(<span class="built_in">str</span>(seg) <span class="keyword">for</span> seg <span class="keyword">in</span> segments)</span><br><span class="line">                    ans.append(ipAddr)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="comment"># 如果还没有找到 4 段 IP 地址就已经遍历完了字符串，那么提前回溯</span></span><br><span class="line">            <span class="keyword">if</span> segStart == <span class="built_in">len</span>(s):</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="comment"># 由于不能有前导零，如果当前数字为 0，那么这一段 IP 地址只能为 0</span></span><br><span class="line">            <span class="keyword">if</span> s[segStart] == <span class="string">&quot;0&quot;</span>:</span><br><span class="line">                segments[segId] = <span class="number">0</span></span><br><span class="line">                dfs(segId + <span class="number">1</span>, segStart + <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 一般情况，枚举每一种可能性并递归</span></span><br><span class="line">            addr = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> segEnd <span class="keyword">in</span> <span class="built_in">range</span>(segStart, <span class="built_in">len</span>(s)):</span><br><span class="line">                addr = addr * <span class="number">10</span> + (<span class="built_in">ord</span>(s[segEnd]) - <span class="built_in">ord</span>(<span class="string">&quot;0&quot;</span>))</span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt; addr &lt;= <span class="number">0xFF</span>:</span><br><span class="line">                    segments[segId] = addr</span><br><span class="line">                    dfs(segId + <span class="number">1</span>, segEnd + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        dfs(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（11）动态规划</title>
    <url>/posts/1QG49BK/</url>
    <content><![CDATA[<h1><span id="五-动态规划"></span></h1><p><img src="https://images.zsxq.com/FvoG8qppuOWSNhXBbj27ShBAJw0G?imageMogr2/auto-orient/quality/100!/ignore-error/1&amp;e=1648742399&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:q36Nzw1NTcg-NvuvnNyOWSZ_mdI=" alt="img"></p>
<p><strong>首先，动态规划问题的一般形式就是==求最值==</strong>。动态规划其实是运筹学的一种最优化方法，只不过在计算机问题上应用比较多，比如说让你求<strong>最长</strong>递增子序列呀，<strong>最小</strong>编辑距离呀等等。<strong>求解动态规划的核心问题是穷举</strong>。首先，动态规划的穷举有点特别，因为这类问题<strong>存在「重叠子问题」</strong>，如果暴力穷举的话效率会极其低下，所以需要「==备忘录」或者「DP table」==来优化穷举过程，避免不必要的计算。而且，动态规划问题一定会<strong>具备「最优子结构」</strong>，才能通过子问题的最值得到原问题的最值。</p>
<p>另外，虽然动态规划的核心思想就是穷举求最值，但是问题可以千变万化，穷举所有可行解其实并不是一件容易的事，只有列出<strong>正确的「状态转移方程」</strong>，才能正确地穷举。</p>
<p>以上提到的<strong>重叠子问题、最优子结构、状态转移方程</strong>就是动态规划三要素。具体什么意思等会会举例详解，但是在实际的算法问题中，<strong>写出状态转移方程是最困难的</strong>，这也就是为什么很多朋友觉得动态规划问题困难的原因，我来提供我研究出来的一个思维框架，辅助你思考状态转移方程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化 base case</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>][...] = base</span><br><span class="line"><span class="comment"># 进行状态转移</span></span><br><span class="line"><span class="keyword">for</span> 状态<span class="number">1</span> <span class="keyword">in</span> 状态<span class="number">1</span>的所有取值：</span><br><span class="line">    <span class="keyword">for</span> 状态<span class="number">2</span> <span class="keyword">in</span> 状态<span class="number">2</span>的所有取值：</span><br><span class="line">        <span class="keyword">for</span> ...</span><br><span class="line">            dp[状态<span class="number">1</span>][状态<span class="number">2</span>][...] = 求最值(选择<span class="number">1</span>，选择<span class="number">2.</span>..)</span><br></pre></td></tr></table></figure>
<h2><span id="51-背包问题"><strong>5.1 背包问题</strong></span></h2><p><a href="https://leetcode-cn.com/problems/coin-change/solution/dai-ma-sui-xiang-lu-dai-ni-xue-tou-wan-q-80r7/">https://leetcode-cn.com/problems/coin-change/solution/dai-ma-sui-xiang-lu-dai-ni-xue-tou-wan-q-80r7/</a></p>
<p><img src="https://img-blog.csdnimg.cn/20210117171307407.png" alt="416.分割等和子集1"></p>
<blockquote>
<p>  <strong>1、确定dp数组以及下标的含义</strong></p>
<p>  <strong>2、确定递推公式</strong></p>
<p>  <strong>3、dp数组如何初始化</strong></p>
<ul>
<li><strong>最大长度</strong></li>
<li><p><strong>递推等号左侧要初始化</strong></p>
<p><strong>4、确定遍历顺序</strong></p>
</li>
<li><p><strong>滚动数组</strong></p>
<p><strong>5、举例推导dp数组</strong></p>
</li>
</ul>
</blockquote>
<h3><span id="511-背包递推公式">5.1.1 背包递推公式</span></h3><p><strong>问==能否装满背包==</strong>（或者<strong>最多装多少</strong>）：dp[j] = max(dp[j], dp[j - nums[i]] + nums[i]); ，对应题目如下：</p>
<ul>
<li><a href="https://programmercarl.com/0416.分割等和子集.html">动态规划：416.分割等和子集(opens new window)</a></li>
<li><a href="https://leetcode-cn.com/problems/partition-to-k-equal-sum-subsets/">划分为k个相等的子集</a></li>
<li><a href="https://programmercarl.com/1049.最后一块石头的重量II.html">动态规划：1049.最后一块石头的重量 II</a></li>
</ul>
<p><strong>==问装满背包有几种方法==</strong>：dp[j] += dp[j - nums[i]] ，对应题目如下：</p>
<ul>
<li><p><a href="https://programmercarl.com/0494.目标和.html">动态规划：494.目标和(opens new window)</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canPartition</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 背包问题</span></span><br><span class="line">        total, n = <span class="built_in">sum</span>(nums), <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">if</span> total % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        target = total // <span class="number">2</span></span><br><span class="line">        dp = [<span class="literal">True</span>] + [<span class="literal">False</span>] * target</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(target, nums[i] - <span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">                <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">                逆序更新</span></span><br><span class="line"><span class="string">                dp[i][j] = dp[i - 1][j] or dp[i - 1][j - nums[i]]</span></span><br><span class="line"><span class="string">                &quot;&quot;&quot;</span></span><br><span class="line">                dp[j] |= dp[j - nums[i]]</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://programmercarl.com/0518.零钱兑换II.html">动态规划：518. 零钱兑换 II(opens new window)</a></p>
</li>
<li><p><a href="https://programmercarl.com/0377.组合总和Ⅳ.html">动态规划：377.组合总和Ⅳ(opens new window)</a></p>
<blockquote>
<p>  <strong>如果求组合数就是外层for循环遍历物品，内层for遍历背包</strong>。</p>
<p>  <strong>如果求排列数就是外层for遍历背包，内层for循环遍历物品</strong>。</p>
</blockquote>
</li>
<li><p><a href="https://programmercarl.com/0070.爬楼梯完全背包版本.html">动态规划：70. 爬楼梯进阶版（完全背包）</a></p>
</li>
</ul>
<p><strong>问背包装满==最大价值==</strong>：dp[j] = max(dp[j], dp[j - weight[i]] + value[i]); ，对应题目如下：</p>
<ul>
<li><a href="https://programmercarl.com/0474.一和零.html">动态规划：474.一和零</a></li>
</ul>
<p><strong>问装满背包所有物品的==最小个数==</strong>：dp[j] = min(dp[j - coins[i]] + 1, dp[j]); ，对应题目如下：</p>
<ul>
<li><a href="https://programmercarl.com/0322.零钱兑换.html">动态规划：322.零钱兑换(opens new window)</a></li>
<li><a href="https://programmercarl.com/0279.完全平方数.html">动态规划：279.完全平方数(opens new window)</a></li>
</ul>
<h3><span id="512-遍历顺序">5.1.2 遍历顺序</span></h3><h4><span id="01背包">01背包</span></h4><p>在<a href="https://programmercarl.com/背包理论基础01背包-1.html">动态规划：关于01背包问题，你该了解这些！ (opens new window)</a>中我们讲解二维dp数组01背包先遍历物品还是先遍历背包都是可以的，且第二层for循环是从小到大遍历。</p>
<p>和<a href="https://programmercarl.com/背包理论基础01背包-2.html">动态规划：关于01背包问题，你该了解这些！（滚动数组） (opens new window)</a>中，我们讲解一维dp数组01背包只能先遍历物品再遍历背包容量，且第二层for循环是从大到小遍历。</p>
<p><strong>一维dp数组的背包在遍历顺序上和二维dp数组实现的01背包其实是有很大差异的，大家需要注意！</strong></p>
<h4><span id="完全背包">完全背包</span></h4><p>说完01背包，再看看完全背包。</p>
<p>在<a href="https://programmercarl.com/背包问题理论基础完全背包.html">动态规划：关于完全背包，你该了解这些！ (opens new window)</a>中，讲解了纯完全背包的一维dp数组实现，先遍历物品还是先遍历背包都是可以的，且第二层for循环是从小到大遍历。</p>
<p>但是仅仅是纯完全背包的遍历顺序是这样的，题目稍有变化，两个for循环的先后顺序就不一样了。</p>
<p><strong>如果求==组合数==就是外层for循环遍历物品，内层for遍历背包</strong>。</p>
<p><strong>如果==求排列数==就是外层for遍历背包，内层for循环遍历物品</strong>。</p>
<p>相关题目如下：</p>
<ul>
<li>求组合数：<a href="https://programmercarl.com/0518.零钱兑换II.html">动态规划：518.零钱兑换II(opens new window)</a></li>
<li>求排列数：<a href="https://mp.weixin.qq.com/s/Iixw0nahJWQgbqVNk8k6gA">动态规划：377. 组合总和 Ⅳ (opens new window)</a>、<a href="https://programmercarl.com/0070.爬楼梯完全背包版本.html">动态规划：70. 爬楼梯进阶版（完全背包）(opens new window)</a></li>
</ul>
<p>如果求最小数，那么两层for循环的先后顺序就无所谓了，相关题目如下：</p>
<ul>
<li>求最小数：<a href="https://programmercarl.com/0322.零钱兑换.html">动态规划：322. 零钱兑换 (opens new window)</a>、<a href="https://programmercarl.com/0279.完全平方数.html">动态规划：279.完全平方数(opens new window)</a></li>
</ul>
<p><strong>对于背包问题，其实递推公式算是容易的，难是难在遍历顺序上，如果把遍历顺序搞透，才算是真正理解了</strong>。</p>
<h2><span id="52-子序列数组问题">5.2 <strong>子序列（数组）问题</strong></span></h2><p><img src="https://code-thinking.cdn.bcebos.com/pics/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E5%AD%90%E5%BA%8F%E5%88%97%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.jpg" alt="img" style="zoom:50%;"></p>
<h4><span id="剑指-offer-42-连续子数组的最大和"></span></h4><p>输入一个整型数组，数组中的一个或连续多个整数组成一个子数组。求所有子数组的和的最大值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxSubArray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums))]</span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            dp[i] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>] + nums[i], nums[i])</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure>
<h4><span id="动态规划-最长递增子序列"></span></h4><p>给你一个整数数组 <code>nums</code> ，找到其中最长严格递增子序列的长度。</p>
<ul>
<li><strong>动态规划</strong> 位置i的最长升序子序列等于j从0到i-1各个位置的最长升序子序列 + 1 的最大值。</li>
<li><strong>贪心 + 二分查找</strong> tail[i] : 记录i长度的末尾是什么</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLIS</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(nums)</span><br><span class="line">        dp = [<span class="number">1</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, i):</span><br><span class="line">                <span class="keyword">if</span> nums[j] &lt; numa[i]:</span><br><span class="line">                    dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line">            result = <span class="built_in">max</span>(result, dp[i]) <span class="comment">#取长的子序列</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">lengthOfLIS</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 贪心 + 二分查找 tail[i] : 记录i长度的末尾是什么</span></span><br><span class="line">        tails, res = [<span class="number">0</span>] * <span class="built_in">len</span>(nums), <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            i = bisect.bisect_left(tails[:res], num)</span><br><span class="line">            <span class="string">&quot;&quot;&quot;左边界</span></span><br><span class="line"><span class="string">            while i &lt; j:</span></span><br><span class="line"><span class="string">                mid = (i + j)//2</span></span><br><span class="line"><span class="string">                if tails[m] &lt; num:</span></span><br><span class="line"><span class="string">                    i = m + 1 </span></span><br><span class="line"><span class="string">                else:</span></span><br><span class="line"><span class="string">                    j = m</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            tails[i] = num</span><br><span class="line">            <span class="keyword">if</span> i == res:</span><br><span class="line">                res += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h5><span id="动态规划最长斐波那契数列"></span></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lenLongestFibSubseq</span>(<span class="params">self, A: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># 基本顺序是 k，i，j 或者 A[k] = A[j] - A[i]</span></span><br><span class="line">    n = <span class="built_in">len</span>(A)</span><br><span class="line">    dic = &#123;&#125;</span><br><span class="line">    <span class="comment"># 创建索引字典，提速</span></span><br><span class="line">    <span class="keyword">for</span> ind,val <span class="keyword">in</span> <span class="built_in">enumerate</span>(A):</span><br><span class="line">        dic[val] = ind</span><br><span class="line">    <span class="comment"># 初始化，行代表的是i，不需要取到n-1，为了给j留出位置</span></span><br><span class="line">    <span class="comment"># 初始为2，只要包含了 j i 位置，则意味着已经有了2个数字。</span></span><br><span class="line">    dp = [[<span class="number">2</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>)]</span><br><span class="line">    ret = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 因此i只能取到n-2，给j留出空间</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n-<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># j从i+1开始，毕竟j在i后面</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,n):</span><br><span class="line">            diff = A[j] - A[i]</span><br><span class="line">            <span class="keyword">if</span> diff <span class="keyword">in</span> dic <span class="keyword">and</span> dic[diff] &lt; i:</span><br><span class="line">                k = dic[diff]</span><br><span class="line">                dp[i][j] = dp[k][i] + <span class="number">1</span> <span class="comment"># 这个1，代表着k位置数字</span></span><br><span class="line">                ret = <span class="built_in">max</span>(ret,dp[i][j])</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>
<h4><span id="动态规划最长重复子数组"></span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findLength</span>(<span class="params">self, A: <span class="type">List</span>[<span class="built_in">int</span>], B: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="number">0</span>] * (<span class="built_in">len</span>(B) + <span class="number">1</span>)</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(A)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(B), <span class="number">0</span>, -<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> A[i-<span class="number">1</span>] == B[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[j] = dp[j-<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[j] = <span class="number">0</span> <span class="comment">#注意这里不相等的时候要有赋0的操作</span></span><br><span class="line">                result = <span class="built_in">max</span>(result, dp[j])</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h4><span id="动态规划最长公共子序列"></span></h4><p>给定两个字符串 <code>text1</code> 和 <code>text2</code>，返回这两个字符串的最长 <strong>公共子序列</strong> 的长度。如果不存在 <strong>公共子序列</strong> ，返回 <code>0</code> 。</p>
<p>一个字符串的 <strong>子序列</strong> 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestCommonSubsequence</span>(<span class="params">self, text1: <span class="built_in">str</span>, text2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        m, n = <span class="built_in">len</span>(text1), <span class="built_in">len</span>(text2)</span><br><span class="line">        dp = [[<span class="number">0</span>] * (n + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> text1[i - <span class="number">1</span>] == text2[j - <span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></table></figure>
<h4><span id="动态规划不相交的线"></span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestCommonSubsequence</span>(<span class="params">self, text1: <span class="built_in">str</span>, text2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 最长子序列 </span></span><br><span class="line">        m, n = <span class="built_in">len</span>(text1), <span class="built_in">len</span>(text2)</span><br><span class="line">        dp = [[<span class="number">0</span>] * (n+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> text2[j-<span class="number">1</span>] == text1[i-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>] +<span class="number">1</span> </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>][-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p><a href="https://programmercarl.com/0053.最大子序和（动态规划）.html">动态规划：最大子序和</a> 【贪心】【动态规划】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxSubArray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># 为什么dp[-1]不是最大？需要res</span></span><br><span class="line">    <span class="comment"># dp[i] 以i结尾的最大子数组和</span></span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    dp = [<span class="number">0</span>] * (n)</span><br><span class="line">    dp[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        dp[i] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>]+nums[i], nums[i])</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure>
<p><a href="https://programmercarl.com/0392.判断子序列.html">动态规划：判断子序列</a> 【双指针】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isSubsequence</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        n, m = <span class="built_in">len</span>(s), <span class="built_in">len</span>(t)</span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> j &lt; m:</span><br><span class="line">            <span class="keyword">if</span> s[i] == t[j]:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> i == n</span><br></pre></td></tr></table></figure>
<p><a href="https://programmercarl.com/0115.不同的子序列.html">动态规划：不同的子序列</a></p>
<p><a href="https://programmercarl.com/0583.两个字符串的删除操作.html">动态规划：两个字符串的删除操作</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dp[i][j] : word1[i-1], word1[j-1] 最少步数</span></span><br><span class="line"><span class="comment"># word1[i - 1] 与 word2[j - 1]不相同的时候，有2种情况 *  min(dp[i-1][j], dp[i][j-1])+1  </span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">minDistance</span>(<span class="params">self, word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        m, n = <span class="built_in">len</span>(word1), <span class="built_in">len</span>(word2)</span><br><span class="line">        dp = [[<span class="number">0</span>] * (n+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][i] = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>):</span><br><span class="line">            dp[j][<span class="number">0</span>] = j</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i-<span class="number">1</span>] == word2[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])+<span class="number">1</span> <span class="comment"># , dp[i-1][j-1]+1</span></span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>][-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p><a href="https://programmercarl.com/0072.编辑距离.html">动态规划：编辑距离</a></p>
<p><a href="https://programmercarl.com/为了绝杀编辑距离，卡尔做了三步铺垫.html">为了绝杀编辑距离，我做了三步铺垫，你都知道么？</a></p>
<h4><span id="剑指-offer-ii-020-回文子字符串的个数"></span></h4><p>给定一个字符串 <code>s</code> ，请计算这个字符串中有多少个回文子字符串。具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被视作不同的子串。</p>
<ul>
<li>动态规划、双指针+中心扩展、<strong>==Manacher算法==</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countSubstrings</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 双指针+中心扩散</span></span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_extend</span>(<span class="params">s, i, j, n</span>):</span><br><span class="line">            res = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">and</span> j &lt; n <span class="keyword">and</span> s[i] == s[j]: <span class="comment"># 确定中心点</span></span><br><span class="line">                i -= <span class="number">1</span></span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">                res += <span class="number">1</span> <span class="comment"># 扩散过程也是答案</span></span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            result += _extend(s, i, i, <span class="built_in">len</span>(s)) <span class="comment">#以i为中心</span></span><br><span class="line">            result += _extend(s, i, i+<span class="number">1</span>, <span class="built_in">len</span>(s)) <span class="comment">#以i和i+1为中心</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countSubstrings</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 动态规划 dp[i][j]: s[i:j] 是否为回文子串</span></span><br><span class="line">        <span class="comment"># dp[i+1][j-1] -&gt; dp[i][j] 遍历顺序 </span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [[<span class="literal">False</span>] * (n) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i,n):</span><br><span class="line">                <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                    <span class="keyword">if</span> j - i &lt;= <span class="number">1</span>:</span><br><span class="line">                        ans += <span class="number">1</span></span><br><span class="line">                        dp[i][j] = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">elif</span> dp[i+<span class="number">1</span>][j-<span class="number">1</span>]:</span><br><span class="line">                        ans += <span class="number">1</span></span><br><span class="line">                        dp[i][j] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<p><a href="https://leetcode-cn.com/problems/longest-palindromic-substring/">最长回文子串</a> ==[Manacher 算法]==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 中心扩展法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># 枚举+中心扩展法</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">extendCenter</span>(<span class="params">left, right</span>):</span><br><span class="line">            <span class="keyword">while</span> <span class="number">0</span> &lt;= left <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">                left -= <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 返回的为while不成立的值</span></span><br><span class="line">            <span class="keyword">return</span> left+<span class="number">1</span>, right-<span class="number">1</span> </span><br><span class="line">        start, end = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            left1, right1 = extendCenter(i,i)</span><br><span class="line">            left2, right2 = extendCenter(i,i+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> right1 - left1 &gt; end - start:</span><br><span class="line">                start, end = left1, right1</span><br><span class="line">            <span class="keyword">if</span> right2 - left2 &gt; end - start:</span><br><span class="line">                start, end = left2, right2</span><br><span class="line">        <span class="keyword">return</span> s[start:end+<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="comment"># dp[i][j]: 是否是回文 dp[i+1][j-1] -&gt; dp[i][j]</span></span><br><span class="line">    <span class="comment"># 返回的子串，不是数字 不能记录长度</span></span><br><span class="line">    n = <span class="built_in">len</span>(s)</span><br><span class="line">    dp = [[<span class="literal">False</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)] <span class="comment"># 右上为1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dp[i][i] = <span class="literal">True</span></span><br><span class="line">    begin = <span class="number">0</span></span><br><span class="line">    maxlen = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 判断是否是回文子串中，如果是则记录begin and len</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>, -<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i,n):</span><br><span class="line">            <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                <span class="keyword">if</span> j - i &lt;=<span class="number">1</span>:</span><br><span class="line">                    dp[i][j] = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">elif</span> dp[i+<span class="number">1</span>][j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> dp[i][j] <span class="keyword">and</span> j - i + <span class="number">1</span> &gt; maxlen:</span><br><span class="line">                maxlen = j - i + <span class="number">1</span></span><br><span class="line">                begin = i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s[begin:begin+maxlen]</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Manacher 算法：<strong>Manacher 算法是在线性时间内求解最长回文子串的算法</strong>。在本题中，我们要求解回文串的个数，为什么也能使用 Manacher 算法呢？这里我们就需要理解一下 Manacher 的基本原理。</p>
<ul>
<li><p>奇偶长度处理： abaaabaa 会被处理成 #a#b#a#a##<em>a</em>#<em>b</em>#<em>a</em>#<em>a</em>#</p>
</li>
<li><p>==<strong>$f(i)$</strong> 来表示以 s 的第 i 位为回文中心，可以拓展出的<strong>最大回文半径 （包括 # ）</strong>==，那么$ f(i) - 1$就是以 i 为中心的最大回文串长度 。</p>
</li>
<li><p>利用已经计算出来的状态来更新 f(i）：<strong>回文右端点rm</strong> ：i + f(i) - 1</p>
</li>
</ul>
</li>
</ul>
<p><a href="https://programmercarl.com/0516.最长回文子序列.html">动态规划：最长回文子序列</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindromeSubseq</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 动态规划 dp[i][j]: s[i:j] 最大回文长度</span></span><br><span class="line">        <span class="comment"># dp[i+1][j-1] -&gt; dp[i][j] 遍历顺序 and j - i &gt;=2</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [[<span class="number">0</span>] * i +[<span class="number">1</span>] * (n-i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)] <span class="comment"># 从定义出发 右上三角 = 1 有意义</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,n): <span class="comment"># j - i &gt;=2</span></span><br><span class="line">                <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                    dp[i][j] = dp[i+<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">2</span> </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i+<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])                </span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][n-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h5><span id="最少回文分割"></span></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minCut</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 动态规划1:判断回文</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        g = [[<span class="literal">True</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">                g[i][j] = (s[i] == s[j]) <span class="keyword">and</span> g[i + <span class="number">1</span>][j - <span class="number">1</span>]</span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        f = [<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)] * n</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> g[<span class="number">0</span>][i]:</span><br><span class="line">                f[i] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">                    <span class="keyword">if</span> g[j + <span class="number">1</span>][i]:</span><br><span class="line">                        <span class="comment"># （0, j) + (j+1, i)</span></span><br><span class="line">                        f[i] = <span class="built_in">min</span>(f[i], f[j] + <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> f[n - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h5><span id="最长连续序列"></span></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestConsecutive</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 动态规划</span></span><br><span class="line">        dic, res = <span class="built_in">dict</span>(), <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> num <span class="keyword">not</span> <span class="keyword">in</span> dic:</span><br><span class="line">                left, right = dic.get(num - <span class="number">1</span>, <span class="number">0</span>), dic.get(num + <span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">                cur = <span class="number">1</span> + left +right</span><br><span class="line">                <span class="keyword">if</span> res &lt; cur:</span><br><span class="line">                    res = cur</span><br><span class="line">                dic[num] = cur</span><br><span class="line">                dic[num - left] = cur</span><br><span class="line">                dic[num + right] = cur</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3><span id="53-打家劫舍问题">5.3 打家劫舍问题</span></h3><ul>
<li><a href="https://programmercarl.com/0198.打家劫舍.html">动态规划：开始打家劫舍</a></li>
<li><a href="https://programmercarl.com/0213.打家劫舍II.html">动态规划：继续打家劫舍 环</a></li>
<li><a href="https://programmercarl.com/0337.打家劫舍III.html">动态规划：还要打家劫舍 树状dp</a></li>
</ul>
<h3><span id="54-股票问题">5.4 股票问题</span></h3><p><img src="https://code-thinking.cdn.bcebos.com/pics/%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.jpg" alt="股票问题总结" style="zoom:50%;"></p>
<p><a href="https://programmercarl.com/0121.买卖股票的最佳时机.html">动态规划：121.买卖股票的最佳时机(opens new window)</a>  </p>
<p>股票只能买卖一次，问最大利润</p>
<ul>
<li><strong>动态规划、贪心算法</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">      	<span class="comment"># 动态规划</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> prices:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        dp = [[<span class="number">0</span>] * <span class="number">2</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices))] </span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = -prices[<span class="number">0</span>]<span class="comment"># 已持有的状态</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">0</span> <span class="comment"># 未持有的状态</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(prices)):</span><br><span class="line">            dp[i][<span class="number">0</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">0</span>], -prices[i])</span><br><span class="line">            dp[i][<span class="number">1</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">1</span>], dp[i-<span class="number">1</span>][<span class="number">0</span>] + prices[i])</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 贪心算法</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> prices:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        buy, profit = prices[<span class="number">0</span>], <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(prices)):</span><br><span class="line">            profit = <span class="built_in">max</span>(profit, prices[i] - buy)</span><br><span class="line">            <span class="keyword">if</span> prices[i] &lt; buy:</span><br><span class="line">                buy = prices[i]</span><br><span class="line">        <span class="keyword">return</span> profit    </span><br></pre></td></tr></table></figure>
<p><a href="https://programmercarl.com/0122.买卖股票的最佳时机II（动态规划）.html">动态规划：122.买卖股票的最佳时机II(opens new window)</a> 可以多次买卖股票，问最大收益。</p>
<p><a href="https://programmercarl.com/0123.买卖股票的最佳时机III.html">动态规划：123.买卖股票的最佳时机III(opens new window)</a> 最多买卖两次，问最大收益。</p>
<p><a href="https://programmercarl.com/0188.买卖股票的最佳时机IV.html">动态规划：188.买卖股票的最佳时机IV(opens new window)</a> 最多买卖k笔交易，问最大收益。</p>
<p><a href="https://programmercarl.com/0309.最佳买卖股票时机含冷冻期.html">动态规划：309.最佳买卖股票时机含冷冻期(opens new window)</a> 可以多次买卖但每次卖出有冷冻期1天。</p>
<p><a href="https://programmercarl.com/0714.买卖股票的最佳时机含手续费（动态规划）.html">动态规划：714.买卖股票的最佳时机含手续费(opens new window)</a> 可以多次买卖，但每次有手续费。</p>
<h3><span id="55-编辑距离问题">5.5 编辑距离问题</span></h3><h4><span id="判断子序列">判断子序列</span></h4><p><a href="https://programmercarl.com/0392.判断子序列.html">动态规划：392.判断子序列 (opens new window)</a>给定字符串 s 和 t ，判断 s 是否为 t 的子序列。</p>
<p>这道题目 其实是可以用<strong>双指针</strong>或者<strong>贪心</strong>的的，但是我在开篇的时候就说了这是编辑距离的入门题目，因为从题意中我们也可以发现，只需要计算删除的情况，不用考虑增加和替换的情况。</p>
<ul>
<li>if (s[i - 1] == t[j - 1])<ul>
<li>t中找到了一个字符在s中也出现了</li>
</ul>
</li>
<li>if (s[i - 1] != t[j - 1])<ul>
<li>相当于t要删除元素，继续匹配</li>
</ul>
</li>
</ul>
<p>状态转移方程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (s[i - <span class="number">1</span>] == t[j - <span class="number">1</span>]) dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span> dp[i][j] = dp[i][j - <span class="number">1</span>];</span><br></pre></td></tr></table></figure>
<h4><span id="不同的子序列">不同的子序列</span></h4><p><a href="https://programmercarl.com/0115.不同的子序列.html">动态规划：115.不同的子序列 (opens new window)</a>给定一个字符串 s 和一个字符串 t ，计算在 s 的子序列中 t 出现的个数。</p>
<p>本题虽然也只有删除操作，不用考虑替换增加之类的，但相对于<a href="https://programmercarl.com/0392.判断子序列.html">动态规划：392.判断子序列 (opens new window)</a>就有难度了，这道题目双指针法可就做不了。</p>
<p>当s[i - 1] 与 t[j - 1]相等时，dp[i][j]可以有两部分组成。</p>
<p>一部分是用s[i - 1]来匹配，那么个数为 dp[i - 1] [j - 1]</p>
<p>一部分是不用s[i - 1]来匹配，个数为 dp[i - 1] [j]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (s[i - <span class="number">1</span>] == t[j - <span class="number">1</span>]) &#123;</span><br><span class="line">    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + dp[i - <span class="number">1</span>][j];</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    dp[i][j] = dp[i - <span class="number">1</span>][j];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="两个字符串的删除操作">两个字符串的删除操作</span></h4><p><a href="https://programmercarl.com/0583.两个字符串的删除操作.html">动态规划：583.两个字符串的删除操作 (opens new window)</a>给定两个单词 word1 和 word2，找到使得 word1 和 word2 相同所需的最小步数，每步可以删除任意一个字符串中的一个字符。</p>
<p>本题和<a href="https://programmercarl.com/0115.不同的子序列.html">动态规划：115.不同的子序列 (opens new window)</a>相比，其实就是两个字符串可以都可以删除了，情况虽说复杂一些，但整体思路是不变的。</p>
<ul>
<li>当word1[i - 1] 与 word2[j - 1]相同的时候</li>
<li>当word1[i - 1] 与 word2[j - 1]不相同的时候</li>
</ul>
<p>当word1[i - 1] 与 word2[j - 1]相同的时候，dp[i][j] = dp[i - 1] [j - 1];</p>
<p>当word1[i - 1] 与 word2[j - 1]不相同的时候，有2种情况：</p>
<p>情况一：删word1[i - 1]，最少操作次数为dp[i - 1] [j] + 1</p>
<p>情况二：删word2[j - 1]，最少操作次数为dp [i] [j - 1] + 1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (word1[i - <span class="number">1</span>] == word2[j - <span class="number">1</span>]) &#123;</span><br><span class="line">    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>];</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    dp[i][j] = <span class="built_in">min</span>(&#123;dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">2</span>, dp[i - <span class="number">1</span>][j] + <span class="number">1</span>, dp[i][j - <span class="number">1</span>] + <span class="number">1</span>&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="编辑距离">编辑距离</span></h4><p><a href="https://programmercarl.com/0072.编辑距离.html">动态规划：72.编辑距离 (opens new window)</a>给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。</p>
<p>编辑距离终于来了，<strong>有了前面三道题目的铺垫，应该有思路了</strong>，本题是两个字符串可以增删改，比 <a href="https://programmercarl.com/0392.判断子序列.html">动态规划：判断子序列 (opens new window)</a>，<a href="https://programmercarl.com/0115.不同的子序列.html">动态规划：不同的子序列 (opens new window)</a>，<a href="https://programmercarl.com/0583.两个字符串的删除操作.html">动态规划：两个字符串的删除操作 (opens new window)</a>都要复杂的多。</p>
<p>在确定递推公式的时候，首先要考虑清楚编辑的几种操作，整理如下：</p>
<ul>
<li>if (word1[i - 1] == word2[j - 1])<ul>
<li>不操作</li>
</ul>
</li>
<li>if (word1[i - 1] != word2[j - 1])<ul>
<li>增</li>
<li>删</li>
<li>换</li>
</ul>
</li>
</ul>
<p>也就是如上四种情况。</p>
<p>if (word1[i - 1] == word2[j - 1]) 那么说明不用任何编辑，dp[i][j] 就应该是 dp[i - 1] [j - 1]，即dp[i][j] = dp[i - 1] [j - 1];</p>
<p><strong>在整个动规的过程中，最为关键就是正确理解dp[i] [j]的定义！</strong></p>
<p>if (word1[i - 1] != word2[j - 1])，此时就需要编辑了，如何编辑呢？</p>
<p>操作一：word1增加一个元素，使其word1[i - 1]与word2[j - 1]相同，那么就是以下标i-2为结尾的word1 与 i-1为结尾的word2的最近编辑距离 加上一个增加元素的操作。</p>
<p>即 dp[i][j] = dp[i - 1] [j] + 1;</p>
<p>操作二：word2添加一个元素，使其word1[i - 1]与word2[j - 1]相同，那么就是以下标i-1为结尾的word1 与 j-2为结尾的word2的最近编辑距离 加上一个增加元素的操作。</p>
<p>即 dp[i][j] = dp[i][j - 1] + 1;</p>
<p>这里有同学发现了，怎么都是添加元素，删除元素去哪了。</p>
<p><strong>word2添加一个元素，相当于word1删除一个元素</strong>，例如 word1 = “ad” ，word2 = “a”，word2添加一个元素d，也就是相当于word1删除一个元素d，操作数是一样！</p>
<p>操作三：替换元素，word1替换word1[i - 1]，使其与word2[j - 1]相同，此时不用增加元素，那么以下标i-2为结尾的word1 与 j-2为结尾的word2的最近编辑距离 加上一个替换元素的操作。</p>
<p>即 dp[i][j] = dp[i - 1] [j - 1] + 1;</p>
<p>综上，当 if (word1[i - 1] != word2[j - 1]) 时取最小的，即：dp[i] [j] = min({dp[i - 1] [j - 1], dp[i - 1] [j], dp[i][j - 1]}) + 1;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (word1[i - <span class="number">1</span>] == word2[j - <span class="number">1</span>]) &#123;</span><br><span class="line">    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    dp[i][j] = <span class="built_in">min</span>(&#123;dp[i - <span class="number">1</span>][j - <span class="number">1</span>], dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]&#125;) + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="让字符串成为最小回文">==让字符串成为最小回文==</span></h4><p><a href="https://leetcode-cn.com/problems/minimum-insertion-steps-to-make-a-string-palindrome/">https://leetcode-cn.com/problems/minimum-insertion-steps-to-make-a-string-palindrome/</a></p>
<h4><span id="剑指-offer-19-正则表达式匹配"></span></h4><p>请实现一个函数用来<strong>匹配包含’. ‘和’*’的正则表达式</strong>。<strong>模式中的字符’.’表示任意一个字符，而’ * ‘表示它前面的字符可以出现任意次（含0次）</strong>。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”ab<em>ac</em>a”匹配，但与”aa.a”和”ab*a”均不匹配。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isMatch</span>(<span class="params">self, s: <span class="built_in">str</span>, p: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 【dp含义】dp[i][j] : s[:i - 1] 可以正则匹配 p[:j - 1]</span></span><br><span class="line">        m, n = <span class="built_in">len</span>(s), <span class="built_in">len</span>(p)</span><br><span class="line">        <span class="comment"># 【初始化】</span></span><br><span class="line">        dp = [[<span class="literal">False</span>] * (n + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>, <span class="number">2</span>):</span><br><span class="line">            dp[<span class="number">0</span>][j] = dp[<span class="number">0</span>][j - <span class="number">2</span>] <span class="keyword">and</span> p[j - <span class="number">1</span>] == <span class="string">&#x27;*&#x27;</span></span><br><span class="line">        <span class="comment"># 【遍历顺序】</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">                <span class="comment"># 【状态转移推导】</span></span><br><span class="line">                <span class="keyword">if</span> p[j - <span class="number">1</span>] == <span class="string">&#x27;*&#x27;</span>: </span><br><span class="line">                    <span class="keyword">if</span> dp[i][j - <span class="number">2</span>]:</span><br><span class="line">                        dp[i][j] = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">elif</span> dp[i - <span class="number">1</span>][j] <span class="keyword">and</span> (p[j - <span class="number">2</span>] == <span class="string">&#x27;.&#x27;</span> <span class="keyword">or</span> s[i - <span class="number">1</span>] == p[j - <span class="number">2</span>]):</span><br><span class="line">                        dp[i][j] = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> dp[i-<span class="number">1</span>][j-<span class="number">1</span>] <span class="keyword">and</span> (p[j - <span class="number">1</span>] == <span class="string">&#x27;.&#x27;</span> <span class="keyword">or</span> s[i - <span class="number">1</span>] == p[j - <span class="number">1</span>]):</span><br><span class="line">                        dp[i][j] = <span class="literal">True</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>][-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h2><span id="六-其他">六、其他</span></h2><h4><span id="剑指-offer-66-构建乘积数组"></span></h4><p>给定一个数组 A[0,1,…,n-1]，请构建一个数组 B[0,1,…,n-1]，其中 B[i] 的值是数组 A 中除了下标 i 以外的元素的积, 即 B[i]=A[0]×A[1]×…×A[i-1]×A[i+1]×…×A[n-1]。不能使用除法。【求三角矩阵非对角线】</p>
<ul>
<li>动态规划双向遍历</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">constructArr</span>(<span class="params">self, a: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        b, tmp = [<span class="number">1</span>] * <span class="built_in">len</span>(a), <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(a)):</span><br><span class="line">            b[i] = b[i - <span class="number">1</span>] * a[i - <span class="number">1</span>] <span class="comment"># 下三角</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a) - <span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            tmp *= a[i + <span class="number">1</span>]            <span class="comment"># 上三角</span></span><br><span class="line">            b[i] *= tmp                <span class="comment"># 下三角 * 上三角</span></span><br><span class="line">        <span class="keyword">return</span> b</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-49-丑数"></span></h4><p>我们<strong>把只包含质因子 2、3 和 5 的数称作丑数</strong>（Ugly Number）。求按从小到大的顺序的第 n 个丑数。</p>
<ul>
<li>三指针、<strong>优先队列（质数的线性筛算法）</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">nthUglyNumber</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 维护3个 2，3，5的动态个数</span></span><br><span class="line">        dp = [<span class="number">0</span>] * (n + <span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        p2 = p3 = p5 = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">          	<span class="comment"># </span></span><br><span class="line">            u2, u3, u5 = <span class="number">2</span> * dp[p2], <span class="number">3</span> * dp[p3], <span class="number">5</span> * dp[p5]</span><br><span class="line">            dp[i] = <span class="built_in">min</span>(u2, u3, u5)</span><br><span class="line">            <span class="keyword">if</span> dp[i] == u5:</span><br><span class="line">                p5 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> dp[i] == u3:</span><br><span class="line">                p3 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> dp[i] == u2:</span><br><span class="line">                p2 += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">nthUglyNumber</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        H = [(<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">5</span>, <span class="number">5</span>)]</span><br><span class="line">        heapify(H)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">            num, p = heappop(H)</span><br><span class="line">            heappush(H, (num * <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            <span class="keyword">if</span> p &gt;= <span class="number">3</span>:</span><br><span class="line">                heappush(H, (num * <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">                <span class="keyword">if</span> p &gt;= <span class="number">5</span>:</span><br><span class="line">                    heappush(H, (num * <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        <span class="comment">#print(len(H)) #n = 1670时, H长度为162</span></span><br><span class="line">        <span class="keyword">return</span> num                </span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-60-n个骰子的点数"></span></h4><p><strong>把n个骰子扔在地上，所有骰子朝上一面的点数之和为s。输入n，打印出s的所有可能的值出现的概率。</strong></p>
<ul>
<li>已知n - 1个骰子的解为f(n - 1), 添加一个骰子，求n个骰子的点数和为x的概率f（n， x）</li>
</ul>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220608212653374.png" alt="image-20220608212653374" style="zoom:50%;"></p>
<p><img src="https://pic.leetcode-cn.com/1614960989-tpJNRQ-Picture2.png" alt="Picture2.png" style="zoom: 33%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dicesProbability</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">float</span>]:</span><br><span class="line">        dp = [<span class="number">1</span> / <span class="number">6</span>] * <span class="number">6</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">            tmp = [<span class="number">0</span>] * (<span class="number">5</span> * i + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dp)):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>): <span class="comment"># 防止越界</span></span><br><span class="line">                    tmp[j + k] += dp[j] / <span class="number">6</span></span><br><span class="line">            dp = tmp</span><br><span class="line">        <span class="keyword">return</span> dp</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（12）图</title>
    <url>/posts/BZK9HE/</url>
    <content><![CDATA[<h2><span id="图论算法从入门到放下">图论算法从入门到放下</span></h2><blockquote>
<p>  <a href="https://leetcode.cn/circle/discuss/FyPTTM/">https://leetcode.cn/circle/discuss/FyPTTM/</a></p>
</blockquote>
<h4><span id="主要内容一览">主要内容一览</span></h4><p><img src="https://pic.leetcode-cn.com/1655461415-YbBmuc-graph_mind.png" alt="graph_mind.png" style="zoom: 33%;"></p>
<h2><span id="岛屿问题">岛屿问题</span></h2><h4><span id="剑指-offer-ii-105-岛屿的最大面积"></span></h4><p>给定一个由 0 和 1 组成的非空二维数组 grid ，用来表示海洋岛屿地图。一个 岛屿 是由一些相邻的 1 (代表土地) 构成的组合，这里的「相邻」要求两个 1 必须在水平或者竖直方向上相邻。你可以假设 grid 的四个边缘都被 0（代表水）包围着。</p>
<p>找到给定的二维数组中最大的岛屿面积。如果没有岛屿，则返回面积为 0 。</p>
<p>示例 1:</p>
<p><img src="https://pic.leetcode-cn.com/1626667010-nSGPXz-image.png" alt="img" style="zoom:33%;"></p>
<p>输入: grid = [[0,0,1,0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,1,1,0,1,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,0,0,1,0,1,0,0],[0,1,0,0,1,1,0,0,1,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,1,1,0,0,0,0]]<br>输出: 6<br>解释: 对于上面这个给定矩阵应返回 6。注意答案不应该是 11 ，因为岛屿只能包含水平或垂直的四个方向的 1 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxAreaOfIsland</span>(<span class="params">self, grid: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 深度优先搜索 空间复杂度o(m*n)</span></span><br><span class="line">        m, n = <span class="built_in">len</span>(grid), <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">x, y</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">            s = <span class="number">1</span></span><br><span class="line">            grid[x][y] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i, j <span class="keyword">in</span> [(x - <span class="number">1</span>, y), (x, y - <span class="number">1</span>), (x + <span class="number">1</span>, y), (x, y + <span class="number">1</span>)]:</span><br><span class="line">                <span class="keyword">if</span> i &lt; <span class="number">0</span> <span class="keyword">or</span> j &lt; <span class="number">0</span> <span class="keyword">or</span> i == m <span class="keyword">or</span> j == n <span class="keyword">or</span> grid[i][j]!=<span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                s += dfs(i, j)</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="keyword">if</span> grid[i][j] == <span class="number">1</span>:</span><br><span class="line">                    res = <span class="built_in">max</span>(res, dfs(i, j))</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="743-网络延迟时间"></span></h4><p>有 n 个网络节点，标记为 1 到 n。给你一个列表 times，表示信号经过 有向 边的传递时间。 times[i] = (ui, vi, wi)，其中 ui 是源节点，vi 是目标节点， wi 是一个信号从源节点传递到目标节点的时间。</p>
<p>现在，从某个节点 K 发出一个信号。需要多久才能使所有节点都收到信号？如果不能使所有节点收到信号，返回 -1 。</p>
<ul>
<li>图最短路径【<strong>优先队列</strong>】</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">networkDelayTime</span>(<span class="params">self, times: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], n: <span class="built_in">int</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        g = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> x, y, time <span class="keyword">in</span> times:</span><br><span class="line">            g[x - <span class="number">1</span>].append((y - <span class="number">1</span>, time))</span><br><span class="line"></span><br><span class="line">        dist = [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] * n</span><br><span class="line">        dist[k - <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">        q = [(<span class="number">0</span>, k - <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            time, x = heapq.heappop(q)</span><br><span class="line">            <span class="keyword">if</span> dist[x] &lt; time:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">for</span> y, time <span class="keyword">in</span> g[x]:</span><br><span class="line">                <span class="keyword">if</span> (d := dist[x] + time) &lt; dist[y]:</span><br><span class="line">                    dist[y] = d</span><br><span class="line">                    heapq.heappush(q, (d, y))</span><br><span class="line"></span><br><span class="line">        ans = <span class="built_in">max</span>(dist)</span><br><span class="line">        <span class="keyword">return</span> ans <span class="keyword">if</span> ans &lt; <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">else</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-106-二分图"></span></h4><p>存在一个 无向图 ，图中有 n 个节点。其中每个节点都有一个介于 0 到 n - 1 之间的唯一编号。给定一个二维数组 graph ，表示图，其中 graph[u] 是一个节点数组，由节点 u 的邻接节点组成。形式上，对于 graph[u] 中的每个 v ，都存在一条位于节点 u 和节点 v 之间的无向边。</p>
<p><strong>二分图 定义</strong>：如果能将一个图的节点集合分割成两个独立的子集 A 和 B ，并使图中的每一条边的两个节点一个来自 A 集合，一个来自 B 集合，就将这个图称为 二分图 。</p>
<p><strong>示例 1：</strong></p>
<p><img src="https://assets.leetcode.com/uploads/2020/10/21/bi2.jpg" alt="img"></p>
<p>输入：graph = [[1,2,3],[0,2],[0,1,3],[0,2]]<br>输出：false<br>解释：不能将节点分割成两个独立的子集，以使每条边都连通一个子集中的一个节点与另一个子集中的一个节点。</p>
<ul>
<li>DFS\BFS【染色问题】、并查集</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isBipartite</span>(<span class="params">self, graph: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 广度优先遍历 [推荐]</span></span><br><span class="line">        n = <span class="built_in">len</span>(graph)</span><br><span class="line">        uncolor, red, green = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">        color = [<span class="number">0</span>] * n</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> color[i] == uncolor:</span><br><span class="line">                color[i] = red</span><br><span class="line">                q = deque([i])</span><br><span class="line">                <span class="keyword">while</span> q:</span><br><span class="line">                    node = q.popleft()</span><br><span class="line">                    nei = green <span class="keyword">if</span> color[node] == red <span class="keyword">else</span> red</span><br><span class="line">                    <span class="keyword">for</span> j <span class="keyword">in</span> graph[node]:</span><br><span class="line">                        <span class="keyword">if</span> color[j] == uncolor:</span><br><span class="line">                            color[j] = nei</span><br><span class="line">                            q.append(j)</span><br><span class="line">                        <span class="keyword">elif</span> color[j] != nei:</span><br><span class="line">                            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isBipartite</span>(<span class="params">self, graph: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 深度优先遍历 【不推荐】</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">i: <span class="built_in">int</span>, c: <span class="built_in">int</span></span>):</span><br><span class="line">            <span class="keyword">nonlocal</span> res</span><br><span class="line">            nei = green <span class="keyword">if</span> c == red <span class="keyword">else</span> red</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> graph[i]:</span><br><span class="line">                <span class="keyword">if</span> color[j] == uncolor:</span><br><span class="line">                    color[j] = nei</span><br><span class="line">                    dfs(j, nei)</span><br><span class="line">                <span class="keyword">elif</span> color[j] != nei:</span><br><span class="line">                    res = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        n = <span class="built_in">len</span>(graph)</span><br><span class="line">        uncolor, red, green = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">        color = [<span class="number">0</span>] * n</span><br><span class="line">        res = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> color[i] == uncolor <span class="keyword">and</span> res:</span><br><span class="line">                dfs(i, red)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># def isBipartite(self, graph: List[List[int]]) -&gt; bool:</span></span><br><span class="line">    <span class="comment">#     # 并查集 【了解】</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（13）结构设计</title>
    <url>/posts/2G3EX7Q/</url>
    <content><![CDATA[<h2><span id="一-前缀树">一、前缀树</span></h2><blockquote>
<p>  一种好用的树结构：Trie树:<a href="https://zhuanlan.zhihu.com/p/508575094">https://zhuanlan.zhihu.com/p/508575094</a></p>
</blockquote>
<h3><span id="11-trie树简介-有限状态自动机-文本词频统计"><strong>1.1 Trie树简介</strong> [有限状态自动机] [文本词频统计]</span></h3><p>在计算机科学中，trie，又称<strong>前缀树</strong>或<strong>字典树</strong>，是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。</p>
<p>Trie这个术语来自于retrieval。根据词源学，trie的发明者Edward Fredkin把它读作/ˈtriː/ “tree”。但是，其他作者把它读作/ˈtraɪ/ “try”。</p>
<p>在图示中，键标注在节点中，值标注在节点之下。每一个完整的英文单词对应一个特定的整数。<strong>Trie可以看作是一个确定有限状态自动机，尽管边上的符号一般是隐含在分支的顺序中的</strong>。 Eg.一个保存了8个单词的字典树的结构如下图所示，8个单词分别是：“A”，“to”，“tea”，“ted”，“ten”，“i” ，“in”，“inn”。</p>
<p><img src="https://pic1.zhimg.com/80/v2-8740aeac82cd2fc980cd1148ab1a64dc_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>另外，<strong>单词查找树，Trie树，是一种树形结构，是一种哈希树的变种</strong>。<strong>典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计</strong>。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。</p>
<h3><span id="12-trie树性质">1.2 <strong>Trie树性质</strong></span></h3><p>它有3个基本性质：</p>
<ul>
<li>根节点不包含字符，除根节点外每一个节点都只包含一个字符；</li>
<li>从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串；</li>
<li>每个节点的所有子节点包含的字符都不相同。</li>
</ul>
<h3><span id="13-基本操作">1.3 <strong>基本操作</strong></span></h3><p>其基本操作有：查找、插入和删除,当然删除操作比较少见。</p>
<h3><span id="14-实现方法"><strong>1.4 实现方法</strong></span></h3><p>搜索字典项目的方法为：</p>
<ul>
<li>从根结点开始一次搜索；</li>
<li>取得要查找关键词的第一个字母，并根据该字母选择对应的子树并转到该子树继续进行检索；</li>
<li>在相应的子树上，取得要查找关键词的第二个字母,并进一步选择对应的子树进行检索。</li>
<li>迭代过程……</li>
<li>在某个结点处，关键词的所有字母已被取出，则读取附在该结点上的信息，即完成查找。 其他操作类似处理</li>
</ul>
<h3><span id="15-实现trie-前缀树">1.5 实现<strong>Trie (前缀树)</strong></span></h3><p>Trie（发音类似 “try”）或者说 前缀树 是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。</p>
<p><strong>请你实现 Trie 类：</strong></p>
<ul>
<li><p><strong>Trie()</strong> :初始化前缀树对象。</p>
<ul>
<li>self.children = [None] <em> 26 , <em>*指向子节点的指针数组 children</em></em></li>
<li>self.isEnd = False , 表示该节点<strong>是否为字符串的结尾</strong>。</li>
</ul>
</li>
<li><p><strong>void insert(String word)</strong> :向前缀树中插入字符串 word 。</p>
<ul>
<li>我们<strong>从字典树的根开始</strong>，插入字符串。对于当前字符对应的子节点，有两种情况：<ul>
<li><strong>子节点存在</strong>。沿着指针移动到子节点，继续处理下一个字符。</li>
<li><strong>子节点不存在</strong>。创建一个新的子节点，记录在children 数组的对应位置上，然后沿着指针移动到子节点，继续搜索下一个字符。</li>
</ul>
</li>
<li>重复以上步骤，直到处理字符串的最后一个字符，然后将当前节点标记为字符串的结尾。</li>
</ul>
</li>
<li><strong>boolean search(String word)</strong> 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。<ul>
<li>我们从字典树的根开始，查找前缀。对于当前字符对应的子节点，有两种情况：<ul>
<li>子节点存在。沿着指针移动到子节点，继续搜索下一个字符。</li>
<li>子节点不存在。说明字典树中不包含该前缀，返回空指针。</li>
</ul>
</li>
<li>重复以上步骤，直到返回空指针或搜索完前缀的最后一个字符。</li>
</ul>
</li>
<li><strong>boolean startsWith(String prefix)</strong> 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。</li>
</ul>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">输入</span><br><span class="line">[&quot;Trie&quot;, &quot;insert&quot;, &quot;search&quot;, &quot;search&quot;, &quot;startsWith&quot;, &quot;insert&quot;, &quot;search&quot;]</span><br><span class="line">[[], [&quot;apple&quot;], [&quot;apple&quot;], [&quot;app&quot;], [&quot;app&quot;], [&quot;app&quot;], [&quot;app&quot;]]</span><br><span class="line">输出</span><br><span class="line">[null, null, true, false, true, null, true]</span><br><span class="line"></span><br><span class="line">解释</span><br><span class="line">Trie trie = new Trie();</span><br><span class="line">trie.insert(&quot;apple&quot;);</span><br><span class="line">trie.search(&quot;apple&quot;);   // 返回 True</span><br><span class="line">trie.search(&quot;app&quot;);     // 返回 False</span><br><span class="line">trie.startsWith(&quot;app&quot;); // 返回 True</span><br><span class="line">trie.insert(&quot;app&quot;);</span><br><span class="line">trie.search(&quot;app&quot;);     // 返回 True</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.children = [<span class="literal">None</span>] * <span class="number">26</span></span><br><span class="line">        self.isEnd = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">searchPrefix</span>(<span class="params">self, prefix: <span class="built_in">str</span></span>) -&gt; <span class="string">&quot;Trie&quot;</span>:</span><br><span class="line">        node = self <span class="comment"># 根节点</span></span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> prefix:</span><br><span class="line">            ch = <span class="built_in">ord</span>(ch) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node.children[ch]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            node = node.children[ch]</span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insert</span>(<span class="params">self, word: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        node = self</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> word:</span><br><span class="line">            ch = <span class="built_in">ord</span>(ch) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node.children[ch]:</span><br><span class="line">                node.children[ch] = Trie()</span><br><span class="line">            node = node.children[ch]</span><br><span class="line">        node.isEnd = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, word: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        node = self.searchPrefix(word)</span><br><span class="line">        <span class="keyword">return</span> node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> node.isEnd</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">startsWith</span>(<span class="params">self, prefix: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">return</span> self.searchPrefix(prefix) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2><span id="二-lru">二、LRU</span></h2><h4><span id="146-lru-缓存"></span></h4><p>请你设计并实现一个满足 <a href="https://baike.baidu.com/item/LRU">LRU (最近最少使用) 缓存</a> 约束的数据结构。</p>
<p>实现 <code>LRUCache</code> 类：</p>
<ul>
<li><code>LRUCache(int capacity)</code> 以 <strong>正整数</strong> 作为容量 <code>capacity</code> 初始化 LRU 缓存</li>
<li><code>int get(int key)</code> 如果关键字 <code>key</code> 存在于缓存中，则返回关键字的值，否则返回 <code>-1</code> 。</li>
<li><code>void put(int key, int value)</code> 如果关键字 <code>key</code> 已经存在，则变更其数据值 <code>value</code> ；如果不存在，则向缓存中插入该组 key-value 。如果插入操作导致关键字数量超过 capacity ，则应该 逐出 最久未使用的关键字。</li>
</ul>
<p>函数 <code>get</code> 和 <code>put</code> 必须以 <code>O(1)</code> 的平均时间复杂度运行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DLinkedNode</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key = <span class="number">0</span>, value = <span class="number">0</span></span>):</span><br><span class="line">        self.key = key</span><br><span class="line">        self.value = value</span><br><span class="line">        self.prev = <span class="literal">None</span></span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capacity: <span class="built_in">int</span></span>):</span><br><span class="line">      	<span class="comment"># 使用伪头部和伪尾部节点    </span></span><br><span class="line">        self.cache = <span class="built_in">dict</span>()</span><br><span class="line">        self.head = DLinkedNode()</span><br><span class="line">        self.tail = DLinkedNode()</span><br><span class="line">        self.head.<span class="built_in">next</span> = self.tail</span><br><span class="line">        self.tail.prev = self.head</span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        self.size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, key: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> self.cache:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        <span class="comment"># 如果 key 存在，先通过哈希表定位，再移到头部</span></span><br><span class="line">        node = self.cache[key]</span><br><span class="line">        self.moveToHead(node)<span class="comment"># &quot;如果数据被访问过。那未来的访问几率更高&quot;</span></span><br><span class="line">        <span class="keyword">return</span> node.value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">put</span>(<span class="params">self, key: <span class="built_in">int</span>, value: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> self.cache:</span><br><span class="line">          	<span class="comment"># 如果 key 不存在，创建一个新的节点</span></span><br><span class="line">            node = DLinkedNode(key, value)</span><br><span class="line">            <span class="comment"># 添加进哈希表</span></span><br><span class="line">            self.cache[key] = node</span><br><span class="line">            <span class="comment"># 添加至双向链表的头部</span></span><br><span class="line">            self.size += <span class="number">1</span></span><br><span class="line">            self.addToHead(node)</span><br><span class="line">            <span class="keyword">if</span> self.size &gt; self.capacity:</span><br><span class="line">              	<span class="comment"># 如果超出容量，删除双向链表的尾部节点</span></span><br><span class="line">                removed = self.removeTail()</span><br><span class="line">                <span class="comment"># 删除哈希表中对应的项</span></span><br><span class="line">                self.cache.pop(removed.key)</span><br><span class="line">                self.size -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          	<span class="comment"># 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部</span></span><br><span class="line">            node = self.cache[key]</span><br><span class="line">            node.value = value</span><br><span class="line">            self.moveToHead(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addToHead</span>(<span class="params">self, node</span>):</span><br><span class="line">        node.prev = self.head</span><br><span class="line">        node.<span class="built_in">next</span> = self.head.<span class="built_in">next</span></span><br><span class="line">        self.head.<span class="built_in">next</span>.prev = node</span><br><span class="line">        self.head.<span class="built_in">next</span> = node</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeNode</span>(<span class="params">self, node</span>): </span><br><span class="line">        node.prev.<span class="built_in">next</span> = node.<span class="built_in">next</span></span><br><span class="line">        node.<span class="built_in">next</span>.prev = node.prev</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">moveToHead</span>(<span class="params">self, node</span>):</span><br><span class="line">        self.removeNode(node)</span><br><span class="line">        self.addToHead(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeTail</span>(<span class="params">self</span>):</span><br><span class="line">        node = self.tail.prev</span><br><span class="line">        self.removeNode(node)</span><br><span class="line">        <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure>
<blockquote>
<p>  <strong>LFU，最近不经常使用</strong>，把数据加入到链表中，按频次排序，一个数据被访问过，把它的频次+1，发生淘汰的时候，把频次低的淘汰掉。<br>   比如有数据 1，1，1，2，2，3<br>   缓存中有（1(3次)，2(2次)）<br>   当3加入的时候，得把后面的2淘汰，变成（1(3次)，3(1次)）<br>   区别：LRU 是得把 1 淘汰。</p>
</blockquote>
<h2><span id="三-并查集">三、并查集</span></h2><h4><span id="剑指-offer-ii-117-相似的字符串"></span></h4><p>如果交换字符串 <code>X</code> 中的两个不同位置的字母，使得它和字符串 <code>Y</code> 相等，那么称 <code>X</code> 和 <code>Y</code> 两个字符串相似。如果这两个字符串本身是相等的，那它们也是相似的。例如，”tars” 和 “rats” 是相似的 (交换 0 与 2 的位置)； “rats” 和 “arts” 也是相似的，但是 “star” 不与 “tars”，”rats”，或 “arts” 相似。</p>
<p>总之，它们通过相似性形成了两个关联组：{“tars”, “rats”, “arts”} 和 {“star”}。注意，”tars” 和 “arts” 是在同一组中，即使它们并不相似。形式上，对每个组而言，要确定一个单词在组中，只需要这个词和该组中至少一个单词相似。</p>
<p>给定一个字符串列表 strs。列表中的每个字符串都是 strs 中其它所有字符串的一个 字母异位词 。请问 strs 中有多少个相似字符串组？</p>
<p><strong>字母异位词（anagram）</strong>，一种把某个字符串的字母的位置（顺序）加以改换所形成的新词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UnionFind</span>:</span><br><span class="line">  	<span class="comment"># 并查集</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.parent = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        self.count = n</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">find</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.parent[x] == x:</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">        self.parent[x] = self.find(self.parent[x])</span><br><span class="line">        <span class="keyword">return</span> self.parent[x]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">union</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        x, y = self.find(x), self.find(y)</span><br><span class="line">        <span class="keyword">if</span> x != y:</span><br><span class="line">            self.parent[x] = y</span><br><span class="line">            self.count -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numSimilarGroups</span>(<span class="params">self, strs: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">      	<span class="comment"># 相似的字符串</span></span><br><span class="line">        m, n = <span class="built_in">len</span>(strs), <span class="built_in">len</span>(strs[<span class="number">0</span>])</span><br><span class="line">        un = UnionFind(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, m):</span><br><span class="line">                cnt = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                    <span class="keyword">if</span> strs[i][k] != strs[j][k]:</span><br><span class="line">                        cnt += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">if</span> cnt &gt; <span class="number">2</span>:</span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>: <span class="comment"># 上下文管理器</span></span><br><span class="line">                    un.union(i, j)</span><br><span class="line">        <span class="keyword">return</span> un.count</span><br></pre></td></tr></table></figure>
<h2><span id="四-序列化与反序列化">四、序列化与反序列化</span></h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/40462507">https://zhuanlan.zhihu.com/p/40462507</a></li>
</ul>
<p><strong>序列化：</strong>把对象转化为可传输的字节序列过程称为序列化。</p>
<p><strong>反序列化：</strong>把字节序列还原为对象的过程称为反序列化。</p>
<h4><span id="41-序列化的定义">4.1 序列化的定义</span></h4><p><strong>序列化</strong>：把对象转化为可传输的字节序列过程称为序列化。</p>
<p><strong>反序列化</strong>：把字节序列还原为对象的过程称为反序列化。</p>
<h4><span id="42-为什么要序列化">4.2 为什么要序列化？</span></h4><p>其实序列化最终的目的是为了对象可以<strong>跨平台存储，和进行网络传输</strong>。而我们进行跨平台存储和网络传输的方式就是IO，而我们的IO支持的数据格式就是字节数组。序列化只是一种拆装组装对象的规则，那么这种规则肯定也可能有多种多样，比如现在常见的序列化方式有：</p>
<p><strong>JDK（不支持跨语言）、JSON、XML、Hessian、Kryo（不支持跨语言）、Thrift、Protostuff、FST（不支持跨语言）</strong></p>
<h4><span id="剑指-offer-37-序列化二叉树"></span></h4><p>请实现两个函数，分别用来序列化和反序列化二叉树。你需要设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构。</p>
<ul>
<li><h5><span id="深度优先遍历">深度优先遍历</span></h5></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Codec</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">serialize</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="comment"># 后续遍历 比 前序遍历快很多 pop(), pop(0)</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; DFS : ncodes a tree to a single string.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: str</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;None&#x27;</span></span><br><span class="line">        <span class="keyword">return</span>  <span class="built_in">str</span>(self.serialize(root.left)) + <span class="string">&#x27;,&#x27;</span> + <span class="built_in">str</span>(self.serialize(root.right)) + <span class="string">&#x27;,&#x27;</span> + <span class="built_in">str</span>(root.val) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deserialize</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Decodes your encoded data to tree.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :type data: str</span></span><br><span class="line"><span class="string">        :rtype: TreeNode</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">dfslist:<span class="built_in">list</span>(<span class="params"></span>)</span>):</span><br><span class="line">            val = dfslist.pop()</span><br><span class="line">            <span class="keyword">if</span> val == <span class="string">&#x27;None&#x27;</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            root = TreeNode(<span class="built_in">int</span>(val))</span><br><span class="line">            root.right = dfs(dfslist)</span><br><span class="line">            root.left = dfs(dfslist)</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        dfslist = data.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> dfs(dfslist)</span><br></pre></td></tr></table></figure>
<h2><span id="五-线段树">五、线段树</span></h2><ul>
<li>算法学习笔记(14): 线段树：<a href="https://zhuanlan.zhihu.com/p/106118909">https://zhuanlan.zhihu.com/p/106118909</a></li>
</ul>
<p><strong>线段树</strong>（Segment Tree）几乎是算法竞赛最常用的数据结构了，它主要用于维护<strong>区间信息</strong>（要求满足结合律）。与树状数组相比，它可以实现 <img src="https://www.zhihu.com/equation?tex=O%28%5Clog+n%29" alt="[公式]"> 的<strong>区间修改</strong>，还可以同时支持<strong>多种操作</strong>（加、乘)，更具通用性。</p>
<p><img src="https://pic1.zhimg.com/80/v2-5e9124a6147143e51cea46755e9a0398_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BIT</span>:</span><br><span class="line">    <span class="comment"># 线段树: 动态维护前缀和数据的结构</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.n = n</span><br><span class="line">        self.tree = [<span class="number">0</span>] * (n+<span class="number">1</span>)</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lowbit</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="comment"># 二进制最小位1的位置</span></span><br><span class="line">        <span class="keyword">return</span> x&amp;(-x)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">query</span>(<span class="params">self, x</span>):</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> x&gt;<span class="number">0</span>:</span><br><span class="line">            ans += self.tree[x]</span><br><span class="line">            x -= BIT.lowbit(x)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">while</span> x&lt;=self.n:</span><br><span class="line">            self.tree[x] += <span class="number">1</span></span><br><span class="line">            x += BIT.lowbit(x)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-51-数组中的逆序对"></span></h4><p>在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。</p>
<ul>
<li><h4><span id="离散化树状数组">离散化树状数组</span></h4><ul>
<li><strong>单点更新 <code>update(i, v)</code>：</strong> 把序列 i<em>i</em> 位置的数加上一个值 v<em>，这题 v</em>=1</li>
<li><strong>区间查询 <code>query(i)</code>：</strong> 查询序列 [1⋯<em>i</em>] 区间的区间和，即 i<em>i</em> 位置的前缀和</li>
</ul>
</li>
</ul>
<p><strong>修改和查询的时间代价都是 O<em>(log</em>n<em>)，其中 n</em>为需要维护前缀和的序列的长度。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reversePairs</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 离散化+线性数组 nums = [7,5,6,4]</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="comment"># 求每个数比他小的有几个</span></span><br><span class="line">        tmp = <span class="built_in">sorted</span>(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            nums[i] = bisect.bisect_left(tmp, nums[i]) + <span class="number">1</span></span><br><span class="line">        <span class="comment"># nums = [4,2,3,1]</span></span><br><span class="line">        bit = BIT(n) <span class="comment"># [0,0,0,0,0]</span></span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            ans += bit.query(nums[i] - <span class="number">1</span>)</span><br><span class="line">            bit.update(nums[i])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h2><span id="自动机">自动机</span></h2><p><strong>字符串处理的题目往往涉及复杂的流程以及条件情况</strong>，如果直接上手写程序，一不小心就会写出极其臃肿的代码。</p>
<p>因此，为了有条理地分析每个输入字符的处理方法，我们可以使用自动机这个概念：</p>
<p>我们的程序在每个时刻有一个状态 s，每次从序列中输入一个字符 c，并根据字符 c 转移到下一个状态 s’。这样，我们只需要建立一个覆盖所有情况的从 s 与 c 映射到 s’ 的表格即可解决题目中的问题。</p>
<h4><span id="算法">算法：</span></h4><p>本题可以建立如下图所示的自动机：</p>
<p><img src="https://assets.leetcode-cn.com/solution-static/8/fig1.png" alt="fig1" style="zoom: 50%;"></p>
<p>我们也可以用下面的表格来表示这个自动机：</p>
<p><img src="../../../../../Library/Application Support/typora-user-images/image-20220617125627816.png" alt="image-20220617125627816" style="zoom:50%;"></p>
<p>接下来编程部分就非常简单了：我们只需要把上面这个状态转换表抄进代码即可。另外自动机也需要记录当前已经输入的数字，只要在 s’ 为 in_number 时，更新我们输入的数字，即可最终得到输入的数字。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">INT_MAX = <span class="number">2</span> ** <span class="number">31</span> - <span class="number">1</span></span><br><span class="line">INT_MIN = -<span class="number">2</span> ** <span class="number">31</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Automaton</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.state = <span class="string">&#x27;start&#x27;</span></span><br><span class="line">        self.sign = <span class="number">1</span></span><br><span class="line">        self.ans = <span class="number">0</span></span><br><span class="line">        self.table = &#123;</span><br><span class="line">            <span class="string">&#x27;start&#x27;</span>: [<span class="string">&#x27;start&#x27;</span>, <span class="string">&#x27;signed&#x27;</span>, <span class="string">&#x27;in_number&#x27;</span>, <span class="string">&#x27;end&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;signed&#x27;</span>: [<span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;in_number&#x27;</span>, <span class="string">&#x27;end&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;in_number&#x27;</span>: [<span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;in_number&#x27;</span>, <span class="string">&#x27;end&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;end&#x27;</span>: [<span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;end&#x27;</span>],</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_col</span>(<span class="params">self, c</span>):</span><br><span class="line">        <span class="keyword">if</span> c.isspace():</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> c == <span class="string">&#x27;+&#x27;</span> <span class="keyword">or</span> c == <span class="string">&#x27;-&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> c.isdigit():</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, c</span>):</span><br><span class="line">        self.state = self.table[self.state][self.get_col(c)]</span><br><span class="line">        <span class="keyword">if</span> self.state == <span class="string">&#x27;in_number&#x27;</span>:</span><br><span class="line">            self.ans = self.ans * <span class="number">10</span> + <span class="built_in">int</span>(c)</span><br><span class="line">            self.ans = <span class="built_in">min</span>(self.ans, INT_MAX) <span class="keyword">if</span> self.sign == <span class="number">1</span> <span class="keyword">else</span> <span class="built_in">min</span>(self.ans, -INT_MIN)</span><br><span class="line">        <span class="keyword">elif</span> self.state == <span class="string">&#x27;signed&#x27;</span>:</span><br><span class="line">            self.sign = <span class="number">1</span> <span class="keyword">if</span> c == <span class="string">&#x27;+&#x27;</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">myAtoi</span>(<span class="params">self, <span class="built_in">str</span>: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        automaton = Automaton()</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">str</span>:</span><br><span class="line">            automaton.get(c)</span><br><span class="line">        <span class="keyword">return</span> automaton.sign * automaton.ans</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（15）排序*</title>
    <url>/posts/3XW082X/</url>
    <content><![CDATA[<h2><span id="排序">排序</span></h2><h3><span id="一-快速排序">一、快速排序</span></h3><h4><span id="剑指-offer-40-最小的k个数"></span></h4><p>输入整数数组 <code>arr</code> ，找出其中<strong>最小的 <code>k</code> 个数</strong>。例如，输入4、5、1、6、2、7、3、8这8个数字，则最小的4个数字是1、2、3、4。</p>
<p><strong>（最小）堆排序算法</strong>：<a href="https://docs.python.org/zh-cn/3/library/heapq.html">https://docs.python.org/zh-cn/3/library/heapq.html</a></p>
<ul>
<li>==<strong>快速选择、堆排序</strong>==</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getLeastNumbers</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 堆排序</span></span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>()</span><br><span class="line">        hp = [ -x <span class="keyword">for</span> x <span class="keyword">in</span> arr[:k]]</span><br><span class="line">        heapq.heapify(hp)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k, <span class="built_in">len</span>(arr)):</span><br><span class="line">            <span class="keyword">if</span> -hp[<span class="number">0</span>] &gt; arr[i]:</span><br><span class="line">                heapq.heappop(hp)</span><br><span class="line">                heapq.heappush(hp, -arr[i])</span><br><span class="line">        arr = [-x <span class="keyword">for</span> x <span class="keyword">in</span> hp]</span><br><span class="line">        <span class="keyword">return</span> arr   </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getLeastNumbers</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 快速选择</span></span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">quickSelect</span>(<span class="params">low, high, k</span>):</span><br><span class="line">            pvoit = random.randint(low, high)</span><br><span class="line">            arr[pvoit], arr[low] = arr[low], arr[pvoit]</span><br><span class="line">            base = arr[low] </span><br><span class="line">            i = low</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(low+<span class="number">1</span>, high+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> arr[j] &lt; base:</span><br><span class="line">                    arr[i+<span class="number">1</span>], arr[j] = arr[j], arr[i+<span class="number">1</span>]</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            arr[low], arr[i] = arr[i], arr[low] </span><br><span class="line">            <span class="keyword">if</span> i == k - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> arr[:k]</span><br><span class="line">            <span class="keyword">elif</span> i &lt; k - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> quickSelect(i+<span class="number">1</span>, high, k)</span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">return</span> quickSelect(low, i-<span class="number">1</span>, k)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> quickSelect(<span class="number">0</span>, <span class="built_in">len</span>(arr) - <span class="number">1</span>, k)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-076-数组中的第-k-大的数字"></span></h4><blockquote>
<p>  <a href="https://leetcode.cn/problems/kth-largest-element-in-an-array/solution/cpython3java-1da-gen-dui-diao-ku-2shou-l-xveq/">https://leetcode.cn/problems/kth-largest-element-in-an-array/solution/cpython3java-1da-gen-dui-diao-ku-2shou-l-xveq/</a></p>
</blockquote>
<p>给定整数数组 <code>nums</code> 和整数 <code>k</code>，请返回数组中第 <code>k</code> 个最大的元素。请注意，你需要找的是<strong>数组排序后的第 <code>k</code> 个最大的元素，而不是第 <code>k</code> 个不同的元素。</strong></p>
<ul>
<li>小根堆调库、快速选择</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findKthLargest</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        heap = []</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(heap) &gt;= k:</span><br><span class="line">                <span class="keyword">if</span> num &gt; heap[<span class="number">0</span>]: <span class="comment"># 注意替换条件</span></span><br><span class="line">                    heapq.heapreplace(heap, num)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                heapq.heappush(heap, num)</span><br><span class="line">        <span class="keyword">return</span> heap[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findKthLargest</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 快速选择</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">findTopk</span>(<span class="params">low, high</span>):</span><br><span class="line">            pvoit = random.randint(low, high)</span><br><span class="line">            nums[pvoit], nums[low] = nums[low], nums[pvoit]</span><br><span class="line">            base = nums[low]</span><br><span class="line">            i = low </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(low + <span class="number">1</span>, high + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> nums[j] &gt; base:</span><br><span class="line">                    nums[i + <span class="number">1</span>], nums[j] = nums[j], nums[i + <span class="number">1</span>]</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            nums[i], nums[low] = nums[low], nums[i]</span><br><span class="line">            <span class="keyword">if</span> i == k - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> nums[i]</span><br><span class="line">            <span class="keyword">elif</span> i &lt; k - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> findTopk(i + <span class="number">1</span>, high)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> findTopk(low, i - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> findTopk(<span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-45-把数组排成最小的数"></span></h4><p>输入一个<strong>非负</strong>整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。</p>
<ul>
<li>【基于快排】+【<strong>拼接字符串</strong>】</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minNumber</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_smallA</span>(<span class="params">a, b</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span> <span class="keyword">if</span> a + b &lt; b + a <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">partition</span>(<span class="params">l, r</span>):</span><br><span class="line">            pivot = random.randint(l, r)</span><br><span class="line">            nums[pivot], nums[l] = nums[l], nums[pivot]</span><br><span class="line">            base = nums[l]</span><br><span class="line">            i = l</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(l+<span class="number">1</span>, r+<span class="number">1</span>): <span class="comment"># r+1 遍历到r</span></span><br><span class="line">                <span class="keyword">if</span> _smallA(nums[j], base):</span><br><span class="line">                    nums[j], nums[i+<span class="number">1</span>] = nums[i+<span class="number">1</span>], nums[j] <span class="comment"># i + 1 与i前面的换</span></span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            nums[l], nums[i] = nums[i], nums[l]</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">quick_sort</span>(<span class="params">l, r</span>):</span><br><span class="line">            <span class="keyword">if</span> r - l &lt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            mid = partition(l, r)</span><br><span class="line">            quick_sort(l, mid - <span class="number">1</span>)</span><br><span class="line">            quick_sort(mid + <span class="number">1</span>, r)</span><br><span class="line">        nums = [<span class="built_in">str</span>(num) <span class="keyword">for</span> num <span class="keyword">in</span> nums]</span><br><span class="line">        quick_sort(<span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(nums).lstrip()</span><br></pre></td></tr></table></figure>
<ul>
<li><h5><span id="内置函数cmp_to_key单个元素并没有一个绝对的大小的情况">内置函数：cmp_to_key，单个元素并没有一个绝对的大小的情况</span></h5></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:    </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">largestNumber</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        nums.sort(key=cmp_to_key(<span class="keyword">lambda</span> x,y: <span class="built_in">int</span>(<span class="built_in">str</span>(y)+<span class="built_in">str</span>(x)) - <span class="built_in">int</span>(<span class="built_in">str</span>(x)+<span class="built_in">str</span>(y))))</span><br><span class="line">        ans = <span class="string">&#x27;&#x27;</span>.join([<span class="built_in">str</span>(num) <span class="keyword">for</span> num <span class="keyword">in</span> nums])</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(<span class="built_in">int</span>(ans))</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-61-扑克牌中的顺子"></span></h4><p>从若干副扑克牌中随机抽 5 张牌，判断是不是一个顺子，即这5张牌是不是连续的。2～10为数字本身，A为1，J为11，Q为12，K为13，而大、小王为 0 ，可以看成任意数字。A 不能视为 14。</p>
<ul>
<li>集合 Set + 遍历，排序 + 遍历</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isStraight</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        pset = <span class="built_in">set</span>()</span><br><span class="line">        mi, ma = <span class="number">14</span>, -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> num == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> num <span class="keyword">in</span> pset:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            mi = <span class="built_in">min</span>(mi, num)</span><br><span class="line">            ma = <span class="built_in">max</span>(ma, num)</span><br><span class="line">            pset.add(num)</span><br><span class="line">        <span class="keyword">return</span> ma - mi &lt; <span class="number">5</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isStraight</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        joker = <span class="number">0</span></span><br><span class="line">        nums.sort() <span class="comment"># 数组排序</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            <span class="keyword">if</span> nums[i] == <span class="number">0</span>: joker += <span class="number">1</span> <span class="comment"># 统计大小王数量</span></span><br><span class="line">            <span class="keyword">elif</span> nums[i] == nums[i + <span class="number">1</span>]: <span class="keyword">return</span> <span class="literal">False</span> <span class="comment"># 若有重复，提前返回 false</span></span><br><span class="line">        <span class="keyword">return</span> nums[<span class="number">4</span>] - nums[joker] &lt; <span class="number">5</span> <span class="comment"># 最大牌 - 最小牌 &lt; 5 则可构成顺子</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-41-数据流中的中位数"></span></h4><p>如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。</p>
<ul>
<li>大小堆排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">heappush(self.B, -heappushpop(self.A, num))<span class="keyword">class</span> <span class="title class_">MedianFinder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        initialize your data structure here.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.minheap = []</span><br><span class="line">        self.maxheap = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addNum</span>(<span class="params">self, num: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.minheap) != <span class="built_in">len</span>(self.maxheap):</span><br><span class="line">          	<span class="comment"># heappush(self.B, -heappushpop(self.A, num))</span></span><br><span class="line">            heappush(self.minheap, num)</span><br><span class="line">            heappush(self.maxheap, -heappop(self.minheap))</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 【len等于时，min多】</span></span><br><span class="line">            heappush(self.maxheap, -num)</span><br><span class="line">            heappush(self.minheap, -heappop(self.maxheap))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMedian</span>(<span class="params">self</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">return</span> self.minheap[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(self.minheap) != <span class="built_in">len</span>(self.maxheap) <span class="keyword">else</span> (self.minheap[<span class="number">0</span>] - self.maxheap[<span class="number">0</span>] ) / <span class="number">2.0</span></span><br></pre></td></tr></table></figure>
<h3><span id="二-归并排序">二、归并排序</span></h3><p>「归并排序」是分治思想的典型应用。</p>
<h4><span id="剑指-offer-51-数组中的逆序对"></span></h4><p>在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。</p>
<p><strong><font color="red">那么求逆序对和归并排序又有什么关系呢？ </font></strong></p>
<p>两个已排序的序列等待<strong>合并</strong>，记录计算逆序对的数量；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reversePairs</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">mergeSort</span>(<span class="params">nums, low, high</span>):</span><br><span class="line">            <span class="keyword">if</span> low &gt;= high:     <span class="comment"># 递归终止</span></span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>        </span><br><span class="line">            ans = <span class="number">0</span>             <span class="comment"># 记录当前逆序对数目</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;递归排序&#x27;&#x27;&#x27;</span></span><br><span class="line">            mid = low+(high-low)//<span class="number">2</span>     </span><br><span class="line">            ans += mergeSort(nums, low, mid)        <span class="comment"># 左半部分逆序对数目</span></span><br><span class="line">            ans += mergeSort(nums, mid+<span class="number">1</span>, high)     <span class="comment"># 右半部分逆序对数目</span></span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;nums[low, mid] 和 nums[mid+1, high] 已排序好&#x27;&#x27;&#x27;</span></span><br><span class="line">            tmp = []        <span class="comment"># 记录nums[low, high]排序结果</span></span><br><span class="line">            left, right = low, mid+<span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> left&lt;=mid <span class="keyword">and</span> right&lt;=high:</span><br><span class="line">                <span class="keyword">if</span> nums[left] &lt;= nums[right]:</span><br><span class="line">                    tmp.append(nums[left])</span><br><span class="line">                    left += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:       <span class="comment"># 后半部分值较小，出现了逆序</span></span><br><span class="line">                    tmp.append(nums[right])</span><br><span class="line">                    right += <span class="number">1</span></span><br><span class="line">                    ans += mid+<span class="number">1</span>-left       <span class="comment"># 当前值 nums[right] 贡献的逆序对个数为 mid+1-left</span></span><br><span class="line">                    <span class="string">&#x27;&#x27;&#x27;解释：若nums[left] &gt; nums[right]，</span></span><br><span class="line"><span class="string">                       则nums[left, mid] &gt; nums[right]均成立，共 mid+1-left 项&#x27;&#x27;&#x27;</span></span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;左或右数组需遍历完（最多只有一个未遍历完）&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">while</span> left&lt;=mid:</span><br><span class="line">                tmp.append(nums[left])</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> right&lt;=high:</span><br><span class="line">                tmp.append(nums[right])</span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">                <span class="comment"># ans += mid+1-left     # 此时，前半部分一定已经遍历完了，即left=mid+1，因此无需再更新结果</span></span><br><span class="line">            nums[low:high+<span class="number">1</span>] = tmp</span><br><span class="line">            <span class="keyword">return</span> ans</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;主程序&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> mergeSort(nums, <span class="number">0</span>, <span class="built_in">len</span>(nums)-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-077-链表归并排序"></span></h4><p>给定链表的头结点 <code>head</code> ，请将其按 <strong>升序</strong> 排列并返回 <strong>排序后的链表</strong> 。</p>
<p><img src="https://assets.leetcode.com/uploads/2020/09/14/sort_list_1.jpg" alt="img" style="zoom: 67%;"></p>
<blockquote>
<p>  输入：head = [4,2,1,3]<br>  输出：[1,2,3,4]</p>
</blockquote>
<ul>
<li><strong>归并排序 + 递归</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortList</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="comment"># 时间空间复杂度分别为 O(nlogn) 和 O(1) 【归并排序】</span></span><br><span class="line">        <span class="comment"># 【切分数组】</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head <span class="keyword">or</span> <span class="keyword">not</span> head.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        slow, fast = head, head.<span class="built_in">next</span> <span class="comment"># 快慢指针【如何】初始化</span></span><br><span class="line">        <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">            fast, slow = fast.<span class="built_in">next</span>.<span class="built_in">next</span>, slow.<span class="built_in">next</span></span><br><span class="line">        mid, slow.<span class="built_in">next</span> = slow.<span class="built_in">next</span>, <span class="literal">None</span> <span class="comment"># None截断</span></span><br><span class="line">        left, right = self.sortList(head), self.sortList(mid) <span class="comment"># 递归</span></span><br><span class="line">        <span class="comment"># 【合并链表】</span></span><br><span class="line">        h = dummy = ListNode(<span class="number">0</span>) <span class="comment"># 新建节点的合并</span></span><br><span class="line">        <span class="keyword">while</span> left <span class="keyword">and</span> right:</span><br><span class="line">            <span class="keyword">if</span> left.val &lt; right.val:</span><br><span class="line">                h.<span class="built_in">next</span> = left</span><br><span class="line">                left = left.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                h.<span class="built_in">next</span> = right</span><br><span class="line">                right = right.<span class="built_in">next</span></span><br><span class="line">            h = h.<span class="built_in">next</span></span><br><span class="line">        h.<span class="built_in">next</span> = left <span class="keyword">if</span> left <span class="keyword">else</span> right</span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-078-合并排序链表"></span></h4><p><strong>给定一个链表数组，每个链表都已经按升序排列</strong>。请将所有链表合并到一个升序链表中，返回合并后的链表。</p>
<blockquote>
<p>  输入：lists = [[1,4,5], [1,3,4], [2,6]]<br>  输出：[1,1,2,3,4,4,5,6]<br>  解释：链表数组如下：<br>  [<br>    1-&gt;4-&gt;5,<br>    1-&gt;3-&gt;4,<br>    2-&gt;6<br>  ]<br>  将它们合并到一个有序链表中得到。<br>  1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6</p>
</blockquote>
<ul>
<li><strong><font color="red"> 【优先队列】[推荐] 维护多个链表对象</font></strong>；时间：O(nlog(k))，<code>n</code> 是所有链表中元素总和，<code>k</code> 是链表个数；</li>
<li>【<strong>归并排序</strong>】链表两两合并，递归占用很多空间；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mergeKLists</span>(<span class="params">self, lists: <span class="type">List</span>[ListNode]</span>) -&gt; ListNode:</span><br><span class="line">        <span class="comment"># 优先队列 维护多个链表对象</span></span><br><span class="line">        h = dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">        heap = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(lists)):</span><br><span class="line">            <span class="keyword">if</span> lists[i]:</span><br><span class="line">                heapq.heappush(heap, (lists[i].val, i))</span><br><span class="line">                lists[i] = lists[i].<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> heap:</span><br><span class="line">            val, idx = heapq.heappop(heap)</span><br><span class="line">            h.<span class="built_in">next</span> = ListNode(val)</span><br><span class="line">            h = h.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">if</span> lists[idx]:</span><br><span class="line">                heapq.heappush(heap, (lists[idx].val, idx))</span><br><span class="line">                lists[idx] = lists[idx].<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mergeKLists</span>(<span class="params">self, lists: <span class="type">List</span>[ListNode]</span>) -&gt; ListNode:</span><br><span class="line">      	<span class="comment"># 【归并排序】链表两两合并</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> lists:<span class="keyword">return</span> </span><br><span class="line">        n = <span class="built_in">len</span>(lists)</span><br><span class="line">        <span class="keyword">return</span> self.merge(lists, <span class="number">0</span>, n-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">merge</span>(<span class="params">self,lists, left, right</span>):</span><br><span class="line">        <span class="keyword">if</span> left == right:</span><br><span class="line">            <span class="keyword">return</span> lists[left]</span><br><span class="line">        mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">        l1 = self.merge(lists, left, mid)</span><br><span class="line">        l2 = self.merge(lists, mid+<span class="number">1</span>, right)</span><br><span class="line">        <span class="keyword">return</span> self.mergeTwoLists(l1, l2)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mergeTwoLists</span>(<span class="params">self,l1, l2</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> l1:<span class="keyword">return</span> l2</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> l2:<span class="keyword">return</span> l1</span><br><span class="line">        <span class="keyword">if</span> l1.val &lt; l2.val:</span><br><span class="line">            l1.<span class="built_in">next</span> = self.mergeTwoLists(l1.<span class="built_in">next</span>, l2)</span><br><span class="line">            <span class="keyword">return</span> l1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l2.<span class="built_in">next</span> = self.mergeTwoLists(l1, l2.<span class="built_in">next</span>)</span><br><span class="line">            <span class="keyword">return</span> l2</span><br></pre></td></tr></table></figure>
<h2><span id="三-堆排序">三、堆排序</span></h2><blockquote>
<p>  Python 十大排序算法：<a href="https://leetcode.cn/problems/sort-an-array/solution/python-shi-xian-de-shi-da-jing-dian-pai-xu-suan-fa/">https://leetcode.cn/problems/sort-an-array/solution/python-shi-xian-de-shi-da-jing-dian-pai-xu-suan-fa/</a></p>
</blockquote>
<p>堆排序是利用堆这个数据结构设计的排序算法。</p>
<h4><span id="算法描述">算法描述：</span></h4><ul>
<li><strong>建堆，从底向上==调整堆==</strong>，使得父亲节点比孩子节点值大，构成大顶堆；</li>
<li>交换堆顶和最后一个元素，重新调整堆。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 递归写法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adjust_heap</span>(<span class="params">nums, startpos, endpos</span>):</span><br><span class="line">        pos = startpos</span><br><span class="line">        chilidpos = pos * <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> chilidpos &lt; endpos:</span><br><span class="line">            rightpos = chilidpos + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> rightpos &lt; endpos <span class="keyword">and</span> nums[rightpos] &gt; nums[chilidpos]:</span><br><span class="line">                chilidpos = rightpos</span><br><span class="line">            <span class="keyword">if</span> nums[chilidpos] &gt; nums[pos]:</span><br><span class="line">                nums[pos], nums[chilidpos] = nums[chilidpos], nums[pos]</span><br><span class="line">                adjust_heap(nums, pos, endpos)</span><br><span class="line">    <span class="comment"># 迭代写法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adjust_heap</span>(<span class="params">nums, startpos, endpos</span>):</span><br><span class="line">        newitem = nums[startpos]</span><br><span class="line">        pos = startpos</span><br><span class="line">        childpos = pos * <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> childpos &lt; endpos:</span><br><span class="line">            rightpos = childpos + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> rightpos &lt; endpos <span class="keyword">and</span> nums[rightpos] &gt;= nums[childpos]:</span><br><span class="line">                childpos = rightpos</span><br><span class="line">            <span class="keyword">if</span> newitem &lt; nums[childpos]:</span><br><span class="line">                nums[pos] = nums[childpos]</span><br><span class="line">                pos = childpos</span><br><span class="line">                childpos = pos * <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        nums[pos] = newitem</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-059-数据流的第-k-大数值"></span></h4><p>设计一个找到数据流中第 <code>k</code> 大元素的类（class）。注意是排序后的第 <code>k</code> 大元素，不是第 <code>k</code> 个不同的元素。</p>
<p><strong>请实现 KthLargest 类：</strong></p>
<ul>
<li>KthLargest(int k, int[] nums) 使用整数 k 和整数流 nums 初始化对象。</li>
<li>int add(int val) 将 val 插入数据流 nums 后，返回当前数据流中第 k 大的元素。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KthLargest</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>):</span><br><span class="line">        self.heap = []</span><br><span class="line">        self.size = k </span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            self._push(num)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_push</span>(<span class="params">self, num</span>):</span><br><span class="line">        <span class="keyword">if</span> self.size &lt;= <span class="built_in">len</span>(self.heap):</span><br><span class="line">            <span class="keyword">if</span> self.heap[<span class="number">0</span>] &lt; num:</span><br><span class="line">                heapq.heappop(self.heap)</span><br><span class="line">                heapq.heappush(self.heap, num)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            heapq.heappush(self.heap, num)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        self._push(val)</span><br><span class="line">        <span class="keyword">return</span> self.heap[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-060-出现频率最高的-k-个数字"></span></h4><p>给定一个整数数组 <code>nums</code> 和一个整数 <code>k</code> ，请返回其中出现频率前 <code>k</code> 高的元素。可以按 <strong>任意顺序</strong> 返回答案。</p>
<ul>
<li>API : <code>return [c[0] for c in Counter(nums).most_common(k)]</code></li>
<li>快速选择</li>
<li>堆排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">topKFrequent</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">findTopk</span>(<span class="params">low, high</span>):</span><br><span class="line">            pivot = random.randint(low, high)</span><br><span class="line">            nums_cnt[pivot], nums_cnt[low] = nums_cnt[low], nums_cnt[pivot]</span><br><span class="line">            base = nums_cnt[low][<span class="number">1</span>]</span><br><span class="line">            i = low</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(low + <span class="number">1</span>, high + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> nums_cnt[j][<span class="number">1</span>] &gt; base:</span><br><span class="line">                    nums_cnt[i + <span class="number">1</span>], nums_cnt[j] = nums_cnt[j], nums_cnt[i + <span class="number">1</span>]</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            nums_cnt[i], nums_cnt[low] = nums_cnt[low], nums_cnt[i]</span><br><span class="line">            <span class="keyword">if</span> i == k - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> nums_cnt[:k]</span><br><span class="line">            <span class="keyword">elif</span> i &lt; k - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> findTopk(i + <span class="number">1</span>, high)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> findTopk(low, i - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        nums_cnt = <span class="built_in">list</span>(Counter(nums).items())</span><br><span class="line">        res = findTopk(<span class="number">0</span>, <span class="built_in">len</span>(nums_cnt) - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> [r[<span class="number">0</span>] <span class="keyword">for</span> r <span class="keyword">in</span> res]</span><br><span class="line">		<span class="keyword">def</span> <span class="title function_">topKFrequent</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 堆排序</span></span><br><span class="line">        <span class="comment"># heapq.nlargest(n, iterable, key=None)</span></span><br><span class="line">        <span class="comment"># 【val, key】 heapq 中优先比[0]</span></span><br><span class="line">        conter = Counter(nums)</span><br><span class="line">        heap = []</span><br><span class="line">        <span class="keyword">for</span> key, val <span class="keyword">in</span> conter.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(heap) &gt;= k:</span><br><span class="line">                <span class="keyword">if</span> val &gt; heap[<span class="number">0</span>][<span class="number">0</span>]:</span><br><span class="line">                    heapq.heapreplace(heap,(val, key))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                heapq.heappush(heap,(val, key))</span><br><span class="line">        <span class="keyword">return</span> [item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> heap]</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-061-和最小的-k-个数对"></span></h4><p>给定两个<strong>以升序排列</strong>的整数数组 nums1 和 nums2 , 以及一个整数 k 。定义一对值 (u,v)，其中第一个元素来自 nums1，第二个元素来自 nums2 。请找到和最小的 k 个数对 (u1,v1),  (u2,v2)  …  (uk,vk) 。</p>
<blockquote>
<p>  <strong>输入</strong>: nums1 = [1,7,11], nums2 = [2,4,6], k = 3<br>  <strong>输出</strong>: [1,2],[1,4],[1,6]<br>  <strong>解释</strong>: 返回序列中的前 3 对数：<br>  [1,2],[1,4],[1,6],[7,2],[7,4],[11,2],[7,6],[11,4],[11,6]</p>
</blockquote>
<ul>
<li><strong>多路归并【优先队列】</strong>、<strong>二分查找</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kSmallestPairs</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># 【BFS】【优先队列, 排序输出交给堆】【去重】</span></span><br><span class="line">        m, n = <span class="built_in">len</span>(nums1), <span class="built_in">len</span>(nums2)</span><br><span class="line">        heap = [(nums1[<span class="number">0</span>] + nums2[<span class="number">0</span>], <span class="number">0</span>, <span class="number">0</span>)]</span><br><span class="line">        res = []</span><br><span class="line">        visited = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">while</span> heap:</span><br><span class="line">            val, i, j = heapq.heappop(heap) </span><br><span class="line">            res.append((nums1[i], nums2[j]))</span><br><span class="line">            visited.add((i, j))</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(res) == k:</span><br><span class="line">                <span class="keyword">return</span> res</span><br><span class="line">            <span class="keyword">if</span> i + <span class="number">1</span> &lt; m <span class="keyword">and</span> (i + <span class="number">1</span>, j) <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                heapq.heappush(heap, (nums1[i + <span class="number">1</span>] + nums2[j], i + <span class="number">1</span>, j))</span><br><span class="line">                visited.add((i + <span class="number">1</span>, j))</span><br><span class="line">            <span class="keyword">if</span> j + <span class="number">1</span> &lt; n <span class="keyword">and</span> (i, j + <span class="number">1</span>) <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                heapq.heappush(heap, (nums1[i] + nums2[j + <span class="number">1</span>], i, j + <span class="number">1</span>))</span><br><span class="line">                visited.add((i, j + <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="56-合并区间"></span></h4><p>以数组 intervals 表示若干个区间的集合，其中单个区间为 intervals[i] = [starti, endi] 。请你合并所有重叠的区间，并返回 一个不重叠的区间数组，该数组需恰好覆盖输入中的所有区间 。</p>
<blockquote>
<p>  输入：intervals = [[1,3],[2,6],[8,10],[15,18]]<br>  输出：[[1,6],[8,10],[15,18]]<br>  解释：区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6].</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">merge</span>(<span class="params">self, intervals: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        n = <span class="built_in">len</span>(intervals)</span><br><span class="line">        intervals.sort(key = <span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">        ans = []</span><br><span class="line">        ans.append(intervals[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> ans[-<span class="number">1</span>][<span class="number">1</span>] &gt;= intervals[i][<span class="number">0</span>]:</span><br><span class="line">                <span class="keyword">if</span> (t := intervals[i][<span class="number">1</span>]) &gt; ans[-<span class="number">1</span>][<span class="number">1</span>]:</span><br><span class="line">                    ans[-<span class="number">1</span>][<span class="number">1</span>] = t</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ans.append(intervals[i])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-075-数组相对排序"></span></h4><p>给定两个数组，arr1 和 arr2，</p>
<ul>
<li>arr2 中的元素各不相同</li>
<li>arr2 中的每个元素都出现在 arr1 中</li>
</ul>
<p>对 arr1 中的元素进行排序，使 arr1 中项的相对顺序和 arr2 中的相对顺序相同。未在 arr2 中出现过的元素需要按照升序放在 arr1 的末尾。</p>
<ul>
<li><strong>自定义排序</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relativeSortArray</span>(<span class="params">self, arr1: <span class="type">List</span>[<span class="built_in">int</span>], arr2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">cmp</span>(<span class="params">k</span>):</span><br><span class="line">            <span class="keyword">return</span> (<span class="number">0</span>, cmpDict[k]) <span class="keyword">if</span> k <span class="keyword">in</span> cmpDict <span class="keyword">else</span> (<span class="number">1</span>, k) </span><br><span class="line">        cmpDict = &#123;val:i <span class="keyword">for</span> i, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(arr2)&#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sorted</span>(arr1, key = cmp)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（1）数组</title>
    <url>/posts/141Q0J3/</url>
    <content><![CDATA[<h4><span id="剑指-offer-17-打印从1到最大的n位数"></span></h4><p>输入数字 <code>n</code>，按顺序打印出从 1 到最大的 n 位十进制数。比如输入 3，则打印出 1、2、3 一直到最大的 3 位数 999。【分治】【==<strong>大数打印解法</strong>==】</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">printNumbers</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">index, num, digit</span>):</span><br><span class="line">            <span class="keyword">if</span> index == digit:</span><br><span class="line">                res.append(<span class="built_in">int</span>(<span class="string">&#x27;&#x27;</span>.join(num)))</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">                num.append(<span class="built_in">str</span>(i))</span><br><span class="line">                dfs(index + <span class="number">1</span>, num, digit)</span><br><span class="line">                num.pop()</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> digit <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> first <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">                num = [<span class="built_in">str</span>(first)]</span><br><span class="line">                dfs(<span class="number">1</span>, num, digit)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="560-和为-k-的子数组"></span></h4><p>给你一个整数数组 <code>nums</code> 和一个整数 <code>k</code> ，请你统计并返回 <em>该数组中和为 <code>k</code> 的<strong>==子数组==</strong>的个数</em> 。</p>
<ul>
<li><code>-1000 &lt;= nums[i] &lt;= 1000</code></li>
<li>前缀和 + 哈希表优化</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">subarraySum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dic = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        dic[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        res = preNum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            preNum += nums[i]</span><br><span class="line">            res += dic[preNum - k]</span><br><span class="line">            dic[preNum] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（2）链表</title>
    <url>/posts/2T4SP03/</url>
    <content><![CDATA[<h2><span id="链表">链表</span></h2><blockquote>
<p>  在对链表进行操作时，一种常用的技巧是添加一个哑节点（dummy node），它的 \textit{next}next 指针指向链表的头节点。这样一来，我们就不需要对头节点进行特殊的判断了。</p>
</blockquote>
<h4><span id="剑指-offer-06-从尾到头打印链表"></span></h4><ul>
<li><strong>递归、辅助栈</strong></li>
</ul>
<p>输入一个链表的头节点，<strong>从尾到头反过来返回每个节点的值（用数组返回）</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">  	<span class="keyword">def</span> <span class="title function_">reversePrint</span>(<span class="params">self, head: ListNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">    		<span class="comment"># 递归法</span></span><br><span class="line">        <span class="keyword">return</span> self.reversePrint(head.<span class="built_in">next</span>) + [head.val] <span class="keyword">if</span> head <span class="keyword">else</span> []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reversePrint</span>(<span class="params">self, head: ListNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">      	<span class="comment"># 辅助栈法</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">while</span> head:</span><br><span class="line">            ans.append(head.val)</span><br><span class="line">            head = head.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">reversed</span>(ans))</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-24-反转链表"></span></h4><p>定义一个函数，输入一个链表的头节点，<strong>反转该链表并输出反转后链表的头节点</strong>。</p>
<ul>
<li><strong>递归、迭代</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">      	<span class="comment"># 迭代</span></span><br><span class="line">        cur, pre = head, <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            tmp = cur.<span class="built_in">next</span> <span class="comment"># 暂存后继节点 cur.next</span></span><br><span class="line">            cur.<span class="built_in">next</span> = pre <span class="comment"># 修改 next 引用指向</span></span><br><span class="line">            pre = cur      <span class="comment"># pre 暂存 cur</span></span><br><span class="line">            cur = tmp      <span class="comment"># cur 访问下一节点</span></span><br><span class="line">        <span class="keyword">return</span> pre</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">      	<span class="comment"># 递归</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">recur</span>(<span class="params">cur, pre</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur: <span class="keyword">return</span> pre     <span class="comment"># 终止条件</span></span><br><span class="line">            res = recur(cur.<span class="built_in">next</span>, cur) <span class="comment"># 递归后继节点</span></span><br><span class="line">            cur.<span class="built_in">next</span> = pre             <span class="comment"># 修改节点引用指向</span></span><br><span class="line">            <span class="keyword">return</span> res                 <span class="comment"># 返回反转链表的头节点</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> recur(head, <span class="literal">None</span>)       <span class="comment"># 调用递归并返回</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-35-复杂链表的复制"></span></h4><p>请实现 copyRandomList 函数，复制一个复杂链表。<strong>在复杂链表中，每个节点除了有一个 next 指针指向下一个节点，还有一个 random 指针指向链表中的任意节点或者 null</strong>。</p>
<ul>
<li><p>defaultdict()，空为0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">copyRandomList</span>(<span class="params">self, head: <span class="string">&#x27;Node&#x27;</span></span>) -&gt; <span class="string">&#x27;Node&#x27;</span>:</span><br><span class="line">        <span class="comment"># 拼接+拆分 【哈希表】</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        dic = collections.defaultdict()</span><br><span class="line">        cur = head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            dic[cur] = Node(cur.val)</span><br><span class="line">            cur = cur.<span class="built_in">next</span></span><br><span class="line">        cur = head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            dic[cur].<span class="built_in">next</span> = dic[cur.<span class="built_in">next</span>] <span class="keyword">if</span> cur.<span class="built_in">next</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            dic[cur].random = dic[cur.random] <span class="keyword">if</span> cur.random <span class="keyword">else</span> <span class="literal">None</span><span class="comment"># keyerror</span></span><br><span class="line">            cur = cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> dic[head]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4><span id="剑指-offer-ii-021-删除链表的倒数第-n-个结点"></span></h4><p>给定一个链表，<strong>删除链表的倒数第 <code>n</code> 个结点</strong>，并且返回链表的头结点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeNthFromEnd</span>(<span class="params">self, head: ListNode, n: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">        dummy = ListNode(<span class="number">0</span>, head)</span><br><span class="line">        first = head</span><br><span class="line">        second = dummy</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            first = first.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> first:</span><br><span class="line">            first = first.<span class="built_in">next</span></span><br><span class="line">            second = second.<span class="built_in">next</span></span><br><span class="line">        second.<span class="built_in">next</span> = second.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-022-链表中环的入口节点"></span></h4><p>给定一个链表，返回链表开始入环的第一个节点。 从链表的头节点开始沿着 next 指针进入环的第一个节点为环的入口节点。如果链表无环，则返回 null。</p>
<p>为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。注意，pos 仅仅是用于标识环的情况，并不会作为参数传递到函数中。</p>
<p>说明：不允许修改给定的链表。</p>
<p><strong>示例 1：</strong></p>
<p><img src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2018/12/07/circularlinkedlist.png" alt="img" style="zoom:50%;"></p>
<blockquote>
<p>  输入：head = [3,2,0,-4], pos = 1<br>  输出：返回索引为 1 的链表节点<br>  解释：链表中有一个环，其尾部连接到第二个节点。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">detectCycle</span>(<span class="params">self, head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        fast = slow = head</span><br><span class="line">        <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">            fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">if</span> fast == slow:</span><br><span class="line">                fast = head</span><br><span class="line">                <span class="keyword">while</span> fast != slow:</span><br><span class="line">                    fast = fast.<span class="built_in">next</span></span><br><span class="line">                    slow = slow.<span class="built_in">next</span></span><br><span class="line">                <span class="keyword">return</span> slow</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-023-两个链表的第一个重合节点"></span></h4><p>给定两个单链表的头节点 <code>headA</code> 和 <code>headB</code> ，请找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 <code>null</code> 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getIntersectionNode</span>(<span class="params">self, headA: ListNode, headB: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> headA <span class="keyword">and</span> <span class="keyword">not</span> headB:</span><br><span class="line">            <span class="keyword">return</span> headA</span><br><span class="line">        p, q = headA, headB</span><br><span class="line">        <span class="keyword">while</span> p != q:</span><br><span class="line">            p = p.<span class="built_in">next</span> <span class="keyword">if</span> p <span class="keyword">else</span> headB</span><br><span class="line">            q = q.<span class="built_in">next</span> <span class="keyword">if</span> q <span class="keyword">else</span> headA</span><br><span class="line">        <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-025-链表中的两数相加"></span></h4><p>给定两个 <strong>非空链表</strong> <code>l1</code>和 <code>l2</code> 来代表两个非负整数。数字最高位位于链表开始位置。它们的每个节点只存储一位数字。将这两数相加会返回一个新的链表。</p>
<ul>
<li>从后往前加【反转链表初始化】、倒序添加</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addTwoNumbers</span>(<span class="params">self, l1: ListNode, l2: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">                <span class="keyword">return</span> head</span><br><span class="line">            pre, cur = <span class="literal">None</span>, head</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                tmp = cur.<span class="built_in">next</span></span><br><span class="line">                cur.<span class="built_in">next</span> = pre</span><br><span class="line">                pre = cur</span><br><span class="line">                cur = tmp</span><br><span class="line">            <span class="keyword">return</span> pre</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> l1 <span class="keyword">and</span> <span class="keyword">not</span> l2:</span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line">        l1 = reverseList(l1)</span><br><span class="line">        l2 = reverseList(l2)</span><br><span class="line">        more = <span class="number">0</span></span><br><span class="line">        p = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">or</span> l2:</span><br><span class="line">            <span class="comment"># 倒序添加</span></span><br><span class="line">            val1 = l1.val <span class="keyword">if</span> l1 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            val2 = l2.val <span class="keyword">if</span> l2 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            total = val1 + val2 + more</span><br><span class="line">            more, cur = <span class="built_in">divmod</span>(total, <span class="number">10</span>)</span><br><span class="line">            p = ListNode(cur, p)</span><br><span class="line">            l1 = l1.<span class="built_in">next</span> <span class="keyword">if</span> l1 <span class="keyword">else</span> l1</span><br><span class="line">            l2 = l2.<span class="built_in">next</span> <span class="keyword">if</span> l2 <span class="keyword">else</span> l2</span><br><span class="line">        <span class="keyword">if</span> more != <span class="number">0</span>:</span><br><span class="line">            p = ListNode(more, p)</span><br><span class="line">        <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-026-重排链表"></span></h4><p>给定一个单链表 L 的头节点 head ，单链表 L 表示为：</p>
<p> L0 → L1 → … → Ln-1 → Ln<br>请将其重新排列后变为：</p>
<p>L0 → Ln → L1 → Ln-1 → L2 → Ln-2 → …</p>
<p>不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。</p>
<ul>
<li><strong>列表的中点、插入节点</strong>; 线性表存储</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reorderList</span>(<span class="params">self, head: ListNode</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Do not return anything, modify head in-place instead.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">            pre, cur = <span class="literal">None</span>, head</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                tmp = cur.<span class="built_in">next</span></span><br><span class="line">                cur.<span class="built_in">next</span> = pre</span><br><span class="line">                pre = cur</span><br><span class="line">                cur = tmp</span><br><span class="line">            <span class="keyword">return</span> pre</span><br><span class="line">        fast, slow = head.<span class="built_in">next</span>, head</span><br><span class="line">        <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">            slow, fast = slow.<span class="built_in">next</span>, fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        mid, slow.<span class="built_in">next</span> = slow.<span class="built_in">next</span>, <span class="literal">None</span></span><br><span class="line">        left, cur = head, reverseList(mid)</span><br><span class="line">        slow.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            tmp = cur.<span class="built_in">next</span></span><br><span class="line">            cur.<span class="built_in">next</span> = left.<span class="built_in">next</span></span><br><span class="line">            left.<span class="built_in">next</span> = cur</span><br><span class="line">            left = cur.<span class="built_in">next</span></span><br><span class="line">            cur = tmp</span><br><span class="line">        <span class="keyword">return</span> head</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-027-回文链表"></span></h4><p>给定一个链表的 <strong>头节点</strong> <code>head</code> <strong>，</strong>请判断其是否为回文链表。如果一个链表是回文，那么链表节点序列从前往后看和从后往前看是相同的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isPalindrome</span>(<span class="params">self, head: ListNode</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">            pre, cur = <span class="literal">None</span>, head</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                tmp = cur.<span class="built_in">next</span></span><br><span class="line">                cur.<span class="built_in">next</span> = pre</span><br><span class="line">                pre = cur</span><br><span class="line">                cur = tmp</span><br><span class="line">            <span class="keyword">return</span> pre</span><br><span class="line">        slow = fast = head</span><br><span class="line">        <span class="keyword">while</span> fast.<span class="built_in">next</span> <span class="keyword">and</span> fast.<span class="built_in">next</span>.<span class="built_in">next</span>:</span><br><span class="line">            slow, fast = slow.<span class="built_in">next</span>, fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        left, right = head, slow.<span class="built_in">next</span></span><br><span class="line">        slow.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">        right = reverseList(right)</span><br><span class="line">        <span class="keyword">while</span> left <span class="keyword">and</span> right:</span><br><span class="line">            <span class="keyword">if</span> left.val != right.val:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            left, right = left.<span class="built_in">next</span>, right.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-028-展平多级双向链表"></span></h4><p>多级双向链表中，除了指向下一个节点和前一个节点指针之外，它还有一个子链表指针，可能指向单独的双向链表。这些子列表也可能会有一个或多个自己的子项，依此类推，生成多级数据结构，如下面的示例所示。</p>
<p>给定位于列表第一级的头节点，请扁平化列表，即将这样的多级双向链表展平成普通的双向链表，使所有结点出现在单级双链表中。</p>
<ul>
<li><strong>深度优先搜索</strong>、<strong>栈迭代</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">flatten</span>(<span class="params">self, head: <span class="string">&#x27;Node&#x27;</span></span>) -&gt; <span class="string">&#x27;Node&#x27;</span>:</span><br><span class="line">      	<span class="comment"># 栈迭代</span></span><br><span class="line">        <span class="keyword">if</span> head == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        dummy = Node(-<span class="number">1</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line">        pre = dummy</span><br><span class="line">        stk = [head]</span><br><span class="line">        <span class="keyword">while</span> stk:</span><br><span class="line">            x = stk.pop()</span><br><span class="line">            pre.<span class="built_in">next</span> = x</span><br><span class="line">            x.prev = pre</span><br><span class="line">            <span class="keyword">if</span> x.<span class="built_in">next</span>:</span><br><span class="line">                stk.append(x.<span class="built_in">next</span>)</span><br><span class="line">            <span class="keyword">if</span> x.child:</span><br><span class="line">                stk.append(x.child)</span><br><span class="line">                x.child = <span class="literal">None</span></span><br><span class="line">            pre = x</span><br><span class="line">        dummy.<span class="built_in">next</span>.prev = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br><span class="line">		<span class="keyword">def</span> <span class="title function_">flatten</span>(<span class="params">self, head: <span class="string">&#x27;Node&#x27;</span></span>) -&gt; <span class="string">&#x27;Node&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> head == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        dummy = Node(-<span class="number">1</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">pre: <span class="string">&#x27;Node&#x27;</span>, cur: <span class="string">&#x27;Node&#x27;</span></span>) -&gt; <span class="string">&#x27;Node&#x27;</span>:</span><br><span class="line">            <span class="keyword">if</span> cur == <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">return</span> pre   </span><br><span class="line">            pre.<span class="built_in">next</span> = cur</span><br><span class="line">            cur.prev = pre</span><br><span class="line"></span><br><span class="line">            nxt_head = cur.<span class="built_in">next</span>         <span class="comment">#相当于4</span></span><br><span class="line"></span><br><span class="line">            tail = dfs(cur, cur.child)  <span class="comment">#相当于dfs(3, 7)</span></span><br><span class="line">            cur.child = <span class="literal">None</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> dfs(tail, nxt_head)  <span class="comment">#相当于dfs(12, 4)</span></span><br><span class="line">        dfs(dummy, head)</span><br><span class="line">        dummy.<span class="built_in">next</span>.prev = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-029-排序的循环链表"></span></h4><p>给定<strong>循环单调非递减列表中的一个点</strong>，<strong>写一个函数向这个列表中插入一个新元素 insertVal</strong> ，使这个列表仍然是循环升序的。给定的可以是这个列表中任意一个顶点的指针，并不一定是这个列表中最小元素的指针。</p>
<p>如果有多个满足条件的插入位置，可以选择任意一个位置插入新的值，插入后整个列表仍然保持有序。如果列表为空（给定的节点是 null），需要创建一个循环有序列表并返回这个节点。否则。请返回原先给定的节点。</p>
<ul>
<li>循环链表边界判断</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insert</span>(<span class="params">self, head: <span class="string">&#x27;Node&#x27;</span>, insertVal: <span class="built_in">int</span></span>) -&gt; <span class="string">&#x27;Node&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            node =  Node(insertVal)</span><br><span class="line">            node.<span class="built_in">next</span> = node</span><br><span class="line">            <span class="keyword">return</span> node</span><br><span class="line">        cur = head</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> cur.val &lt;= insertVal &lt;= cur.<span class="built_in">next</span>.val:</span><br><span class="line">                cur.<span class="built_in">next</span> = Node(insertVal, cur.<span class="built_in">next</span>)</span><br><span class="line">                <span class="keyword">return</span> head</span><br><span class="line">            <span class="keyword">elif</span> cur.<span class="built_in">next</span>.val &lt; cur.val:</span><br><span class="line">                <span class="keyword">if</span> insertVal &gt;= cur.val <span class="keyword">or</span> insertVal &lt;= cur.<span class="built_in">next</span>.val:</span><br><span class="line">                    cur.<span class="built_in">next</span> = Node(insertVal, cur.<span class="built_in">next</span>)</span><br><span class="line">                    <span class="keyword">return</span> head</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 走到原点了，还是没有</span></span><br><span class="line">                <span class="keyword">if</span> cur.<span class="built_in">next</span> == head:</span><br><span class="line">                    cur.<span class="built_in">next</span> = Node(insertVal, cur.<span class="built_in">next</span>)</span><br><span class="line">                    <span class="keyword">return</span> head</span><br><span class="line">            cur = cur.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>
<h4><span id="列表反转二">列表反转二</span></h4><h3><span id>#</span></h3>]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（3）二叉树</title>
    <url>/posts/1G12BTE/</url>
    <content><![CDATA[<h2><span id="二叉树">二叉树</span></h2><h4><span id="剑指-offer-07-重建二叉树"></span></h4><p>输入某二叉树的<strong>前序遍历</strong>和<strong>中序遍历</strong>的结果，请构建该二叉树并返回其根节点。</p>
<ul>
<li>数组切片、<strong>字典递归</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildTree</span>(<span class="params">self, preorder: <span class="type">List</span>[<span class="built_in">int</span>], inorder: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; TreeNode:</span><br><span class="line">        <span class="comment"># 切片</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(preorder) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        rootVal = preorder[<span class="number">0</span>]</span><br><span class="line">        rootindex = inorder.index(rootVal)</span><br><span class="line">        leftsize = rootindex</span><br><span class="line">        root = TreeNode(rootVal)</span><br><span class="line">        root.left = self.buildTree(preorder[<span class="number">1</span>:leftsize+<span class="number">1</span>], inorder[:rootindex])</span><br><span class="line">        root.right = self.buildTree(preorder[leftsize+<span class="number">1</span>:], inorder[rootindex+<span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildTree</span>(<span class="params">self, preorder: <span class="type">List</span>[<span class="built_in">int</span>], inorder: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; TreeNode:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">recur</span>(<span class="params">root, left, right</span>):</span><br><span class="line">            <span class="keyword">if</span> left &gt; right: <span class="keyword">return</span>                               <span class="comment"># 递归终止</span></span><br><span class="line">            node = TreeNode(preorder[root])                       <span class="comment"># 建立根节点</span></span><br><span class="line">            i = dic[preorder[root]]                               <span class="comment"># 划分根节点、左子树、右子树</span></span><br><span class="line">            node.left = recur(root + <span class="number">1</span>, left, i - <span class="number">1</span>)              <span class="comment"># 开启左子树递归</span></span><br><span class="line">            node.right = recur(i - left + root + <span class="number">1</span>, i + <span class="number">1</span>, right) <span class="comment"># 开启右子树递归</span></span><br><span class="line">            <span class="keyword">return</span> node                                           <span class="comment"># 回溯返回根节点</span></span><br><span class="line"></span><br><span class="line">        dic = &#123;val: i <span class="keyword">for</span> i, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(inorder)&#125;</span><br><span class="line">        <span class="keyword">return</span> recur(<span class="number">0</span>, <span class="number">0</span>, <span class="built_in">len</span>(inorder) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-33-二叉搜索树的后序遍历序列"></span></h4><p>输入一个整数数组，<strong>判断该数组是不是某二叉搜索树的后序遍历结果</strong>。如果是则返回 <code>true</code>，否则返回 <code>false</code>。假设输入的数组的任意两个数字都互不相同。</p>
<ul>
<li>分治、<strong>辅助单调栈</strong></li>
</ul>
<p><strong>辅助单调栈：</strong></p>
<p><img src="https://pic.leetcode-cn.com/0b0f77f90c68ecf5d0d154f66971f32fa6feb5d50f01a2b2b627df2029a0a103-Picture10.png" alt="Picture10.png" style="zoom:50%;"></p>
<ul>
<li>借助一个单调栈 stack 存储值递增的节点；</li>
<li>每当遇到值递减的节点 $r_i$ ，则通过出栈来更新节点 $r_i$的父节点 $root$ ；</li>
<li>每轮判断 $r_i和 root$的值关系：<ul>
<li>若 $r_i &gt; root$则说明不满足二叉搜索树定义，直接返回 false。</li>
<li>若 $r_i &lt; root$ 则说明满足二叉搜索树定义，则继续遍历。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">verifyPostorder</span>(<span class="params">self, postorder: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(postorder) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        inorder = <span class="built_in">sorted</span>(postorder)</span><br><span class="line">        rootval = postorder[-<span class="number">1</span>]</span><br><span class="line">        rootindex = inorder.index(rootval)</span><br><span class="line">        rightsize = <span class="built_in">len</span>(inorder) - rootindex - <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">set</span>(inorder[rootindex+<span class="number">1</span>:]) != <span class="built_in">set</span>(postorder[-<span class="number">1</span>-rightsize:-<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">set</span>(inorder[:rootindex]) != <span class="built_in">set</span>(postorder[:-<span class="number">1</span>-rightsize]):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> self.verifyPostorder(postorder[-<span class="number">1</span>-rightsize:-<span class="number">1</span>]) <span class="keyword">and</span> self.verifyPostorder(postorder[:-<span class="number">1</span>-rightsize])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">verifyPostorder</span>(<span class="params">self, postorder: [<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 分治递归</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">recur</span>(<span class="params">i, j</span>):</span><br><span class="line">            <span class="keyword">if</span> i &gt;= j: </span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            p = i</span><br><span class="line">            <span class="keyword">while</span> postorder[p] &lt; postorder[j]: </span><br><span class="line">                p += <span class="number">1</span></span><br><span class="line">            m = p</span><br><span class="line">            <span class="keyword">while</span> postorder[p] &gt; postorder[j]: </span><br><span class="line">                p += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> p == j <span class="keyword">and</span> recur(i, m - <span class="number">1</span>) <span class="keyword">and</span> recur(m, j - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> recur(<span class="number">0</span>, <span class="built_in">len</span>(postorder) - <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">verifyPostorder</span>(<span class="params">self, postorder: [<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 单调队列</span></span><br><span class="line">        stack, root = [], <span class="built_in">float</span>(<span class="string">&quot;+inf&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(postorder) - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> postorder[i] &gt; root: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">while</span>(stack <span class="keyword">and</span> postorder[i] &lt; stack[-<span class="number">1</span>]):</span><br><span class="line">                root = stack.pop()</span><br><span class="line">            stack.append(postorder[i])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-044-二叉树每层的最大值"></span></h4><p>给定一棵二叉树的根节点 <code>root</code> ，请找出该二叉树中每一层的最大值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">largestValues</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        q = deque([root])</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            k = <span class="built_in">len</span>(q)</span><br><span class="line">            tmp = <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                node = q.popleft()</span><br><span class="line">                <span class="keyword">if</span> (t:= node.val) &gt; tmp:</span><br><span class="line">                    tmp = t</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    q.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    q.append(node.right)</span><br><span class="line">            ans.append(tmp)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="513-找树左下角的值"></span></h4><p>给定一个二叉树的 <strong>根节点</strong> <code>root</code>，请找出该二叉树的 <strong>最底层 最左边</strong> 节点的值。</p>
<ul>
<li>深度优先遍历、广度优先遍历</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findBottomLeftValue</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        curVal = curHeight = <span class="number">0</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node: <span class="type">Optional</span>[TreeNode], height: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> node <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            height += <span class="number">1</span></span><br><span class="line">            dfs(node.left, height)</span><br><span class="line">            dfs(node.right, height)</span><br><span class="line">            <span class="keyword">nonlocal</span> curVal, curHeight</span><br><span class="line">            <span class="keyword">if</span> height &gt; curHeight:</span><br><span class="line">                curHeight = height</span><br><span class="line">                curVal = node.val</span><br><span class="line">        dfs(root, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> curVal</span><br><span class="line">		<span class="keyword">def</span> <span class="title function_">findBottomLeftValue</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        q = deque([root])</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            node = q.popleft()</span><br><span class="line">            <span class="keyword">if</span> node.right:</span><br><span class="line">                q.append(node.right)</span><br><span class="line">            <span class="keyword">if</span> node.left:</span><br><span class="line">                q.append(node.left)</span><br><span class="line">            ans = node.val</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h2><span id="搜索">搜索</span></h2><h4><span id="剑指-offer-32-i-从上到下打印二叉树层序遍历">【层序遍历】</span></h4><p>从上到下打印出二叉树的每个节点，同一层的节点按照从左到右的顺序打印。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">levelOrder</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> []</span><br><span class="line">        res, queue = [], collections.deque()</span><br><span class="line">        queue.append(root)</span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            node = queue.popleft()</span><br><span class="line">            res.append(node.val)</span><br><span class="line">            <span class="keyword">if</span> node.left: queue.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right: queue.append(node.right)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-32-ii-从上到下打印二叉树-ii"></span></h4><p>从上到下按层打印二叉树，同一层的节点按从左到右的顺序打印，每一层打印到一行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">levelOrder</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        ans, q = [], collections.deque()</span><br><span class="line">        q.append(root)</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            tmp = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q)):</span><br><span class="line">                node = q.popleft()</span><br><span class="line">                tmp.append(node.val)</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    q.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    q.append(node.right)</span><br><span class="line">            ans.append(tmp)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-32-iii-从上到下打印二叉树-iii"></span></h4><p>请实现一个函数按照之字形顺序打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右到左的顺序打印，第三行再按照从左到右的顺序打印，其他行以此类推。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">levelOrder</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        ans, q = [], collections.deque()</span><br><span class="line">        q.append(root)</span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            tmp = []</span><br><span class="line">            k = <span class="built_in">len</span>(q)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                node = q.popleft()</span><br><span class="line">                tmp.append(node.val)</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    q.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    q.append(node.right)</span><br><span class="line">            ans.append(tmp[::-<span class="number">1</span>] <span class="keyword">if</span> <span class="built_in">len</span>(ans) % <span class="number">2</span> <span class="keyword">else</span> tmp)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="104-二叉树的最大深度"></span></h4><p>给定一个二叉树，<strong>找出其最大深度</strong>。二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDepth</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        left_high = self.maxDepth(root.left)</span><br><span class="line">        right_high = self.maxDepth(root.right)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(left_high, right_high)+<span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDepth</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 空树，高度为 0</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 初始化队列和层次</span></span><br><span class="line">        queue = [root]</span><br><span class="line">        depth = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 当队列不为空</span></span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            <span class="comment"># 当前层的节点数</span></span><br><span class="line">            n = <span class="built_in">len</span>(queue)</span><br><span class="line">            <span class="comment"># 弹出当前层的所有节点，并将所有子节点入队列</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                node = queue.pop(<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    queue.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    queue.append(node.right)</span><br><span class="line">            depth += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 二叉树最大层次即为二叉树最深深度</span></span><br><span class="line">        <span class="keyword">return</span> depth</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-27-二叉树的镜像"></span></h4><p>请完成一个函数，输入一个二叉树，该函数<strong>输出它的镜像</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mirrorTree</span>(<span class="params">self, root: TreeNode</span>) -&gt; TreeNode:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        stack = [root]</span><br><span class="line">        <span class="keyword">while</span> stack:</span><br><span class="line">            node = stack.pop()</span><br><span class="line">            node.left, node.right = node.right, node.left</span><br><span class="line">            <span class="keyword">if</span> node.left:</span><br><span class="line">                stack.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right:</span><br><span class="line">                stack.append(node.right)</span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mirrorTree</span>(<span class="params">self, root: TreeNode</span>) -&gt; TreeNode:</span><br><span class="line">        <span class="comment"># 反转左右子树</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        root.left, root.right = root.right, root.left</span><br><span class="line">        root.left = self.mirrorTree(root.left)</span><br><span class="line">        root.right = self.mirrorTree(root.right)</span><br><span class="line">   </span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-26-树的子结构-辅助函数"> [辅助函数]</span></h4><p> <strong>输入两棵二叉树A和B，判断B是不是A的子结构</strong>。(约定空树不是任意一个树的子结构)，B是A的子结构， 即 A中有出现和B相同的结构和节点值。</p>
<ul>
<li><strong>==对称性递归==</strong>：<a href="https://leetcode.cn/problems/shu-de-zi-jie-gou-lcof/solution/yi-pian-wen-zhang-dai-ni-chi-tou-dui-che-uhgs/">https://leetcode.cn/problems/shu-de-zi-jie-gou-lcof/solution/yi-pian-wen-zhang-dai-ni-chi-tou-dui-che-uhgs/</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isSubStructure</span>(<span class="params">self, A: TreeNode, B: TreeNode</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">recur</span>(<span class="params">A, B</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> B: </span><br><span class="line">              	<span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> A <span class="keyword">or</span> A.val != B.val: </span><br><span class="line">              	<span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> recur(A.left, B.left) <span class="keyword">and</span> recur(A.right, B.right)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">bool</span>(A <span class="keyword">and</span> B) <span class="keyword">and</span> (recur(A, B) <span class="keyword">or</span> self.isSubStructure(A.left, B) <span class="keyword">or</span> self.isSubStructure(A.right, B))</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-28-对称的二叉树-辅助函数"> [辅助函数]</span></h4><p><strong>请实现一个函数，用来判断一棵二叉树是不是对称的</strong>。如果一棵二叉树和它的镜像一样，那么它是对称的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isSymmetric</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">recur</span>(<span class="params">left, right</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> left <span class="keyword">and</span> <span class="keyword">not</span> right:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> left <span class="keyword">or</span> <span class="keyword">not</span> right <span class="keyword">or</span> left.val != right.val:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> recur(left.left, right.right) <span class="keyword">and</span> recur(left.right, right.left)</span><br><span class="line">        <span class="keyword">return</span> recur(root.left, root.right)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-36-二叉搜索树与双向链表"></span></h4><p>==<strong>输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的循环双向链表</strong>==。要求不能创建任何新的节点，只能调整树中节点指针的指向。为了让您更好地理解问题，以下面的二叉搜索树为例：</p>
<p><img src="https://assets.leetcode.com/uploads/2018/10/12/bstdlloriginalbst.png" alt="img" style="zoom:50%;"></p>
<p>我们希望<strong>将这个二叉搜索树转化为双向循环链表</strong>。链表中的每个节点都有一个前驱和后继指针。对于双向循环链表，第一个节点的前驱是最后一个节点，最后一个节点的后继是第一个节点。</p>
<p><img src="https://assets.leetcode.com/uploads/2018/10/12/bstdllreturndll.png" alt="img" style="zoom:50%;"></p>
<h5><span id="算法流程">算法流程：</span></h5><p><strong><code>dfs(cur):</code></strong> 递归法中序遍历；</p>
<ul>
<li><strong>终止条件：</strong> 当节点 <code>cur</code> 为空，代表越过叶节点，直接返回；</li>
<li>递归左子树，即 <code>dfs(cur.left)</code> ；</li>
<li><strong>构建链表：</strong><ul>
<li><strong>当 <code>pre</code> 为空时：</strong> 代表正在访问链表头节点，记为 <code>head</code> ；</li>
<li><strong>当 <code>pre</code> 不为空时：</strong> 修改双向节点引用，即 <code>pre.right = cur</code> ， <code>cur.left = pre</code> ；</li>
<li><strong>保存 <code>cur</code> ：</strong> 更新 <code>pre = cur</code> ，即节点 <code>cur</code> 是后继节点的 <code>pre</code> ；</li>
</ul>
</li>
<li>递归右子树，即 <code>dfs(cur.right)</code> ；</li>
</ul>
<p><strong><code>treeToDoublyList(root)：</code></strong></p>
<ul>
<li><strong>特例处理：</strong> 若节点 <code>root</code> 为空，则直接返回；</li>
<li><strong>初始化：</strong> 空节点 <code>pre</code> ；</li>
<li><strong>转化为双向链表：</strong> 调用 <code>dfs(root)</code> ；</li>
<li><strong>构建循环链表：</strong> 中序遍历完成后，<code>head</code> 指向头节点， <code>pre</code> 指向尾节点，因此修改 <code>head</code> 和 <code>pre</code> 的双向节点引用即可；</li>
<li><strong>返回值：</strong> 返回链表的头节点 <code>head</code> 即可；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">treeToDoublyList</span>(<span class="params">self, root: <span class="string">&#x27;Node&#x27;</span></span>) -&gt; <span class="string">&#x27;Node&#x27;</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">cur</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> cur: </span><br><span class="line">              	<span class="keyword">return</span></span><br><span class="line">            dfs(cur.left) <span class="comment"># 递归左子树</span></span><br><span class="line">            <span class="keyword">if</span> self.pre: <span class="comment"># 修改节点引用</span></span><br><span class="line">                self.pre.right, cur.left = cur, self.pre</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 记录头节点</span></span><br><span class="line">                self.head = cur</span><br><span class="line">            self.pre = cur <span class="comment"># 保存 cur</span></span><br><span class="line">            dfs(cur.right) <span class="comment"># 递归右子树</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span></span><br><span class="line">        self.pre = <span class="literal">None</span></span><br><span class="line">        dfs(root)</span><br><span class="line">        self.head.left, self.pre.right = self.pre, self.head</span><br><span class="line">        <span class="keyword">return</span> self.head</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-54-二叉搜索树的第k大节点"></span></h4><p>给定一棵二叉搜索树，请找出其中<strong>第 <code>k</code> 大的节点的值</strong>。</p>
<ul>
<li><strong>从大到小：倒序遍历</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kthLargest</span>(<span class="params">self, root: TreeNode, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 中序遍历是单调的</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">nonlocal</span> k, res</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root: </span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            dfs(root.right)</span><br><span class="line">            k -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> k == <span class="number">0</span>: </span><br><span class="line">                res = root.val</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            dfs(root.left)</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        dfs(root)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kthLargest</span>(<span class="params">self, root: TreeNode, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">TreeSize</span>(<span class="params">root: TreeNode</span>):<span class="comment"># 求子树节点个数</span></span><br><span class="line">          	<span class="comment"># 子树的深度</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root: </span><br><span class="line">              	<span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right: </span><br><span class="line">             		<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> TreeSize(root.left) + TreeSize(root.right) + <span class="number">1</span></span><br><span class="line">        nright = TreeSize(root.right)</span><br><span class="line">        <span class="keyword">if</span> nright &gt;= k: <span class="comment">#在右子树</span></span><br><span class="line">            <span class="keyword">return</span> self.kthLargest(root.right, k)</span><br><span class="line">        <span class="keyword">elif</span> nright == k - <span class="number">1</span>: <span class="keyword">return</span> root.val</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> self.kthLargest(root.left, k - nright - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-55-i-二叉树的深度"></span></h4><p>输入一棵二叉树的根节点，求该树的深度。从根节点到叶节点依次经过的节点（含根、叶节点）形成树的一条路径，最长路径的长度为树的深度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDepth</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        queue, res = deque([root]), <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            k = <span class="built_in">len</span>(queue)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                node = queue.popleft()</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    queue.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right: </span><br><span class="line">                    queue.append(node.right)</span><br><span class="line">            res += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDepth</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + <span class="built_in">max</span>(self.maxDepth(root.left), self.maxDepth(root.right))</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-55-ii-平衡二叉树-辅助函数"> [辅助函数]</span></h4><p>输入一棵二叉树的根节点，判断该树是不是平衡二叉树。如果某二叉树中任意节点的左右子树的深度相差不超过1，那么它就是一棵平衡二叉树。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isBalanced</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">height</span>(<span class="params">root: TreeNode</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            leftH = height(root.left)</span><br><span class="line">            rightH = height(root.right)</span><br><span class="line">            <span class="keyword">if</span> leftH == -<span class="number">1</span> <span class="keyword">or</span> rightH == -<span class="number">1</span> <span class="keyword">or</span> <span class="built_in">abs</span>(leftH - rightH) &gt; <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">max</span>(leftH, rightH) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> height(root) &gt;= <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-64-求12n"></span></h4><p>求 <code>1+2+...+n</code> ，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sumNums</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">sumStop</span>(<span class="params">n</span>):</span><br><span class="line">            <span class="keyword">return</span> n &gt;= <span class="number">1</span> <span class="keyword">and</span> n + sumStop(n-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> sumStop(n)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-68-i-二叉搜索树的最近公共祖先"></span></h4><p>给定一个<strong>二叉搜索树</strong>, 找到该树中两个指定节点的最近公共祖先。</p>
<ul>
<li>若 root.val &lt; p.val，则 pp 在 root 右子树 中；</li>
<li>若 root.val &gt; p.val ，则 pp 在 root 左子树 中；</li>
</ul>
<blockquote>
<p>  输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8<br>  输出: 6<br>  解释: 节点 2 和节点 8 的最近公共祖先是 6。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lowestCommonAncestor</span>(<span class="params">self, root: <span class="string">&#x27;TreeNode&#x27;</span>, p: <span class="string">&#x27;TreeNode&#x27;</span>, q: <span class="string">&#x27;TreeNode&#x27;</span></span>) -&gt; <span class="string">&#x27;TreeNode&#x27;</span>:</span><br><span class="line">      	<span class="comment"># 迭代优化</span></span><br><span class="line">        <span class="keyword">if</span> p.val &gt; q.val: p, q = q, p <span class="comment"># 保证 p.val &lt; q.val</span></span><br><span class="line">        <span class="keyword">while</span> root:</span><br><span class="line">            <span class="keyword">if</span> root.val &lt; p.val: <span class="comment"># p,q 都在 root 的右子树中</span></span><br><span class="line">                root = root.right <span class="comment"># 遍历至右子节点</span></span><br><span class="line">            <span class="keyword">elif</span> root.val &gt; q.val: <span class="comment"># p,q 都在 root 的左子树中</span></span><br><span class="line">                root = root.left <span class="comment"># 遍历至左子节点</span></span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">              	<span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lowestCommonAncestor</span>(<span class="params">self, root: <span class="string">&#x27;TreeNode&#x27;</span>, p: <span class="string">&#x27;TreeNode&#x27;</span>, q: <span class="string">&#x27;TreeNode&#x27;</span></span>) -&gt; <span class="string">&#x27;TreeNode&#x27;</span>:</span><br><span class="line">      	<span class="comment"># 递归</span></span><br><span class="line">        <span class="keyword">if</span> root.val &lt; p.val <span class="keyword">and</span> root.val &lt; q.val:</span><br><span class="line">            <span class="keyword">return</span> self.lowestCommonAncestor(root.right, p, q)</span><br><span class="line">        <span class="keyword">if</span> root.val &gt; p.val <span class="keyword">and</span> root.val &gt; q.val:</span><br><span class="line">            <span class="keyword">return</span> self.lowestCommonAncestor(root.left, p, q)</span><br><span class="line">        <span class="keyword">return</span> root	</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-68-ii-二叉树的最近公共祖先"></span></h4><p>给定一个<strong>二叉树,</strong> 找到该树中两个指定节点的最近公共祖先。深度优先遍历是从下往上返回，所以第一个满足的就是<strong>最近公共祖先节点</strong> 。</p>
<h5><span id="递归解析">递归解析：</span></h5><ul>
<li><strong>终止条件：</strong><ul>
<li>当越过叶节点，则直接返回 null；</li>
<li>当 root等于 p, q ，则直接返回 root；</li>
</ul>
</li>
<li><strong>递推工作：</strong><ul>
<li>开启递归左子节点，返回值记为 left；</li>
<li>开启递归右子节点，返回值记为 right ；</li>
</ul>
</li>
<li><strong>返回值</strong>：根据 left 和 right ，可展开为四种情况；<ul>
<li>当 left 和 right 同时为空 ：说明 root的左 / 右子树中都不包含 p,q ，返回 null；</li>
<li>当 left 和 right 同时不为空 ：说明 p, q分列在 root的 异侧 （分别在 左 / 右子树），因此 root为最近公共祖先，返回 root；</li>
<li>当 left为空 ，right不为空 ：p,q都不在 root的左子树中，直接返回 right。具体可分为两种情况：<ul>
<li>p,q 其中一个在 root的 右子树 中，此时 right 指向 p（假设为 p ）；</li>
<li>p,q 两节点都在 root的 右子树 中，此时的 right指向 <strong>最近公共祖先节点</strong> ；</li>
</ul>
</li>
<li>当 left不为空 ， right为空 ：与情况 3. 同理；</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lowestCommonAncestor</span>(<span class="params">self, root: TreeNode, p: TreeNode, q: TreeNode</span>) -&gt; TreeNode:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root <span class="keyword">or</span> root == q <span class="keyword">or</span> root == p:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        left = self.lowestCommonAncestor(root.left, p, q)</span><br><span class="line">        right = self.lowestCommonAncestor(root.right, p, q)</span><br><span class="line">        <span class="keyword">if</span> left <span class="keyword">and</span> right:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">return</span> left <span class="keyword">if</span> left <span class="keyword">else</span> right</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-049-从根节点到叶节点的路径数字之和"></span></h4><p>给定一个二叉树的根节点 root ，树中每个节点都存放有一个 0 到 9 之间的数字。每条从根节点到叶节点的路径都代表一个数字：</p>
<p>例如，从根节点到叶节点的路径 1 -&gt; 2 -&gt; 3 表示数字 123 。计算从根节点到叶节点生成的 所有数字之和 。</p>
<ul>
<li><strong>深度优先遍历</strong>、回溯【没有return】</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sumNumbers</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># dfs , 回溯</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root, prevTotal</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            total = prevTotal * <span class="number">10</span> + root.val</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">                <span class="keyword">return</span> total</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> dfs(root.left, total) + dfs(root.right, total)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtrace</span>(<span class="params">root, tmp</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> root</span><br><span class="line">            tmp.append(<span class="built_in">str</span>(root.val))</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">                ans.append(tmp[:])</span><br><span class="line">                <span class="comment"># 【没有return, 就会执行一次 pop() 】</span></span><br><span class="line">            backtrace(root.left, tmp)</span><br><span class="line">            backtrace(root.right, tmp)</span><br><span class="line">            tmp.pop()</span><br><span class="line">        backtrace(root, [])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>([<span class="built_in">int</span>(<span class="string">&quot;&quot;</span>.join(nums)) <span class="keyword">for</span> nums <span class="keyword">in</span> ans])</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-047-二叉树剪枝"></span></h4><p>给定一个二叉树 <strong>根节点</strong> <code>root</code> ，树的每个节点的值要么是 <code>0</code>，要么是 <code>1</code>。请剪除该二叉树中所有节点的值为 <code>0</code> 的子树。节点 <code>node</code> 的子树为 <code>node</code> 本身，以及所有 <code>node</code> 的后代。</p>
<ul>
<li>建树【dfs 有返回值】</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pruneTree</span>(<span class="params">self, root: TreeNode</span>) -&gt; TreeNode:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> root</span><br><span class="line">            root.left = dfs(root.left)</span><br><span class="line">            root.right = dfs(root.right)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> root.val == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">return</span> root    </span><br><span class="line">        <span class="keyword">return</span> dfs(root)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-050-向下的路径节点之和"></span></h4><p>给定一个二叉树的根节点 <code>root</code> ，和一个整数 <code>targetSum</code> ，求该二叉树里节点值之和等于 <code>targetSum</code> 的 <strong>路径</strong> 的数目。<strong>路径</strong> 不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。</p>
<p>示例 1：</p>
<p><img src="https://assets.leetcode.com/uploads/2021/04/09/pathsum3-1-tree.jpg" alt="img" style="zoom:50%;"></p>
<p>输入：root = [10,5,-3,3,2,null,11,3,-2,null,1], targetSum = 8<br>输出：3<br>解释：和等于 8 的路径有 3 条，如图所示。</p>
<ul>
<li><strong>前缀和+回溯</strong> o(n)、深度优先遍历【双重递归 0（n^2) 】</li>
</ul>
<p>DFS中存在许多重复计算，我们定义节点的前缀和为：</p>
<ul>
<li><strong>curr</strong> 由根结点到当前结点的路径上所有节点的和；</li>
<li>利用先序遍历二叉树，<strong>记录下根节点 root 到当前节点 <em>p</em> 的路径上除当前节点以外所有节点的前缀和；</strong></li>
<li>在已保存的路径前缀和中<strong>查找是否存在前缀和刚好等于当前节点到根节点的前缀和 curr 减去 targetSum。</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 深度优先遍历</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root, total</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            ans = <span class="number">0</span></span><br><span class="line">            total += root.val</span><br><span class="line">            <span class="keyword">if</span> total == targetSum:</span><br><span class="line">                ans += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> ans + dfs(root.left, total) + dfs(root.right, total)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> dfs(root, <span class="number">0</span>) + self.pathSum(root.left, targetSum) + self.pathSum(root.right, targetSum)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 前缀和 + 回溯</span></span><br><span class="line">        pre = collections.defaultdict(<span class="built_in">int</span>) <span class="comment"># 根节点到当前节点和</span></span><br><span class="line">        pre[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backstace</span>(<span class="params">root, curr</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            curr += root.val <span class="comment"># 根节点到当前节点和</span></span><br><span class="line">            ans = <span class="number">0</span></span><br><span class="line">            ans += pre[curr - targetSum]</span><br><span class="line">            <span class="comment"># 回溯</span></span><br><span class="line">            pre[curr] += <span class="number">1</span></span><br><span class="line">            ans += backstace(root.left, curr)</span><br><span class="line">            ans += backstace(root.right, curr)</span><br><span class="line">            pre[curr] -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> ans</span><br><span class="line">        <span class="keyword">return</span> backstace(root, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-051-节点之和最大的路径"></span></h4><p><strong>路径</strong> 被定义为一条从树中任意节点出发，沿父节点-子节点连接，达到任意节点的序列。同一个节点在一条路径序列中 <strong>至多出现一次</strong> 。该路径 <strong>至少包含一个</strong> 节点，且不一定经过根节点。</p>
<p>给定一个二叉树的根节点 <code>root</code> ，返回其 <strong>最大路径和</strong>，即所有路径上节点值之和的最大值。</p>
<p><strong>示例 2：</strong></p>
<p><img src="https://assets.leetcode.com/uploads/2020/10/13/exx2.jpg" alt="img" style="zoom:50%;"></p>
<p>输入：root = [-10,9,20,null,null,15,7]<br>输出：42<br>解释：<strong>最优路径是 15 -&gt; 20 -&gt; 7 ，路径和为 15 + 20 + 7 = 42</strong></p>
<ul>
<li><strong>节点贡献值</strong> root + max(left, right)  / 最大路径和：当前节点 + 左子树max + 右子树max</li>
<li><strong>动态过程保存</strong>：ans</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxPathSum</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        ans = -inf</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">nonlocal</span> ans</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            left = <span class="built_in">max</span>(pathSum(root.left), <span class="number">0</span>)</span><br><span class="line">            right = <span class="built_in">max</span>(pathSum(root.right), <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> (t := root.val + left + right) &gt; ans:</span><br><span class="line">                ans = t</span><br><span class="line">            <span class="keyword">return</span> root.val + <span class="built_in">max</span>(left, right)</span><br><span class="line">        pathSum(root)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-052-展平二叉搜索树"></span></h4><p>给你一棵二叉搜索树，请 <strong>按中序遍历</strong> 将其重新排列为一棵递增顺序搜索树，使树中最左边的节点成为树的根节点，并且每个节点没有左子节点，只有一个右子节点。</p>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（3）哈希表</title>
    <url>/posts/6M6SM4/</url>
    <content><![CDATA[<h2><span id="dict-字典">dict 字典</span></h2><h4><span id="剑指-offer-39-数组中出现次数超过一半的数字"></span></h4><p>数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。</p>
<ul>
<li>Counter(nums).most_common(1)[0] [0]</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">majorityElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> Counter(nums).most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">majorityElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        majority_count = <span class="built_in">len</span>(nums) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            candidate = random.choice(nums)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> elem <span class="keyword">in</span> nums <span class="keyword">if</span> elem == candidate) &gt; majority_count:</span><br><span class="line">                <span class="keyword">return</span> candidate</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-034-外星语言是否排序"></span></h4><p>某种外星语也使用英文小写字母，但可能顺序 order 不同。字母表的顺序（order）是一些小写字母的排列。</p>
<p>给定一组用外星语书写的单词 words，以及其字母表的顺序 order，只有当给定的单词在这种外星语中按字典序排列时，返回 true；否则，返回 false。</p>
<p><strong>示例 1：</strong></p>
<p>输入：words = [“hello”,”leetcode”], order = “hlabcdefgijkmnopqrstuvwxyz”<br>输出：true<br>解释：在该语言的字母表中，’h’ 位于 ‘l’ 之前，所以单词序列是按字典序排列的。</p>
<ul>
<li>迭代器 pairwise</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isAlienSorted</span>(<span class="params">self, words: <span class="type">List</span>[<span class="built_in">str</span>], order: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">       index = &#123;c: i <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(order)&#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="built_in">all</span>(s &lt;= t <span class="keyword">for</span> s, t <span class="keyword">in</span> pairwise([index[c] <span class="keyword">for</span> c <span class="keyword">in</span> word] <span class="keyword">for</span> word <span class="keyword">in</span> words))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（4）字符串</title>
    <url>/posts/4VXEX1/</url>
    <content><![CDATA[<h2><span id="字符串">字符串</span></h2><h4><span id="剑指-offer-05-替换空格"></span></h4><p>请实现一个函数，把字符串 <code>s</code> 中的每个空格替换成”%20”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">replaceSpace</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">return</span> s.replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;%20&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-58-ii-左旋转字符串"></span></h4><p>字符串的左旋转操作是把字符串前面的若干个字符转移到字符串的尾部。请定义一个函数实现字符串左旋转操作的功能。比如，输入字符串”abcdefg”和数字2，该函数将返回左旋转两位得到的结果”cdefgab”。</p>
<ul>
<li>字符串切片、列表遍历拼接、字符串遍历拼接</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseLeftWords</span>(<span class="params">self, s: <span class="built_in">str</span>, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">return</span> s[n:] + s[:n]</span><br><span class="line">		<span class="keyword">def</span> <span class="title function_">reverseLeftWords</span>(<span class="params">self, s: <span class="built_in">str</span>, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">      	<span class="comment"># 列表遍历拼接</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n, <span class="built_in">len</span>(s)):</span><br><span class="line">            res.append(s[i])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            res.append(s[i])</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(res)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseLeftWords</span>(<span class="params">self, s: <span class="built_in">str</span>, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">      	<span class="comment"># 字符串遍历拼接</span></span><br><span class="line">        res = <span class="string">&quot;&quot;</span> <span class="comment"># 字符为不可变对象，每轮都新建</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n, <span class="built_in">len</span>(s)):</span><br><span class="line">            res += s[i]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            res += s[i]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>以 Python 为例开展三种方法的效率测试：</p>
<p><img src="https://pic.leetcode-cn.com/ef68413b3366b97af3ed76037c6a9d1e40ac09c74fd6e5cb6d5173cbd7116beb-Picture4.png" alt="Picture4.png" style="zoom:48%;"></p>
<h4><span id="567-字符串的排列"></span></h4><p>给你两个字符串 <code>s1</code> 和 <code>s2</code> ，写一个函数来判断 <code>s2</code> 是否包含 <code>s1</code> 的排列。如果是，返回 <code>true</code> ；否则，返回 <code>false</code> 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkInclusion</span>(<span class="params">self, s1: <span class="built_in">str</span>, s2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        m, n = <span class="built_in">len</span>(s1), <span class="built_in">len</span>(s2)</span><br><span class="line">        <span class="keyword">if</span> m &gt; n:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        key = [<span class="number">0</span>] * <span class="number">26</span></span><br><span class="line">        cur = [<span class="number">0</span>] * <span class="number">26</span> </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            key[<span class="built_in">ord</span>(s1[i]) - <span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)] += <span class="number">1</span></span><br><span class="line">            cur[<span class="built_in">ord</span>(s2[i]) - <span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> key == cur:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n - m):</span><br><span class="line">            cur[<span class="built_in">ord</span>(s2[j]) - <span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)] -= <span class="number">1</span></span><br><span class="line">            cur[<span class="built_in">ord</span>(s2[j + m]) - <span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> key == cur:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4><span id="438-找到字符串中所有字母异位词"></span></h4><p>给定两个字符串 <code>s</code> 和 <code>p</code>，找到 <code>s</code> 中所有 <code>p</code> 的 <strong>异位词</strong> 的子串，返回这些子串的起始索引。不考虑答案输出的顺序。</p>
<ul>
<li><strong>字典数组</strong>、滑动窗口</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findAnagrams</span>(<span class="params">self, s: <span class="built_in">str</span>, p: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        s_len, p_len = <span class="built_in">len</span>(s), <span class="built_in">len</span>(p)</span><br><span class="line">        <span class="keyword">if</span> s_len &lt; p_len:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        ans = []</span><br><span class="line">        s_count = [<span class="number">0</span>] * <span class="number">26</span></span><br><span class="line">        p_count = [<span class="number">0</span>] * <span class="number">26</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(p_len):</span><br><span class="line">            s_count[<span class="built_in">ord</span>(s[i]) - <span class="number">97</span>] += <span class="number">1</span></span><br><span class="line">            p_count[<span class="built_in">ord</span>(p[i]) - <span class="number">97</span>] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> s_count == p_count:</span><br><span class="line">            ans.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(s_len - p_len):</span><br><span class="line">            s_count[<span class="built_in">ord</span>(s[i]) - <span class="number">97</span>] -= <span class="number">1</span></span><br><span class="line">            s_count[<span class="built_in">ord</span>(s[i + p_len]) - <span class="number">97</span>] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> s_count == p_count:</span><br><span class="line">                ans.append(i + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-016-不含重复字符的最长子字符串"></span></h4><p>给定一个字符串 <code>s</code> ，请你找出其中不含有重复字符的 <strong>最长连续子字符串</strong> 的长度。</p>
<ul>
<li><strong>滑动窗口</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s:<span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        left, lookup, n = <span class="number">0</span>, <span class="built_in">set</span>(), <span class="built_in">len</span>(s)</span><br><span class="line">        max_len, cur_len = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            cur_len += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> s[i] <span class="keyword">in</span> lookup:</span><br><span class="line">                lookup.remove(s[left]) <span class="comment"># 移除做元素直到无重复，到最小无重复子串</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> cur_len &gt; max_len:</span><br><span class="line">                max_len = cur_len</span><br><span class="line">            lookup.add(s[i])</span><br><span class="line">        <span class="keyword">return</span> max_len</span><br></pre></td></tr></table></figure>
<h4><span id="76-最小覆盖子串"></span></h4><p>给你一个字符串 <code>s</code> 、一个字符串 <code>t</code> 。返回 <code>s</code> 中涵盖 <code>t</code> 所有字符的最小子串。如果 <code>s</code> 中不存在涵盖 <code>t</code> 所有字符的子串，则返回空字符串 <code>&quot;&quot;</code> 。</p>
<ul>
<li><strong>滑动窗口</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minWindow</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        s_len, t_len = <span class="built_in">len</span>(s), <span class="built_in">len</span>(t)</span><br><span class="line">        <span class="keyword">if</span> s_len &lt; t_len:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 0 不删除</span></span><br><span class="line">        sdict, tdict = defaultdict(<span class="built_in">int</span>), Counter(t)</span><br><span class="line">        left, conut = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        res = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(s_len):</span><br><span class="line">            sdict[s[i]] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> sdict[s[i]] &lt;= tdict[s[i]]:</span><br><span class="line">                conut += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> left &lt;= i <span class="keyword">and</span> sdict[s[left]] &gt; tdict[s[left]]:</span><br><span class="line">                <span class="comment"># 加一个A就减一个A...(无用后缀)</span></span><br><span class="line">                sdict[s[left]] -= <span class="number">1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> conut == t_len:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> res <span class="keyword">or</span> (i - left + <span class="number">1</span> &lt; <span class="built_in">len</span>(res)):</span><br><span class="line">                    res = s[left:i + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="125-验证回文串"></span></h4><p>给定一个字符串，验证它是否是回文串，只考虑字母和数字字符，可以忽略字母的大小写。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        left, right = <span class="number">0</span>, n - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> <span class="keyword">not</span> s[left].isalnum():</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> <span class="keyword">not</span> s[right].isalnum():</span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> left &lt; right:</span><br><span class="line">                <span class="keyword">if</span> s[left].lower() != s[right].lower():</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                left, right = left + <span class="number">1</span>, right - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-019-最多删除一个字符得到回文"></span></h4><p>给定一个非空字符串 <code>s</code>，请判断如果 <strong>最多</strong> 从字符串中删除一个字符能否得到一个回文字符串。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 回溯， 广度优先 【字符串长度，暴搜必定超时】</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">checkPalind</span>(<span class="params">l, r</span>):</span><br><span class="line">            <span class="keyword">while</span> l &lt; r:</span><br><span class="line">                <span class="keyword">if</span> s[l] != s[r]:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">                r -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        left, right = <span class="number">0</span>, n - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            <span class="keyword">if</span> s[left] == s[right]:</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> checkPalind(left + <span class="number">1</span>, right) <span class="keyword">or</span> checkPalind(left, right - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-020-回文子字符串的个数"></span></h4><p>给定一个字符串 <code>s</code> ，请计算这个字符串中有多少个回文子字符串。具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被视作不同的子串。</p>
<ul>
<li>动态规划、双指针+中心扩展、<strong>==Manacher算法==</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countSubstrings</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 双指针+中心扩散</span></span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_extend</span>(<span class="params">s, i, j, n</span>):</span><br><span class="line">            res = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">and</span> j &lt; n <span class="keyword">and</span> s[i] == s[j]: <span class="comment"># 确定中心点</span></span><br><span class="line">                i -= <span class="number">1</span></span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">                res += <span class="number">1</span> <span class="comment"># 扩散过程也是答案</span></span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            result += _extend(s, i, i, <span class="built_in">len</span>(s)) <span class="comment">#以i为中心</span></span><br><span class="line">            result += _extend(s, i, i+<span class="number">1</span>, <span class="built_in">len</span>(s)) <span class="comment">#以i和i+1为中心</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countSubstrings</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 动态规划 dp[i][j]: s[i:j] 是否为回文子串</span></span><br><span class="line">        <span class="comment"># dp[i+1][j-1] -&gt; dp[i][j] 遍历顺序 </span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [[<span class="literal">False</span>] * (n) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i,n):</span><br><span class="line">                <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                    <span class="keyword">if</span> j - i &lt;= <span class="number">1</span>:</span><br><span class="line">                        ans += <span class="number">1</span></span><br><span class="line">                        dp[i][j] = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">elif</span> dp[i+<span class="number">1</span>][j-<span class="number">1</span>]:</span><br><span class="line">                        ans += <span class="number">1</span></span><br><span class="line">                        dp[i][j] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（4）查找算法</title>
    <url>/posts/24PQ9K6/</url>
    <content><![CDATA[<h2><span id="查找算法">查找算法</span></h2><blockquote>
<p>  <strong>bisect 文档</strong>:<a href="https://docs.python.org/zh-cn/3.6/library/bisect.html">https://docs.python.org/zh-cn/3.6/library/bisect.html</a></p>
<ul>
<li><p><strong>二分查找求左边界</strong> <a href="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/">第一个 False </a> : 不小于target的第一个数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">leftMargin</span>(): <span class="comment"># 左边界</span></span><br><span class="line">   left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">	<span class="keyword">while</span> left &lt;= right:</span><br><span class="line">       mid = (left + right) // <span class="number">2</span></span><br><span class="line">       <span class="keyword">if</span> nums[mid] &lt; target: <span class="comment"># ... Ture # False ...</span></span><br><span class="line">          left = mid + <span class="number">1</span></span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">          right = mid - <span class="number">1</span></span><br><span class="line">   <span class="keyword">return</span> left <span class="keyword">if</span> nums[left] == target <span class="keyword">else</span> -<span class="number">1</span> <span class="comment"># 越界检验</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>二分查找右边界</strong> [第一个 True ]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rightMargin</span>(): <span class="comment"># 右边界</span></span><br><span class="line">		left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">    		mid = (left + right) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &gt; target: <span class="comment"># ... False # Ture ...</span></span><br><span class="line">           right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">           left = mid + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> right <span class="keyword">if</span> nums[right] == target <span class="keyword">else</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<h4><span id="剑指-offer-03-数组中重复的数字"></span></h4><p><strong>找出数组中重复的数字</strong>。<strong>==在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内==</strong>。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。</p>
<ul>
<li>哈希表 / Set、原地交换（空间0（1））</li>
<li>合理利用 <strong>val</strong> 和 <strong>key</strong> 的关系</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findRepeatNumber</span>(<span class="params">self, nums: [<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dic = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> num <span class="keyword">in</span> dic: <span class="keyword">return</span> num</span><br><span class="line">            dic.add(num)</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">		<span class="keyword">def</span> <span class="title function_">findRepeatNumber</span>(<span class="params">self, nums: [<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(nums):</span><br><span class="line">            <span class="keyword">if</span> nums[i] == i:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> nums[nums[i]] == nums[i]: </span><br><span class="line">              	<span class="keyword">return</span> nums[i]</span><br><span class="line">            nums[nums[i]], nums[i] = nums[i], nums[nums[i]]</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-53-i-在排序数组中查找数字-i"></span></h4><p>统计一个数字在<strong>排序数组</strong>中出现的<strong>次数</strong>。</p>
<ul>
<li>找到目标值「最后」出现的分割点，并「往前」进行统计</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="comment"># 求右边界</span></span><br><span class="line">        <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">            mid = (right + left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &gt; target:</span><br><span class="line">                right = mid - <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">        <span class="comment"># 最后要检查 right 越界的情况</span></span><br><span class="line">        <span class="keyword">if</span> right &lt; <span class="number">0</span> <span class="keyword">or</span> nums[right] != target:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> right &gt;= <span class="number">0</span> <span class="keyword">and</span> nums[right] == target :</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">            ans += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-53-ii-0~n-1中缺失的数字"></span></h4><p><strong>一个长度为n-1的递增排序数组</strong>中的所有数字都是唯一的，并且每个数字都在范围0～n-1之内。在范围0～n-1内的n个数字中有且只有一个数字不在该数组中，请找出这个数字。</p>
<ul>
<li>排序数组中的搜索问题，首先想到 <strong>二分法</strong> 解决。<ul>
<li>根据题意，数组可以按照以下规则划分为两部分<ul>
<li><strong>左子数组：</strong> nums[i] = i</li>
<li><strong>右子数组：</strong> nums[i] \ = i</li>
</ul>
</li>
</ul>
</li>
<li>缺失的数字等于 <strong>“右子数组的首位元素”</strong> 对应的索引；因此考虑使用二分法查找 “右子数组的首位元素” 。</li>
</ul>
<p><img src="https://pic.leetcode-cn.com/df7e04fbab0937ff74e5f29e958c7b1d531af066789ff363be5e1c8e75f17f56-Picture1.png" alt="Picture1.png" style="zoom:48%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">missingNumber</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        i, j = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> i &lt;= j:</span><br><span class="line">            m = (i + j) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[m] == m: </span><br><span class="line">              	i = m + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">              	j = m - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-04-二维数组中的查找"></span></h4><p>在一个 n <em> m 的<strong>二维数组</strong>中，每一行都按照<strong>从左到右递增</strong>的顺序排序，每一列都按照<em>*从上到下递增</em></em>的顺序排序。请完成一个高效的函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p>
<ul>
<li>我们将矩阵逆时针旋转 45° ，并将其转化为图形式，发现其类似于 <strong>二叉搜索树</strong></li>
</ul>
<p><img src="https://pic.leetcode-cn.com/6584ea93812d27112043d203ea90e4b0950117d45e0452d0c630fcb247fbc4af-Picture1.png" alt="Picture1.png" style="zoom:48%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findNumberIn2DArray</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        i, j = <span class="built_in">len</span>(matrix) - <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">and</span> j &lt; <span class="built_in">len</span>(matrix[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">if</span> matrix[i][j] &gt; target: </span><br><span class="line">              	i -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> matrix[i][j] &lt; target: </span><br><span class="line">              	j += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">              	<span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-11-旋转数组的最小数字"></span></h4><p><strong>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转</strong>。给你一个可能存在 重复 元素值的数组 numbers ，它原来是一个升序排列的数组，并按上述情形进行了一次旋转。请返回旋转数组的最小元素。例如，数组 [3,4,5,1,2] 为 [1,2,3,4,5] 的一次旋转，该数组的最小值为 1。  </p>
<ul>
<li>二分查找</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minArray</span>(<span class="params">self, numbers: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 无法判断左右， right -= 1 缩小范围</span></span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(numbers) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> numbers[mid] &lt; numbers[right]:</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">elif</span> numbers[mid] &gt; numbers[right]:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> numbers[left]</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-50-第一个只出现一次的字符"></span></h4><p>在字符串 s 中找出第一个只出现一次的字符。如果没有，返回一个单空格。 s 只包含小写字母。</p>
<ul>
<li>哈希表存储频数、索引</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">firstUniqChar</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        frequency = collections.Counter(s)</span><br><span class="line">        <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line">            <span class="keyword">if</span> frequency[ch] == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> ch</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>队列：使用了「<strong>延迟删除</strong>」这一技巧</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">firstUniqChar</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        position = <span class="built_in">dict</span>()</span><br><span class="line">        q = collections.deque()</span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> position:</span><br><span class="line">                position[ch] = i</span><br><span class="line">                q.append((s[i], i))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                position[ch] = -<span class="number">1</span></span><br><span class="line">                <span class="keyword">while</span> q <span class="keyword">and</span> position[q[<span class="number">0</span>][<span class="number">0</span>]] == -<span class="number">1</span>:</span><br><span class="line">                    q.popleft()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span> <span class="keyword">if</span> <span class="keyword">not</span> q <span class="keyword">else</span> q[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-51-数组中的逆序对"></span></h4><p>在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。</p>
<ul>
<li><code>from sortedcontainers import SortedList</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reversePairs</span>(<span class="params">self, nums</span>):</span><br><span class="line">        <span class="comment"># 逆序 + 二分插入</span></span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        q = []</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            s = nums[i]</span><br><span class="line">            j = bisect.bisect_left(q, s)</span><br><span class="line">            res += j</span><br><span class="line">            q[j:j] = [s] </span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reversePairs</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)       </span><br><span class="line">        sl = SortedList()</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):        <span class="comment"># 反向遍历</span></span><br><span class="line">            cnt = sl.bisect_left(nums[i])   <span class="comment"># 找到右边比当前值小的元素个数</span></span><br><span class="line">            ans += cnt                      <span class="comment"># 记入答案</span></span><br><span class="line">            sl.add(nums[i])                 <span class="comment"># 将当前值加入有序数组中</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="167-两数之和-ii-输入有序数组"></span></h4><p>给你一个下标从 1 开始的整数数组 numbers ，该<strong>数组已按非递减顺序排列  ，请你从数组中找出满足相加之和等于目标数 target 的两个数</strong>。如果设这两个数分别是 numbers[index1] 和 numbers[index2] ，则 1 &lt;= index1 &lt; index2 &lt;= numbers.length 。</p>
<ul>
<li>二分查找、双指针</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, numbers: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 二分查找</span></span><br><span class="line">        n = <span class="built_in">len</span>(numbers)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            left, right = i + <span class="number">1</span>, n - <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">                mid = (left + right) // <span class="number">2</span></span><br><span class="line">                <span class="keyword">if</span> numbers[mid] == target - numbers[i]:</span><br><span class="line">                    <span class="keyword">return</span> [i + <span class="number">1</span>, mid + <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">elif</span> numbers[mid] &gt; target - numbers[i]:</span><br><span class="line">                    right = mid - <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> [-<span class="number">1</span>, -<span class="number">1</span>] </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, numbers: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 双指针</span></span><br><span class="line">        low, high = <span class="number">0</span>, <span class="built_in">len</span>(numbers) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            total = numbers[low] + numbers[high]</span><br><span class="line">            <span class="keyword">if</span> total == target:</span><br><span class="line">                <span class="keyword">return</span> [low + <span class="number">1</span>, high + <span class="number">1</span>]</span><br><span class="line">            <span class="keyword">elif</span> total &lt; target:</span><br><span class="line">                low += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> [-<span class="number">1</span>, -<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h4><span id="15-三数之和"></span></h4><p>给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。</p>
<ul>
<li>排序 + 双指针 + 去重</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">threeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># 排序 + 双指针 + 去冲</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        nums.sort()</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> nums[i] &gt; <span class="number">0</span>: <span class="comment"># 剪枝加速</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i-<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            left, right = i + <span class="number">1</span>, n - <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> left &lt; right:</span><br><span class="line">                cur = nums[i] + nums[left] + nums[right]</span><br><span class="line">                <span class="keyword">if</span> cur == <span class="number">0</span>:</span><br><span class="line">                    ans.append([nums[i], nums[left], nums[right]])</span><br><span class="line">                    <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[right] == nums[right - <span class="number">1</span>]: right -= <span class="number">1</span></span><br><span class="line">                    right -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[left] == nums[left + <span class="number">1</span>]: left += <span class="number">1</span></span><br><span class="line">                    left += <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> cur &gt; <span class="number">0</span>:</span><br><span class="line">                    right -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-008-和大于等于-target-的最短子数组"></span></h4><p>给定一个含有 <code>n</code> 个<strong>正整数</strong>的数组和一个正整数 <code>target</code> <strong>。</strong>找出该数组中满足其和 <strong>≥ target</strong> 的<strong>长度最小的连续子数组</strong> [numsl, numsl+1, …, numsr-1, numsr] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。</p>
<ul>
<li>前缀和 + 二分查找 o(nlogn)、<strong>滑动窗口o(n)</strong>【连续，正整数】</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 【前缀和 + 二分查找 o(nlogn)】</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(nums) &lt; target:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        pre = <span class="built_in">list</span>(accumulate([<span class="number">0</span>] + nums)) <span class="comment"># itertools.accumulate()</span></span><br><span class="line">        ans = n + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>):</span><br><span class="line">            cur = target + pre[i]</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            left = bisect.bisect_left(pre, cur)</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span> </span><br><span class="line">            left = i + <span class="number">1</span></span><br><span class="line">            right = n</span><br><span class="line">            <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">                mid = (left + right) // <span class="number">2</span></span><br><span class="line">                <span class="keyword">if</span> pre[mid] &lt; cur:</span><br><span class="line">                    left = mid + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    right = mid - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> left != n+<span class="number">1</span>: </span><br><span class="line">                ans = <span class="built_in">min</span>(ans, left - i)</span><br><span class="line">        <span class="keyword">return</span> ans </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(nums) &lt; target:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        cur, res = <span class="number">0</span>, <span class="built_in">len</span>(nums) + <span class="number">1</span></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j, var <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            cur += var</span><br><span class="line">            <span class="keyword">while</span> i &lt;= j <span class="keyword">and</span> cur &gt;= target:</span><br><span class="line">                <span class="keyword">if</span> (t:= j - i + <span class="number">1</span>) &lt; res:</span><br><span class="line">                    res = t </span><br><span class="line">                cur -= nums[i]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-009-乘积小于-k-的子数组"></span></h4><p>给定一个正整数数组 <code>nums</code>和整数 <code>k</code> ，请找出该数组内乘积小于 <code>k</code> 的连续的子数组的个数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numSubarrayProductLessThanK</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 前缀和单调 + 二分查找</span></span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        k = math.log(k)</span><br><span class="line">        pre = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            pre.append(pre[-<span class="number">1</span>] + math.log(num))</span><br><span class="line">        cur = res =  <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(pre)):</span><br><span class="line">            cur = k + pre[i - <span class="number">1</span>]</span><br><span class="line">            j = bisect.bisect_left(pre, cur, lo = i)</span><br><span class="line">            res += j - i </span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numSubarrayProductLessThanK</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 滑动窗口</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        left, right = <span class="number">0</span>, n - <span class="number">1</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        i, cur =<span class="number">0</span>,  <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            cur *= num</span><br><span class="line">            <span class="keyword">while</span> i &lt;= j <span class="keyword">and</span> cur &gt;= k: <span class="comment"># 左跟着右</span></span><br><span class="line">                cur //= nums[i]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            res += j - i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-070-排序数组中只出现一次的数字"></span></h4><p>给定一个只包含整数的<strong>有序数组 nums</strong> ，每个元素都会出现两次，唯有一个数只会出现一次，请找出这个唯一的数字。<strong>你设计的解决方案必须满足 O(log n) 时间复杂度和 O(1) 空间复杂度。</strong></p>
<ul>
<li>二分查找 o(logN)</li>
</ul>
<p>利用按位异或的性质，可以得到mid 和相邻的数之间的如下关系，其中 ⊕ 是按位异或运算符：</p>
<ul>
<li><p>当mid 是偶数时，mid+1=mid⊕1；</p>
</li>
<li><p>当mid 是奇数时，mid−1=mid⊕1。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">singleNonDuplicate</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            mid = (low + high) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == nums[mid ^ <span class="number">1</span>]:</span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid</span><br><span class="line">        <span class="keyword">return</span> nums[low]</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-071-按权重生成随机数"></span></h4><p>给定一个正整数数组 w ，其中 w[i] 代表下标 i 的权重（下标从 0 开始），请写一个函数 pickIndex ，它可以随机地获取下标 i，选取下标 i 的概率与 w[i] 成正比。</p>
<p>例如，对于 w = [1, 3]，挑选下标 0 的概率为 1 / (1 + 3) = 0.25 （即，25%），而选取下标 1 的概率为 3 / (1 + 3) = 0.75（即，75%）。</p>
<p>也就是说，选取下标 i 的概率为 w[i] / sum(w) 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, w: <span class="type">List</span>[<span class="built_in">int</span>]</span>):</span><br><span class="line">        self.pre = <span class="built_in">list</span>(accumulate(w))</span><br><span class="line">        self.total = <span class="built_in">sum</span>(w)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pickIndex</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 前缀和 + 二分查找</span></span><br><span class="line">        r = random.randint(<span class="number">1</span>, self.total)</span><br><span class="line">        <span class="keyword">return</span> bisect_left(self.pre, r)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-073-狒狒吃香蕉"></span></h4><p><strong>狒狒喜欢吃香蕉。这里有 n 堆香蕉，第 i 堆中有 piles[i] 根香蕉。警卫已经离开了，将在 h 小时后回来。</strong>狒狒可以决定她吃香蕉的速度 k （单位：根/小时）。每个小时，她将会选择一堆香蕉，从中吃掉 k 根。如果这堆香蕉少于 k 根，她将吃掉这堆的所有香蕉，然后这一小时内不会再吃更多的香蕉，下一个小时才会开始吃另一堆的香蕉。<strong>返回她可以在 h 小时内吃掉所有香蕉的最小速度 k（k 为整数）。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minEatingSpeed</span>(<span class="params">self, piles: <span class="type">List</span>[<span class="built_in">int</span>], h: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        left, right = <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">sum</span>(piles) // h), <span class="built_in">max</span>(piles)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">k</span>):</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> pile <span class="keyword">in</span> piles:</span><br><span class="line">                count += ceil(pile / k)</span><br><span class="line">            <span class="keyword">return</span> count &gt; h</span><br><span class="line">        <span class="keyword">while</span> left &lt;= right: </span><br><span class="line">            mid = (left + right) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> check(mid): <span class="comment"># ... Ture # False(r) ...</span></span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>
<h4><span id="378-有序矩阵中第-k-小的元素"></span></h4><p>给你一个 n x n 矩阵 matrix ，其中<strong>每行和每列元素均按升序排序</strong>，<strong>找到矩阵中第 k 小的元素</strong>。<br>请注意，它是 排序后 的第 k 小元素，而不是第 k 个 不同 的元素。</p>
<p><strong>你必须找到一个内存复杂度优于 O(n2) 的解决方案。</strong></p>
<blockquote>
<p>  输入：matrix = [[1,5,9],[10,11,13],[12,13,15]], k = 8<br>  输出：13<br>  解释：矩阵中的元素为 [1,5,9,10,11,12,13,13,15]，第 8 小元素是 13</p>
</blockquote>
<ul>
<li><strong>优先队列</strong>：这个矩阵的每一行均为一个有序数组。问题即转化为从这 n<em>n</em> 个有序数组中找第 k<em>k</em> 大的数，可以想到利用归并排序的做法，归并到第 k<em>k</em> 个数即可停止。<strong>需要用小根堆维护，以优化时间复杂度。</strong></li>
<li><strong>二分查找</strong><ul>
<li>初始位置在 matrix [n - 1] [0]（即左下角）；</li>
<li>设当前位置为 matrix [i] [j]。若 matrix [i] [j] ≤mid，则将当前所在列的不大于 mid 的数的数量（即 i + 1i+1）累加到答案中，<strong>并向右移动，否则向上移动</strong>；</li>
<li><strong>可以线性计算对于任意一个 mid，矩阵中有多少数不大于它。这满足了二分查找的性质。</strong></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kthSmallest</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 优先队列</span></span><br><span class="line">        n = <span class="built_in">len</span>(matrix)</span><br><span class="line">        heap = [(matrix[i][<span class="number">0</span>], i, <span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        heapq.heapify(heap)</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k - <span class="number">1</span>):</span><br><span class="line">            num, x, y = heapq.heappop(heap)</span><br><span class="line">            <span class="keyword">if</span> y != n - <span class="number">1</span>:</span><br><span class="line">                heapq.heappush(heap, (matrix[x][y + <span class="number">1</span>], x, y + <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> heap[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kthSmallest</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 二分查找</span></span><br><span class="line">        n = <span class="built_in">len</span>(matrix)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">mid</span>):</span><br><span class="line">            i, j = n - <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">and</span> j &lt; n:</span><br><span class="line">                <span class="keyword">if</span> matrix[i][j] &lt;= mid:</span><br><span class="line">                    num += i + <span class="number">1</span></span><br><span class="line">                    j += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    i -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> num &lt; k </span><br><span class="line">        <span class="comment"># 【第一个False】</span></span><br><span class="line">        left, right = matrix[<span class="number">0</span>][<span class="number">0</span>], matrix[-<span class="number">1</span>][-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">            mid = (left + right) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> check(mid):</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（5）双指针</title>
    <url>/posts/2WGWZ88/</url>
    <content><![CDATA[<h2><span id="双指针">双指针</span></h2><h4><span id="剑指-offer-48-最长不含重复字符的子字符串"></span></h4><p>请从字符串中找出一个最长的<strong>不包含重复字符的子字符串</strong>，<strong>计算该最长子字符串的长度</strong>。</p>
<ul>
<li><strong>双指针、队列</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># 回溯 + set</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        que = collections.deque()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        que.append(s[<span class="number">0</span>])</span><br><span class="line">        res = <span class="number">1</span></span><br><span class="line">        tmp = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> s[i] <span class="keyword">not</span> <span class="keyword">in</span> que:</span><br><span class="line">                tmp += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">while</span> que <span class="keyword">and</span> que.popleft() != s[i]:</span><br><span class="line">                    tmp -= <span class="number">1</span></span><br><span class="line">            que.append(s[i])</span><br><span class="line">            res = <span class="built_in">max</span>(res, tmp)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">      	<span class="comment"># 双指针 + 哈希表</span></span><br><span class="line">        dic, res, i = &#123;&#125;, <span class="number">0</span>, -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            <span class="keyword">if</span> s[j] <span class="keyword">in</span> dic:</span><br><span class="line">                i = <span class="built_in">max</span>(dic[s[j]], i) <span class="comment"># 更新左指针 i</span></span><br><span class="line">            dic[s[j]] = j <span class="comment"># 哈希表记录</span></span><br><span class="line">            res = <span class="built_in">max</span>(res, j - i) <span class="comment"># 更新结果</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-18-删除链表的节点"></span></h4><p>给定单向链表的头指针和一个要删除的节点的值，定义一个<strong>函数删除该节点</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteNode</span>(<span class="params">self, head: ListNode, val: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        cur = dummy = ListNode(<span class="number">0</span>, head)</span><br><span class="line">        <span class="keyword">while</span> cur.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">if</span> cur.<span class="built_in">next</span>.val == val:</span><br><span class="line">                cur.<span class="built_in">next</span> = cur.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur = cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-22-链表中倒数第k个节点"></span></h4><p>输入一个链表，输出该链表中倒数第k个节点。为了符合大多数人的习惯，本题从1开始计数，即链表的尾节点是倒数第1个节点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getKthFromEnd</span>(<span class="params">self, head: ListNode, k: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">        fast, slow = head, head</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            fast = fast.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> fast:</span><br><span class="line">            fast = fast.<span class="built_in">next</span></span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> slow</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-25-合并两个排序的链表"></span></h4><p>输入两个递增排序的链表，<strong>合并这两个链表并使新链表中的节点仍然是递增排序的。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mergeTwoLists</span>(<span class="params">self, l1: ListNode, l2: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        cur = dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">and</span> l2:</span><br><span class="line">            <span class="keyword">if</span> l1.val &gt; l2.val:</span><br><span class="line">                cur.<span class="built_in">next</span>, l2 = l2, l2.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur.<span class="built_in">next</span>, l1 = l1, l1.<span class="built_in">next</span></span><br><span class="line">            cur = cur.<span class="built_in">next</span></span><br><span class="line">        cur.<span class="built_in">next</span> = l1 <span class="keyword">if</span> l1 <span class="keyword">else</span> l2</span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-52-两个链表的第一个公共节点"></span></h4><p>输入两个<strong>链表</strong>，找出它们的<strong>第一个公共节点</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getIntersectionNode</span>(<span class="params">self, headA: ListNode, headB: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        p1, p2 = headA, headB</span><br><span class="line">        <span class="keyword">while</span> p1 != p2:</span><br><span class="line">            p1 = p1.<span class="built_in">next</span> <span class="keyword">if</span> p1 <span class="keyword">else</span> headB</span><br><span class="line">            p2 = p2.<span class="built_in">next</span> <span class="keyword">if</span> p2 <span class="keyword">else</span> headA</span><br><span class="line">        <span class="keyword">return</span> p1</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-21-调整数组顺序使奇数位于偶数前面"></span></h4><p>输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有奇数在数组的前半部分，所有偶数在数组的后半部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">exchange</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)-<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            <span class="keyword">while</span> nums[left]%<span class="number">2</span> == <span class="number">1</span> <span class="keyword">and</span> left &lt; right: </span><br><span class="line">              	left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> nums[right]%<span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> left &lt; right: </span><br><span class="line">              	right -= <span class="number">1</span></span><br><span class="line">            nums[left], nums[right] = nums[right], nums[left]</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">exchange</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">    		<span class="keyword">return</span> <span class="built_in">sorted</span>(nums, key = <span class="keyword">lambda</span> x: x%<span class="number">2</span>, reverse = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-57-和为s的两个数字"></span></h4><p>输入一个递增排序的数组和一个数字s，在数组中查找两个数，使得它们的和正好是s。如果有多对数字的和等于s，则输出任意一对即可。</p>
<h5><span id="解题思路">解题思路：</span></h5><ul>
<li><p>利用 HashMap 可以通过遍历数组找到数字组合，时间和空间复杂度均为 O(N)；</p>
</li>
<li><p>注意本题的 nums 是<strong>排序数组</strong> ，因此可使用 双指针法 将空间复杂度降低至 O(1)。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        ndict = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> target-nums[i] <span class="keyword">in</span> ndict:</span><br><span class="line">                <span class="keyword">return</span> [target-nums[i], nums[i]]</span><br><span class="line">            ndict[nums[i]] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)-<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            s = nums[left] + nums[right]</span><br><span class="line">            <span class="keyword">if</span> s &gt; target:</span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> s &lt; target:</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> [nums[left], nums[right]]</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-58-i-翻转单词顺序"></span></h4><p>输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。为简单起见，标点符号和普通字母一样处理。例如输入字符串”I am a student. “，则输出”student. a am I”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseWords</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot; &quot;</span>.join(<span class="built_in">reversed</span>(s.strip().split()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseWords</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        s = s.strip() <span class="comment"># 删除首尾空格</span></span><br><span class="line">        i = j = <span class="built_in">len</span>(s) - <span class="number">1</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">while</span> i &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">and</span> s[i] != <span class="string">&#x27; &#x27;</span>: i -= <span class="number">1</span> <span class="comment"># 搜索首个空格</span></span><br><span class="line">            res.append(s[i + <span class="number">1</span>: j + <span class="number">1</span>]) <span class="comment"># 添加单词</span></span><br><span class="line">            <span class="keyword">while</span> s[i] == <span class="string">&#x27; &#x27;</span>: i -= <span class="number">1</span> <span class="comment"># 跳过单词间空格</span></span><br><span class="line">            j = i <span class="comment"># j 指向下个单词的尾字符</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(res) <span class="comment"># 拼接并返回</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（6）栈</title>
    <url>/posts/3C0HSVJ/</url>
    <content><![CDATA[<h2><span id="栈">栈</span></h2><h4><span id="剑指-offer-30-包含min函数的栈"></span></h4><p>定义栈的数据结构，请在该类型中实现一个能够<strong>得到栈的最小元素的 min 函数在该栈中</strong>，调用 min、push 及 pop 的时间复杂度都是 <strong>O(1)</strong>。</p>
<ul>
<li>self.smin = [] <strong>存储最小栈元素</strong>，记录当前最小值是什么。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MinStack</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.sdata = []</span><br><span class="line">        self.smin = []</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, x: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.sdata.append(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.smin <span class="keyword">or</span> self.smin[-<span class="number">1</span>] &gt;= x:</span><br><span class="line">            self.smin.append(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pop</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> self.sdata.pop() == self.smin[-<span class="number">1</span>]:</span><br><span class="line">            self.smin.pop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">top</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self.sdata[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">min</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self.smin[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h4><span id="150-逆波兰表达式求值"></span></h4><p>有效的算符包括 +、-、<em>、/ 。每个运算对象可以是整数，也可以是另一个逆波兰表达式。<em>*注意 两个整数之间的除法只保留整数部分。</em></em></p>
<p>示例 1：</p>
<p>输入：tokens = [“2”,”1”,”+”,”3”,”<em>“]<br>输出：9<br>解释：该算式转化为常见的中缀算术表达式为：((2 + 1) </em> 3) = 9</p>
<ul>
<li><strong><font color="red"> 函数字典</font></strong>，<strong>isdigit只能判断正整数</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evalRPN</span>(<span class="params">self, tokens: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(tokens)</span><br><span class="line">        op = &#123;</span><br><span class="line">                <span class="string">&#x27;+&#x27;</span>: add,</span><br><span class="line">                <span class="string">&#x27;-&#x27;</span>: sub,</span><br><span class="line">                <span class="string">&#x27;*&#x27;</span>: mul,</span><br><span class="line">                <span class="string">&#x27;/&#x27;</span>: <span class="keyword">lambda</span> x, y: <span class="built_in">int</span>(x / y)</span><br><span class="line">        &#125;</span><br><span class="line">        stack = []</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> tokens:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                num = <span class="built_in">int</span>(t)</span><br><span class="line">            <span class="keyword">except</span> ValueError:</span><br><span class="line">                b = stack.pop()</span><br><span class="line">                a = stack.pop()</span><br><span class="line">                num = op[t](a, b)</span><br><span class="line">            <span class="keyword">finally</span>:</span><br><span class="line">                stack.append(num)</span><br><span class="line">        <span class="keyword">return</span> stack[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h2><span id="单调栈">单调栈</span></h2><h4><span id="剑指-offer-ii-039-直方图最大矩形面积"></span></h4><p>给定非负整数数组 <code>heights</code> ，数组中的数字用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 <code>1</code> 。求在该柱状图中，能够勾勒出来的矩形的最大面积。</p>
<p><strong>示例 1:</strong></p>
<p><img src="https://assets.leetcode.com/uploads/2021/01/04/histogram.jpg" alt="img"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">输入：heights = [<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">输出：<span class="number">10</span></span><br><span class="line">解释：最大的矩形为图中红色区域，面积为 <span class="number">10</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">largestRectangleArea</span>(<span class="params">self, heights: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        heights = [<span class="number">0</span>] + heights + [<span class="number">0</span>] <span class="comment"># 哨兵</span></span><br><span class="line">        stack, res = [<span class="number">0</span>], <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(heights)):</span><br><span class="line">            <span class="keyword">while</span> stack <span class="keyword">and</span> heights[i] &lt;= heights[stack[-<span class="number">1</span>]]:</span><br><span class="line">                tmp = stack.pop()</span><br><span class="line">                <span class="keyword">if</span> stack:</span><br><span class="line">                    weith = i - stack[-<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> (t := heights[tmp] * weith) &gt; res:</span><br><span class="line">                        res = t</span><br><span class="line">            stack.append(i)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-040-矩阵中最大的矩形"></span></h4><p>给定一个由 <code>0</code> 和 <code>1</code> 组成的矩阵 <code>matrix</code> ，找出只包含 <code>1</code> 的最大矩形，并返回其面积。</p>
<p><strong>注意：</strong>此题 <code>matrix</code> 输入格式为一维 <code>01</code> 字符串数组。</p>
<p><strong>示例 1：</strong></p>
<p><img src="https://assets.leetcode.com/uploads/2020/09/14/maximal.jpg" alt="img"></p>
<figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line">输入：<span class="built_in">matrix</span> = [<span class="string">&quot;10100&quot;</span>,<span class="string">&quot;10111&quot;</span>,<span class="string">&quot;11111&quot;</span>,<span class="string">&quot;10010&quot;</span>]</span><br><span class="line">输出：<span class="number">6</span></span><br><span class="line">解释：最大矩形如上图所示。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maximalRectangle</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">maximalhist</span>(<span class="params">heights</span>):</span><br><span class="line">            heights = [<span class="number">0</span>] + heights + [<span class="number">0</span>]</span><br><span class="line">            res, stack = <span class="number">0</span>, [<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(heights)):</span><br><span class="line">                <span class="keyword">while</span> stack <span class="keyword">and</span> heights[i] &lt;= heights[stack[-<span class="number">1</span>]]:</span><br><span class="line">                    tmp = stack.pop()</span><br><span class="line">                    <span class="keyword">if</span> stack:</span><br><span class="line">                        weith = i - stack[-<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">                        <span class="keyword">if</span> (t := heights[tmp] * weith) &gt; res:</span><br><span class="line">                            res = t </span><br><span class="line">                stack.append(i)</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> matrix:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        m, n = <span class="built_in">len</span>(matrix), <span class="built_in">len</span>(matrix[<span class="number">0</span>])</span><br><span class="line">        pre = [<span class="number">0</span>] * n</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="comment"># if else 缩减</span></span><br><span class="line">                <span class="keyword">if</span> matrix[i][j] != <span class="string">&#x27;0&#x27;</span>:</span><br><span class="line">                    pre[j] += <span class="built_in">int</span>(matrix[i][j])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    pre[j] = <span class="number">0</span></span><br><span class="line">            ans = <span class="built_in">max</span>(ans, maximalhist(pre))</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-037-小行星碰撞"></span></h4><p>给定一个整数数组 asteroids，表示在同一行的小行星。对于数组中的每一个元素，其绝对值表示小行星的大小，正负表示小行星的移动方向（正表示向右移动，负表示向左移动）。每一颗小行星以相同的速度移动。<strong>找出碰撞后剩下的所有小行星。碰撞规则：两个行星相互碰撞，较小的行星会爆炸。如果两颗行星大小相同，则两颗行星都会爆炸。两颗移动方向相同的行星，永远不会发生碰撞。</strong></p>
<blockquote>
<p>  <strong>示例 1：</strong></p>
<p>  输入：asteroids = [5,10,-5]<br>  输出：[5,10]<br>  解释：10 和 -5 碰撞后只剩下 10 。 5 和 10 永远不会发生碰撞。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">asteroidCollision</span>(<span class="params">self, asteroids: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        n = <span class="built_in">len</span>(asteroids)</span><br><span class="line">        stack = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">while</span> stack <span class="keyword">and</span> asteroids[stack[-<span class="number">1</span>]] &gt; <span class="number">0</span> <span class="keyword">and</span> asteroids[i] &lt; <span class="number">0</span>:</span><br><span class="line">                a = asteroids[stack[-<span class="number">1</span>]]</span><br><span class="line">                b = asteroids[i]</span><br><span class="line">                <span class="keyword">if</span> a + b &lt; <span class="number">0</span>:</span><br><span class="line">                    stack.pop()</span><br><span class="line">                <span class="keyword">elif</span> a + b &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">elif</span> a + b == <span class="number">0</span>:</span><br><span class="line">                    stack.pop()</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stack.append(i)</span><br><span class="line">        <span class="keyword">return</span> [asteroids[i] <span class="keyword">for</span> i <span class="keyword">in</span> stack]</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-038-每日温度"></span></h4><p>给定一个整数数组 temperatures ，表示每天的温度，<strong>返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后</strong>。如果气温在这之后都不会升高，请在该位置用 0 来代替。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dailyTemperatures</span>(<span class="params">self, temperatures: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        n = <span class="built_in">len</span>(temperatures)</span><br><span class="line">        stack, ans = [<span class="number">0</span>], [<span class="number">0</span>] * n</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">while</span> stack <span class="keyword">and</span> temperatures[stack[-<span class="number">1</span>]] &lt; temperatures[i]:</span><br><span class="line">                tmp = stack.pop()</span><br><span class="line">                ans[tmp] = i - tmp</span><br><span class="line">            stack.append(i)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（7）队列</title>
    <url>/posts/33W7JW8/</url>
    <content><![CDATA[<h2><span id="队列">队列</span></h2><h4><span id="剑指-offer-09-用两个栈实现队列"></span></h4><p>用两个栈实现一个队列。队列的声明如下，<strong>请实现它的两个函数 appendTail 和 deleteHead</strong> ，<strong>分别完成在队列尾部插入整数和在队列头部删除整数的功能</strong>。(若队列中没有元素，deleteHead 操作返回 -1 )</p>
<ul>
<li>初始化<strong>两个栈</strong>（s1输入栈、s2输出栈）</li>
<li><strong>将输入栈导出到输出栈</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CQueue</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.s1 = [] <span class="comment"># 输入栈</span></span><br><span class="line">        self.s2 = [] <span class="comment"># 输出栈</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">appendTail</span>(<span class="params">self, value: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.s1.append(value)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteHead</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> self.s2:</span><br><span class="line">            <span class="keyword">return</span> self.s2.pop()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.s1:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> self.s1: <span class="comment"># 清空输入栈</span></span><br><span class="line">            self.s2.append(self.s1.pop())</span><br><span class="line">        <span class="keyword">return</span> self.s2.pop()</span><br></pre></td></tr></table></figure>
<h2><span id="优先队列">优先队列</span></h2><h4><span id="239-滑动窗口最大值"></span></h4><p>给你一个整数数组 <code>nums</code>，有一个大小为 <code>k</code> 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 <code>k</code> 个数字。滑动窗口每次只向右移动一位。</p>
<blockquote>
<p>  示例 1：</p>
<p>  输入：nums = [1,3,-1,-3,5,3,6,7], k = 3<br>  输出：[3,3,5,5,6,7]<br>  解释：<br>  滑动窗口的位置                最大值</p>
<hr>
<p>  [1  3  -1] -3  5  3  6  7       3<br>   1 [3  -1  -3] 5  3  6  7       3<br>   1  3 [-1  -3  5] 3  6  7       5<br>   1  3  -1 [-3  5  3] 6  7       5<br>   1  3  -1  -3 [5  3  6] 7       6<br>   1  3  -1  -3  5 [3  6  7]      7</p>
</blockquote>
<ul>
<li><strong>单调队列</strong>  0(1) 线性时间算出滑动窗口最大值 </li>
<li><strong>优先队列</strong>（<strong>大根堆</strong>） 时间复杂度：O(nlogn) 空间复杂度：O(n) 【<strong>注意 Python 默认的优先队列是小根堆</strong>】</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxSlidingWindow</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 单调队列 0(1) 线性时间算出滑动窗口最大值 </span></span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        q = collections.deque() <span class="comment"># 保存最大索引</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="keyword">while</span> q <span class="keyword">and</span> nums[i] &gt;= nums[q[-<span class="number">1</span>]]:</span><br><span class="line">                <span class="comment"># 较小的备用,交大的删除前面小的</span></span><br><span class="line">                q.pop()</span><br><span class="line">            q.append(i)</span><br><span class="line">        ans = [nums[q[<span class="number">0</span>]]] <span class="comment"># 第一个窗口初始化</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k, n):</span><br><span class="line">            <span class="keyword">while</span> q <span class="keyword">and</span> nums[i] &gt;= nums[q[-<span class="number">1</span>]]:</span><br><span class="line">                <span class="comment"># 较小的备用,交大的删除前面小的</span></span><br><span class="line">                q.pop()</span><br><span class="line">            q.append(i)</span><br><span class="line">            <span class="keyword">while</span> q[<span class="number">0</span>] &lt;= i-k: <span class="comment"># 最大的退役了</span></span><br><span class="line">                q.popleft()</span><br><span class="line">            ans.append(nums[q[<span class="number">0</span>]])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxSlidingWindow</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 优先队列（大根堆） 时间复杂度：O(nlogn) 空间复杂度：O(n)</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="comment"># 注意 Python 默认的优先队列是小根堆</span></span><br><span class="line">        q = [(-nums[i], i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k)]</span><br><span class="line">        heapq.heapify(q) <span class="comment">#list2最小堆</span></span><br><span class="line">        ans = [-q[<span class="number">0</span>][<span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k, n):</span><br><span class="line">            heapq.heappush(q, (-nums[i], i))</span><br><span class="line">            <span class="keyword">while</span> q[<span class="number">0</span>][<span class="number">1</span>] &lt;= i - k: <span class="comment"># 弹出历史最大值的索引</span></span><br><span class="line">                heapq.heappop(q)</span><br><span class="line">            ans.append(-q[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-041-滑动窗口的平均值"></span></h4><p>给定一个整数数据流和一个窗口大小，根据该滑动窗口的大小，计算滑动窗口里所有数字的平均值。</p>
<p><strong>实现 MovingAverage 类：</strong></p>
<ul>
<li>MovingAverage(int size) 用窗口大小 size 初始化对象。</li>
<li>double next(int val) 成员函数 next 每次调用的时候都会往滑动窗口增加一个整数，请计算并返回数据流中最后 size 个值的移动平均值，即滑动窗口里所有数字的平均值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MovingAverage</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize your data structure here.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.nums = deque()</span><br><span class="line">        self.total = <span class="number">0</span></span><br><span class="line">        self.size = size</span><br><span class="line">        self.<span class="built_in">len</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next</span>(<span class="params">self, val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">if</span> self.<span class="built_in">len</span> &gt;= self.size:</span><br><span class="line">            self.total -= self.nums.popleft()</span><br><span class="line">            self.<span class="built_in">len</span> -= <span class="number">1</span></span><br><span class="line">        self.nums.append(val)</span><br><span class="line">        self.total += val</span><br><span class="line">        self.<span class="built_in">len</span> += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.total / self.<span class="built_in">len</span></span><br></pre></td></tr></table></figure>
<h4><span id="剑指-offer-ii-042-最近请求次数"></span></h4><p>写一个 RecentCounter 类来计算特定时间范围内最近的请求。</p>
<p><strong>请实现 RecentCounter 类：</strong></p>
<ul>
<li>RecentCounter() 初始化计数器，请求数为 0 。</li>
<li><p>int ping(int t) 在时间 t 添加一个新请求，其中 t 表示以毫秒为单位的某个时间，并返回过去 3000 毫秒内发生的所有请求数（包括新请求）。确切地说，返回在 [t-3000, t] 内发生的请求数。</p>
</li>
<li><p><strong>优先队列、二分查找</strong></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RecentCounter</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    def __init__(self):</span></span><br><span class="line"><span class="string">        self.q = deque()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def ping(self, t: int) -&gt; int:</span></span><br><span class="line"><span class="string">        while self.q and (self.q[0] + 3000) &lt; t:</span></span><br><span class="line"><span class="string">            self.q.popleft()</span></span><br><span class="line"><span class="string">        self.q.append(t)</span></span><br><span class="line"><span class="string">        return len(self.q)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.ping_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ping</span>(<span class="params">self, t: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        self.ping_list.append(t)</span><br><span class="line">        left = bisect_left(self.ping_list, t-<span class="number">3000</span>) </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.ping_list) - left</span><br></pre></td></tr></table></figure>
<h2><span id="单调队列">单调队列</span></h2>]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（8）BFS&amp;DFS</title>
    <url>/posts/1TJZXJZ/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>算法长征（8）贪心</title>
    <url>/posts/2T8VZBZ/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>工程设计-DaaS-Client</title>
    <url>/posts/3BJB9WW/</url>
    <content><![CDATA[<h2><span id="how-自动部署开源ai模型到生产环境">HOW 自动部署开源AI模型到生产环境？</span></h2><blockquote>
<p>  DaaS-Client、Sklearn、XGBoost、LightGBM、和PySpark<em>关注 AI/ML 模型上线、模型部署-程序员宅基地</em></p>
<p>  DaaS-Client：<a href="https://github.com/autodeployai/daas-client">https://github.com/autodeployai/daas-client</a></p>
<p>3万字长文 PySpark入门级学习教程，框架思维:<a href="https://zhuanlan.zhihu.com/p/395431025">https://zhuanlan.zhihu.com/p/395431025</a></p>
<p><strong>DMatrix 格式</strong> 在xgboost当中运行速度更快，性能更好。</p>
</blockquote>
<h3><span id="一-背景介绍">一、背景介绍</span></h3><p>AI的广泛应用是由AI在开源技术的进步推动的，利用功能强大的开源模型库，数据科学家们可以很容易的训练一个性能不错的模型。但是因为模型生产环境和开发环境的不同，涉及到不同角色人员：模型训练是数据科学家和数据分析师的工作，但是模型部署是开发和运维工程师的事情，导致模型上线部署却不是那么容易。</p>
<p><strong>DaaS（Deployment-as-a-Service）是AutoDeployAI公司推出的基于Kubernetes的AI模型自动部署系统，提供一键式自动部署开源AI模型生成REST API，以方便在生产环境中调用</strong>。下面，我们主要演示在DaaS中如何部署经典机器学习模型，包括<strong>Scikit-learn、XGBoost、LightGBM、和PySpark ML Pipelines</strong>。关于深度学习模型的部署，会在下一章中介绍。</p>
<h3><span id="二-部署准备">二、部署准备</span></h3><p>我们使用DaaS提供的Python客户端（DaaS-Client）来部署模型，对于XGBoost和LightGBM，我们同样使用它们的Python API来作模型训练。在训练和部署模型之前，我们需要完成以下操作。</p>
<ul>
<li><strong>安装Python DaaS-Client</strong></li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install --upgrade git+https://github.com/autodeployai/daas-client.git</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>初始化DaasClient</strong>。使用DaaS系统的URL、账户、密码登陆系统，文本使用的DaaS演示系统安装在本地的Minikube上。完整Jupyter Notebook，请参考：<a href="https://github.com/aipredict/ai-deployment/blob/master/deploy-ai-models-in-daas/deploy-sklearn-xgboost-lightgbm-pyspark.ipynb">deploy-sklearn-xgboost-lightgbm-pyspark.ipynb</a></li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">from daas_client import DaasClient</span><br><span class="line"></span><br><span class="line">client = DaasClient(&#x27;https://192.168.64.3:30931&#x27;, &#x27;username&#x27;, &#x27;password&#x27;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>创建项目</strong>。DaaS使用项目管理用户不同的分析任务，一个项目中可以包含用户的各种分析资产：模型、部署、程序脚本、数据、数据源等。项目创建成功后，设置为当前活动项目，发布的模型和创建的部署都会存储在该项目下。<code>create_project</code>函数接受三个参数：<ul>
<li><strong>项目名称</strong>：可以是任意有效的Linux文件目录名。</li>
<li><strong>项目路由</strong>：使用在部署的REST URL中来唯一表示当前项目，只能是小写英文字符(a-z)，数字(0-9)和中横线<code>-</code>，并且<code>-</code>不能在开头和结尾处。</li>
<li><strong>项目说明</strong>（可选）：可以是任意字符。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">project = <span class="string">&#x27;部署测试&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> client.project_exists(project):</span><br><span class="line">    client.create_project(project, <span class="string">&#x27;deployment-test&#x27;</span>, <span class="string">&#x27;部署测试项目&#x27;</span>)</span><br><span class="line">client.set_project(project)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>初始化数据</strong>。我们使用流行的分类数据集<code>iris</code>来训练不同的模型，并且把数据分割为训练数据集和测试数据集以方便后续使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">seed = <span class="number">123456</span></span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris_target_name = <span class="string">&#x27;Species&#x27;</span></span><br><span class="line">iris_feature_names = iris.feature_names</span><br><span class="line">iris_df = pd.DataFrame(iris.data, columns=iris_feature_names)</span><br><span class="line">iris_df[iris_target_name] = iris.target</span><br><span class="line"></span><br><span class="line">X, y = iris_df[iris_feature_names], iris_df[iris_target_name]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=seed)    </span><br></pre></td></tr></table></figure>
<ul>
<li><strong>模型部署流程。主要包含以下几步</strong>：<ul>
<li><strong>训练模型</strong>。使用模型库提供的API，在<code>iris</code>数据集上训练模型。</li>
<li><strong>发布模型</strong>。调用<code>publish</code>函数发布模型到DaaS系统。</li>
<li><strong>测试模型</strong>（可选）。调用<code>test</code>函数获取测试API信息，可以使用任意的REST客户端程序测试模型在DaaS中是否工作正常，使用的是DaaS系统模型测试API。第一次执行<code>test</code>会比较慢，因为DaaS系统需要启动测试运行时环境。</li>
<li><strong>部署模型</strong>。发布成功后，调用<code>deploy</code>函数部署部署模型。可以使用任意的REST客户端程序测试模型部署，使用的是DaaS系统正式部署API。</li>
</ul>
</li>
</ul>
<h3><span id="三-部署scikit-learn模型">三、部署Scikit-learn模型</span></h3><ul>
<li><strong>训练一个Scikit-learn分类模型</strong>：SVC</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">model = SVC(probability=<span class="literal">True</span>, random_state=seed)</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>发布Scikit-learn模型</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">publish_resp = client.publish(model,</span><br><span class="line">                            name=<span class="string">&#x27;iris&#x27;</span>,</span><br><span class="line">                            mining_function=<span class="string">&#x27;classification&#x27;</span>,</span><br><span class="line">                            X_test=X_test,</span><br><span class="line">                            y_test=y_test,</span><br><span class="line">                            description=<span class="string">&#x27;A SVC model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>test</strong>函数必须要指定前两个参数，第一个<strong>model</strong>是训练的模型对象，第二个是模型名称，其余是可选参数：</p>
<ul>
<li><strong>mining_function</strong>：指定挖掘功能，可以指定为<code>regression</code>（回归）、<code>classification</code>（分类）、和<code>clustering</code>（聚类）。</li>
<li><strong>X_test和y_test</strong>：指定测试训练集，发布时计算模型评估指标，比如针对分类模型，计算正确率（Accuracy），对于回归模型，计算可释方差（explained Variance）。</li>
<li><strong>data_test</strong>： 同样是指定测试训练集，但是该参数用在Spark模型上，非Spark模型通过<code>X_test</code>和<code>y_test</code>指定。</li>
<li><strong>description</strong>：模型描述。</li>
<li><strong>params</strong>：记录模型参数设置。</li>
</ul>
<p><strong>publish_resp</strong>是一个字典类型的结果，记录了模型名称，和发布的模型版本。该模型是<code>iris</code>模型的第一个版本。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span>&#x27;model_name&#x27;<span class="punctuation">:</span> &#x27;iris&#x27;<span class="punctuation">,</span> &#x27;model_version&#x27;<span class="punctuation">:</span> &#x27;<span class="number">1</span>&#x27;<span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>测试Scikit-learn模型</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_resp = client.test(publish_resp[<span class="string">&#x27;model_name&#x27;</span>],</span><br><span class="line">                        model_version=publish_resp[<span class="string">&#x27;model_version&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><code>test_resp</code>是一个字典类型的结果，记录了测试REST API信息。如下，其中<code>access_token</code>是访问令牌，一个长字符串，这里没有显示出来。<code>endpoint_url</code>指定测试REST API地址，<code>payload</code>提供了测试当前模型需要输入的请求正文格式。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;access_token&#x27;<span class="punctuation">:</span> &#x27;A-LONG-STRING-OF-BEARER-TOKEN-USED-IN-HTTP-HEADER-AUTHORIZATION&#x27;<span class="punctuation">,</span></span><br><span class="line">			&#x27;endpoint_url&#x27;<span class="punctuation">:</span> &#x27;https<span class="punctuation">:</span><span class="comment">//192.168.64.3:30931/api/v1/test/deployment-test/daas-python37-faas/test&#x27;,</span></span><br><span class="line">			&#x27;payload&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      				&#x27;args&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">              			&#x27;X&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span> &#x27;petal length (cm)&#x27;<span class="punctuation">:</span> <span class="number">1.5</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;petal width (cm)&#x27;<span class="punctuation">:</span> <span class="number">0.4</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;sepal length (cm)&#x27;<span class="punctuation">:</span> <span class="number">5.7</span><span class="punctuation">,</span></span><br><span class="line">                            &#x27;sepal width (cm)&#x27;<span class="punctuation">:</span> <span class="number">4.4</span></span><br><span class="line">                          <span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">               &#x27;model_name&#x27;<span class="punctuation">:</span> &#x27;iris&#x27;<span class="punctuation">,</span></span><br><span class="line">               &#x27;model_version&#x27;<span class="punctuation">:</span> &#x27;<span class="number">1</span>&#x27;<span class="punctuation">&#125;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>使用requests调用测试API，这里我们直接使用<strong>test_resp</strong>返回的测试payload，您也可以使用自定义的数据<code>X</code>，但是参数<code>model_name</code>和<code>model_version</code>必须使用上面输出的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = requests.post(test_resp[<span class="string">&#x27;endpoint_url&#x27;</span>],</span><br><span class="line">                        headers=&#123;</span><br><span class="line">      <span class="string">&#x27;Authorization&#x27;</span>: <span class="string">&#x27;Bearer &#123;token&#125;&#x27;</span>.<span class="built_in">format</span>(token=test_resp[<span class="string">&#x27;access_token&#x27;</span>])&#125;,</span><br><span class="line">                        json=test_resp[<span class="string">&#x27;payload&#x27;</span>],</span><br><span class="line">                        verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>返回结果，不同于正式部署API，除了预测结果，测试API会同时返回标准控制台输出和标准错误输出内容，以方便用户碰到错误时，查看相关信息。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"># response.json()</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;result&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;PredictedValue&#x27;<span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">            &#x27;Probabilities&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">0.8977133931668801</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="number">0.05476023239878367</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="number">0.047526374434336216</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">			&#x27;stderr&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">			&#x27;stdout&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h3><span id="四-部署pyspark模型">四、部署PySpark模型</span></h3><ul>
<li><strong>训练一个PySpark分类模型</strong>：RandomForestClassifier。PySpark模型必须是一个<code>PipelineModel</code>，也就是说必须使用Pipeline来建立模型，哪怕只有一个Pipeline节点。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.getOrCreate()</span><br><span class="line">df = spark.createDataFrame(iris_df)</span><br><span class="line"></span><br><span class="line">df_train, df_test = df.randomSplit([<span class="number">0.7</span>, <span class="number">0.3</span>], seed=seed)</span><br><span class="line">assembler = VectorAssembler(inputCols=iris_feature_names,</span><br><span class="line">                            outputCol=<span class="string">&#x27;features&#x27;</span>)</span><br><span class="line"></span><br><span class="line">rf = RandomForestClassifier(seed=seed).setLabelCol(iris_target_name)</span><br><span class="line">pipe = Pipeline(stages=[assembler, rf])</span><br><span class="line">model = pipe.fit(df_train)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>发布PySpark模型</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">publish_resp = client.publish(model,</span><br><span class="line">                            name=<span class="string">&#x27;iris&#x27;</span>,</span><br><span class="line">                            mining_function=<span class="string">&#x27;classification&#x27;</span>,</span><br><span class="line">                            data_test=df_test,</span><br><span class="line">                            description=<span class="string">&#x27;A RandomForestClassifier of Spark model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="五-模型部署管理">五、模型部署管理</span></h3><p>打开浏览器，登陆DaaS管理系统。进入项目<code>部署测试</code>，切换到<code>模型</code>标签页，有一个<code>iris</code>模型，最新版本是<code>v4</code>，类型是<code>Spark</code>即我们最后发布的模型。<br><img src="https://img-blog.csdnimg.cn/20190916185003145.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTYyNjkwMQ==,size_16,color_FFFFFF,t_70" alt="Daas-models" style="zoom: 33%;"></p>
<p>点击模型，进入模型主页（概述）。当前<code>v4</code>是一个Spark Pipeline模型，正确率是94.23%，并且显示了<code>iris</code>不同版本正确率历史图。下面罗列了模型的输入和输出变量，以及评估结果，当前为空，因为还没有在DaaS中执行任何的模型评估任务。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2FpcHJlZGljdC9haS1kZXBsb3ltZW50L21hc3Rlci9kZXBsb3ktYWktbW9kZWxzLWluLWRhYXMvZGFhcy1tb2RlbC1vdmVydmlldy12NC5qcGc?x-oss-process=image/format,png" alt="Daas-model-overview-v4" style="zoom: 50%;"></p>
<p>点击<code>v4</code>，可以自由切换到其他版本。比如，切换到<code>v1</code>。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2FpcHJlZGljdC9haS1kZXBsb3ltZW50L21hc3Rlci9kZXBsb3ktYWktbW9kZWxzLWluLWRhYXMvZGFhcy1tb2RlbC12ZXJzaW9ucy5qcGc?x-oss-process=image/format,png" alt="DaaS-model-versions" style="zoom:50%;"></p>
<p><code>v1</code>版本是一个Scikit-learn SVM分类模型，正确率是98.00%。其他信息与<code>v4</code>类似。<br><img src="https://img-blog.csdnimg.cn/20190916185146237.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTYyNjkwMQ==,size_16,color_FFFFFF,t_70" alt="DaaS-model-overview-v1" style="zoom:50%;"></p>
<p>切换到模型<code>部署</code>标签页，有一个我们刚才创建的部署<code>iris-svc</code>，鼠标移动到操作菜单，选择<code>修改设置</code>。可以看到，当前部署服务关联的是模型<code>v1</code>，就是我们刚才通过<code>deploy</code>函数部署的<code>iris</code>第一个版本Scikit-learn模型。选择最新的<code>v4</code>，点击命令<code>保存并且重新部署</code>，该部署就会切换到<code>v4</code>版本。</p>
]]></content>
      <categories>
        <category>模型部署</category>
      </categories>
  </entry>
  <entry>
    <title>工程设计-Treelite加速</title>
    <url>/posts/2A2FA4H/</url>
    <content><![CDATA[<h1><span id="treelite树模型部署加速工具支持xgboost-lightgbm和sklearn">Treelite：树模型部署加速工具（支持XGBoost、LightGBM和Sklearn）</span></h1><blockquote>
<p>  项目链接：<a href="https://link.zhihu.com/?target=https%3A//treelite.readthedocs.io/">https://treelite.readthedocs.io/</a></p>
<p>  项目论文：<a href="https://link.zhihu.com/?target=https%3A//mlsys.org/Conferences/doc/2018/196.pdf">https://mlsys.org/Conferences/doc/2018/196.pdf</a></p>
<p>  支持模型：XGB、LGB、SKlearn树模型</p>
</blockquote>
<h3><span id="一-treelite介绍"><strong>一、TreeLite介绍</strong></span></h3><p>TreeLite能够树模型编译优化为单独库，可以很方便的用于模型部署。经过优化后可以将XGBoost模型的预测速度提高2-6倍。</p>
<p><img src="https://pic2.zhimg.com/80/v2-f3b1978eb5c3a4fe463f93cbbad9dbf5_1440w.jpg" alt="img"></p>
<p>如上图，黑色曲线为XGBoost在不同batch size下的吞吐量，红色曲线为XGBoost经过TreeLite编译后的吞吐量。</p>
<p>Treelite支持众多的树模型，特别是随机森林和GBDT。同时Treelite可以很好的支持XGBoost, LightGBM和 scikit-learn，也可以将自定义模型根据要求完成编译。</p>
<h3><span id="二-treelite原理"><strong>二、TreeLite原理</strong></span></h3><p><strong>TreeLite主要在两方面完成了改进。</strong></p>
<ul>
<li><p><strong>逻辑分支</strong></p>
<ul>
<li><p>这个指令是<strong>gcc引入</strong>的，作用是<strong>允许程序员将最有可能执行的分支告诉编译器</strong>。这个指令的写法为：<code>__builtin_expect(EXP, N)</code>。意思是：EXP==N的概率很大。【<strong>减少重新取跳转地址</strong>】</p>
<blockquote>
<p>  __builtin_expect 说明: <a href="https://www.jianshu.com/p/2684613a300f">https://www.jianshu.com/p/2684613a300f</a></p>
</blockquote>
</li>
<li><p><strong>构建树模型+ 计算好每个分支下面样本的个数+提前预知哪一个叶子节点被执行的可能性更大，进而可以提前执行子节点逻辑。</strong></p>
</li>
</ul>
</li>
<li><p><strong>逻辑比较</strong></p>
<ul>
<li><strong>浮点数比较，转化为整数型比较</strong>。</li>
</ul>
</li>
</ul>
<h4><span id="21-逻辑分支"><strong>2.1 逻辑分支</strong></span></h4><p>对于树模型而言，节点的分类本质使用if语句完成，而CPU在执行if语句时会等待条件逻辑的计算。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> ( [conditional expression] ) &#123;</span><br><span class="line">  <span class="built_in">foo</span>();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">bar</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>如果在构建树模型时候，提前计算好每个分支下面样本的个数，则可以提前预知哪一个叶子节点被执行的可能性更大，进而可以提前执行子节点逻辑。</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-4e8806e27e4186d058b87c772b27a087_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>借助于编译命令，可以完成逻辑计算加速。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* expected to be false */</span></span><br><span class="line"><span class="keyword">if</span>( __builtin_expect([condition],<span class="number">0</span>))&#123;</span><br><span class="line">  ...</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="22-逻辑比较">2.2 逻辑比较</span></h4><p>原始的分支比较可能会有浮点数比较逻辑，可以量化为数值比较逻辑。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (data[<span class="number">3</span>].fvalue &lt; <span class="number">1.5</span>) &#123;  </span><br><span class="line"><span class="comment">/* floating-point comparison */</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (data[<span class="number">3</span>].qvalue &lt; <span class="number">3</span>) &#123;     </span><br><span class="line"><span class="comment">/* integer comparison */</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>模型部署</category>
      </categories>
  </entry>
  <entry>
    <title>工程设计-XGBoost4J</title>
    <url>/posts/27C0J2T/</url>
    <content><![CDATA[<h2><span id="分布式-xgboost4j-spark-基本原理">分布式 XGBoost4J - Spark 基本原理</span></h2><p><strong>XGBoost4J-Spark</strong>是一个项目，旨在通过使XGBoost适应Apache Spark的MLLIB框架，无缝集成XGBoost和Apache Spark。通过集成，用户不仅可以使用XGBoost的高性能算法实现，还可以利用Spark强大的数据处理引擎实现以下功能：</p>
<ul>
<li>特征工程：特征提取，变换，降维和选择等。</li>
<li>管道：构造，评估和调整ML管道</li>
<li>持久性：持久化并加载机器学习模型，甚至整个管道</li>
</ul>
<p>本文将介绍使用XGBoost4J-Spark构建机器学习管道的端到端过程。讨论</p>
<ul>
<li>使用Spark预处理数据以适合XGBoost / XGBoost4J-Spark的数据接口</li>
<li>使用XGBoost4J-Spark训练XGBoost模型</li>
<li>使用Spark服务XGBoost模型（预测）</li>
<li>使用XGBoost4J-Spark构建机器学习管道</li>
<li>在生产中运行XGBoost4J-Spark</li>
</ul>
<h3><span id="模型上线之模型即服务三自动上线框架"></span></h3><h3><span id="机器学习中的并行计算"></span></h3>]]></content>
      <categories>
        <category>模型部署</category>
      </categories>
  </entry>
  <entry>
    <title>工程设计-在线部署</title>
    <url>/posts/3J5RV40/</url>
    <content><![CDATA[<h2><span id="模型在线部署-tensorflow-serving">模型在线部署-TensorFlow Serving</span></h2><p>TensorFlow Serving 是 TensorFlow 提供的一个高性能、灵活、可扩展的模型服务框架，用于部署和扩展 TensorFlow 模型服务。它支持多种模型格式和多种协议，能够快速部署和扩展模型服务，适用于各种规模和复杂度的深度学习模型。TensorFlow Serving 的主要特点包括：</p>
<ol>
<li>支持多种模型格式：TensorFlow Serving 支持多种常见的模型格式，包括 TensorFlow SavedModel 格式、TensorFlow Hub 格式、Session Bundle 格式等，能够适应不同类型和规模的深度学习模型。</li>
<li>支持多种协议：TensorFlow Serving 支持多种常见的网络协议，包括 gRPC、RESTful API、Apache Kafka、Apache Pulsar 等，能够方便地与其他系统集成，实现模型服务的高效和灵活。</li>
<li>高性能和高可用性：TensorFlow Serving 提供了多种性能优化和扩展机制，包括模型预热、预加载、多线程和多进程等，能够快速响应请求并保证高可用性。</li>
<li>易于部署和管理：TensorFlow Serving 提供了多种部署和管理工具，包括 Docker 容器、Kubernetes、Triton Inference Server 等，能够快速部署和管理模型服务，并支持自动化部署和扩展。</li>
</ol>
<p>通过使用 TensorFlow Serving，用户可以方便地将训练好的 TensorFlow 模型部署到云端或本地服务器上，提供高效、灵活、可扩展的模型服务。TensorFlow Serving 已广泛应用于各种领域，包括计算机视觉、自然语言处理、语音识别等，是深度学习模型部署和推断的重要基础框架之一。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型名称和路径</span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;mnist&quot;</span></span><br><span class="line">MODEL_BASE_PATH = <span class="string">&quot;./models/mnist&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 TensorFlow Serving</span></span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line">subprocess.Popen([<span class="string">&quot;tensorflow_model_server&quot;</span>, </span><br><span class="line">                  <span class="string">&quot;--port=8500&quot;</span>, </span><br><span class="line">                  <span class="string">&quot;--rest_api_port=8501&quot;</span>, </span><br><span class="line">                  <span class="string">f&quot;--model_name=<span class="subst">&#123;MODEL_NAME&#125;</span>&quot;</span>, </span><br><span class="line">                  <span class="string">f&quot;--model_base_path=<span class="subst">&#123;MODEL_BASE_PATH&#125;</span>&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送推断请求</span></span><br><span class="line">data = np.random.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>).tolist()</span><br><span class="line">json_data = &#123;<span class="string">&quot;instances&quot;</span>: data&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">&quot;content-type&quot;</span>: <span class="string">&quot;application/json&quot;</span>&#125;</span><br><span class="line">url = <span class="string">f&quot;http://localhost:8501/v1/models/<span class="subst">&#123;MODEL_NAME&#125;</span>:predict&quot;</span></span><br><span class="line"></span><br><span class="line">response = requests.post(url, json=json_data, headers=headers)</span><br><span class="line"><span class="built_in">print</span>(response.json())</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>模型部署</category>
      </categories>
  </entry>
  <entry>
    <title>工程设计-模型部署</title>
    <url>/posts/3W84T2F/</url>
    <content><![CDATA[<h2><span id="机器学习一模型训练及线上部署相关笔记">机器学习（一）：模型训练及线上部署相关笔记</span></h2><p><a href="https://www.shangmayuan.com/a/2331a95928b344d782ac08ac.html">https://www.shangmayuan.com/a/2331a95928b344d782ac08ac.html</a></p>
<ul>
<li>GBDT+LR模型训练及线上部署</li>
<li>Java调用jpmml类</li>
</ul>
]]></content>
      <categories>
        <category>模型部署</category>
      </categories>
  </entry>
  <entry>
    <title>工程设计-部署优化</title>
    <url>/posts/GN92QG/</url>
    <content><![CDATA[<h1><span id="模型部署优化的学习路线是什么"></span></h1><p>模型部署优化这个方向其实比较宽泛。从模型完成训练，到最终将模型部署到实际硬件上，整个流程中会涉及到很多不同层面的工作，每一个环节对技术点的要求也不尽相同。</p>
<p><strong>部署的流程大致可以分为以下几个环节：</strong></p>
<p><img src="https://segmentfault.com/img/bVcT3R0" alt="模型部署流程" style="zoom: 67%;"></p>
<h3><span id="一-模型转换">一、模型转换</span></h3><p>从训练框架得到模型后，根据需求转换到相应的模型格式。模型格式的选择通常是根据公司业务端 SDK 的需求，通常为 caffe 模型或 onnx 模型，以方便模型在不同的框架之间适配。该环节的工作需要对相应的训练框架以及 caffe/onnx 等模型格式有所了解。常用的 Pytorch 和 TensorFlow 等框架都有十分成熟的社区和对应的博客或教程；caffe 和 onnx 模型格式也有很多可参考和学习的公开文档。</p>
<p>即使没找到有可参考的文章时，好在二者都是开源的，依然可以通过对源码和样例代码的阅读来寻找答案。</p>
<h3><span id="二-模型优化">二、模型优化</span></h3><p>此处的模型优化是指与后端无关的通用优化，比如<strong>常量折叠</strong>、<strong>算数优化</strong>、<strong>依赖优化</strong>、<strong>函数优化</strong>、<strong>算子融合</strong>以及<strong>模型信息简化</strong>等等。</p>
<p>部分训练框架会在训练模型导出时就包含部分上述优化过程，同时如果模型格式进行了转换操作，不同 IR 表示之间的差异可能会引入一些冗余或可优化的计算，因此在模型转换后通常也会进行一部分的模型优化操作。</p>
<p><strong>该环节的工作需要对计算图的执行流程、各个 op 的计算定义、程序运行性能模型有一定了解，才能知道如果进行模型优化，如何保证优化后的模型具有更好的性能。</strong></p>
<p>了解得越深入，越可以挖掘到更多的模型潜在性能。</p>
<h3><span id="三-模型压缩">三、模型压缩</span></h3><p>广义上来讲，模型压缩也属于模型优化的一部分。模型压缩本身也包括很多种方法，比如剪枝、蒸馏、量化等等。模型压缩的根本目的是希望获得一个较小的模型，减少存储需求的同时降低计算量，从而达到加速的目的。</p>
<p><strong>该环节的工作需要对压缩算法本身、模型涉及到的算法任务及模型结构设计、硬件平台计算流程三个方面都有一定的了解。</strong></p>
<p>当因模型压缩操作导致模型精度下降时，对模型算法的了解，和该模型在硬件上的计算细节有足够的了解，才能分析出精度下降的原因，并给出针对性的解决方案。</p>
<p><strong>对于模型压缩更重要的往往是工程经验，</strong> 因为在不同的硬件后端上部署相同的模型时，由于硬件计算的差异性，对精度的影响往往也不尽相同，这方面只有通过积累工程经验来不断提升。</p>
<p>OpenPPL也在逐步开源自己的模型压缩工具链，并对上述提到的模型算法、压缩算法和硬件平台适配等方面的知识进行介绍。</p>
<h3><span id="四-模型部署">四、模型部署</span></h3><p>模型部署是整个过程中最复杂的环节。从工程上讲，主要的核心任务是<strong>模型打包</strong>、<strong>模型加密</strong>，并进行SDK封装。</p>
<p><strong>在一个实际的产品中，往往会用到多个模型。模型打包是指将模型涉及到的前后处理，以及多个模型整合到一起，并加入一些其他描述性文件。</strong>模型打包的格式和模型加密的方法与具体的 SDK 相关。<strong>在该环节中主要涉及到的技能与 SDK 开发更为紧密。</strong></p>
<p>从功能上讲，对部署最后的性能影响最大的肯定是SDK中包含的后端库，即实际运行模型的推理库。开发一个高性能推理库所需要的技能点就要更为广泛，并且专业。</p>
<p>并行计算的编程思想在不同的平台上是通用的，但不同的硬件架构的有着各自的特点，推理库的开发思路也不尽相同，这也就要求对开发后端的架构体系有着一定的了解。</p>
<p>具体到不同架构的编程学习，建议参考当前各大厂开源的推理库来进一步学习。</p>
]]></content>
      <categories>
        <category>模型部署</category>
      </categories>
  </entry>
  <entry>
    <title>模型部署Q&amp;A</title>
    <url>/posts/35SVZYH/</url>
    <content><![CDATA[<h3><span id="一-离线模型offline">一、离线模型(Offline)</span></h3><p>离线模型存在于很多业务场景中，其中最常见的业务场景就是用在推荐系统的召回阶段，由于在推荐系统中，<strong>召回并不要求是实时的</strong>，可以根据业务的需要，调整成每天一次，或者每几个小时跑一次即可，因此，这类的模型，一般我们只需要使用<strong>Linux下的crontab定时任务脚本</strong>，每隔一段时间来启动一次就可以，然后将log文件输出到指定的文件下即可。这种方式一般来讲仅限离线模型的部署，其本质上就是一段定时任务的代码。</p>
<h3><span id="二-在线online近似在线nearline模型">二、在线(Online)/近似在线(NearLine)模型</span></h3><p>在生产系统中，实时推理和预测是最常见的需求，也是对于很多深度学习模型来说所必须达到的点。下面简介一些深度学习模型在实时预测时常见的几种部署方法：</p>
<h4><span id="21-将模型预测直接打包成http接口">2.1 将模型预测直接打包成http接口</span></h4><p>将模型直接打包成一个http接口的形式是在企业中比较常见的模型上线的方式，<strong>所谓的将预测直接打包成http接口实际上一般是指将我们训练好的模型直接在线上进行预测</strong>。我们来试想一个场景，当一个模型训练好之后，我们如果想要验证这个模型的好坏，我们首先能想到的办法就是找一批数据来测试一下。实际上，将模型预测直接打包成http接口也是利用了这样的思路。</p>
<p>在这里，我们可以<strong>将训练好的模型提前进行加载，并初始化若干个消息队列和worker，当有新的待预测数据进入的时候，我们直接将数据通过消息队列传入到模型中进行推理和预测，最终得到结果。</strong></p>
<p>而<strong>对于外层接收输入，我们一般可以将接收的地方使用==flask打包成一个http接口==，等待传入即可。</strong></p>
<p>使用这种方式直接打包成http接口的好处在于打包和部署相对比较方便，对于一些相对比较<strong>轻量级且对并发量要求不是很高的情况下相对还是比较好用的。使用值得注意的是，如果对于一个相对比较大的模型来讲，这种方式推理的时间相对就会比较长，从用户输入到结果返回可能需要200ms左右。</strong></p>
<h4><span id="22-pmml">==2.2 PMML==</span></h4><blockquote>
<p>  使用PMML部署机器学习模型 : <a href="https://zhuanlan.zhihu.com/p/79197337">https://zhuanlan.zhihu.com/p/79197337</a></p>
</blockquote>
<p>PMML (Predictive Model Markup Language) 是一套通用的且与平台和环境无关的模型表示语言，<strong>机器学习在模型部署方面的一种标准部署方案，其形式是采用XML语言标记形式</strong>。我们可以将自己训练的机器学习模型打包成PMML模型文件的形式，然后使用目标环境的解析PMML模型的库来完成模型的加载并做预测。PMML是一套基于XML的标准，通过 XML Schema 定义了使用的元素和属性，主要由以下核心部分组成：</p>
<p><strong>数据字典（Data Dictionary）</strong>，描述输入数据。</p>
<p><strong>数据转换（Transformation Dictionary和Local Transformations）</strong>，应用在输入数据字段上生成新的派生字段。</p>
<p><strong>模型定义 （Model）</strong>，每种模型类型有自己的定义。</p>
<p><strong>输出（Output）</strong>，指定模型输出结果。</p>
<p>目前，大部分机器学习库都支持直接打包成PMML模型文件的相关函数，例如在Python中的LightGBM库，XGBoost库，Keras库等，都有对PMML的支持，直接使用相应的命令就可以生成，而在Java、R等语言中，也有相关的库可以进行PMML文件生成的命令。一般来讲，使用PMML文件进行预测的过程如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-7b62592015b357a9e77d2d29857f3367_1440w.jpg" alt="img" style="zoom: 50%;"></p>
<p><strong>由于其平台无关性，导致PMML可以实现跨平台部署，是企业中部署机器学习模型的常见解决方案。</strong></p>
<h4><span id="23-tensorflow-serving">2.3 TensorFlow Serving</span></h4><p>使用TensorFlow Serving进行服务部署，一般需要2台以上机器。</p>
<ul>
<li>其中一台作为TensorFlow Serving的服务器，这台服务器是专门来做模型部署和预测用，对于这台服务器，一般建议使用GPU服务器，这样会使整个推理预测的过程变得很快；</li>
<li>另外一台服务器是业务服务器，也就是接收用户的输入以及其他业务处理的服务器。我们可以把模型部署到TensorFlow Serving的服务器上，而一般我们只需要先在服务器上使用docker创建一个TensorFlow Serving服务，然后将模型文件上传上去，当有请求进来的时候，业务服务会直接对模型所在的服务器发起服务调用，并得到模型预测的结果。</li>
</ul>
<h3><span id="三-实际用时的一些部署方法组合">三、实际用时的一些部署方法组合</span></h3><h4><span id="31-rpmmlsparkairflow调度">3.1 R+pmml+spark+airflow调度</span></h4><p>用R语言训练模型并转为pmml文件，然后使用spark将这个pmml文件封装为jar，使用==airflow==提交到yarn。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val is: InputStream = fs.open(path)</span><br><span class="line"></span><br><span class="line">val pmml: PMML = PMMLUtil.unmarshal(is)</span><br><span class="line"></span><br><span class="line">modelEvaluator = ModelEvaluatorFactory.newInstance.newModelEvaluator(pmml)</span><br></pre></td></tr></table></figure>
<h4><span id="32-pythonsklearnairflow调度">3.2 python+sklearn+airflow调度</span></h4><p>使用python训练好sklearn模型，并joblib.dumps()保存，然后在python文件中joblib.load()加载改文件，使用==airflow离线调度==。</p>
<h4><span id="33-xgboostsparkxgb4j">==3.3 xgboost+spark+xgb4j==</span></h4><ul>
<li>使用分布式的<strong>spark版的xgboost</strong>，训练好的模型直接保存为model.booster.saveModel(hdfsOutStream)二进制文件.然后<strong>xgboost4j加载该文件XGBoost.loadModel(is)实现线上实时预测</strong>。</li>
</ul>
<h4><span id="34-tensorflowtensorflow的java库">3.4 tensorflow+tensorflow的java库</span></h4><h3><span id="四-不同需求对应的不同平台部署方案">四、不同需求对应的不同平台部署方案</span></h3><h4><span id="1-放到服务器上跑要求吞吐和时延重点是吞吐这种应用在互联网企业居多一般是互联网产品的后端ai计算例如人脸验证-语音服务-应用了深度学习的智能推荐等">1、放到服务器上跑，要求吞吐和时延（重点是吞吐）这种应用在互联网企业居多，一般是互联网产品的后端AI计算，例如人脸验证、语音服务、应用了深度学习的智能推荐等。</span></h4><p>由于一般是大规模部署，这时不仅仅要考虑吞吐和时延，还要考虑功耗和成本。所以除了软件外，硬件也会下功夫，比如使用推理专用的NVIDIA P4、寒武纪MLU100等。这些推理卡比桌面级显卡功耗低，单位能耗下计算效率更高，且硬件结构更适合高吞吐量的情况软件上，一般都不会直接上深度学习框架。对于NVIDIA的产品，一般都会使用TensorRT来加速（不仅可以加速前传，还包含调度功能）。TensorRT用了CUDA、CUDNN，而且还有<strong>图优化、==fp16、int8量化==</strong>等。</p>
<h3><span id="五-模型部署时一些常用的trick">五、模型部署时一些常用的trick</span></h3><h4><span id="1-模型压缩">1、模型压缩</span></h4><p>基于参数修剪和共享的方法针对模型参数的冗余性，试图去除冗余和不重要的项。基于低秩因子分解的技术使用矩阵/张量分解来估计深度学习模型的信息参数。基于传输/紧凑卷积滤波器的方法设计了特殊的结构卷积滤波器来降低存储和计算复杂度。知识蒸馏方法通过学习一个蒸馏模型，训练一个更紧凑的神经网络来重现一个更大的网络的输出。</p>
<p>一般来说，参数修剪和共享，低秩分解和知识蒸馏方法可以用于全连接层和卷积层的CNN，但另一方面，使用转移/紧凑型卷积核的方法仅支持卷积层。低秩因子分解和基于转换/紧凑型卷积核的方法提供了一个端到端的流水线，可以很容易地在CPU/GPU环境中实现。相反参数修剪和共享使用不同的方法，如矢量量化，二进制编码和稀疏约束来执行任务，这导致常需要几个步骤才能达到目标。</p>
]]></content>
      <categories>
        <category>模型部署</category>
      </categories>
  </entry>
  <entry>
    <title>README</title>
    <url>/posts/1VWTGBN/</url>
    <content><![CDATA[<ul>
<li>360 BDCI</li>
<li>邮件安全</li>
<li>Datacon 2020</li>
<li>Datacon 2021</li>
<li>Datacon 2022</li>
</ul>
]]></content>
      <categories>
        <category>算法比赛</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程 vs 表示学习</title>
    <url>/posts/S0Q57W/</url>
    <content><![CDATA[<h2><span id="特征工程与表示学习"></span></h2><h3><span id="1-表示学习"><strong>1. 表示学习</strong></span></h3><p>当我们学习一个复杂概念时，总想有一条捷径可以化繁为简。机器学习模型也不例外，如果有经过提炼的对于原始数据的更好表达，往往可以使得后续任务事倍功半。<strong>这也是表示学习的基本思路，即找到对于原始数据更好的表达，以方便后续任务（比如分类）</strong>。</p>
<p>举个简单的例子，假设我们有 <img src="https://www.zhihu.com/equation?tex=%5C%7B%7Bx%2Cy%5C%7D%7D" alt="[公式]"> ，想要寻找<em>x</em>与<em>y</em>之间的关系。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5C%5B+x%3D+%5Cbegin%7Bbmatrix%7D+1+%26+2+%26+1+%26+0+%5C%5C+2+%26+3+%26+2+%26+1+%5C%5C+1+%26+6+%26+1+%26+4+%5C%5C+0+%26+0+%26+0+%26+1+%5C%5C+1+%26+1+%26+1+%26+17+%5Cend%7Bbmatrix%7D+%5C%5D" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%5C%5B+y%3D+%5Cbegin%7Bbmatrix%7D+6+%5C%5C+10+%5C%5C+14+%5C%5C+18+%5C%5C+22+%5Cend%7Bbmatrix%7D+%5C%5D" alt="[公式]"> </p>
<p>如果单用肉眼看的话，<em>x</em>这个矩阵其实还是比较复杂的，无法直接发现与<em>y</em>间的关系。但如果我们非常幸运，发现<em>x</em>每行相加后的结果 <img src="https://www.zhihu.com/equation?tex=%5B4%2C8%2C12%2C16%2C20%5D%5ET" alt="[公式]"> ，就可以直接看出<em>x</em>与<em>y</em>之间的关系是 <img src="https://www.zhihu.com/equation?tex=y%3Dx%2B2" alt="[公式]"> 。这个例子是为了说明：<strong>同样的数据的不同表达，会直接决定后续任务的难易程度，==因此找到好的数据表示往往是机器学习的核心任务==</strong>。值得注意的是，在现实情况中我们所提炼的到表示往往是很复杂的，往往对于高维矩阵提取到特征也是高维矩阵。这个例子仅供抛砖引玉之用，表示学习不等于维度压缩或者特征选择。</p>
<h3><span id="2-特征工程与表示学习人工-vs-自动"><strong>2. 特征工程与表示学习：人工 vs. 自动</strong></span></h3><p><strong>正因为数据表示的重要性，机器学习一般有两种思路来提升原始数据的表达</strong>：</p>
<ol>
<li>特征<strong>学习</strong>(feature <strong>learning</strong>)，又叫<strong>表示学习</strong>(representation learning)或者表征学习，一般指的是<strong>自动</strong>学习有用的数据特征。<strong>深度学习的最终目标，就是完全自动化的广义数据处理。</strong></li>
<li>特征<strong>工程</strong>(feature <strong>engineering</strong>)，主要指对于数据的<strong>人为</strong>处理提取，有时候也代指“洗数据”。</li>
</ol>
<p>不难看出，两者的主要区别在于前者是“<strong>学习的过程</strong>”，而后者被认为是一门“<strong>人为的工程</strong>”。用更加白话的方式来说，<strong>特征学习是</strong>从数据中自动抽取特征或者表示的方法，这个学习过程是<strong>模型自主的</strong>。而<strong>特征工程</strong>的过程是<strong>人为的</strong>对数据进行处理，<strong>得到我们认为的、适合后续模型使用的样式</strong>。根据这个思路，机器学习模型对于数据的处理可以被大致归类到两个方向：</p>
<ul>
<li>表示学习：<strong>模型自动</strong>对输入数据进行学习，得到更有利于使用的特征(*可能同时做出了预测)。代表的算法大致包括：<ul>
<li>深度学习，包括大部分常见的模型如CNN/RNN/DBN等。</li>
<li>某些无监督学习算法，如<strong>主成分分析(PCA) </strong>及 <strong>自编码器（autoencoder）</strong>通过对数据转化而使得输入数据更有意义。</li>
<li>某些树模型可以自动的学习到数据中的特征并同时作出预测。</li>
</ul>
</li>
<li>特征工程：模型依赖<strong>人为处理</strong>的数据特征，而模型的主要任务是预测，比如简单的线性回归期待良好的输入数据(如离散化后的数据)</li>
</ul>
<h3><span id="3-模型选择"><strong>3. 模型选择</strong></span></h3><p>回归到问题的本质，就要谈谈什么时候用「手工提取」什么时候用「表示学习」。一种简单的看法是，<strong>要想自动学习到数据的良好表达，就需要大量的数据。这个现象也解释了为什么「特征工程」往往在中小数据集上表现良好，而「表示学习」在大量复杂数据上更有用武之地。</strong></p>
<p>而一切的根本，其实在于<strong>假设</strong>。比如我们会假设数据分布，会假设映射函数的性质，也会假设预测值与输入值间的关系。<strong>这一切假设其实并非凭空猜想，而是基于我们对于问题的理解，从某种角度来看，这是一种先验，是贝叶斯模型</strong>。在中小数据集上的机器学习往往使用的就是强假设模型（人类知识先验）+一个简单线性分类器。当数据愈发复杂，数据量逐渐加大后，我们对于数据的理解越来越肤浅，做出的假设也越来越倾向于随机，那么此时人工特征工程往往是有害的，而需要使用摆脱了人类先验的模型，比如深度学习或者集成模型。</p>
<p><strong>换句话说，模型选择的过程其实也是在衡量我们对于问题及数据的理解是否深刻，是在人类先验与数据量之间的一场博弈。</strong>从这个角度来看，深度学习首先革的是传统机器学习模型的命：最先被淘汰的不是工人，而是特定场景下的传统机器学习模型。</p>
<p>但话说回来，在很多领域数据依然是稀缺的，我们依然需要人工的手段来提炼数据。而这样的尝试其实并不罕见，我也写过一篇<a href="https://zhuanlan.zhihu.com/p/32896968">「Stacking」与「神经网络」</a>介绍如何模拟神经网络在中小数据集上无监督的抽取特征，并最终提升数据的表示。另一个相关的问题是，到底多少数据才算多？可以参考这篇文章：<a href="https://zhuanlan.zhihu.com/p/34523880">「机器学习」到底需要多少数据？</a>。</p>
<p>然而，<strong>相同的数据对于不同的任务也要求不同的数据表达，最优的数据表示并非是绝对的</strong>。类比来看，人类是由细胞组成的，器官也是由细胞组成的。在器官层面来看，细胞是很好的表达。而从人类角度来看，器官是比较好的表达，因为我们可以通过身高体重来区分人，而无法直观地通过细胞来区分人。然而再往前看一步，每个人的细胞携带不同的遗传信息，因此也可以被认为是一种很强的数据表达。讲这个故事的目的是说明，<strong>什么是好的数据表达，其实是非常模棱两可的问题，在不同语境下可能大不相同</strong>。</p>
]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程（0）【Nan】数据清洗</title>
    <url>/posts/29WR3P/</url>
    <content><![CDATA[<h2><span id="特征工程-数据清洗">特征工程-数据清洗</span></h2><blockquote>
<p>  特征工程 - 未来达摩大师的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/476659737">https://zhuanlan.zhihu.com/p/476659737</a></p>
<p>  这9个特征工程使用技巧，解决90%机器学习问题！ - Python与数据挖掘的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/462744763">https://zhuanlan.zhihu.com/p/462744763</a></p>
<p>  有哪些精彩的特征工程案例？ - 京东科技风险算法与技术的回答 - 知乎 <a href="https://www.zhihu.com/question/400064722/answer/1308358333">https://www.zhihu.com/question/400064722/answer/1308358333</a></p>
</blockquote>
<p><img src="https://pic1.zhimg.com/v2-3baead31deae5b339481aa843a13e21c_b.jpg" alt="img"></p>
<p>数据格式内容错误数据来源有多种，有些是传感器采集，然后算法提取的特征数据；有些是采集的控制器的数据；还有一些应用场合，则是用户/访客产生的，数据肯定存在格式和内容上不一致的情况，所以在进行模型构建之前需要先进行数据的格式内容清洗操作。逻辑错误清洗主要是通过简单的逻辑推理发现数据中的问题数据，防止分析结果走偏，主要包含以下几个步骤：</p>
<p><strong><em>1.数据去重，去除或替换不合理的值；</em></strong></p>
<p><strong><em>2.去除或重构不可靠的字段值（修改矛盾的内容）；</em></strong></p>
<p><strong><em>3.去除异常点数据。</em></strong></p>
<h2><span id="采样">采样</span></h2><blockquote>
<p>  随机采样方法整理与讲解（MCMC、Gibbs Sampling等） - 向阳树的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/109978580">https://zhuanlan.zhihu.com/p/109978580</a></p>
</blockquote>
]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程（2）特征选择</title>
    <url>/posts/T8EY5H/</url>
    <content><![CDATA[<h2><span id="二-特征选择">二、特征选择</span></h2><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/74198735">【机器学习】特征选择(Feature Selection)方法汇总</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/306057603"><em>特征选择方法</em>全面总结</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/479948993"><em>特征选择</em>的基本<em>方法</em>总结</a></p>
</blockquote>
<p> <img src="https://pic4.zhimg.com/v2-05756baf02bd7a023f7b27842594bc2b_b.jpg" alt="img"></p>
<p>训练数据包含许多冗余或无用的特征，移除这些特征并不会导致丢失信息。其中冗余是指一个本身很有用的特征与另外一个有用的特征强相关，或它包含的信息能从其它特征推演出来; 特征很多但样本相对较少</p>
<ul>
<li><p>产生过程：产生特征或特征子集候选集合；</p>
</li>
<li><p>评价函数：衡量特征或特征子集的重要性或者好坏程度，即量化特征变量和目标变量之间的联系以及特征之间的相互联系。为了避免过拟合，可用交叉验证的方式来评估特征的好坏；</p>
</li>
<li><p>停止准则：为了减少计算复杂度，需设定一个阈值，当评价函数值达到阈值后搜索停止</p>
</li>
<li><p>验证过程：在验证数据集上验证选出来的特征子集的有效性</p>
</li>
</ul>
<h3><span id="21-特征选择的目的"><strong>2.1 特征选择的目的</strong></span></h3><p>1.<strong>简化模型</strong>，使模型更易于理解：去除不相关的特征会降低学习任务的难度。并且可解释性能对模型效果的稳定性有更多的把握</p>
<p>2.<strong>改善性能</strong>：节省存储和计算开销</p>
<p>3.<strong>改善通用性、降低过拟合风险</strong>：减轻维数灾难，特征的增多会大大增加模型的搜索空间，大多数模型所需要的训练样本随着特征数量的增加而显著增加。特征的增加虽然能更好地拟合训练数据，但也可能增加方差</p>
<h3><span id="22-特征选择常见方法">2.2 特征选择常见方法</span></h3><ul>
<li><p>==<strong>Filter(过滤法)</strong>==</p>
<ul>
<li><strong>覆盖率</strong></li>
<li><strong>方差选择</strong></li>
<li><strong>Pearson(皮尔森)相关系数</strong></li>
<li><strong>卡方检验</strong></li>
<li><strong>互信息法(KL散度、相对熵)和最大信息系数</strong> </li>
<li>Fisher得分</li>
<li>相关特征选择</li>
<li>最小冗余最大相关性</li>
</ul>
</li>
<li><p><strong>Wrapper(包装法)</strong></p>
<ul>
<li>完全搜索</li>
<li>启发搜索</li>
<li>随机搜索</li>
</ul>
</li>
<li>==<strong>Embedded(嵌入法)</strong>==<ul>
<li>L1 正则项</li>
<li>树模型选择</li>
<li>不重要性特征选择</li>
</ul>
</li>
</ul>
<h3><span id="221-filter过滤法-特征集">2.2.1 <strong>Filter(过滤法)</strong> 【特征集】</span></h3><p><img src="https://pic4.zhimg.com/v2-d91b1bdb2bf6034b9e26b3620bf8a233_b.jpg" alt="img"></p>
<h4><span id="定义"><strong>定义</strong></span></h4><ul>
<li><strong>过滤法的思想就是不依赖模型，仅从特征的角度来做特征的筛选</strong>，具体又可以分为两种方法，一种是根据特征里面包含的信息量，如方差选择法，如果一列特征的方差很小，每个样本的取值都一样的话，说明这个特征的作用不大，可以直接剔除。另一种是对每一个特征，都计算关于目标特征的<a href="https://www.zhihu.com/search?q=相关度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;479948993&quot;}">相关度</a>，然后根据这个相关度来筛选特征，只保留高于某个阈值的特征，这里根据相关度的计算方式不同就可以衍生出一下很多种方法。</li>
</ul>
<h4><span id="分类"><strong>分类</strong></span></h4><ul>
<li><strong>单变量过滤方法</strong>：不需要考虑特征之间的相互关系，按照特征变量和目标变量之间的相关性或互信息对特征进行排序，过滤掉最不相关的特征变量。优点是计算效率高、不易过拟合。</li>
<li><strong>多变量过滤方法</strong>：考虑特征之间的相互关系，常用方法有基于相关性和一致性的特征选择</li>
</ul>
<h4><span id="覆盖率">==覆盖率==</span></h4><ul>
<li>即特征在训练集中出现的比例。若覆盖率很小，如有10000个样本，但某个特征只出现了5次，则次覆盖率对模型的预测作用不大，可删除</li>
</ul>
<h4><span id="方差选择法">==<strong>方差选择法</strong>==</span></h4><ul>
<li>先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="comment"># 方差选择法，返回值为特征选择后的数据</span></span><br><span class="line"><span class="comment"># 参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>
<h4><span id="pearson皮尔森相关系数用于度量两个变量x和y之间的线性相关性">==Pearson皮尔森相关系数==:用于度量两个变量X和Y之间的线性相关性</span></h4><ul>
<li><strong>用于度量两个变量X和Y之间的线性相关性</strong>，结果的取值区间为[-1, 1]， -1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关性</li>
<li>计算方法为两个变量之间的<strong>协方差</strong>和<strong>标准差</strong>的商</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">  <span class="comment"># 选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">  <span class="comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，</span></span><br><span class="line">  <span class="comment"># 输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。</span></span><br><span class="line">  <span class="comment"># 在此为计算相关系数</span></span><br><span class="line">  <span class="comment"># 其中参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, </span><br><span class="line">              k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h4><span id="卡方检验自变量对因变量的相关性">==<strong>卡方检验</strong>==:自变量对因变量的相关性</span></h4><ul>
<li><strong>检验定性自变量对定性因变量的相关性</strong>。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量:</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cchi%5E%7B2%7D%3D%5Csum+%5Cfrac%7B%28A-E%29%5E%7B2%7D%7D%7BE%7D+%5C%5C" alt="[公式]"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="comment">#选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h4><span id="psi互信息法kl散度-相对熵和最大信息系数-mutual-information-and-maximal-information-coefficient-mic">==PSI互信息法==(KL散度、相对熵)和==最大信息系数== Mutual information and maximal information coefficient (MIC)</span></h4><blockquote>
<p>  <strong><font color="red"> 风控模型—群体稳定性指标(PSI)深入理解应用</font></strong>:<a href="https://zhuanlan.zhihu.com/p/79682292">https://zhuanlan.zhihu.com/p/79682292</a></p>
</blockquote>
<ul>
<li>评价定性自变量对定性因变量的相关性，评价类别型变量对类别型变量的相关性，互信息越大表明两个变量相关性越高，互信息为0时，两个变量相互独立。互信息的计算公式为</li>
<li><img src="https://www.zhihu.com/equation?tex=I%28X+%3B+Y%29%3D%5Csum%5Climits_%7Bx+%5Cin+X%7D+%5Csum%5Climits_%7By+%5Cin+Y%7D+p%28x%2C+y%29+%5Clog+%5Cfrac%7Bp%28x%2C+y%29%7D%7Bp%28x%29+p%28y%29%7D%3DD_%7BK+L%7D%28p%28x%2C+y%29+%5C%7C+p%28x%29+p%28y%29%29+%5C%5C" alt="[公式]"></li>
</ul>
<h4><span id="fisher得分">==<strong>Fisher得分</strong>==</span></h4><p>对于分类问题，<strong>好的特征应该是在同一个类别中的取值比较相似</strong>，<strong>而在不同类别之间的取值差异比较大</strong>。因此特征i的重要性可用Fiser得分<img src="https://www.zhihu.com/equation?tex=S_i" alt="[公式]">来表示</p>
<p><img src="https://www.zhihu.com/equation?tex=S_%7Bi%7D%3D%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E%7BK%7D+n_%7Bj%7D%5Cleft%28%5Cmu_%7Bi+j%7D-%5Cmu_%7Bi%7D%5Cright%29%5E%7B2%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BK%7D+n_%7Bj%7D+%5Crho_%7Bi+j%7D%5E%7B2%7D%7D+%5C%5C" alt="[公式]"> </p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=u_%7Bij%7D" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=%5Crho_%7Bij%7D" alt="[公式]">分别是特征i在类别j中均值和方差，<img src="https://www.zhihu.com/equation?tex=%5Cmu_i" alt="[公式]">为特征i的均值，<img src="https://www.zhihu.com/equation?tex=n_j" alt="[公式]">为类别j中的<a href="https://www.zhihu.com/search?q=样本数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;306057603&quot;}">样本数</a>。Fisher得分越高，<strong>特征在不同类别之间的差异性越大、在同一类别中的差异性越小，则特征越重要</strong>;</p>
<h4><span id="相关特征选择">==<strong>相关特征选择</strong>==</span></h4><p>该方法基于的假设是，好的特征集合包含跟目标变量非常相关的特征，但这些特征之间彼此不相关</p>
<h4><span id="最小冗余最大相关性-mrmr">==<strong>最小冗余最大相关性( mRMR)</strong>==</span></h4><p>由于单变量过滤法只考虑了单特征变量和目标变量之间的相关性，因此选择的特征子集可能过于冗余。mRMR在进行特征时考虑到了特征之间的冗余性，具体做法是对跟已选择特征相关性较高的冗余特征进行惩罚;</p>
<h3><span id="222-wrapper包装法-特征集模型"><strong>2.2.2 Wrapper(包装法)</strong> 【特征集+模型】</span></h3><p><img src="https://pic4.zhimg.com/v2-bd321ff1e16c011d1a2bce86a5939a17_b.jpg" alt="img"></p>
<ul>
<li><p>使用<strong>机器学习算法评估特征子集</strong>的效果，可以检测两个或多个特征之间的交互关系，而且选择的特征子集让模型的效果达到最优。</p>
</li>
<li><p>这是<strong>特征子集搜索</strong>和<strong>评估指标相结合</strong>的方法。前者提供候选的新特征子集，后者基于新特征子集训练一个模型，并用验证集进行评估，为每一组特征子集进行打分。</p>
</li>
<li><p>最简单的方法是在<strong>每一个特征子集上训练并评估模型</strong>，从而找出最优的特征子集</p>
</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>需要对每一组特征子集训练一个模型，<strong>计算量很大</strong></li>
<li>样本不够充分的情况下<strong>容易过拟合</strong></li>
<li>特征变量较多时计算复杂度太高</li>
</ul>
<h4><span id="完全搜索">==完全搜索==</span></h4><ul>
<li>即穷举法，<strong>遍历所有可能的组合达到全局最优</strong>，时间复杂度<img src="https://www.zhihu.com/equation?tex=2%5En" alt="[公式]"></li>
</ul>
<h4><span id="启发式搜索">==<strong>启发式搜索</strong>==</span></h4><ul>
<li>序列向前选择：特征子集从空集开始，每次只加入一个特征，时间复杂度为<img src="https://www.zhihu.com/equation?tex=O%28n%2B%28n-1%29%2B%28n-2%29%2B%5Cldots%2B1%29%3DO%5Cleft%28n%5E%7B2%7D%5Cright%29" alt="[公式]"></li>
<li>序列向后选择：特征子集从全集开始，每次删除一个特征，时间复杂度为<img src="https://www.zhihu.com/equation?tex=O%28n%5E%7B2%7D%29" alt="[公式]"></li>
</ul>
<h4><span id="随机搜索">==<strong>随机搜索</strong>==</span></h4><ul>
<li>执行序列向前或向后选择时，随机选择特征子集</li>
</ul>
<h4><span id="递归特征消除法">==<strong>递归特征消除法</strong>==</span></h4><ul>
<li>使用一个基模型进行多轮训练，每轮训练后通过学习器返回的<strong>coef</strong><em>或者<strong>feature_importances</strong></em>消除若干权重较低的特征，再基于新的特征集进行下一轮训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#递归特征消除法，返回特征选择后的数据</span></span><br><span class="line"><span class="comment">#参数estimator为基模型</span></span><br><span class="line"><span class="comment">#参数n_features_to_select为选择的特征个数</span></span><br><span class="line">RFE(estimator=LogisticRegression(), </span><br><span class="line">    n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, </span><br><span class="line">                                          iris.target)</span><br></pre></td></tr></table></figure>
<h3><span id="223-embedded嵌入法">2.2.3 Embedded(嵌入法)</span></h3><p><img src="https://pic4.zhimg.com/v2-9420bbe9f86094fd683ff1fb8919631f_b.jpg" alt="img"></p>
<p>将特征选择嵌入到模型的构建过程中，具有<strong>包装法与机器学习算法相结合的优点</strong>，也具有<strong>过滤法计算效率高的优点</strong></p>
<h4><span id="lasso方法-l1正则项">==<strong>LASSO方法</strong>== L1正则项</span></h4><p>通过对回归系数添加<img src="https://www.zhihu.com/equation?tex=L_1" alt="[公式]">惩罚项来防止过拟合，可以让特定的回归系数变为0，从而可以选择一个不包含那些系数的更简单的模型；实际上，L1惩罚项降维的原理是，在多个对实际上，L1惩罚项降维的原理是，在多个对<a href="https://www.zhihu.com/search?q=目标值&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;306057603&quot;}">目标值</a>具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要具有同等相关性的特征中，只保留一个，所以没保留的特征并不代表不重要。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">#带L1惩罚项的逻辑回归作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(LogisticRegression(</span><br><span class="line">          penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(</span><br><span class="line">               iris.data,iris.target)</span><br></pre></td></tr></table></figure>
<h4><span id="基于树模型的特征选择方法">==<strong>基于树模型的特征选择方法</strong>==</span></h4><ul>
<li>在<a href="https://www.zhihu.com/search?q=决策树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;306057603&quot;}">决策树</a>中，深度较浅的节点一般对应的特征分类能力更强(可以将更多的样本区分开)</li>
<li>对于基于决策树的算法，如<strong>随机森林</strong>，重要的特征更有可能出现在深度较浅的节点，而且出现的次数可能越多</li>
<li>即可基于树模型中<strong>特征出现次数</strong>等指标对特征进行重要性排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="comment">#GBDT作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(</span><br><span class="line">    GradientBoostingClassifier()).fit_transform(</span><br><span class="line">      iris.data,iris.target)</span><br></pre></td></tr></table></figure>
<h4><span id="使用特征重要性来筛选特征的缺陷">使用特征重要性来筛选特征的缺陷？</span></h4><p>1、特征重要性只能说明哪些特征在训练时起到作用了，<strong>并不能说明特征和目标变量之间一定存在依赖关系</strong>。举例来说，随机生成一大堆没用的特征，然后用这些特征来训练模型，一样可以得到特征重要性，但是这个特征重要性并不会全是0，这是完全没有意义的。</p>
<p>2、<strong>特征重要性容易高估数值特征和基数高的类别特征的重要性</strong>。这个道理很简单，特征重要度是根据决策树分裂前后节点的不纯度的减少量（基尼系数或者MSE）来算的，那么对于数值特征或者基础高的类别特征，不纯度较少相对来说会比较多。</p>
<p>3、<strong>特征重要度在选择特征时需要决定阈值</strong>，要保留多少特征、删去多少特征，这些需要人为决定，并且删掉这些特征后模型的效果也不一定会提升。</p>
<h4><span id="non-importance-选择">==Non importance 选择==</span></h4>]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程（1）特征预处理*</title>
    <url>/posts/3PJZDND/</url>
    <content><![CDATA[<h2><span id="特征工程-特征处理">特征工程-特征处理</span></h2><blockquote>
<p>  <strong>机器学习中的特征工程（四）—— ==特征离散化处理方法==：</strong><a href="https://www.jianshu.com/p/918649ce379a">https://www.jianshu.com/p/918649ce379a</a></p>
<p>  <strong>机器学习中的特征工程（三）—— ==序数和类别特征处理方法==</strong>：<a href="https://www.jianshu.com/p/3d828de72cd4">https://www.jianshu.com/p/3d828de72cd4</a></p>
<p>  <strong>机器学习中的特征工程（二）—— ==数值类型数据处理==</strong>：<a href="https://www.jianshu.com/p/b0cc0710ef55">https://www.jianshu.com/p/b0cc0710ef55</a></p>
<p>  机器学习中的特征工程（一）—— 概览：<a href="https://www.jianshu.com/p/172677f4ea4c">https://www.jianshu.com/p/172677f4ea4c</a></p>
<p>  特征工程完全手册 - 从预处理、构造、选择、降维、不平衡处理，到放弃：<a href="https://zhuanlan.zhihu.com/p/94994902">https://zhuanlan.zhihu.com/p/94994902</a></p>
<p>  <strong>这9个特征工程使用技巧，解决90%机器学习问题！</strong> - Python与数据挖掘的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/462744763">https://zhuanlan.zhihu.com/p/462744763</a></p>
</blockquote>
<h3><span id="一-数值类型处理">一、 数值类型处理</span></h3><blockquote>
<p>  <strong>pandas 显示所有列：</strong></p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#显示所有列</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#显示所有行</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_rows&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#设置value的显示长度为100，默认为50</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;max_colwidth&#x27;</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>  <strong>pandas 查看缺失特征:</strong></p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.isnull().<span class="built_in">sum</span>().sort_values(ascending = <span class="literal">False</span>) / train.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>  <strong>pandas 查看某一列的分布:</strong></p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.loc[:,col_name].value_counts()</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>特征提取方式是可以深挖隐藏在数据背后更深层次的信息的</strong>。其次，数值类型数据也并不是直观看上去那么简单易用，因为不同的数值类型的计量单位不一样，比如个数、公里、千克、DB、百分比之类，同样数值的大小也可能横跨好几个量级，比如小到头发丝直径约为0.00004米， 大到热门视频播放次数成千上万次。</p>
<h4><span id="11-数据归一化">1.1 数据归一化</span></h4><blockquote>
<p>  <strong>为什么要数据归一化？</strong></p>
<p>   <a href="../../AI深度学习/深度学习（3）Normalization*.md">深度学习（3）Normalization*.md</a> </p>
<ul>
<li><strong>可解释性</strong>：<strong>回归模型【无正则化】</strong>中自变量X的量纲不一致导致了<strong>==回归系数无法直接解读==</strong>或者错误解读；需要将X都处理到统一量纲下，这样才可比【可解释性】；<strong>取决于我们的逻辑回归是不是用了正则化</strong>。如果你不用正则，标准化并不是必须的，如果用正则，那么标准化是必须的。</li>
<li><strong>距离计算</strong>：机器学习任务和统计学任务中有很多地方要用到<strong>==“距离”的计算==</strong>，比如<strong>PCA、KNN，kmeans和SVM</strong>等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；</li>
<li><p><strong>加速收敛</strong>：参数估计时使用<strong>==梯度下降==</strong>，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即<strong>==提升模型的收敛速度==</strong>。</p>
<p><strong>需要归一化的模型：</strong>利用梯度下降法求解的模型一般需要归一化，<strong>线性回归、LR、SVM、KNN、神经网络</strong></p>
</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\tilde{x}=\frac{x-\min (x)}{\max (x)-\min (x)}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># define data </span></span><br><span class="line">data = np.asarray([[<span class="number">100</span>, <span class="number">0.001</span>], </span><br><span class="line">                   [<span class="number">8</span>, <span class="number">0.05</span>], </span><br><span class="line">                   [<span class="number">50</span>, <span class="number">0.005</span>], </span><br><span class="line">                   [<span class="number">88</span>, <span class="number">0.07</span>], </span><br><span class="line">                   [<span class="number">4</span>, <span class="number">0.1</span>]])</span><br><span class="line"><span class="comment"># define min max scaler</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line"><span class="comment"># transform data</span></span><br><span class="line">scaled = scaler.fit_transform(data) </span><br></pre></td></tr></table></figure>
<h4><span id="12-数据标准化">1.2 数据标准化</span></h4><p>数据标准化是指通过改变数据的分布得到均值为0，标准差为1的服从标准正态分布的数据。主要目的是为了让不同特征之间具有相同的尺度（Scale），这样更有理化模型训练收敛。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment"># define standard scaler</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"><span class="comment"># transform data</span></span><br><span class="line">scaled = scaler.fit_transform(data) </span><br><span class="line"><span class="built_in">print</span>(scaled)</span><br></pre></td></tr></table></figure>
<h4><span id="13-对数转换">==1.3 对数转换==</span></h4><p>log函数的定义为<img src="https://math.jianshu.com/math?formula=log_a(%5Calpha%5Ex" alt="log_a(\alpha^x) = x">%20%3D%20x)，其中a是log函数的底数，<img src="https://math.jianshu.com/math?formula=%5Calpha" alt="\alpha">是一个正常数，<img src="https://math.jianshu.com/math?formula=x" alt="x">可以是任何正数。由于<img src="https://math.jianshu.com/math?formula=%5Calpha%5E0%20%3D%201" alt="\alpha^0 = 1">，我们有<img src="https://math.jianshu.com/math?formula=log_a(1" alt="log_a(1) = 0">%20%3D%200)。这意味着log函数可以将一些介于0~1之间小范围的数字映射到<img src="https://math.jianshu.com/math?formula=(-%5Cinfty%2C%200" alt="(-\infty, 0)">)范围内。比如当a=10时，函数<img src="https://math.jianshu.com/math?formula=log_%7B10%7D(x" alt="log_{10}(x)">)可以将[1,10]映射到[0,1]，将[1,100]映射到[1,2]。<strong>换句话说，log函数压缩了大数的范围，扩大了小数的范围</strong>。x越大，log(x)增量越慢。log(x)函数的图像如下：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426154119370.png" alt="image-20220426154119370" style="zoom: 25%;"></p>
<p><strong>Log函数可以极大压缩数值的范围，相对而言就扩展了小数字的范围。该转换方法适用于长尾分布且值域范围很大的特征，变换后的特征趋向于正态分布。</strong>对数值类型使用对数转换一般有以下几种好处：</p>
<ul>
<li>缩小数据的绝对数值</li>
<li>取对数后，可以将乘法计算转换成加法计算</li>
<li>在数据的整个值域中不同区间的差异带来的影响不同</li>
<li>取对数后不会改变数据的性质和相关关系，但压缩了变量的尺度。</li>
<li>得到的数据易消除异方差问题</li>
</ul>
<h3><span id="二-序数和类别特征处理">二、序数和类别特征处理</span></h3><p>本文主要说明特征工程中关于<strong>序数特征</strong>和<strong>类别特征</strong>的常用处理方法。主要包含<strong>LabelEncoder</strong>、<strong>One-Hot编码</strong>、<strong>DummyCoding</strong>、<strong>FeatureHasher</strong>以及要重点介绍的<strong>WOE编码</strong>。</p>
<h4><span id="21-序数特征处理">2.1 序数特征处理</span></h4><p><strong>序数特征指的是有序但无尺度的特征</strong>。比如表示‘学历’的特征，’高中’、’本科’、’硕士’，这些特征彼此之间是有顺序关系的，但是特征本身无尺度，并且也可能不是数值类型。在实际应用中，一般是字符类型居多，为了将其转换成模型能处理的形式，通常需要先进行编码，比如LabelEncoding。如果序数特征本身就是数值类型变量，则可不进行该步骤。下面依次介绍序数特征相关的处理方式。</p>
<ul>
<li><h4><span id="label-encoding">Label Encoding</span></h4></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">x = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">encoder = LabelEncoder()</span><br><span class="line">x1 = encoder.fit_transform(x)</span><br><span class="line"></span><br><span class="line">x2 = pd.Series(x).astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">x2.cat.codes.values</span><br><span class="line"><span class="comment"># pandas 因子化</span></span><br><span class="line">x2, uniques = pd.factorize(x)</span><br><span class="line"><span class="comment"># pandas 二值化</span></span><br><span class="line">x2 = pd.Series(x)</span><br><span class="line">x2 = (x2 &gt;= <span class="string">&#x27;b&#x27;</span>).astype(<span class="built_in">int</span>) <span class="comment">#令大于等于&#x27;b&#x27;的都为1</span></span><br></pre></td></tr></table></figure>
<h4><span id="22-类别特征处理">2.2 类别特征处理</span></h4><p><strong>类别特征由于没有顺序也没有尺度</strong>，因此处理较为麻烦，但是在CTR等领域却是非常常见的特征。比如<strong>商品的类型，颜色，用户的职业，兴趣</strong>等等。类别变量编码方法中最常使用的就是<strong>One-Hot编码</strong>，接下来结合具体实例来介绍。</p>
<ul>
<li><h4><span id="one-hot编码">One-Hot编码</span></h4></li>
</ul>
<p>One-Hot编码，又称为’独热编码’，其变换后的单列特征值只有一位是1。如下例所示，一个特征中包含3个不同的特征值(a,b,c)，编码转换后变成3个子特征，其中每个特征值中只有一位是有效位1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line"></span><br><span class="line">one_feature = [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">feature = label_encoder.fit_transform(one_feature)</span><br><span class="line">onehot_encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line">onehot_encoder.fit_transform(feature.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="labelbinarizer">LabelBinarizer</span></h4></li>
</ul>
<p>sklearn中的LabelBinarizer也具有同样的作用，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line">feature = np.array([<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">LabelBinarizer().fit_transform(feature)</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="虚拟编码dummy-coding">虚拟编码Dummy Coding</span></h4></li>
</ul>
<p>同样，<strong>pandas中也内置了对应的处理方式,使用起来比Sklearn更加方便</strong>，产生n-1个特征。实例如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">one_feature = [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">pd.get_dummies(one_feature, prefix=<span class="string">&#x27;test&#x27;</span>) <span class="comment"># 设置前缀test</span></span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="特征哈希feature-hashing"><strong><font color="red"> 特征哈希（feature hashing）</font></strong></span></h4></li>
</ul>
<p>按照上述编码方式，如果某个特征具有100个类别值，那么经过编码后将产生100个或99个新特征，这极大地增加了特征维度和特征的稀疏度，同时还可能会出现内存不足的情况。<strong>sklearn中的FeatureHasher接口采用了hash的方法，将不同的值映射到用户指定长度的数组中，使得输出特征的维度是固定的，该方法占用内存少，效率高，可以在多类别变量值中使用，但是由于采用了Hash函数的方式，所以具有冲突的可能，即不同的类别值可能映射到同一个特征变量值中。</strong></p>
<blockquote>
<p>  Feature hashing(特征哈希): <a href="https://blog.csdn.net/laolu1573/article/details/79410187">https://blog.csdn.net/laolu1573/article/details/79410187</a></p>
<p>  <a href="https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing">https://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing</a></p>
<p>  <a href="https://www.zhihu.com/question/264165760/answer/277634591">如何用通俗的语言解释CTR和推荐系统中常用的<em>Feature</em> <em>Hashing</em>技术以及其对应的优缺点？</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line"></span><br><span class="line">h = FeatureHasher(n_features=<span class="number">5</span>, input_type=<span class="string">&#x27;string&#x27;</span>)</span><br><span class="line">test_cat = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;f&#x27;</span>,<span class="string">&#x27;g&#x27;</span>,<span class="string">&#x27;h&#x27;</span>,<span class="string">&#x27;i&#x27;</span>,<span class="string">&#x27;j&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">f = h.transform(test_cat)</span><br><span class="line">f.toarray()</span><br></pre></td></tr></table></figure>
<p><strong>如果hash的目标空间足够大，并且hash函数本身足够散列，不会损失什么特征信息。</strong></p>
<p>feature hashing简单来说和<strong>kernal的思想</strong>是类似的，就是把输入的特征映射到一个具有一些我们期望的较好性质的空间上去。在feature hasing这个情况下我们希望目标的空间具有如下的性质：</p>
<ol>
<li><strong>样本无关的维度大小，因为当在线学习，或者数据量非常大，提前对数据观察开销非常大的时候，这可以使得我们能够提前给算法分配存储和切分pattern。大大提高算法的工程友好性</strong>。</li>
<li>这个空间一般来说比输入的特征空间维度小很多。</li>
<li>另外我们假设在原始的特征空间里，样本的分布是非常稀疏的，只有很少一部分子空间是被取值的。</li>
<li><strong>保持内积的无偏</strong>（不变肯定是不可能的，因为空间变小了），否则很多机器学习方法就没法用了。</li>
</ol>
<blockquote>
<p>  <strong>原理</strong>：假设输入特征是一个N维的0/1取值的向量x。一个N-&gt;M的<a href="https://www.zhihu.com/search?q=哈希函数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">哈希函数</a>h。那么 <img src="https://www.zhihu.com/equation?tex=%5Cphi_j%3D%5Csum_%7Bh%28i%29%3Dj%7D%7Bx_i%7D" alt="[公式]"></p>
<p>  好处：</p>
<ul>
<li>从某种程度上来讲，使得训练样本的特征在对应空间里的<strong>分布更均匀</strong>了。这个好处对于实际训练过程是非常大的，某种程度上起到了<strong>shuffle的作用</strong>。</li>
<li>特征的空间变小了，而且是一个可以预测的大小。比如说加入输入特征里有个东西叫做user_id，那么显然你也不知道到底有多少<a href="https://www.zhihu.com/search?q=userid&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">userid</a>的话，你需要先扫描一遍并且分配足够的空间给到它不然学着学着oom了。你也不能很好地提前优化<a href="https://www.zhihu.com/search?q=分片&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;277634591&quot;}">分片</a>。</li>
<li><p>对在线学习非常友好。</p>
<p>坏处：</p>
</li>
<li><p>会给debug增加困难，为了debug你要保存记录h计算的过程数据，否则如果某个特征有毛病，你怎么知道到底是哪个原始特征呢？</p>
</li>
<li>没选好哈希函数的话，<strong>可能会造成碰撞</strong>，如果原始特征很稠密并且碰撞很严重，那可能会带来坏的训练效果。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hashing_vectorizer</span>(<span class="params">features, N</span>):</span><br><span class="line">  	x = [<span class="number">0</span>] * N</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> features:</span><br><span class="line">      	h = <span class="built_in">hash</span>(f)</span><br><span class="line">        idx = h % N</span><br><span class="line">        <span class="keyword">if</span> xt(f) == <span class="number">1</span>: <span class="comment"># xt 2值hash函数减少hash冲突</span></span><br><span class="line">          	x[idx] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">         		x[idx] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="多类别值处理方式-基于统计的编码方法">多类别值处理方式 -==- 基于统计的编码方法==</span></h4></li>
</ul>
<p>当类别值过多时，<strong>One-Hot 编码或者Dummy Coding都可能导致编码出来的特征过于稀疏</strong>，其次也会占用过多内存。<strong>如果使用FeatureHasher，n_features的设置不好把握，可能会造成过多冲突，造成信息损失</strong>。这里提供一种基于统计的编码方法，包括<strong>基于特征值的统计</strong>或者<strong>基于标签值的统计</strong>——基于标签的编码。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line"><span class="built_in">test</span> = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="built_in">df</span> = pd.DataFrame(<span class="built_in">test</span>, columns=[<span class="string">&#x27;alpha&#x27;</span>])</span><br><span class="line">sns.countplot(<span class="built_in">df</span>[<span class="string">&#x27;alpha&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426130800412.png" alt="image-20220426130800412" style="zoom:50%;"></p>
<p>首先我们将每个类别值出现的频数计算出来，比如我们设置阈值为1，那么所有小于阈值1的类别值都会被编码为同一类，大于1的类别值会分别编码，如果出现频数一样的类别值，既可以都统一分为一个类，也可以按照某种顺序进行编码，这个可以根据业务需要自行决定。那么根据上图，可以得到其编码值为：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220426130824066.png" alt="image-20220426130824066" style="zoom:50%;"></p>
<p> <strong>即（a,c）分别编码为一个不同的类别，（e,b,d）编码为同一个类别。</strong></p>
<h3><span id="23-二阶">2.3 二阶</span></h3><blockquote>
<p>  w * h = s</p>
</blockquote>
<h3><span id="三-特征离散化处理方法">三、 特征离散化处理方法</span></h3><p><strong>特征离散化指的是将连续特征划分离散的过程</strong>：将原始定量特征的一个区间一一映射到单一的值。离散化过程也被表述成分箱（Binning）的过程。特征离散化常应用于<strong>逻辑回归</strong>和金融领域的评分卡中，同时在规则提取，特征分类中也有对应的应用价值。本文主要介绍几种常见的分箱方法，包括<strong>等宽分箱、等频分箱、信息熵分箱</strong>、<strong>基于决策树分箱、卡方分箱</strong>等。</p>
<p><strong>可以看到在分箱之后，数据被规约和简化，有利于理解和解</strong>释。总来说特征离散化，即 分箱之后会带来如下优势：</p>
<ul>
<li>有助于模型部署和应用，加快模型迭代</li>
<li>增强模型鲁棒性</li>
<li>增加非线性表达能力：连续特征不同区间对模型贡献或者重要程度不一样时，分箱后不同的权重能直接体现这种差异，离散化后的特征再进行特征 交叉衍生能力会进一步加强。</li>
<li>提升模型的泛化能力</li>
<li><strong>扩展数据在不同各类型算法中的应用范围</strong></li>
</ul>
<p>当然特征离散化也有其缺点，总结如下：</p>
<ul>
<li>分箱操作必定会导致一定程度的信息损失</li>
<li>增加流程：建模过程中加入了额外的的离散化步骤</li>
<li>影响模型稳定性： 当一个特征值处于分箱点的边缘时，此时微小的偏差会造成该特征值的归属从一箱跃迁到另外一箱，影响模型的稳定性。</li>
</ul>
<h4><span id="31-等宽分箱equal-width-binning">3.1 等宽分箱（Equal-Width Binning)</span></h4><p><strong>等宽分箱指的是每个分隔点或者划分点的距离一样，即等宽</strong>。实践中一般指定分隔的箱数，等分计算后得到每个分隔点。例如将数据序列分为n份，则 分隔点的宽度计算公式为：</p>
<p><img src="https://math.jianshu.com/math?formula=w%20%3D%20%5Cfrac%20%7Bmax%20-%20min%7D%20%7Bn%7D" alt="w = \frac {max - min} {n}"></p>
<p>这样就将原始数据划分成了n个等宽的子区间，一般情况下，分箱后每个箱内的样本数量是不一致的。使用pandas中的cut函数来实现等宽分箱，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">value, cutoff = pd.cut(df[<span class="string">&#x27;mean radius&#x27;</span>], bins=<span class="number">4</span>, retbins=<span class="literal">True</span>, precision=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>等宽分箱计算简单，但是当数值方差较大时，即数据离散程度很大，那么很可能出现没有任何数据的分箱</strong>，这个问题可以通过自适应数据分布的分箱方法—等频分箱来避免</p>
<h4><span id="32-等频分箱equal-frequency-binning">3.2 等频分箱（Equal-Frequency Binning）</span></h4><p><strong>等频分箱理论上分隔后的每个箱内得到数据量大小一致</strong>，但是当某个值出现次数较多时，会出现等<strong>分边界是同一个值</strong>，导致同一数值分到不同的箱内，这是不正确的。具体的实现可以<strong>去除分界处的重复值</strong>，但这也导致每箱的数量不一致。如下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1 = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">value, cutoff = pd.qcut(s1, <span class="number">3</span>, retbins=<span class="literal">True</span>)</span><br><span class="line">sns.countplot(value)</span><br></pre></td></tr></table></figure>
<p><strong>上述的等宽和等频分箱容易出现的问题是每箱中信息量变化不大</strong>。例如，等宽分箱不太适合分布不均匀的数据集、离群值；等频方法不太适合特定的值占比过多的数据集，如<strong>长尾分布</strong>。</p>
<h4><span id="33-信息熵分箱有监督">3.3 信息熵分箱【有监督】</span></h4><p><strong>如果分箱后箱内样本对y的区分度好，那么这是一个好的分箱</strong>。通过信息论理论，我们可知信息熵衡量了这种区分能力。当特征按照某个分隔点划分为上下两部分后能达到最大的信息增益，那么这就是一个好的分隔点。由上可知，信息熵分箱是有监督的分箱方法。<strong><font color="red"> 其实决策树的节点分裂原理也是基于信息熵。</font></strong></p>
<p>首先我们需要明确信息熵和信息增益的计算方式，分别如下：<br><img src="https://math.jianshu.com/math?formula=Entropy(y" alt="Entropy(y) = - \sum_{i=1}^m p_i log_2{p_i} \\ Gain(x) = Entropy(y) - Info_{split}(x)">%20%3D%20-%20%5Csum<em>%7Bi%3D1%7D%5Em%20p_i%20log_2%7Bp_i%7D%20%5C%5C%20Gain(x)%20%3D%20Entropy(y)%20-%20Info</em>%7Bsplit%7D(x))</p>
<p>在二分类问题中，<img src="https://math.jianshu.com/math?formula=m%3D2" alt="m=2">。 信息增益的物理含义表达为：x的分隔带来的信息对y的不确定性带来的增益。<br>对于二值化的单点分隔，如果我们找到一个分隔点将数据一分为二，分成<img src="https://math.jianshu.com/math?formula=P_1" alt="P_1">和<img src="https://math.jianshu.com/math?formula=P_2" alt="P_2">两部分，那么划分后的信息熵的计算方式为：</p>
<p><img src="https://math.jianshu.com/math?formula=Info_%7Bsplit%7D(x" alt="Info_{split}(x) = P1_{ratio}Entropy(x_{p1}) + P2_{ratio}Entropy(x_{p2})">%20%3D%20P1<em>%7Bratio%7DEntropy(x</em>%7Bp1%7D)%20%2B%20P2<em>%7Bratio%7DEntropy(x</em>%7Bp2%7D))</p>
<p>同时也可以看出，当分箱后，某个箱中的标签y的类别（0或者1）的比例相等时，其熵值最大，表明此特征划分几乎没有区分度。而当某个箱中的数据的标签y为单个类别时，那么该箱的熵值达到最小的0，即纯度最纯，最具区分度。从结果上来看，最大信息增益对应分箱后的总熵值最小。</p>
<h4><span id="34-决策树分箱有监督">3.4 决策树分箱【有监督】</span></h4><p><strong>由于决策树的结点选择和划分也是根据信息熵来计算的，因此我们其实可以利用决策树算法来进行特征分箱</strong>，具体做法如下：</p>
<p>还是以乳腺癌数据为例，首先取其中‘mean radius’字段，和标签字段‘target’来拟合一棵决策树，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>, max_depth=<span class="number">3</span>) <span class="comment"># 树最大深度为3</span></span><br><span class="line">dt.fit(df[<span class="string">&#x27;mean radius&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>), df[<span class="string">&#x27;target&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>接着我们取出这课决策树的所有叶节点的分割点的阈值，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">qts = dt.tree_.threshold[np.where(dt.tree_.children_left &gt; -<span class="number">1</span>)]</span><br><span class="line">qts = np.sort(qts)</span><br><span class="line">res = [np.<span class="built_in">round</span>(x, <span class="number">3</span>) <span class="keyword">for</span> x <span class="keyword">in</span> qts.tolist()]</span><br></pre></td></tr></table></figure>
<h4><span id="35-卡方分箱-有监督">3.5 卡方分箱 【有监督】</span></h4><blockquote>
<p>  <strong>特征选择之卡方分箱、WOE/IV</strong> - 云水僧的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/101771771">https://zhuanlan.zhihu.com/p/101771771</a></p>
</blockquote>
<p><strong><font color="red"> 卡方检验可以用来评估两个分布的相似性，因此可以将这个特性用到数据分箱的过程中。卡方分箱认为：理想的分箱是在同一个区间内标签的分布是相同的</font>;</strong> <strong>卡方分布是概率统计常见的一种概率分布，是卡方检验的基础。卡方分布定义为</strong>：若n个独立的随机变量<img src="https://math.jianshu.com/math?formula=Z_1%2C%20Z_2%2C%20...%2CZ_k" alt="Z_1, Z_2, ...,Z_k">满足标准正态分布<img src="https://math.jianshu.com/math?formula=N(0%2C1" alt="N(0,1)">)，则n个随机变量的平方和<img src="https://math.jianshu.com/math?formula=X%3D%5Csum_%7Bi%3D0%7D%5Ek%20Z_i%5E2" alt="X=\sum_{i=0}^k Z_i^2">为服从自由度为k的卡方分布，记为<img src="https://math.jianshu.com/math?formula=X%20%5Csim%20%5Cchi%5E2" alt="X \sim \chi^2">。参数n称为自由度（样本中独立或能自由变化的自变量的个数)，不同的自由度是不同的分布。</p>
<p><strong>卡方检验</strong> ：卡方检验属于非参数假设检验的一种，其本质都是度量频数之间的差异。其假设为：<strong>观察频数与期望频数无差异或者两组变量相互独立不相关。</strong></p>
<p><img src="https://math.jianshu.com/math?formula=%5Cchi%5E2%20%3D%20%5Csum%20%5Cfrac%20%7B(O-E" alt="\chi^2 = \sum \frac {(O-E)^2}{E}">%5E2%7D%7BE%7D)</p>
<ul>
<li>卡方拟合优度检验：用于检验样本是否来自于某一个分布，比如检验某样本是否为正态分布</li>
<li>独立性卡方检验，查看两组类别变量分布是否有差异或者相关，以列联表的形式比较。以列联表形式的卡方检验中，卡方统计量由上式给出。</li>
</ul>
<h4><span id="步骤">步骤：</span></h4><p>卡方分箱是自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。基本思想: 对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</p>
<p><strong>理想的分箱是在同一个区间内标签的分布是相同的</strong>。卡方分箱就是不断的计算相邻区间的卡方值（卡方值越小表示分布越相似），将分布相似的区间（卡方值最小的）进行合并，直到相邻区间的分布不同，达到一个理想的分箱结果。下面用一个例子来解释：</p>
<p><img src="/Users/apple/Library/Application Support/typora-user-images/image-20220708173638301.png" alt="image-20220708173638301" style="zoom: 33%;"></p>
<p> 由上图，第一轮中初始化是5个区间，分别计算相邻区间的卡方值。找到1.2是最小的，合并2、3区间，为了方便，将合并后的记为第2区间，因此得到4个区间。第二轮中，由于合并了区间，影响该区间与前面的和后面的区间的卡方值，因此重新计算1和2,2和4的卡方值，由于4和5区间没有影响，因此不需要重新计算，这样就得到了新的卡方值列表，找到最小的取值2.5，因此该轮会合并2、4区间，并重复这样的步骤，一直到满足终止条件。</p>
<h4><span id="36-woe编码-有监督"><strong><font color="red"> 3.6 WOE编码 【有监督】</font></strong></span></h4><blockquote>
<p>  <strong><font color="red"> 风控模型—WOE与IV指标的深入理解应用</font></strong>: <a href="https://zhuanlan.zhihu.com/p/80134853">https://zhuanlan.zhihu.com/p/80134853</a></p>
</blockquote>
<p><strong>WOE（Weight of Evidence，证据权重）编码利用了标签信息，属于有监督的编码方式。该方式广泛用于金融领域信用风险模型中，是该领域的经验做法。</strong>下面先给出WOE的计算公式：<br> <img src="https://math.jianshu.com/math?formula=WOE_i%20%3D%20ln%5Clbrace%20%5Cfrac%20%7BP_%7By1%7D%7D%7BP_%7By0%7D%7D%20%5Crbrace%20%3D%20ln%5Clbrace%20%5Cfrac%20%7BB_i%20%2F%20B%7D%7BG_i%2FG%7D%20%5Crbrace" alt="WOE_i = ln\lbrace \frac {P_{y1}}{P_{y0}} \rbrace = ln\lbrace \frac {B_i / B}{G_i/G} \rbrace"><br> ==<strong><img src="https://math.jianshu.com/math?formula=WOE_i" alt="WOE_i">值可解释为第<img src="https://math.jianshu.com/math?formula=i" alt="i">类别中好坏样本分布比值的对数</strong>==。其中各个分量的解释如下：</p>
<ul>
<li><img src="https://math.jianshu.com/math?formula=P_%7By1%7D" alt="P_{y1}">表示该类别中坏样本的分布</li>
<li><img src="https://math.jianshu.com/math?formula=P_%7By0%7D" alt="P_{y0}">表示该类别中好样本的分布</li>
<li><img src="https://math.jianshu.com/math?formula=B_i%2FB" alt="B_i/B">表示该类别中坏样本的数量在总体坏样本中的占比</li>
<li><img src="https://math.jianshu.com/math?formula=G_i%2FG" alt="G_i/G">表示该类别中好样本的数量在总体好样本中的占比</li>
</ul>
<p>很明显，如果整个分数的值大于1，那么WOE值为正，否则为负，所以WOE值的取值范围为正负无穷。<br> <strong>WOE值直观上表示的实际上是“当前分组中坏客户占所有坏客户的比例”和“当前分组中好客户占所有坏客户的比例”的差异。</strong>转化公式以后，也可以理解为：当前这个组中坏客户和好客户的比值，和所有样本中这个比值的差异。这个差异为这两个比值的比值，再取对数来表示的。 WOE越大，这种差异越大，这个分组里的样本坏样本可能性就越大，WOE越小，差异越小，这个分组里的坏样本可能性就越小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 随机生成1000行数据</span></span><br><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;x&#x27;</span>: np.random.choice([<span class="string">&#x27;R&#x27;</span>,<span class="string">&#x27;G&#x27;</span>,<span class="string">&#x27;B&#x27;</span>], <span class="number">1000</span>),</span><br><span class="line">    <span class="string">&#x27;y&#x27;</span>: np.random.randint(<span class="number">2</span>, size=<span class="number">1000</span>)</span><br><span class="line">&#125;)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<h3><span id="四-缺失值处理解析">四、缺失值处理解析</span></h3><blockquote>
<p>  看不懂你打我，史上最全的缺失值解析: <a href="https://zhuanlan.zhihu.com/p/379707046">https://zhuanlan.zhihu.com/p/379707046</a></p>
<p>  <a href="https://zhuanlan.zhihu.com/p/137175585">https://zhuanlan.zhihu.com/p/137175585</a></p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>机器学习模型</th>
<th>是否支持缺失值</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>XGBoost</strong></td>
<td>是</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>是</td>
</tr>
<tr>
<td>线性回归</td>
<td>否</td>
</tr>
<tr>
<td>逻辑回归（LR）</td>
<td>否</td>
</tr>
<tr>
<td>随机森林（RF）</td>
<td>否</td>
</tr>
<tr>
<td>SVM</td>
<td>否</td>
</tr>
<tr>
<td>因子分解机(FM)</td>
<td>否</td>
</tr>
<tr>
<td>朴实贝叶斯（NB）</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="41-缺失值的替换">4.1 <strong>缺失值的替换</strong></span></h4><p><strong>scikit-learn中填充缺失值的API是Imputer类，使用方法如下：</strong></p>
<p>参数strategy有三个值可选：mean(平均值)，median(中位数)，most_frequent(众数)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rom sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 缺失值填补的时候必须得是float类型</span></span><br><span class="line"><span class="comment"># 缺失值要填充为np.nan，它是浮点型，strategy是填充的缺失值类型，这里填充平均数，axis代表轴，这里第0轴是列</span></span><br><span class="line">im = Imputer(missing_values=<span class="string">&#x27;NaN&#x27;</span>,strategy=<span class="string">&#x27;mean&#x27;</span>,axis=<span class="number">0</span>)</span><br><span class="line">data = im.fit_transform([[<span class="number">1</span>, <span class="number">2</span>], </span><br><span class="line">                         [np.nan, <span class="number">3</span>], </span><br><span class="line">                         [<span class="number">7</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<h4><span id="42-缺失值的删除">4.2 缺失值的删除</span></h4><h3><span id="五-异常值处理">五、异常值处理</span></h3><h2><span id="数据预处理qampa">数据预处理Q&amp;A</span></h2><h3><span id="1-lr为什么要离散化"><strong><font color="red"> 1、LR为什么要离散化？</font></strong></span></h3><h4><span id="学习-连续特征的离散化在什么情况下将连续的特征离散化之后可以获得更好的效果"></span></h4><p><strong>问题描述：</strong>发现CTR预估一般都是用LR，而且特征都是离散的，为什么一定要用离散特征呢？这样做的好处在哪里？求大拿们解答。</p>
<h4><span id="答案一严林"><strong>答案一（<a href="https://www.zhihu.com/search?q=严林&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">严林</a>）：</strong></span></h4><p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ol>
<li>离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>【鲁棒性】离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则为0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>【模型假设】<strong>逻辑回归属于广义线性模型，表达能力受限</strong>；单变量离散化为N个后，每个变量有独立的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li>
<li>【特征交叉】离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>特征离散化以后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险；</li>
</ol>
<p><strong><font color="red"> <a href="https://www.zhihu.com/search?q=李沐&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">李沐</a>曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型”同“少量连续特征+复杂模型”的权衡。</font></strong></p>
<blockquote>
<p>  这里我写下我关于上面某些点的理解，有问题的欢迎指出：</p>
<ol>
<li><p>假设目前有两个连续的特征：『年龄』和『收入』，预测用户的『魅力指数』；</p>
<p>第三点: <strong>LR是广义线性模型</strong>，因此如果特征『年龄』不做离散化直接输入，那么只能得到『年龄』和魅力指数的一个线性关系。但是这种线性关系是不准确的，并非年龄越大魅力指一定越大；如果将年龄划分为M段，则可以针对每段有一个对应的权重；这种分段的能力为模型带来类似『折线』的能力，也就是所谓的非线性<br><strong>连续变量的划分，naive的可以通过人为先验知识划分，也可以通过训练单特征的决策树桩，根据Information Gain/Gini系数等来有监督的划分。</strong><br>假如『年龄』离散化后，共有N段，『收入』离散化后有M段；此时这两个离散化后的特征类似于<strong>CategoryFeature</strong>，对他们进行<strong>OneHotEncode</strong>，即可以得到 M + N的 01向量；例如： 0 1 0 0， 1 0 0 0 0；<br>第四点: <strong>特征交叉</strong>，可以理解为上述两个向量的互相作用，作用的方式可以例如是 &amp;和|操作（这种交叉方式可以产生一个 M * N的01向量；）<br>上面特征交叉，可以类比于决策树的决策过程。例如进行&amp;操作后，得到一个1，则可以认为产生一个特征 （a &lt; age &lt; b &amp;&amp; c &lt; income &lt; d）;将特征空间进行的非线性划分，也就是所谓的引入非线性；</p>
</li>
</ol>
</blockquote>
<h4><span id="答案二周开拓"><strong>答案二（周开拓）：</strong></span></h4><p><strong><font color="red"> 机器学习里当然并没有free lunch，一个方法能work，必定是有假设的。如果这个假设和真实的问题及数据比较吻合，就能work。</font></strong></p>
<p>对于LR这类的模型来说，假设基本如下：</p>
<ul>
<li><strong>局部<a href="https://www.zhihu.com/search?q=平坦性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">平坦性</a>，或者说连续性</strong>。对于连续特征x来说，在任何一个取值x0的邻域附近，这个特征对预估目标y的影响也在一个足够小的邻域内变化。比如，人年龄对点击率的影响，x0=30岁假设会产生一定的影响，那么x=31或者29岁，这个影响和x0=30岁的影响差距不会太大；</li>
<li><strong>x对y的影响，这个函数虽然局部比较平坦，但是不太规律，如果你知道这个影响是个严格的直线</strong>（或者你有先验知识知道这个影响一定可以近似于一个参数不太多的函数），<strong>显然也没必要去做离散化</strong>。当然这条基本对于绝大多数问题都是成立的，因为基本没有这种好事情。</li>
</ul>
<p>假设一个最简单的问题，binary classification，y=0/1，x是个连续值。你希望学到一个logloss足够小的y=f(x)。</p>
<p>那么有一种做法就是，在数据轴上切若干段，每一段观察训练样本里y为1的比例，以这个比例作为该段上y=f(x)的值。这个当然不是LR训练的过程，但是就是离散化的思想。你可以发现：</p>
<ul>
<li><strong>如果每一段里面都有足够多的样本，那么在这一段里的y=f(x)值的点估计就比较可信</strong>；</li>
<li><font color="red">如果x在数轴上分布不太均匀，比如是<strong>指数分布或者周期分布</strong>的，这么做可能会有问题，因而你要先<strong>对x取个log，或者去掉周期性</strong></font>；</li>
</ul>
<p>这就告诉了你应该怎么做离散化：<strong><font color="red"> 尽可能保证每个分段里面有足够多的样本，尽量让样本的分布在数轴上均匀一些。</font></strong></p>
<p>结语：<strong>本质上连续特征离散化，可以理解为连续信号怎么转化为数字信号，好比我们计算机画一条曲线，也是变成了画一系列线段的问题。</strong>用分段函数来表达一个连续的函数在大多数情况下，都是work的。想取得好的效果需要：</p>
<ul>
<li>你的分段足够小，以使得在每个分段内x对y的影响基本在一个不大的邻域内，或者你可以忍受这个变化的幅度；</li>
<li>你的分段足够大，以使得在每个分段内有足够的样本，以获得可信的f(x)也就是权重；</li>
<li>你的分段策略使得在每个x的分段中，样本的分布尽量均匀（当然这很难），一般会根据先验知识先对x做一些变化以使得变得均匀一些；</li>
<li>如果你有非常强的x对y的先验知识，比如严格线性之类的，也未必做离散化，但是这种先验在计算广告或者推荐系统里一般是不存在的，也许其他领域比如CV之类的里面是可能存在的；</li>
</ul>
<p>最后还有个特别大的<strong>LR用离散特征的好处就是LR的特征是并行的，每个特征是并行同权的</strong>，如果有异常值的情况下，如果这个异常值没见过，那么LR里因为没有这个值的权重，最后对<a href="https://www.zhihu.com/search?q=score&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;122387176&quot;}">score</a>的贡献为0，最多效果不够好，但是不会错的太离谱。另外，如果你debug，很容易查出来是哪个段上的权重有问题，比较好定位和解决。</p>
<h3><span id="2-树模型为什么离散化"><strong><font color="red">2、树模型为什么离散化？</font></strong></span></h3><h4><span id="cart树的离散化">Cart树的离散化：</span></h4><p><strong>分类：</strong></p>
<ul>
<li><p><strong><font color="red">如果特征值是连续值：CART的处理思想与C4.5是相同的，即将连续特征值离散化。唯一不同的地方是度量的标准不一样，</font></strong> <strong>CART采用基尼指数，而C4.5采用信息增益比</strong>。</p>
</li>
<li><p>如果当前节点为连续属性，<strong>CART树中该属性（剩余的属性值）后面还可以参与子节点的产生选择过程</strong>。</p>
</li>
</ul>
<p><strong>回归：</strong></p>
<p>对于连续值的处理，<strong>CART 分类树采用基尼系数的大小来度量特征的各个划分点</strong>。<strong>在回归模型中，我们使用常见的和方差度量方式</strong>，对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> ，求出使 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 各自<strong>集合的均方差最小</strong>，同时 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=c_1" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 数据集的样本输出均值， <img src="https://www.zhihu.com/equation?tex=c_2" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 数据集的样本输出均值。</p>
<h4><span id="lgb直方图算法优点">LGB直方图算法优点：</span></h4><p><strong>内存小、复杂度降低、直方图加速【分裂、并行通信、缓存优化】</strong></p>
<ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于 直方图算法，则只需要(1x样本数x维 度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的 bin 值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p>
</li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为$k$的树的时间复杂度：对特征所有取值的排序为$O(NlogN)$，$N$为样本点数目，若有$D$维特征，则$O(kDNlogN)$，而直方图算法需要$O(kD \times bin)$ (bin是histogram 的横轴的数量，一般远小于样本数量$N$)。</p>
</li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>==两个维度==</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的$k$个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
</li>
<li><p><strong>数据并行优化</strong>，用 histgoram 可以大幅降低通信代价。用 pre-sorted 算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst 在并行的时候也使用 histogram 进行通信。</p>
</li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM 所使用直方图算法对 Cache 天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</p>
</li>
</ul>
<h3><span id="3-归一化">3、归一化？</span></h3><h3><span id="4-赵兄-特征处理">4、赵兄-特征处理</span></h3><p><img src="../../../../../Library/Containers/com.tencent.xinWeChat/Data/Library/Application Support/com.tencent.xinWeChat/2.0b4.0.9/076b7987d1501ed1ebeee6aecab0dccc/Message/MessageTemp/f381afde205729b015fb0a76a283f729/Image/891647863651_.pic.jpg" alt="891647863651_.pic"></p>
<ul>
<li>滑动窗口、随机采样、加权差分特征</li>
<li>均值插补插补，稀疏矩阵技术问题</li>
<li>分层统计、差分特征</li>
<li><strong>二阶交叉统计</strong></li>
</ul>
]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程（3）不平衡数据集*</title>
    <url>/posts/XZ18TA/</url>
    <content><![CDATA[<h2><span id="不平衡数据问题">不平衡数据问题</span></h2><blockquote>
<p>  <strong>实际上，很多时候，数据不平衡并没有啥负面影响，并不是数据不平衡了，就一定要处理。如果你只是为了做而做，我有99%的自信告诉你，你做了也是白做，啥收益都没有。</strong></p>
<ul>
<li>机器学习中不平衡数据的预处理：<a href="https://capallen.gitee.io/2019/Deal-with-imbalanced-data-in-ML.html">https://capallen.gitee.io/2019/Deal-with-imbalanced-data-in-ML.html</a></li>
<li>如何处理数据中的「类别不平衡」？：<a href="https://zhuanlan.zhihu.com/p/32940093">https://zhuanlan.zhihu.com/p/32940093</a></li>
<li><strong>==不平衡数据集处理方法==</strong>：<a href="https://blog.csdn.net/asialee_bird/article/details/83714612">https://blog.csdn.net/asialee_bird/article/details/83714612</a></li>
<li><strong>==不平衡数据究竟出了什么问题？==</strong>：<a href="https://www.zhihu.com/column/jiqizhixin">https://www.zhihu.com/column/jiqizhixin</a></li>
<li>数据挖掘时，当正负样本不均，代码如何实现改变正负样本权重? - 十三的回答 - 知乎 <a href="https://www.zhihu.com/question/356640889/answer/2299286791">https://www.zhihu.com/question/356640889/answer/2299286791</a></li>
<li><strong>样本权重对逻辑回归评分卡的影响探讨</strong> - 求是汪在路上的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/110982479">https://zhuanlan.zhihu.com/p/110982479</a></li>
</ul>
</blockquote>
<h4><span id="为什么很多模型在训练数据不均衡时会出问题">为什么很多模型在训练数据不均衡时会出问题？</span></h4><p><strong>本质原因是</strong>：<strong>模型在训练时优化的目标函数和在测试时使用的评价标准不一致。</strong>这种”不一致“可能是训练数据的样本分布与测试数据分布不一致；</p>
<h2><span id="一-不平衡数据集的主要处理方式"><strong>一、不平衡数据集的主要处理方式？</strong></span></h2><h3><span id="11-数据的角度">1.1 <strong>数据的角度</strong></span></h3><p>主要方法为采样，分为<strong>欠采样</strong>和<strong>过采样</strong>以及对应的一些改进方法。[<strong>python imblearn库</strong>]<strong><font color="red">尊重真实样本分布，人为主观引入样本权重，反而可能得出错误的结论。</font></strong></p>
<h4><span id="业务角度"><strong><font color="red"> 业务角度</font></strong>：</span></h4><ul>
<li><strong>时间因素</strong>，对近期样本提高权重，较远样本降低权重。这是考虑近期样本与未来样本之间的“相似度”更高，希望模型学到更多近期样本的模式。</li>
<li><strong>贷款类型</strong>，不同额度、利率、期限的样本赋予不同权重，这需要结合业务未来的发展方向。例如，未来业务模式希望是小额、短期、低利率，那就提高这批样本的权重。</li>
<li><strong>样本分群</strong>，不同群体赋予不同权重。例如，按流量获客渠道，如果未来流量渠道主要来自平台A，那么就提高这批样本权重。</li>
</ul>
<h4><span id="技术角度"><strong><font color="red"> 技术角度：</font></strong></span></h4><ul>
<li><p><strong>欠采样</strong>：</p>
<ul>
<li><p><strong>EasyEnsemble</strong>：从多数类$S_{max}$上随机抽取一个子集，与其他类训练一个分类器；重复若干次，多个分类器融合。</p>
</li>
<li><p><font color="red"><strong>BalanceCascade</strong></font>：从多数类$S_{max}$上随机抽取一个子集，与其他类训练一个分类器；<strong>剔除能被分类正确的分类器</strong>，重复若干次，多个分类器融合。</p>
</li>
<li><p>==<strong>NearMIss</strong>：利用K邻近信息挑选具有代表性的样本==。</p>
</li>
<li><p>==<strong>one-side Selection</strong>：采用数据清洗技术==。</p>
</li>
</ul>
</li>
<li><p><strong>==过采样==</strong>：</p>
<ul>
<li><p><strong>随机采样</strong></p>
</li>
<li><p><strong>SMOTE算法</strong>：对少数类$S_{min}$中每个样本x的K近邻随机选取一个样本y，在x，y的连线上随机选取一个点作为新的样本点。</p>
</li>
<li><p>==<strong>Borderline-SMOTE、ADASYN改进算法等</strong>==</p>
</li>
</ul>
</li>
<li><h5><span id="分层抽样技术批量训练分类器的分层抽样技术-当面对不平衡类问题时这种技术通过消除批次内的比例差异可使训练过程更加稳定">==分层抽样技术==：批量训练分类器的「分层抽样」技术。当面对不平衡类问题时，这种技术（通过消除批次内的比例差异）可使训练过程更加稳定。</span></h5></li>
</ul>
<h3><span id="1-2-算法的角度"><strong>1. 2 算法的角度</strong></span></h3><p>考虑<strong>不同误分类情况代价的差异性</strong>对算法进行优化，主要是基于<strong>代价敏感学习算法</strong>(Cost-Sensitive Learning)，代表的算法有<strong>==adacost==</strong>。<a href="实现基于代价敏感的AdaCost算法">实现基于代价敏感的AdaCost算法</a></p>
<ul>
<li><p><strong>代价函数</strong>：可以增加小类样本的权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。刚开始，可以设置每个类别的权值与样本个数比例的倒数，然后可以使用过采样进行调优。</p>
<blockquote>
<p>  这种方法的难点在于设置合理的权重，实际应用中一般让各个分类间的加权损失值近似相等。当然这并不是通用法则，还是需要具体问题具体分析。</p>
</blockquote>
</li>
<li><h5><span id="xgb自定义损失函数-jhwjhw0123imbalance-xgboost-focal-loss">XGB自定义损失函数 /<strong><a href="https://github.com/jhwjhw0123/Imbalance-XGBoost">Imbalance-XGBoost </a></strong>【Focal Loss】：</span></h5><p> <img src="https://camo.githubusercontent.com/d79dd87cc30238a9124e1d26f787aed887a587d6061dcba4cdea9ebaec84836a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c6470697b3135307d2673706163653b4c5f7b777d2673706163653b3d2673706163653b2d5c73756d5f7b693d317d5e7b6d7d5c6861747b797d5f7b697d28312d795f7b697d295e7b5c67616d6d617d5c746578747b6c6f677d28795f7b697d292673706163653b2b2673706163653b28312d5c6861747b797d5f7b697d29795f7b697d5e7b5c67616d6d617d5c746578747b6c6f677d28312d795f7b697d29" alt="img" style="zoom: 67%;"></p>
</li>
</ul>
<h3><span id="13-分类方式">1.3 <strong>分类方式</strong></span></h3><p>可以把小类样本作为<strong>异常点</strong>(outliers)，把问题转化为<strong>异常点检测问题(anomaly detection)</strong>。此时分类器需要学习到大类的决策分界面，即分类器是一个<strong>单个类分类器（One Class Classifier）</strong>。代表的算法有<font color="red"> <strong>One-class SVM</strong></font>。</p>
<blockquote>
<h4><span id="一类分类算法">一类分类算法:</span></h4><p>  不平衡数据集的一类分类算法:<a href="https://machinelearningmastery.com/one-class-classification-algorithms/">https://machinelearningmastery.com/one-class-classification-algorithms/</a></p>
<p>  一类分类是机器学习的一个领域，它提供了异常值和异常检测的技术,如何使一类分类算法适应具有严重偏斜类分布的不平衡分类,如何拟合和评估 SVM、隔离森林、椭圆包络、局部异常因子等一类分类算法。</p>
<h4><span id="不平数据集的划分方法">不平数据集的划分方法？</span></h4><ul>
<li>K折交叉验证？</li>
<li><p>自助法？</p>
<h4><span id="不平数据集的评价方法">不平数据集的评价方法？</span></h4><p>G-Mean和ROC曲线和AUC。Topk@P</p>
</li>
<li><p><strong>AP衡量的是学出来的模型在每个类别上的好坏，==mAP衡量==的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。</strong></p>
</li>
</ul>
</blockquote>
<h3><span id="二-类别不平衡如何得到一个不错的分类器">二、「类别不平衡」如何得到一个不错的分类器？</span></h3><blockquote>
<p>  <strong>微调</strong>：<a href="https://zhuanlan.zhihu.com/p/32940093">如何处理数据中的「类别不平衡」？</a></p>
</blockquote>
<p>机器学习中常常会遇到数据的<strong>类别不平衡（class imbalance）</strong>，也叫数据偏斜（class skew）。以常见的二分类问题为例，我们希望预测病人是否得了某种罕见疾病。但在历史数据中，阳性的比例可能很低（如百分之0.1）。在这种情况下，学习出好的分类器是很难的，而且在这种情况下得到结论往往也是很具迷惑性的。</p>
<p>以上面提到的场景来说，如果我们的分类器<strong>总是</strong>预测一个人未患病，即预测为反例，那么我们依然有高达99.9%的预测准确率。然而这种结果是没有意义的，这提出了今天的第一个问题，如何有效在类别不平衡的情况下评估分类器？</p>
<p><strong>当然，本文最终希望解决的问题是：在数据偏斜的情况下，如何得到一个不错的分类器？如果可能，是否可以找到一个较为简单的解决方法，而规避复杂的模型、数据处理，降低我们的工作量。</strong></p>
<h4><span id="21-类别不平衡下的评估问题"><strong>2.1 类别不平衡下的评估问题</strong>?</span></h4><p>而<strong>当类别不平衡时，准确率就非常具有迷惑性</strong>，而且意义不大。给出几种主流的评估方法：</p>
<ul>
<li><strong>ROC</strong>是一种常见的替代方法，全名receiver operating curve，计算ROC曲线下的面积是一种主流方法</li>
<li><strong>Precision-recall curve</strong>和ROC有相似的地方，但定义不同，计算此曲线下的面积也是一种方法</li>
<li><strong>Precision@n</strong>是另一种方法，特制将分类阈值设定得到恰好n个正例时分类器的precision</li>
<li>Average precision也叫做平均精度，主要描述了precision的一般表现，在异常检测中有时候会用</li>
<li>直接使用Precision也是一种想法，但此时的假设是分类器的阈值是0.5，因此意义不大</li>
</ul>
<blockquote>
<p>  本文的目的不是介绍一般的分类评估标准，简单的科普可以参看：<a href="https://www.zhihu.com/question/19645541">如何解释召回率与准确率？</a></p>
</blockquote>
<h4><span id="22-解决类别不平衡中的奇淫巧技有什么"><strong>2.2 解决类别不平衡中的“奇淫巧技”有什么？</strong></span></h4><p>对于类别不平衡的研究已经有很多年了，在资料[1]中就介绍了很多比较复杂的技巧。结合我的了解举几个简单的例子：</p>
<blockquote>
<p>  [1] He, H. and Garcia, E.A., 2009. Learning from imbalanced data. <em>IEEE Transactions on knowledge and data engineering</em>, <em>21</em>(9), pp.1263-1284.</p>
</blockquote>
<ul>
<li>对数据进行采用的过程中通过相似性同时生成并插样“少数类别数据”，叫做SMOTE算法</li>
<li>对数据先进行聚类，再将大的簇进行随机欠采样或者小的簇进行数据生成</li>
<li>把监督学习变为无监督学习，舍弃掉标签把问题转化为一个无监督问题，如异常检测</li>
<li><strong>先对多数类别进行随机的欠采样，并结合boosting算法进行集成学习</strong></li>
</ul>
<h4><span id="23-简单通用的算法有哪些">==<strong>2.3 简单通用的算法有哪些？</strong>==</span></h4><ul>
<li>对较多的那个类别进行欠采样(under-sampling)，舍弃一部分数据，使其与较少类别的数据相当</li>
<li>对较少的类别进行过采样(over-sampling)，重复使用一部分数据，使其与较多类别的数据相当</li>
<li><strong>阈值调整（threshold moving）</strong>，将原本默认为0.5的阈值调整到 较少类别/（较少类别+较多类别）即可</li>
</ul>
<p>当然很明显我们可以看出，<strong>第一种和第二种方法都会明显的改变数据分布，我们的训练数据假设不再是真实数据的无偏表述</strong>。在第一种方法中，我们<strong>浪费了很多数据</strong>。而第二类方法中有无中生有或者重复使用了数据，会导致过拟合的发生。</p>
<p><strong>因此欠采样的逻辑中往往会结合集成学习来有效的使用数据，假设正例数据n，而反例数据m个。我们可以通过欠采样，随机无重复的生成（k=n/m）个反例子集，并将每个子集都与相同正例数据合并生成k个新的训练样本。我们在k个训练样本上分别训练一个分类器，最终将k个分类器的结果结合起来，比如求平均值。这就是一个简单的思路，也就是==Easy Ensemble== [5]。</strong></p>
<p>但不难看出，其实这样的过程是需要花时间处理数据和编程的，对于很多知识和能力有限的人来说难度比较大。特此推荐两个简单易行且效果中上的做法：</p>
<ul>
<li>简单的调整阈值，不对数据进行任何处理。此处特指将分类阈值从0.5调整到正例比例</li>
<li>使用现有的集成学习分类器，如随机森林或者xgboost，<strong>==并调整分类阈值==</strong></li>
</ul>
<p><strong>提出这样建议的原因有很多。首先，==简单的阈值调整从经验上看往往比过采样和欠采样有效==</strong> [6]。其次，如果你对统计学知识掌握有限，而且编程能力一般，在集成过程中更容易出错，还不如使用现有的集成学习并调整分类阈值。</p>
<h4><span id="24-一个简单但有效的方案"><strong>2.4 一个简单但有效的方案</strong></span></h4><p>经过了上文的分析，我认为一个比较靠谱的解决方案是：</p>
<ul>
<li>不对数据进行过采样和欠采样，但使用现有的集成学习模型，如随机森林</li>
<li>输出随机森林的预测概率，<strong>调整阈值得到最终结果</strong></li>
<li><strong>选择合适的评估标准，如precision@n</strong></li>
</ul>
<h3><span id="三-脉脉数据集不平衡应该思考什么">三、脉脉：数据集不平衡应该思考什么</span></h3><p>首先, 猜测一下, 你研究的数据存在着较 大的不平衡, 你还是比较关注正类(少数类) 样本的, 比如【想要识别出 有信用风险 的 人】那么就要谈一下你所说的【模型指标还行】这个问题。<strong>auc这种复合指标先不提, precision代表的是, 你预测的信用风险人群, 其中有多少是真的信用风险人群。recall 代表的是, “真的信用风险人群”有多少被你识别出来了</strong>；</p>
<ul>
<li>所以, 倘若你比较关注的是【我想要找出 所有”可能有违约风险的人”】宁可错杀也不 放过。那么你应该重点关注的就是召回率 recall。在此基础上, 尽量提高precision。</li>
<li>你把训练集的正负样本控制在64左右, 那 么你是怎么控制的呢, 是单纯用了数据清理技术, 还是单纯生成了一些新的样本, 还是怎么做的？</li>
<li><font color="red">**如果条件允许, 可以查看一下你被错分的 样本, 看看被错分的原因可能是什么, 是因为类重叠, 还是有少数类的分离还是单纯的因为不平衡比太夸张所以使分类器产生偏倚?** </font></li>
<li><font color="red">不知道你用的什么模型, 但是现在有一些把重采样和分类器结合在一起的集成学习方法, 可以试试看。</font></li>
<li>维度太高的时候, <strong>特征的提取很重要</strong>呀！</li>
<li>当做异常检测问题可能会好一些?</li>
</ul>
<h3><span id="四-样本准备与权重指标">四、样本准备与权重指标</span></h3><blockquote>
<p>  样本权重对逻辑回归评分卡的影响探讨: <a href="https://zhuanlan.zhihu.com/p/110982479">https://zhuanlan.zhihu.com/p/110982479</a></p>
</blockquote>
<h4><span id="风控业务背景"><strong>风控业务背景</strong></span></h4><p><strong>在统计学习建模分析中，样本非常重要，它是我们洞察世界的窗口</strong>。在建立逻辑回归评分卡时，我们也会考虑对样本赋予不同的权重weight，希望模型学习更有侧重点。</p>
<p>诚然，我们可以通过实际数据测试来检验赋权前后的差异，但我们更希望从理论上分析其合理性。毕竟理论可以指导实践。本文尝试探讨样本权重对逻辑回归评分卡的影响，以及从业务和技术角度分析样本权重调整的操作方法。</p>
<h4><span id="part-1-样本加权对woe的影响"><strong>Part 1. 样本加权对WOE的影响</strong></span></h4><p>在<strong>《</strong><a href="https://zhuanlan.zhihu.com/p/80134853">WOE与IV指标的深入理解应用</a><strong>》</strong>一文中，我们介绍了WOE的概念和计算方法。在逻辑回归评分卡中，其具有重要意义。其公式定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=++WOE_i+%3D+ln%28+%5Cfrac%7BGood_i%7D%7BGood_T%7D%2F+%5Cfrac%7BBad_i%7D%7BBad_T%7D+%29+%3D+ln%28%5Cfrac%7BGood_i%7D%7BBad_i%7D%29+-+ln%28%5Cfrac%7BGood_T%7D%7BBad_T%7D%29+%5Ctag%7B1%7D" alt="[公式]"></p>
<p><strong>现在，我们思考在计算WOE时，是否要考虑样本权重呢？</strong></p>
<p>如图1所示的样本，我们希望对某些样本加强学习，因此对年龄在46岁以下的样本赋予权重1.5，而对46岁以上的样本赋予权重1.0，也就是加上权重列weight。此时再计算WOE值，我们发现数值发生变化。这是因为权重的改变，既影响了局部bucket中的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> ，也影响了整体的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> 。</p>
<p><img src="https://pic1.zhimg.com/80/v2-538659592a96c40bd4098f29d30d144c_1440w.jpg" alt="img">我们有2种对样本赋权后的模型训练方案，如图2所示。</p>
<ul>
<li>方案1：WOE变换利用原训练集，LR模型训练时利用加权后的样本。</li>
<li>方案2: WOE变换和LR模型训练时，均使用加权后的样本。</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-af2b686a4cdddff60556e90545378f91_1440w.jpg" alt="img"></p>
<p>个人更倾向于第一种方案，原因在于：<strong>WOE变换侧重变量的可解释性，引入样本权重会引起不可解释的困扰。</strong></p>
<h4><span id="part-2-采样对lr系数的影响"><strong>Part 2. 采样对LR系数的影响</strong></span></h4><p>我们定义特征向量 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D+%3D+x_1%2C+x_2%2C...%2C+x_n" alt="[公式]"> 。记 <img src="https://www.zhihu.com/equation?tex=G+%3D+Good%2C+B+%3D+Bad" alt="[公式]"> ，那么逻辑回归的公式组成便是：</p>
<p><img src="https://www.zhihu.com/equation?tex=Ln%28odds%28G%7C%5Ctextbf%7Bx%7D%29%29+%3D+Ln%28%5Cfrac%7Bp_Gf%28%5Ctextbf%7Bx%7D%7CG%29%7D%7Bp_Bf%28%5Ctextbf%7Bx%7D%7CB%29%7D%29+%3D+Ln%28%5Cfrac%7Bp_G%7D%7Bp_B%7D%29+%2B+Ln%28%5Cfrac%7Bf%28%5Ctextbf%7Bx%7D%7CG%29%7D%7Bf%28%5Ctextbf%7Bx%7D%7CB%29%7D%29+%5C%5C+%3D+Ln%28%5Cfrac%7Bp_G%7D%7Bp_B%7D%29+%2B+Ln%28%5Cfrac%7Bf%28x_1%2Cx_2%2C...%2Cx_n%7CG%29%7D%7Bf%28x_1%2Cx_2%2C...%2Cx_n%7CB%29%7D%29+%5C%5C+%3D+Ln%28%5Cfrac%7Bp_G%7D%7Bp_B%7D%29+%2B+Ln%28%5Cfrac%7Bf%28x_1%7CG%29%7D%7Bf%28x_1%7CB%29%7D%29+%2B+Ln%28%5Cfrac%7Bf%28x_2%7CG%29%7D%7Bf%28x_2%7CB%29%7D+%2B+%5Cspace+...+%5Cspace+%2B+Ln%28%5Cfrac%7Bf%28x_n%7CG%29%7D%7Bf%28x_n%7CB%29%7D%5C%5C+%3D+Ln%28odds_%7Bpop%7D%29+%2B+Ln%28odds_%7Binfo%7D+%28%5Ctextbf%7Bx%7D%29%29+%5Ctag%7B2%7D" alt="[公式]"></p>
<p>其中，第2行到第3行的变换是基于朴素贝叶斯假设，即<strong>自变量 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 之间相互独立</strong>。</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=odds_%7Bpop%7D" alt="[公式]"> 是指总体（训练集）的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]">，指<strong>先验信息</strong><img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]">。</li>
<li><img src="https://www.zhihu.com/equation?tex=odds_%7Binfo%7D+%28%5Ctextbf%7Bx%7D%29" alt="[公式]"> 是指自变量引起的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> 变化，我们称为<strong>后验信息</strong><img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> 。</li>
</ul>
<p><strong><font color="red"> 因此，随着观察信息的不断加入，对群体的好坏 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> 判断将越来越趋于客观。</font></strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-0021358b05ebb5fea25073cceea1da2d_1440w.jpg" alt="img"></p>
<h5><span id="样本权重调整直接影响先验项也就是截距-那对系数的影响呢">样本权重调整直接影响先验项，也就是截距。那对系数的影响呢？</span></h5><p>接下来，我们以过采样（Oversampling）和欠采样（Undersampling）为例，分析采样对LR系数的影响。如图4所示，对于不平衡数据集，过采样是指对正样本简单复制很多份；欠采样是指对负样本随机抽样。最终，正负样本的比例将达到1:1平衡状态。<br><img src="https://pic4.zhimg.com/80/v2-c49197fdd37023e75daabd7e74feb11b_1440w.jpg" alt="img">图 4 - 欠采样（左）和过采样（右）</p>
<p>我们同样从贝叶斯角度进行解释：</p>
<h2><span id><img src="https://www.zhihu.com/equation?tex=P%28B%7C%5Ctextbf%7Bx%7D%29+%3D+%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CB%29P%28B%29%7D%7BP%28%5Ctextbf%7Bx%7D%29%7D++%3D+%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CB%29P%28B%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29P%28B%29+%2B+P%28%5Ctextbf%7Bx%7D%7CG%29P%28G%29%7D+%5C%5C+%5CLeftrightarrow+%5Cfrac%7B1%7D%7BP%28B%7C%5Ctextbf%7Bx%7D%29%7D+%3D++1+%2B+%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CG%29P%28G%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29P%28B%29%7D+%5C%5C+%5CLeftrightarrow++Ln%28%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CG%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29%7D%29+%3D+Ln%28%5Cfrac%7B1%7D%7BP%28B%7C%5Ctextbf%7Bx%7D%29%7D+-+1%29++-+Ln%28%5Cfrac%7BP%28G%29%7D%7BP%28B%29%7D%29+%5C%5C+%5CLeftrightarrow++Ln%28%5Cfrac%7BP%28G%7C%5Ctextbf%7Bx%7D%29%7D%7BP%28B%7C%5Ctextbf%7Bx%7D%29%7D%29++%3D+Ln%28%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CG%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29%7D%29+%2B+Ln%28%5Cfrac%7BP%28G%29%7D%7BP%28B%29%7D%29+%5Ctag%7B3%7D" alt="[公式]"></span></h2><p>假设采样处理后的训练集为 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D%27" alt="[公式]"> 。记 <img src="https://www.zhihu.com/equation?tex=%5C%23+B" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5C%23+G" alt="[公式]"> 分别表示正负样本数，那么显然：</p>
<p><img src="https://www.zhihu.com/equation?tex=odds+%3D+%5Cfrac%7B%5C%23+G%7D%7B%5C%23+B%7D+%5Cne+%5Cfrac%7B%5C%23+G%27%7D%7B%5C%23+B%27%7D+%3D+odds%27+%5Ctag%7B4%7D" alt="[公式]"></p>
<p>由于 <img src="https://www.zhihu.com/equation?tex=Ln%28%5Cfrac%7BP%28G%29%7D%7BP%28B%29%7D%29++%3D+Ln%28%5Cfrac%7B%5C%23+G%7D%7B%5C%23+B%7D%29+" alt="[公式]"> ，因此对应<strong>截距将发生变化</strong>。</p>
<p>无论是过采样，还是欠采样，处理后的新样本都和原样本服从同样的分布，即满足：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28%5Ctextbf%7Bx%7D%7CG%29+%3D+P%28%5Ctextbf%7Bx%7D%27%7CG%27%29+%5C%5C+P%28%5Ctextbf%7Bx%7D%7CB%29+%3D+P%28%5Ctextbf%7Bx%7D%27%7CB%27%29+%5Ctag%7B5%7D" alt="[公式]"></p>
<p>因此， <img src="https://www.zhihu.com/equation?tex=Ln%28%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%7CG%29%7D%7BP%28%5Ctextbf%7Bx%7D%7CB%29%7D%29++%3D+Ln%28%5Cfrac%7BP%28%5Ctextbf%7Bx%7D%27%7CG%27%29%7D%7BP%28%5Ctextbf%7Bx%7D%27%7CB%27%29%7D%29+" alt="[公式]"> ，即<strong>系数不发生变化</strong>。</p>
<p><strong>实践证明，按照做评分卡的方式，做WOE变换，然后跑LR，单变量下确实只有截距影响。而对于多变量，理想情况下，当各自变量相互独立时，LR的系数是不变的，但实际自变量之间多少存在一定的相关性，所以还是会有一定的变化。</strong></p>
<h4><span id="part-3-样本准备与权重指标"><strong><font color="red"> Part 3. 样本准备与权重指标</font></strong></span></h4><p>风控建模的基本假设是<strong>未来样本和历史样本的分布是一致</strong>的。模型从历史样本中拟合 <img src="https://www.zhihu.com/equation?tex=X_%7Bold%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 之间的关系，并根据未来样本的 <img src="https://www.zhihu.com/equation?tex=X_%7Bnew%7D" alt="[公式]"> 进行预测。因此，我们总是在思考，<strong>如何选择能代表未来样本的训练样本。</strong></p>
<p>如图5所示，不同时间段、不同批次的样本总是存在差异，即<strong>服从不同的总体分布</strong>。因此，我们需要<strong>从多个维度来衡量两个样本集之间的相似性。</strong></p>
<p>从迁移学习的角度看，这是一个从源域（source domain）中学习模式，并应用到目标域（target domain）的过程。在这里，源域是训练集，目标域指测试集，或者未来样本。</p>
<p>这就会涉及到一些难点：</p>
<ul>
<li>假设测试集OOT与未来总体分布样本基本一致，但未来样本是不可知且总是在发生变化。</li>
<li>面向测试集效果作为评估指标，会出现在测试集上过拟合现象。<br><img src="https://pic4.zhimg.com/80/v2-b40bd56da51ab37d01fbffbdb8bc91f7_1440w.jpg" alt="img"></li>
</ul>
<p><strong>那么，建模中是否可以考虑建立一个权重指标体系，即综合多方面因素进行样本赋权？我们采取2种思路来分析如何开展。</strong></p>
<p><strong><font color="red"> 业务角度</font></strong>：</p>
<ol>
<li><strong>时间因素</strong>，对近期样本提高权重，较远样本降低权重。这是考虑近期样本与未来样本之间的“相似度”更高，希望模型学到更多近期样本的模式。</li>
<li><strong>贷款类型</strong>，不同额度、利率、期限的样本赋予不同权重，这需要结合业务未来的发展方向。例如，未来业务模式希望是小额、短期、低利率，那就提高这批样本的权重。</li>
<li><strong>样本分群</strong>，不同群体赋予不同权重。例如，按流量获客渠道，如果未来流量渠道主要来自平台A，那么就提高这批样本权重。</li>
</ol>
<p>结合以上各维度，可得到总体采样权重的一种融合方式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=w+%3D+w_1+%2A+w_2+%2A+w_3+%5Ctag%7B6%7D" alt="[公式]"></p>
<p><strong>这种业务角度的方案虽然解释性强，但实际拍定多大的权重显得非常主观，实践中往往需要不断尝试，缺少一些理论指导。</strong></p>
<p><strong><font color="red"> 技术角度：</font></strong></p>
<ol>
<li><strong>过采样、欠采样等，从样本组成上调整正负不平衡</strong>。</li>
<li>代价敏感学习，在损失函数对好坏样本加上不同的代价。比如，坏样本少，分错代价更高。</li>
<li>借鉴Adaboost的做法，对误判样本在下一轮训练时提高权重。</li>
</ol>
<p>在机器学习中，有一个常见的现象——<strong>Covariate Shift</strong>，是指当训练集的样本分布和测试集的样本分布不一致的时候，训练得到的模型无法具有很好的泛化 (Generalization) 能力。</p>
<p>其中一种做法，既然是希望让训练集尽可能像测试集，那就让模型帮助我们做这件事。如图6所示，将测试集标记为1，训练集标记为0，训练一个LR模型，在训练集上预测，概率越高，说明这个样例属于测试集的可能性越大。以此达到样本权重调整的目的。</p>
<p><img src="https://pic1.zhimg.com/80/v2-6a218af9de452a4c58e36a9e8e6d4bb0_1440w.jpg" alt="img"></p>
<h4><span id="part-4-常见工具包的样本赋权"><strong>Part 4. 常见工具包的样本赋权</strong></span></h4><p>现有Logistic Regression模块主要来自sklearn和scipy两个包。很不幸，scipy包并不支持直接赋予权重列。这是为什么呢？有统计学家认为，<strong><font color="red"> 尊重真实样本分布，人为主观引入样本权重，反而可能得出错误的结论。</font></strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-4942078e60d225438f77362c675bec20_1440w.jpg" alt="img"></p>
<p>因此，我们只能选择用scikit-learn。样本权重是如何体现在模型训练过程呢？查看源码后，发现目前主要是体现在<strong>损失函数</strong>中，即<strong>代价敏感学习。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Logistic loss is the negative of the log of the logistic function.</span></span><br><span class="line"><span class="comment"># 添加L2正则项的逻辑回归对数损失函数</span></span><br><span class="line">out = -np.<span class="built_in">sum</span>(sample_weight * log_logistic(yz)) + <span class="number">.5</span> * alpha * np.dot(w, w)</span><br></pre></td></tr></table></figure>
<p><strong><font color="red"> 样本权重对决策分割面的影响:</font></strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-699ad7ac670c27a5b0963b8b104ad7fc_1440w.jpg" alt="img"></p>
<p>以下是scikit-learn包中的逻辑回归参数列表说明，可以发现调节样本权重的方法有两种：</p>
<ul>
<li>在class_weight参数中使用balanced</li>
<li>在调用fit函数时，通过sample_weight调节每个样本权重。</li>
</ul>
<p>如果同时设置上述2个参数，<strong>那么样本的真正权重是class_weight * sample_weight.</strong></p>
<p><strong>那么，在评估模型的指标时，是否需要考虑抽样权重，即还原真实场景下的模型评价指标？</strong>笔者认为，最终评估还是需要还原到真实场景下。例如，训练集正负比例被调节为1:1，但这并不是真实的 <img src="https://www.zhihu.com/equation?tex=odds" alt="[公式]"> ，在预测时将会偏高。因此，仍需要进行模型校准。</p>
<h4><span id="part-5-总结"><strong>Part 5. 总结</strong></span></h4><p>本文系统整理了样本权重的一些观点，但目前仍然没有统一的答案。据笔者所知，目前在实践中还是采取以下几种方案：</p>
<ol>
<li>尊重原样本分布，不予处理，LR模型训练后即为真实概率估计。</li>
<li>结合权重指标综合确定权重，训练完毕模型后再进行校准，还原至真实概率估计。</li>
</ol>
<p>值得指出的是，大环境总是在发生变化，造成样本分布总在偏移。因此，尽可能增强模型的鲁棒性，以及策略使用时根据实际情况灵活调整，两者相辅相成，可能是最佳的使用方法。欢迎大家一起讨论业界的一些做法。</p>
]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程（4）Auto工具</title>
    <url>/posts/GTE114/</url>
    <content><![CDATA[<h2><span id="四-autoeda-工具">四、AutoEDA 工具</span></h2><blockquote>
<p>  盘点Kaggle中常见的AutoEDA工具库： <a href="https://zhuanlan.zhihu.com/p/444405236">https://zhuanlan.zhihu.com/p/444405236</a></p>
</blockquote>
<h4><span id="41-pandas-profiling">4.1 <strong>Pandas Profiling</strong></span></h4><ul>
<li><a href="http://link.zhihu.com/?target=https%3A//pandas-profiling.github.io/pandas-profiling/docs/master/index.html">https://pandas-profiling.github.io/pandas-profiling/docs/master/index.html</a></li>
</ul>
<p><strong>Pandas Profiling</strong>是款比较成熟的工具，可以直接传入DataFrame即可完成分析过程，将结果展示为HTML格式，同时分析功能也比较强大。</p>
<ul>
<li>功能：<strong>字段类型分析、变量分布分析、相关性分析、缺失值分析、重复行分析</strong></li>
<li>耗时：较少</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-97af4843870e76e9d9ef1a4518a27bb2_720w.jpg?source=d16d100b" alt="img" style="zoom: 67%;"></p>
<h4><span id="42-autoviz"><strong>4.2 AutoViz</strong></span></h4><ul>
<li><a href="http://link.zhihu.com/?target=https%3A//github.com/AutoViML/AutoViz">https://github.com/AutoViML/AutoViz</a></li>
</ul>
<p><strong>AutoViz是款美观的数据分析工具</strong>，在进行可视化的同时将结果保存为图片格式。</p>
<ul>
<li>功能：<strong>相关性分析、数值变量箱线图、数值变量分布图</strong></li>
<li>耗时：较多</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-cc73734146a68cd619ed33dd8e5525b1_1440w.jpg?source=d16d100b" alt="img" style="zoom: 67%;"></p>
<h4><span id="43-dataprep">==4.3 <strong>Dataprep</strong>==</span></h4><ul>
<li><a href="http://link.zhihu.com/?target=https%3A//dataprep.ai/">https://dataprep.ai/</a></li>
</ul>
<p><strong>Dataprep是款比较灵活也比较强大的工具，也是笔者最喜欢的。它可以指定列进行分析，同时也可以在Notebook中进行交互式分析。</strong></p>
<ul>
<li>功能：<strong>字段类型分析、变量分布分析、相关性分析、缺失值分析、交互式分析</strong>。</li>
<li>耗时：较多</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-622241d523ff43ded380d414d040c5d7_1440w.jpg?source=d16d100b" alt="img" style="zoom: 67%;"></p>
<h4><span id="44-sweetviz"><strong>4.4 SweetViz</strong></span></h4><ul>
<li><a href="http://link.zhihu.com/?target=https%3A//github.com/fbdesignpro/sweetviz">https://github.com/fbdesignpro/sweetviz</a></li>
</ul>
<p><strong>SweetViz是款强大的数据分析工具，可以很好的分析训练集和测试集，以及目标标签与特征之间的关系</strong>。</p>
<ul>
<li>功能：数据集对比分析、字段类型分析、变量分布分析、目标变量分析</li>
<li>耗时：中等<img src="https://pica.zhimg.com/80/v2-45b365b953f62a41680381891eedca9b_1440w.jpg?source=d16d100b" alt="img" style="zoom:67%;"></li>
</ul>
<h4><span id="45-d-tale">4.5 D-Tale</span></h4><ul>
<li><a href="http://link.zhihu.com/?target=https%3A//github.com/man-group/dtale">https://github.com/man-group/dtale</a></li>
</ul>
<p><code>D-Tale</code>是款功能最为强大的数据分析工具，对单变量的分析过程支持比较好。</p>
<ul>
<li>功能：字段类型分析、变量分布分析、相关性分析、缺失值分析、交互式分析。</li>
<li>耗时：中等</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-7c66f31393a92e6ba7c87e03478277e1_1440w.jpg?source=d16d100b" alt="img" style="zoom:67%;"></p>
<h2><span id="五-警惕特征工程中的陷阱">五、</span></h2><blockquote>
<p>  特征工程(Feature Engineering)是机器学习中的重要环节。在传统的项目中，百分之七十以上的时间都花在了预处理数据上(Data Preprocessing)，其中特征工程消耗了很多时间。</p>
<p>  一般来说，特征工程涵盖的内容非常广泛，包括从<strong>缺失值补全、特征选择、维度压缩，到对输入数据的范围进行变换（Data Scaling）等</strong>。举个简单的例子，一个K-近邻算法的输入数据有两个特征 <img src="https://www.zhihu.com/equation?tex=%7BX_1%2CX_2%7D" alt="[公式]"> ，但 <img src="https://www.zhihu.com/equation?tex=X_1" alt="[公式]"> 这个特征的取值范围在 <img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[公式]"> 而 <img src="https://www.zhihu.com/equation?tex=X_2" alt="[公式]"> 的范围在<img src="https://www.zhihu.com/equation?tex=%5B-1000%2C1000%5D" alt="[公式]"> 。不可避免的，K-近邻的结果取决于距离，那么很容易被取值范围大的特征，也就是此处的 <img src="https://www.zhihu.com/equation?tex=X_2" alt="[公式]"> 所“垄断”。在这种情况下，把 <img src="https://www.zhihu.com/equation?tex=%7BX_1%2CX_2%7D" alt="[公式]"> 的取值调整到可比较的范围上就成了必须。常见的做法有归一化或者标准化，此处不再赘述，可以参考[1]。为了简化内容，本文中的例子仅以归一化作为唯一的特征工程。今天主要说的是：特征工程中的面临的进退两难。</p>
</blockquote>
<h4><span id="51-如何保证-训练集-测试集-预测数据-有相同的输入">5.1 <strong>如何保证 训练集、测试集、预测数据 有相同的输入？</strong></span></h4><p>以刚才的例子为基础，我们把所有数据按照70:30的比例分为训练集和测试集，并打算使用K-近邻进行训练。那么一个令人困扰的问题是，对训练集的特征做归一化后，测试集的特征怎么办？这是一个非常关键的问题，因为训练集<strong>特征归一化</strong>后，测试集的特征范围可能就不同了，因此模型失效。一般有几种思路：</p>
<ul>
<li><strong>方法1：把训练集和测试集合在一起做归一化</strong>，这样特征范围就统一了。之后用训练集做训练，那测试集做测试。<strong>但很明显的，在训练模型时，不应该包括任何测试集的信息</strong>。这种做法会导致存在人为偏差的模型，不能用。</li>
<li><strong>方法2：对训练集单独做归一化，之后对测试集单独做归一化</strong>。这种看法看似也可以，重点在于数据量以及数据的排列顺序。<strong>在数据量大且数据被充分打乱的前提下，这种做法是可行的</strong>。但换句话说，如果有这样的前提假设，那么方法1的结论也是可行的。</li>
<li><strong>方法3：对训练集先做归一化，并保留其归一化参数（如最大、最小值），之后用训练集的归一化参数对测试集做处理。</strong>这种做法看似是可以的。<strong>但风险在于数据量有限的前提下，训练集的参数会导致测试集的结果异常，如产生极大或者极小的数值</strong>。</li>
</ul>
<blockquote>
<p>  其实不难看出，从某种意义上说，三种做法是等价的。在数据量大且充分打乱的前提下，训练集和验证集有相同的分布假设，因此用任意一种其实差别不大。然而这样的假设过于乐观，<strong>且我们在真实情况下应该只有{==训练集+1个测试数据==}，因此方法2是明显不行的</strong>。</p>
<p>  方法1常常被认为是错误的操作，原因是在训练阶段引入了测试数据，这属于未知数据。即使仅仅引入了1个测试数据，如果取值非常极端，依然会导致输出范围有较大的波动。其次，如果对于每一个测试数据都需要用整个训练集来归一的话，那么运算开销会非常大。</p>
<p>  那么似乎备选的只有方案3，即保<strong>留验证集上的归一化参数</strong>，并运用于测试集。这样的做法看似可以，但有不少风险：</p>
<ul>
<li><strong>不是每种特征工程都可以保存参数，很多特征工程是非常繁复的</strong>。</li>
<li>如果测试集数据和训练集数据有很大的差别，那么用测试集的参数会产生异常数据。</li>
</ul>
</blockquote>
<h4><span id="52-可能的解决方案">5.2 可能的解决方案</span></h4><p>在<strong>模型评估阶段</strong>，如果我们假设拥有大量数据，且充分打乱其顺序。那么在划分训练集和测试集前，可以对整体数据进行统一的特征工程。不难看出，这和统计学的大数定理有异曲同工之妙。这种做法是最为高效的，需要的运算量最小。而将“测试数据”暴露给训练模型的风险也并不大，因为大数据量使得分布比较稳定，可以忽略。换个角度来看，当数据量非常大的时候，使用其他方法进行特征工程的开销会过大，不利于模型评估。因此，在<strong>模型评估阶段</strong>，如果符合以上假设，可以用这种方法（也就是上文的方法1）。但退一步说，如果满足这个条件，那么方法3也是等价的。</p>
<p>在<strong>预测阶段</strong>，每次假设我们只有1个测试点，那么最佳方案还是保存训练集上特征工程的参数或者模型，并直接用于未知数据的特征工程（也就是上文的方法3）。</p>
<p>但在<strong>预测阶段</strong>，一个一个数据的预测是非常昂贵的，我们一般会做<strong>“批处理”(batch operation)</strong>。换句话说，就是攒够一定量的预测数据后统一进行预测。在这种情况下，我们：</p>
<ul>
<li>利用方法3，按照顺序对每个训练数据进行处理</li>
<li>利用方法1，风险在于（方法1）会影响训练数据且需要重新训模型</li>
<li><strong>利用方法2，此时较为稳妥</strong>。在批的尺寸较大，且与训练数据分布相同（接近）时，效果应该与方法3一致，但效率可以得到提升</li>
</ul>
<h3><span id="53-总结"><strong>5.3 总结</strong></span></h3><p>这篇文章的重点是：“特征工程虽然重要，但极容易在使用中带来风险。”比如在训练时同时误用了测试数据进行特征工程，也叫做数据泄露(data leakage)。但数据泄露其实也是个伪命题，<strong>当数据量大且分布相同时，使用哪一种方法得到结果应该都近似等价，而更重要的是运行效率</strong>。分类讨论的话，方法1、2、3都有可能是适合的方法。</p>
<p>但我们依然希望能避免类似的风险，因此尽量避免不必要的特征工程，有以下建议：</p>
<ul>
<li><strong>选择对于特征学习能力强的模型，在数据量允许的情况下可以选择深度学习</strong></li>
<li><strong>避免不必要的特征工程，数据范围比较良好的情况下省略某些特征工程</strong></li>
<li><strong>优先选择对于特征工程要求低的模型，如xgboost等</strong></li>
</ul>
<h2><span id="六-业务角度看特征工程">六、业务角度看特征工程</span></h2><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/505480353">如何从业务角度看特征工程</a></p>
</blockquote>
<p>前两天刷某知名社交软件的时候看到有人问特征工程现在还重要吗？觉得是个很有意思的事情。其实工业界能够支持的起大规模稀疏向量的场景大概并不是想象中的那么多，大多数场景面对极为稀疏的行为数据下都很难在ID层面得到很好的emb表达。在这个前提下，没有好的特征工程，其余的模型结构优化或者各种花里胡哨的模型结构都是纸上谈兵。真正被小场景捶打过的朋友，比如我，绝对会在一次又一次的生活毒打中明白，抛弃那些ppt上的高级多塔多注意力，直面特征工程的人生吧！</p>
<p>有竞赛经验的小伙伴都明白，一个强特能一飞冲天，一个灵机一动能直上top榜。但是，长久的可持续的特征工程决不能够靠简单的灵机一动来实现，特别是当手上有无数的芝麻大小的场景时，一个系统的特征工程思维就尤为重要了。本文将从以下几个方面来阐述特征工程中的方方面面。提前说明的是，一般的特征工程常用方法，例如one-hot，hash-encoding，分桶等等不会作为本文的重点，因为这是器的维度，文末有一篇非常全面的文章供参考，本文主要聚焦在术的维度，也就是怎么去思考和选用方法的层面。<strong>首先，我会给出一个特征工程树，这个属于一个主流版本，希望在屏蔽场景特殊性的情况下，给出一般场景的思考方法</strong>。接下来，我会介绍上文提到的特征树的细节，包括涉及到的具体特征例子。第三部分，则包括特征之间可能存在的相互作用和不同特征适合的模型类型。最后，我给出了一个具体场景的具体例子，并说明这个场景的一般性和特殊性，给出针对具体业务场景的特征工程思路。</p>
<p>此外，一个基础认知是，这里的特征是指输入模型的信息，包括偏置或者先验，这些特征的使用方式除了作为模型的输入，也可以通过其他的方式引入，例如样本工程或者损失函数，这个就不在本文讨论范围之内了。当然还是那句老话，个人的认知是有限的，欢迎有经验的小伙伴交流和指正。</p>
<h3><span id="6-1-基础特征树">6. 1 基础特征树</span></h3><p>不管是基于已有的模型迭代优化，又或者是从0到1构建一个场景的全部特征，都需要自己梳理一个完整的基础特征树。这是了解一个场景的开始。做这件事情我推荐的方法是<strong>先体验这个场景，然后分类列出所有可能影响你优化目标决策的因素以及优化目标的历史信息</strong>。</p>
<p>对于绝大多数业务来讲，基础的特征树都可以分为以下3大部分。</p>
<ul>
<li><strong>供给侧</strong>：对于大部分to c的互联网应用，供给侧都是item，可能是音乐，doc或者一条推送消息。</li>
<li><strong>消费侧</strong>：有关用户的一切描述，其中比较特殊的是序列特征。</li>
<li><strong>上下文</strong>：场景测的因素，包括特定的时刻，特定的展现形式等。</li>
<li><strong>交叉特征</strong>：以上三个部分任意两部分或全部的交叉特征。</li>
</ul>
]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程（5）特征融合</title>
    <url>/posts/2TRK228/</url>
    <content><![CDATA[<h2><span id="特征工程数据清洗预处理-特征生成-特征拼接">特征工程｜数据清洗（预处理）、特征生成、特征拼接</span></h2><ul>
<li>这或许是全网最全机器学习模型融合方法总结！：<a href="https://zhuanlan.zhihu.com/p/511246278">https://zhuanlan.zhihu.com/p/511246278</a></li>
</ul>
<p>特征工程的完整流程是：特征设计 -&gt; 特征获取 -&gt; 特征处理 -&gt; 特征存储 -&gt; 特征监控。前边介绍了那么多，相当于是对特征设计、特征获取、特征存储进行了说明，而特征工程中最重要的环节则是特征处理。特征处理中还包括：数据清洗、特征生成、特征拼接、特征处理、特征转换、特征选择。本篇主要介绍<strong>数据清洗、特征生成、特征拼接</strong>。</p>
<h3><span id="一-数据清洗">一、数据清洗：</span></h3><p>从特征工程角度讲，数据清洗是特征工程的前置阶段（但是也会贯穿整个数据应用过程），其本义是对数据进行重新的<strong>审查和校验</strong>，目的在于<strong>删除重复信息</strong>、纠正存在的错误数据，并保证数据的一致性。数据清洗是整个数据分析过程中不可缺少的一个环节，其结果质量直接关系到模型效果和最终结论。</p>
<p>一个特征处理的完整流程可以表示为：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/02kicEWsInicg2tpfKoZNzmSRj6mXo9Ppic3w2e5VGe7PqXlEiaibdcgmj8PMZqUOcLCrcDLNVkERGvib4MYOjJDu4Lw/640?wx_fmt=png" alt="img"></p>
<p>因此基础数据的准确性、完备性、一致性决定了后续特征数据的有效性。在我们日常使用的公开数据集中，很多都是已经被处理后的了，比如学术界中使用很广泛的MovieLens数据集，但是在真实的业务场景中，我们拿到的数据其实直接是没有办法使用的，可能包含了<strong>大量的缺失值</strong>，可能包含<strong>大量的噪音</strong>，也可能因为人工录入错误导致有异常点存在，对我们挖据出有效信息造成了一定的困扰，所以我们需要通过一些方法，尽量提高数据的质量。</p>
<p><strong>初期数据清洗更多的是针对单条样本数据的清洗和检测</strong>，包括：</p>
<ul>
<li>数据表示一致性处理</li>
<li>逻辑错误值处理</li>
<li>缺失值处理</li>
<li>非必要性数据剔除</li>
</ul>
<p>在实际的业务场景中，数据是由系统收集或用户填写而来，有很大可能性在格式和内容上存在一些问题，同样类型的数据在不同的团队上报过程中产出的内容或格式会不一致，同样不同数据源采集而来的数据内容和格式也会不一致。</p>
<blockquote>
<p>  数据格式的一致性。比如<strong>时间信息</strong>（以2020年6月18日，11点11分12秒为例），有的用毫秒时间戳表示（1592449872000），有的用秒时间戳表示（1592449872），有的用带横线的时间表示（2020-06-18 11:11:12），有的则用不带横线的时间表示（20200618 11:11:12）。</p>
<p>  数据类型的一致性。比如不同的数据表中，同样表示用户ID的字段，有的可能是string类型，有的是int类型。如果用string类型表示，如果用户id缺失的话，正常可以留空，但是不同团队，不同人对于缺失的id处理方式也会不一致，比如有的用None，有的用Null，有的则用0表示，可谓是千奇百怪。小编在日常工作中也会经常遇到这种情况，被折磨的体无完肤。</p>
</blockquote>
<h3><span id="二-特征生成">二、特征生成</span></h3><p>对基础数据进行清理之后需要做的就是生成我们需要的特征，在特征设计部分提到特征主要分为四大维度，根据小编的经验特征又可以根据其值的属性划分为：</p>
<ul>
<li><strong>类别特征</strong>：即特征的属性值是一个有限的集合，比如用户性别、事物的类别、事物的ID编码类特征等</li>
<li><strong>连续特征</strong>：即用户行为、类别、组合特征之类的统计值，比如用户观看的视频部数、某类别下事物的个数等</li>
<li><strong>时间序列特征</strong>：即和时间相关的特征，比如用户来访时间、用户停留时长、当前时间等。</li>
<li>组合特征：即多种类别的组合特征，比如用户在某个类别下的行为统计特征、当天内事物被访问次数特征等</li>
<li><strong>文本特征</strong>：即和文本相关的特征，比如评论数据、商品描述、新闻内容等。</li>
<li><strong>Embedding特征</strong>：即一些基础特征的高层次表示，比如用户ID编码的Embedding表示、事物ID编码的Embedding表示、<strong>用户访问事物序列的Embedding编码</strong>等。</li>
</ul>
<h3><span id="三-特征融合">==三、特征融合==</span></h3><blockquote>
<p>  多模态特征融合三部曲: <a href="https://zhuanlan.zhihu.com/p/390668652">https://zhuanlan.zhihu.com/p/390668652</a></p>
<p>  推荐系统（六）—— 特征融合 :  <a href="https://zhuanlan.zhihu.com/p/459012483">https://zhuanlan.zhihu.com/p/459012483</a></p>
</blockquote>
<h4><span id="31-特征处理">3.1 特征处理</span></h4><p>假设你有三种类型数据，或者说可以是三个不同维度的向量</p>
<p><img src="https://www.zhihu.com/equation?tex=x_1%5Cin%5Cmathbb%7BR%7D%5E%7Bn_1%7D%2Cx_2%5Cin%5Cmathbb%7BR%7D%5E%7Bn_2%7D%2Cx_3%5Cin%5Cmathbb%7BR%7D%5E%7Bn_3%7D" alt="[公式]"></p>
<p>第一种融合手段就是在训练前进行的</p>
<ol>
<li><strong>三个向量直接concat，可能维度会比较高，再进行个PCA</strong></li>
<li><strong>自编码器结构</strong>：三个向量通过MLP映射成一个维度后相加，用还原回去；融合特征再用来做后续的模型设计和训练就好了。</li>
</ol>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220519212856682.png" alt="image-20220519212856682" style="zoom: 33%;"></p>
<h4><span id="32-模型结构">3.2 模型结构</span></h4><p>一种直接的思想就是分而治之，多分支网络；还有一种比较出名的在中间层进行融合的方法，多模态双线性矩阵分解池化方法（MFB），本质上就是对不同模态数据进行双线性融合，借助矩阵分解的思想，再对原始特征进行高维映射，然后element-wise相乘后再做pooling操作。</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220519220722481.png" alt="image-20220519220722481" style="zoom: 33%;"></p>
<p><img src="https://pic3.zhimg.com/80/v2-c4ecc29092d26822fef3f420c2bf5bb2_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h4><span id="33-后处理">3.3 后处理</span></h4><p>后处理其实也是分而治之的思想，多模态数据分别训练不同的模型，再将不同模型的预测输出进行融合，比如平均、加权，或者fix住原来的多个网络，后面再加一层进行微调。</p>
<p><img src="https://pic3.zhimg.com/80/v2-3e0669ac057b924e8814f34578d4acee_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h4><span id="34-特征融合是什么和特征交叉有什么区别呢">3.4 特征融合是什么？和特征交叉有什么区别呢？</span></h4><p>在上一篇文章（<a href="https://zhuanlan.zhihu.com/p/457853657">yu-lzn：推荐系统（五）—— 特征交叉</a>）中，我们讨论了特征交叉，特征交叉也称为特征组合，旨在提高模型对非线性的建模能力，从而提高模型的性能。<strong>特征融合</strong>和特征交叉有相同的目的，都是为了提高模型的性能。特征融合是想更好地利用不同特性的特征。</p>
<blockquote>
<p>  随着信息时代的发展，<strong>在推荐系统中，多模态信息的融合也变得越来越重要</strong>。以淘宝购物为例，用户在决策是否购买物品时，会考虑<strong>物品的属性</strong>、<strong>物品图片的展示</strong>、其他<strong>用户的评论信息</strong>、甚至是观看<strong>物品的介绍视频</strong>等等。换句话说，这些<strong>多模态信息（文本、图片、视频）会影响用户的行为</strong>。所以如何利用这些多模态信息来建模，是提高推荐系统准确度的一个途径。那么如何去融合这些不同来源的信息便是一个关键的问题。</p>
</blockquote>
]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程（6）【Nan】时间序列处理</title>
    <url>/posts/35TG484/</url>
    <content><![CDATA[<h2><span id="时间序列数据的预处理">时间序列数据的预处理</span></h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/466086665"><em>时间序列</em>数据的预<em>处理</em></a></li>
<li><a href="https://mp.weixin.qq.com/s/vyDZfDdaH2Y7k75NNsMNJA">如何在实际场景中使用异常检测？阿里云Prometheus智能检测算子来了</a> </li>
</ul>
<blockquote>
<p>  在本文中，我们将主要讨论以下几点：</p>
<ul>
<li>时间序列数据的定义及其重要性。</li>
<li>时间序列数据的预处理步骤。</li>
<li>构建时间序列数据，查找缺失值，对特征进行去噪，并查找数据集中存在的异常值。</li>
</ul>
</blockquote>
<h3><span id="时间序列的定义">时间序列的定义</span></h3><p><strong>时间序列是在特定时间间隔内记录的一系列均匀分布的观测值</strong>。时间序列的一个例子是黄金价格。在这种情况下，我们的观察是在固定时间间隔后一段时间内收集的黄金价格。时间单位可以是分钟、小时、天、年等。但是任何两个连续样本之间的时间差是相同的。</p>
]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>特征工程（7）【Nan】特征重要性</title>
    <url>/posts/3MAMQRP/</url>
    <content><![CDATA[<h2><span id="特征重要性-模型的可解释性">特征重要性 - 模型的可解释性</span></h2><blockquote>
<p>  机器学习模型可解释性进行到底 —— <strong>SHAP值理论</strong>（一） - 悟乙己的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/364919024">https://zhuanlan.zhihu.com/p/364919024</a></p>
<p>  <strong>Permutation Importance</strong></p>
<p>  机器学习模型可解释性进行到底——特征重要性（四） - 悟乙己的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/364922142">https://zhuanlan.zhihu.com/p/364922142</a></p>
</blockquote>
]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（18）TF-IDF</title>
    <url>/posts/2MXAK3G/</url>
    <content><![CDATA[<h2><span id="tf-idf">TF-IDF</span></h2><blockquote>
<p>  <a href="https://blog.csdn.net/u010417185/article/details/87905899">https://blog.csdn.net/u010417185/article/details/87905899</a></p>
</blockquote>
<p><strong>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)</strong>是一种用于资讯检索与资讯探勘的常用<a href="https://www.zhihu.com/search?q=加权技术&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;97273457&quot;}">加权技术</a>。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p>
<p>上述引用总结就是, <strong>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。</strong>这也就是TF-IDF的含义。</p>
<h4><span id="11-tf"><strong>1.1 TF</strong></span></h4><p><strong>TF(Term Frequency, ==词频==)</strong>表示词条在文本中出现的频率，这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。TF用公式表示如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=TF_%7Bi%2Cj%7D%3D%5Cfrac%7Bn_%7Bi%2Cj%7D%7D%7B%5Csum_%7Bk%7D%7Bn_%7Bk%2Cj%7D%7D%7D%5Ctag%7B1%7D+%5C%5C" alt="[公式]"></p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=n_%7Bi%2Cj%7D" alt="[公式]"> 表示词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 在文档 <img src="https://www.zhihu.com/equation?tex=d_j" alt="[公式]"> 中出现的次数，<img src="https://www.zhihu.com/equation?tex=TF_%7Bi%2Cj%7D" alt="[公式]"> 就是表示词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 在文档 <img src="https://www.zhihu.com/equation?tex=d_j" alt="[公式]"> 中出现的频率。</p>
<p>但是，需要注意， 一些<strong>通用的词语对于主题并没有太大的作用</strong>， <strong>反倒是一些出现频率较少的词才能够表达文章的主题</strong>， 所以单纯使用是TF不合适的。权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章的主题的作用很大，这些词的权重应该设计的较大。IDF就是在完成这样的工作。</p>
<h4><span id="12-idf"><strong>1.2 IDF</strong></span></h4><p><strong>IDF(Inverse Document Frequency, ==逆文件频率==)</strong>表示关键词的普遍程度。如果包含词条 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 的文档越少， <strong>IDF</strong>越大，则说明该词条具有很好的类别区分能力。某一特定词语的<strong>IDF</strong>，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到:</p>
<p><img src="https://www.zhihu.com/equation?tex=IDF_i%3D%5Clog%5Cfrac%7B%5Cleft%7CD+%5Cright%7C%7D%7B1%2B%5Cleft%7Cj%3A+t_i+%5Cin+d_j%5Cright%7C%7D%5Ctag%7B2%7D+%5C%5C" alt="[公式]"></p>
<p>其中，<img src="https://www.zhihu.com/equation?tex=%5Cleft%7CD+%5Cright%7C" alt="[公式]"> 表示所有<strong>文档的数量</strong>，<img src="https://www.zhihu.com/equation?tex=%5Cleft%7Cj%3A+t_i+%5Cin+d_j%5Cright%7C" alt="[公式]"> 表示包<strong>含词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 的文档数量</strong>，为什么这里要加 1 呢？主要是<strong>防止包含词条 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 的数量为 0 从而导致运算出错的现象发生</strong>。</p>
<p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于<strong>过滤掉常见的词语，保留重要的词语</strong>，表达为</p>
<p><img src="https://www.zhihu.com/equation?tex=TF+%5Ctext%7B-%7DIDF%3D+TF+%5Ccdot+IDF%5Ctag%7B3%7D+%5C%5C" alt="[公式]"></p>
<p>==<strong>最后</strong>在计算完文档中每个字符的tfidf之后，对其进行归一化，将值保留在0-1之间，并保存成稀疏矩阵。==</p>
<h2><span id="tf-idf-qampa">TF-IDF Q&amp;A</span></h2><h3><span id="1-究竟应该是对整个语料库进行tf-idf呢还是先对训练集进行tf-idf然后再对xtest进行tf-idf呢两者有什么区别"><strong>1、究竟应该是对整个语料库进行tf-idf呢？还是先对训练集进行tf-idf，然后再对xtest进行tf-idf呢？两者有什么区别？</strong></span></h3><blockquote>
<h4><span id="fit">fit</span></h4><p>  学习输入的数据有多少个不同的单词，以及每个单词的idf</p>
<h4><span id="transform-训练集">transform 训练集</span></h4><p>  返回我们一个document-term matrix.</p>
<h4><span id="transform-测试集">transform 测试集</span></h4></blockquote>
<p>transform的过程也很让人好奇。要知道，他是将测试集的数据中的文档数量纳入进来，重新计算每个词的idf呢，还是<strong>直接用训练集学习到的idf去计算测试集里面每一个tf-idf</strong>呢？</p>
<p><strong>如果纳入了测试集新词，就等于预先知道测试集中有什么词，影响了idf的权重。这样预知未来的行为，会导致算法丧失了泛化性。</strong></p>
<h3><span id="2-tf-idf-模型加载太慢">2、TF-IDF 模型加载太慢</span></h3><blockquote>
<p>  <a href="https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/">https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> idfs <span class="keyword">import</span> idfs <span class="comment"># numpy array with our pre-computed idfs</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># subclass TfidfVectorizer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyVectorizer</span>(<span class="title class_ inherited__">TfidfVectorizer</span>):</span><br><span class="line">    <span class="comment"># plug our pre-computed IDFs</span></span><br><span class="line">    TfidfVectorizer.idf_ = idfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiate vectorizer</span></span><br><span class="line">vectorizer = MyVectorizer(lowercase = <span class="literal">False</span>,</span><br><span class="line">                          min_df = <span class="number">2</span>,</span><br><span class="line">                          norm = <span class="string">&#x27;l2&#x27;</span>,</span><br><span class="line">                          smooth_idf = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plug _tfidf._idf_diag</span></span><br><span class="line">vectorizer._tfidf._idf_diag = sp.spdiags(idfs,</span><br><span class="line">                                         diags = <span class="number">0</span>,</span><br><span class="line">                                         m = <span class="built_in">len</span>(idfs),</span><br><span class="line">                                         n = <span class="built_in">len</span>(idfs))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>【Nan】Contrastive Autoencoder for Drifting detection and Explanation</title>
    <url>/posts/3B7BMZZ/</url>
    <content><![CDATA[<h1><span id="cade-contrastive-autoencoder-for-drifting-detection-and-explanation">CADE: Contrastive Autoencoder for Drifting detection and Explanation</span></h1><p>The repository contains the code for detecting and explaining a specific type of concept drift (i.e., previously unseen families) in security applications like malware attribution and network intrusion classification.</p>
<ul>
<li>代码：<a href="https://github.com/whyisyoung/CADE">https://github.com/whyisyoung/CADE</a></li>
</ul>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>【Nan】Hunting Stealthy Malware</title>
    <url>/posts/2ZYZKWX/</url>
    <content><![CDATA[<p><a href="https://mzgao.blog.csdn.net/article/details/118355956">https://mzgao.blog.csdn.net/article/details/118355956</a></p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>【Nan】Soghos-家族语义标签问题</title>
    <url>/posts/95QGZ5/</url>
    <content><![CDATA[<p><a href="https://mzgao.blog.csdn.net/article/details/118943477">https://mzgao.blog.csdn.net/article/details/118943477</a></p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>【Nan】算法优化（1）SliceLine- Fast, Linear-Algebra-based Slice Finding for ML Model Debugging</title>
    <url>/posts/BGCJ54/</url>
    <content><![CDATA[<h2><span id="sliceline-fast-linear-algebra-based-slice-finding-for-ml-model-debugging-sigmod-21">SliceLine: Fast, Linear-Algebra-based Slice Finding for ML Model Debugging （SIGMOD ’21）</span></h2><blockquote>
<ul>
<li>论文下载：<a href="https://mboehm7.github.io/resources/sigmod2021b_sliceline.pdf">https://mboehm7.github.io/resources/sigmod2021b_sliceline.pdf</a></li>
<li></li>
</ul>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p>切片查找——最近一项关于调试机器学习（ML）模型的工作旨在查找前K个数据切片（例如，谓词的连接词，如性别女性和博士学位），其中经过训练的模型的表现明显不如整个训练/测试数据。这些切片可以用于为有问题的子集获取更多数据、添加规则或以其他方式改进模型。与决策树相比，一般的切片查找问题允许重叠的切片。由此产生的搜索空间是巨大的，因为它涵盖了特征的所有子集及其不同的值。因此，现有的工作主要依赖于启发式，并专注于适合单个节点内存的小型数据集。在本文中，我们从算法和系统的角度，以整体的方式解决了切片查找的这些可扩展性限制。我们利用切片大小、错误和结果分数的单调性财产来促进有效的修剪。<strong>此外，我们提出了一种优雅的基于线性代数的枚举算法，它允许在现有ML系统的基础上进行快速枚举和自动并行化。</strong>对不同真实世界回归和分类数据集的实验表明，有效的修剪和高效的稀疏线性代数使精确枚举变得可行，即使对于具有许多特征、相关性和数据大小超过单节点内存的数据集也是如此。</p>
<h3><span id="一-说明">一、说明</span></h3><p>机器学习（ML）和数据驱动的应用程序从根本上改变了IT格局的许多方面，从面向用户的应用程序到后端决策系统，再到软件和硬件堆栈的优化[21，36，56]。开发和部署用于生产的ML管道过程中的关键步骤是<strong>数据验证（分析输入数据特征）</strong>[56，59]和<strong>模型调试（分析有效的ML模型特征）</strong>[22，56，60]。需要考虑的方面包括数据误差（如异质性、人为误差、测量误差）、缺乏模型泛化（如过拟合、不平衡、域外预测）以及系统偏差和缺乏公平性。缺乏模型验证和调试可能会导致无声但严重的问题[56]。例如，种族偏见的监狱风险评估[6]、基于积雪的狼检测[58]和基于图像水印的马检测[38]。模型调试旨在识别此类问题。</p>
<p>模型调试技术：除了基本数据调试和验证[56，59]、服务期间的模型准确性监测和比较[56，60]，以及通过混淆矩阵进行手动模型误差分析（例如，正确标签与预测标签的矩阵可视化）外，还存在几种先进的模型调试技术。计算机视觉领域的例子有显著性图[30，63，70]、逐层相关性传播[8，38]和基于遮挡的解释[75]，它们都旨在找到对预测有重大影响的输入图像区域。数据管理界最近通过利用这种计算中固有的重叠，为基于遮挡的解释贡献了有效的增量计算方法[47，48]。然而，对于具有连续和分类特征的结构化数据和预测任务，文献相对较少。现有的工作包括解释表[25]（主要关注数据摘要）和切片查找器[18，19]，其目的是查找前K个数据切片（例如，谓词的连接词，如性别女性和博士学位），其中训练的模型的表现明显不如整个数据集。找到这样的有问题的切片对于理解缺乏训练数据或模型偏差非常有用，但也可以作为改进模型的途径</p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>对抗攻击与防御（1）Adversarial Attacks and Defenses in Deep Learning: From a Perspective of Cybersecurity</title>
    <url>/posts/1HGMZ3J/</url>
    <content><![CDATA[<h2><span id="adversarial-attacks-and-defenses-in-deep-learning-from-a-perspective-of-cybersecurity">Adversarial Attacks and Defenses in Deep Learning: From a Perspective of Cybersecurity</span></h2><ul>
<li><a href="https://dl.acm.org/doi/10.1145/3547330">https://dl.acm.org/doi/10.1145/3547330</a></li>
</ul>
<h3><span id="摘要">摘要</span></h3><p>深度神经网络的卓越性能推动了深度学习在广泛领域的应用。然而，对抗性样本带来的潜在风险阻碍了深度学习的大规模部署。在这些场景中，人眼无法察觉的对抗性扰动会显著降低模型的最终性能。已经发表了许多关于对抗性攻击及其在深度学习领域的对策的论文。大多数论文都集中在逃避攻击上，即在测试时发现对抗性示例，而不是在训练数据中插入中毒数据的中毒攻击。此外，由于没有标准的评估方法，很难评估对抗性攻击的真实威胁或深度学习模型的稳健性。因此，在本文中，我们回顾了迄今为止的文献。此外，我们试图为系统地理解对抗性攻击提供第一个分析框架。该框架是从网络安全的角度构建的，为对抗性攻击和防御提供生命周期。</p>
<h3><span id="说明">说明</span></h3><p>机器学习技术已被应用于广泛的场景，并取得了广泛的成功，尤其是在深度学习方面，它正迅速成为各种任务中的关键工具。然而，在许多情况下，机器学习或深度学习模型的失败可能会导致严重的安全问题。例如，在自动驾驶汽车中，无法识别交通标志可能会导致严重事故[1]。因此，在大规模部署之前，训练一个准确稳定的模型至关重要。不幸的是，近年来，许多研究揭示了模型安全中一个令人失望的现象，即深度学习模型可能容易受到对抗性示例的影响，即被对手恶意干扰的样本。以这种方式被篡改的模型很有可能会产生错误的预测，尽管它们可能在良性样本中显示出很高的准确性[2-5]。对抗性攻击可以被广泛定义为一类攻击，其目的是通过将对抗性示例插入训练阶段（称为中毒攻击[6-8]）或推理阶段（称称为逃避攻击[2，3]）来欺骗机器学习模型。无论哪种攻击都会显著降低深度学习模型的鲁棒性，并引发模型安全问题。此外，最近在现实世界中发现了深受这种模型安全问题困扰的深度学习解决方案的漏洞，这引发了人们对深度学习技术的信任程度的担忧</p>
<p>由于在深度学习技术的实际应用中，隐私和安全的潜在威胁多种多样，越来越多的组织，如ISO、IEEE和NIST，正在参与人工智能标准化的过程。对一些国家来说，这项工作被认为类似于一些国家的新基础设施建设[9]。ISO提出了一个关于人工智能系统生命周期的项目，该项目将技术的生命周期分为八个阶段，包括初始化、设计和开发、检查和验证、部署、运行监控、持续验证、重新评估和放弃[10]。本周期中没有进一步解决的是对抗性攻击如何阻碍深度学习模型的商业部署。因此，评估模型安全威胁是人工智能项目生命周期中的一个关键组成部分。此外，考虑到可能的对抗性攻击和防御的分散性、独立性和多样性，如何分析模型的安全威胁也应该标准化。迫切需要的是一个风险图，以帮助准确确定项目生命周期每个阶段的多种类型的风险。更严重的是，攻击的防御仍处于早期阶段，因此对更复杂的分析技术提出了更高的要求。</p>
<p>近年来，一些与对抗性机器学习相关的调查已经发表。Chakraborty等人[11]描述了安全相关环境中对抗性示例的灾难性后果，并回顾了一些强有力的对策。然而，他们的结论表明，它们都不能成为应对所有挑战的灵丹妙药。Hu等人[12]首先介绍了基于人工智能的系统的生命周期，用于分析每个阶段的安全威胁和研究进展。对抗性攻击分为训练和推理阶段。Serban等人[13]和Machado等人[14]分别回顾了在对象识别和图像分类中关于对抗性机器学习的现有工作，并总结了存在对抗性示例的原因。Serban等人[13]还描述了对抗性示例在不同模型之间的可转移性。与参考文献[14]提供设计防御的相关指导类似，Zhang等人[15]还从防御者的角度对相关工作进行了全面调查，并总结了深度神经网络对抗性示例起源的假设。还有一些关于对抗性机器学习在特定领域的应用的调查，如推荐系统[16]、网络安全领域[17]和医疗系统[18]。</p>
<blockquote>
<p>  [17]</p>
</blockquote>
<p>先前已经观察到，在网络安全背景下，高级持续威胁（APT）通常是高度有组织的，成功的可能性极高[19]。以Stuxnet为例，这是最著名的APT攻击之一。它于2009年发射，摧毁了伊朗的核武器计划[19]。APT的工作流程系统地考虑了安全问题，这使得APT相关技术既能获得出色的成功率，即绕过这些防御，又能以高度有效的方式评估系统的安全性。受APT工作流程的启发，我们将此系统分析工具应用于网络安全问题，作为分析对抗性攻击威胁的潜在方法。APT主要由多种类型的现有底层网络空间攻击（如SQL注入和恶意软件）组成。不同类型的底层攻击的组合策略及其五阶段工作流程意味着，与单一攻击相比，APT的成功率极高。然而，有趣的是，根据现有的对抗性攻击策略，可以将其巧妙地划分为这五个阶段。基于这一观察，我们发现在评估对抗性攻击的威胁时，类似于APT的工作流程是有效的。因此，这构成了我们分析和对策框架的基础。尽管一些综述论文总结了模型安全方面的工作，但攻击方法或防御通常仍分为依赖类和分段类。这意味着不同方法之间的关系尚未明确确定。在这篇文章中，我们从APT的角度系统地对现有的对抗性攻击和防御进行了全面系统的回顾。我们的贡献如下：</p>
<ul>
<li>我们提供了一个新颖的网络安全视角来研究深度学习的安全问题。我们首次提出将APT概念纳入深度学习中对抗性攻击和防御的分析中。其结果是为模型安全性提供了一个标准的类似APT的分析框架。与之前部分关注机制[13，20]、威胁模型[21]或情景[22]的调查不同，我们的工作可以为理解和研究这个问题提供全局和系统层面的视角。具体而言，先前的研究倾向于在小组中讨论具有类似策略的方法。分别研究了不同策略的对抗性攻击，忽略了不同攻击群之间的关系。相反，我们的工作将对抗性攻击视为一个全球系统，每一组具有相似策略的攻击都只是这个全球系统的一部分。与网络安全类似，考虑不同群体之间的关系有助于进一步提高攻击的有效性；</li>
<li>基于类似APT的分析框架，我们对现有的对抗性攻击方法进行了系统的审查。根据APT的逻辑，对抗性攻击可以明确地分为五个阶段。在每个阶段，都确定了共同的基本组成部分和短期目标，这有助于以深入的顺序提高攻击性能；</li>
<li>我们还回顾了在类似APT的分析框架内对对抗性攻击的防御。同样，防御分为五个阶段，提供自上而下的顺序来消除对抗性例子的威胁。可以识别不同阶段的防御方法之间的关系，从而激发一种可能的策略，将多种防御结合起来，为深度学习模型提供更高的鲁棒性</li>
<li>我们分别从数据和模型的角度总结了对抗性示例存在的假设，并全面介绍了对抗性机器学习中常用的数据集</li>
</ul>
<p>我们希望这项工作能激励其他研究人员在系统层面上看待模型的安全风险（甚至隐私威胁），并在全球范围内评估这些风险。如果可以建立一个标准，那么各种财产，例如健壮性，就可以在更短的时间内更准确地访问。因此，用户对深度学习模型的信心将会增强。</p>
<h3><span id="二-相关研究">二、相关研究</span></h3><p>深度学习是指建立在深度神经网络（DNN）上的一组机器学习算法，已广泛应用于预测和分类等任务[21]。DNN是一种数学模型，包括具有大量计算神经元和非线性激活函数的多层。典型的深度学习系统的工作流程包括两个阶段：训练阶段和推理阶段。两个阶段的详细流程如下所示，如图1所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191546299.png" alt="image-20230402204215480"></p>
<p>图1：深度学习系统的一般工作流程。在训练阶段，基于训练数据迭代更新参数θ。得到最佳参数θ后 , 在推理阶段，输入将被输入到经过训练的模型中，这将为决策提供相应的输出</p>
<ul>
<li><p>在训练阶段，DNN的参数通过迭代前馈和反向传播不断更新。反向传播中的梯度下降方向是通过优化损失函数来引导的，该函数量化了预测标签和地面实况标签之间的误差。具体地，给定输入空间X和标签空间Y，最优参数θ  期望DNN f的损失函数L最小化训练数据集（X，Y）上的损失函数。因此，在训练过程中要找到最佳θ  可以定义为：</p>
<script type="math/tex; mode=display">
\theta^{\star}=\underset{\theta}{\arg \min } \sum_{x_i \in \mathcal{X}, y_i \in \mathcal{Y}} \mathcal{L}\left(f_\theta\left(x_i\right), y_i\right)</script></li>
</ul>
<pre><code>其中fθ是要训练的DNN模型；xi∈X是从训练数据集中采样的数据实例，yi和fθ（xi）分别表示相应的地面真实标签和预测标签。
</code></pre><ul>
<li>在推理阶段，训练的模型f  具有固定最优参数的θ  被应用来提供关于未被包括在训练数据集中的看不见的输入的决策。给定看不见的输入xj，可以通过单个前馈过程来计算相应的模型决策yj（即xj的预测标签）：yj=f  θ（xj）。值得注意的是，一次成功的对抗性攻击通常会在推理阶段导致被操纵的预测yõ，这可能与xj的正确标签相去甚远。</li>
</ul>
<h4><span id="22-针对dnn的威胁模型">2.2 针对DNN的威胁模型</span></h4><p>为了对这些攻击进行分类，我们通过引入模型攻击的关键组件来指定针对DNN的威胁模型。威胁被分解为三个维度：对手的目标、对手的特异性和对手的知识。这三个维度可以帮助我们识别潜在的风险，并了解对抗性攻击环境中的攻击行为。在图2中，我们概述了对抗性攻击中的威胁模型。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191546534.png" alt="image-20230403202614103"></p>
<h5><span id="221-攻击目标">2.2.1 攻击目标</span></h5><ul>
<li>中毒攻击。在中毒攻击中，攻击者可以访问和修改训练数据集，以影响最终训练的模型[23-26]。攻击者在训练数据中注入假样本以生成有缺陷的模型的方式可以被视为“中毒”。中毒攻击通常会导致准确性下降[25]或对给定测试样本的错误分类[26]。•躲避攻击。</li>
<li>在规避攻击中，对手的目标是攻击训练有素且固定的DNN，而没有任何权限修改目标模型的参数[13，18，23]。通过这种方式，攻击者不再需要训练数据集的可访问性。相反，攻击者生成目标模型无法识别的欺骗性测试样本，以逃避最终检测[27，28]。</li>
</ul>
<h5><span id="222-对抗性特异性">2.2.2 对抗性特异性</span></h5><p>对抗性特异性的差异取决于攻击者是否能够在推理阶段为给定的对抗性样本预先定义特定的欺诈预测。</p>
<ul>
<li>非目标攻击。在无目标攻击中，对手的唯一目的是欺骗目标模型生成错误预测，而不关心选择哪个标签作为最终输出[29-32]。</li>
<li>有针对性的攻击。在有针对性的攻击中，对于给定的样本，攻击者不仅希望目标模型做出错误的预测，而且还旨在诱导模型提供特定的错误预测[33-36]。一般来说，有针对性的攻击不会像无针对性的那样成功。</li>
</ul>
<h5><span id="223-敌人的知识">2.2.3 敌人的知识</span></h5><ul>
<li>白盒攻击。在白盒设置中，对手能够访问目标模型的细节，包括结构信息、梯度和所有可能的参数[30，34，37]。因此，对手可以通过利用手头的所有信息来精心制作对抗性样本。</li>
<li>黑匣子攻击。在黑盒设置中，攻击者在不了解目标模型的情况下实施攻击。攻击者可以通过与目标模型交互来获取一些信息[36，38，39]。这是通过将输入查询样本输入到模型中并分析相应的输出来完成的。</li>
</ul>
<h4><span id="23-对抗性攻击">2.3 对抗性攻击</span></h4><h5><span id="231-逃逸攻击">2.3.1 逃逸攻击</span></h5><p>Szegedy等人[2]首先在对抗性攻击中引入了对抗性示例的概念，它可以在推理阶段以高成功率误导目标模型。他们提出了一种通过等式（1）搜索具有目标标签的最小失真对抗性示例的方法：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191546941.png" alt="image-20230402210029648" style="zoom:50%;"></p>
<p>通过这个方程，他们可以找到最接近的x  通过最小化与良性样本x的距离 x  − x 22，并且将被f（x）的条件错误地分类为目标标签t ) = 时间。这个问题可以导致方程（2）中的目标，该目标可以通过L-BFGS算法来解决：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191546006.png" alt="image-20230402210112765" style="zoom:50%;"></p>
<h5><span id="232-投毒攻击">2.3.2 投毒攻击</span></h5><p>与推理攻击中发生的规避攻击不同，中毒攻击旨在通过污染训练数据来降低模型的准确性。攻击者需要一些权限来操纵训练数据，例如数据注入和数据修改[11]。因此，发起中毒攻击的目标可以分为两类：可用性违规和完整性违规。前者旨在降低受害者模型的置信度或准确性，破坏整个系统，而后者试图在不影响其他正常样本的情况下，通过引入后门，在某些特定样本上误导受害者模型[11]。具体而言，针对仅涉及训练样本的神经网络的中毒实例可以通过以下两种策略来制作：双层优化和特征碰撞[23]。</p>
<ul>
<li>双层优化：修改数据的经典数据中毒可以形式化为双层优化问题[23]。然而，对于非凸神经网络，双层优化问题是棘手的。Mu~noz-Gonz´alez等人[24]提出了“反向梯度下降”来近似内部问题的解，然后对外部损失进行梯度下降，尽管这在计算上仍然很昂贵。为了加快中毒样品的生产，Yang等人[25]引入GAN来产生毒素。MetaPoison等人[26]也被提出作为一阶方法，使用集成策略近似解决生产毒药的双层优化。</li>
<li>特征冲突：基于双层优化的方法通常对迁移学习和端到端训练都有效，而特征冲突策略可以用于在有针对性的错误分类设置中设计针对迁移学习的有效攻击。例如，Shafahi等人[40]开发了一种方法，在特征空间中生成与目标样本相似的中毒样本，同时在输入空间中接近原始良性样本。这两类方法只需要操作训练的权限，而不需要标签，并且训练数据的语义将保持不变。这些带有干净标签的中毒样本将更难被检测到。</li>
</ul>
<p>此外，除了仅操纵训练数据的中毒攻击外，另一种类型的中毒攻击是后门攻击，它需要在推理阶段向输入插入触发器的额外能力[23]。</p>
<p>后门攻击：后门攻击中的对手通常可以修改训练样本的标签[41]。这些带有后门触发器的错误标记数据将被注入到训练数据集中。因此，基于该数据集的训练模型将被迫为新样本（使用触发器）分配所需的目标标签。大多数后门攻击都需要在制作毒药的过程中给训练样本贴上错误的标签，而这些毒药更有可能被防御者识别出来。因此，还提出了一些干净标签后门攻击[42]，并使用参考文献[40]中提出的特征碰撞策略来制作后门样本。</p>
<h4><span id="24-advanced-persistent-threats">2.4 Advanced Persistent Threats</span></h4><p>高级持久性威胁（APT）是一系列新定义的攻击过程，其中攻击是长时间连续的，例如数年。分析APT的工作流程构成了我们对抗性攻击和防御分析框架的基础。</p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（11）T&amp;-Blackhat 2022-Malware Classification With Machine Learning Enhanced by Windows Kernel Emulation</title>
    <url>/posts/2Z5SQWP/</url>
    <content><![CDATA[<h2><span id="malware-classification-with-machine-learning-enhanced-by-windows-kernel-emulation"><font color="red"> Malware Classification With Machine Learning Enhanced by Windows Kernel Emulation</font></span></h2><blockquote>
<p>论文：<a href="http://i.blackhat.com/USA-22/Thursday/US-22-Trizna-Malware-Classification-with-machine-learning.pdf">http://i.blackhat.com/USA-22/Thursday/US-22-Trizna-Malware-Classification-with-machine-learning.pdf</a></p>
<p>项目：<a href="https://github.com/dtrizna/quo.vadis">https://github.com/dtrizna/quo.vadis</a></p>
<p>作者：<strong>Dmitrijs Trizna</strong></p>
</blockquote>
<p>我们提出了一种混合机器学习架构，该架构同时使用多个深度学习模型来分析Windows可移植可执行文件的上下文和行为特征，并根据元模型的决策生成最终预测。当代机器学习Windows恶意软件分类器中的检测启发式通常基于样本的静态属性，因为通过虚拟化进行的动态分析对于大量样本是具有挑战性的。为了克服这一限制，我们采用了一种Windows内核仿真，它允许以最小的时间和计算成本跨大型语料库获取行为模式。<strong>我们与一家安全供应商合作，收集了超过10万个类似于当代威胁场景的野生样本，其中包含执行时应用程序的原始PE文件和文件路径</strong>。获取的数据集至少比行为恶意软件分析相关工作中报告的数据集大十倍。训练数据集中的文件由专业威胁情报团队使用手动和自动逆向工程工具进行标记。我们通过在获得训练集三个月后收集样本外测试集来估计混合分类器的操作效用。我们报告了一种改进的检测率，高于当前最先进模型的能力，特别是在低误报要求下。此外，我们还发现了元模型在验证和测试集中识别恶意活动的能力，即使没有一个单独的模型表达足够的信心来将样本标记为恶意。我们得出结论，元模型可以从不同分析技术产生的表示组合中学习恶意样本的典型模式。此外，我们公开发布预先训练的模型和仿真报告的非命名数据集。</p>
<p><strong>因此实现了与虚拟化相比的高分析速率</strong>。由于数据异构性，我们考虑使用多个单独的预训练模块和元模型的组合解决方案，而不是构建具有端到端可训练架构的单个特征向量。该架构允许通过仅重新训练元模型，以最小的努力扩展决策启发式的模块性。在本出版物的范围内，混合ML体系结构的评估依赖于三种不同的分析技术</p>
<p>缺乏出版物人工制品是任何研究中一个臭名昭著的缺陷，并导致了科学的再现性危机。因此，我们披露了源代码和预训练的PyTorch[19]和scikit learn[25]模型，并为我们的模型提供了类似scikit learn[20]的API，遵循机器学习对象广泛采用的接口。据我们所知，我们是安全研究社区中第一个发布基于（a）上下文、（b）静态和（c）PE文件动态属性的单一决策启发的模型。</p>
<p>模型训练三个月后，我们收集了一个样本外数据集。我们承认，与任何单独的方法能力相比，基于软件混合表示的恶意软件分类可以提高检测性能，降低针对恶意逻辑不断演变的误报率</p>
<p><strong>仿真器不需要调动成熟的操作系统操作，因为它们允许以合理的速度获取大量遥测数据，而不需要虚拟化基础设施</strong>。因此，利用仿真器作为ML模型的遥测源进行动态恶意软件分析研究很有前景，但尚未普遍采用。据我们所知，Athiwaratkun和Stokes在2017年首次报告了系统调用收集的仿真器利用率[4]。他们的模型类似于自然语言处理（NLP）中使用的递归方案。Agrawal等人[2]进一步发展了这项工作，他们提出了一种类似的架构，用于在仿真器的帮助下获取的任意长API调用序列。Mandiant的数据科学团队通过基于仿真的动态分析进行了有前景的研究。具体而言，Li等人[16]提供了一个扩展摘要，报告了仿真器[18]在与我们的架构类似的混合分析中的使用情况</p>
<p>基于仿真的行为分析工作的稀疏性是由于它们的局限性。仿真是在模拟器运行的操作系统之上的抽象，不会与硬件发生直接交互。理论上，完美的仿真器可以欺骗任何系统调用背后的逻辑。尽管如此，像Windows N这样的内核并没有集成大量的功能，从而实现了难以置信的一对一复制。我们在第3.2节中报告了与相关的基于虚拟化的工作数据集的详细错误率和数据多样性比较。根据经验证据，我们认为现代Windows内核仿真在ML恶意软件检测器中具有巨大潜力。仿真报告生成丰富多样的遥测，绕过静态分析限制。它包含可执行文件调用的一系列内核API调用，并描述对文件或注册表项的操作以及尝试的网络通信。</p>
<p><strong>例如，我们在训练和验证数据集报告中获取了2822个唯一的API调用</strong>。与相关工作数据集相比，这种行为的异构性明显更高-Athiwaratkun和Stokes[4]总共有114个独特的API调用，Kolosnjaji等人[14]报告了60个独特的API调用，Yen等人[33]有286个不同的API调用。<strong>Rosenberg等人[23]有314个单独的API调用</strong>。由于独特调用的数量与样本数量正相关，因此我们的数据集的较大容量可以部分地描述这种观察。然而，我们强调这是仿真技术效率的证据。仿真报告生成丰富多样的遥测数据，其质量相当于用于动态分析目的的沙箱。</p>
<h3><span id="三-数据集">三、数据集</span></h3><p>本工作中提出的混合解决方案的功能基于输入数据，包括（a）适用于动态分析的武装PE文件和（b）上下文文件路径信息。获取上下文数据的必要性导致无法利用公共数据收集，因为对于每个数据样本，我们都需要拥有原始PE字节和文件路径数据。</p>
<p>据我们所知，所有已知数据集均未提供文件执行时具有文件路径值的PE样本的上下文信息。例如，Kyadige和Rudd等人[15]依赖专有的Sophos威胁情报源，不公开发布其数据集。上下文数据的私有性质是可以理解的，因为这种遥测将包含敏感组件，如个人计算机上的目录。</p>
<p>因此，我们与一家未公开的安全供应商合作，收集了大量数据集，其中包含原始PE文件和来自个人客户系统的样本文件路径，类似于最新的威胁场景。根据所有客户接受的隐私政策处理数据。因此，我们不会公开发布文件路径和原始PE数据集。敏感数据组件（如用户名或自定义环境变量）来自预处理阶段的遥测，在模型参数或仿真报告中没有相似之处。</p>
<h4><span id="31-数据集结构">3.1 数据集结构</span></h4><p>我们分两次收集数据集。第一部分构成了我们分析的基础，包括98966个样本，329GB的原始PE字节。80%的语料库被用作固定的训练集，20%形成样本内验证集。我们使用这些数据对模型进行预训练，并研究我们的混合解决方案配置。</p>
<p>第二次数据集采集会议发生在三个月后，从27500个样本（约100GB的数据）形成了样本外测试集。该语料库用于评估混合模型的实际效用，并研究模型在进化的恶意景观上的行为。</p>
<p>数据集中的PE文件由专业威胁情报团队标记，利用恶意软件分析师操作的手动和自动逆向工程工具。数据集涵盖七个恶意软件家族和benignware，详细分布参数如表1所示。除“干净”外，所有标签均表示恶意文件。因此，我们收集了相对更多的“干净”样本，以平衡数据集中的恶意和良性标签。</p>
<p>由于大多数恶意软件被编译为x86二进制文件，因此我们关注32位（x86）图像，并有意跳过64位（x64）图像的集合，以保持数据集的同质性和标签平衡。此外，恶意软件作者更喜欢x86二进制文件，因为Microsoft向后兼容性允许在64位系统上执行32位二进制文件，但反之亦然。数据集由可执行文件（.exe）组成，我们有意省略库PE文件（.dll）。</p>
<h4><span id="32-样本评估">3.2 样本评估</span></h4><p>表1中表示的所有示例都是用Windows内核仿真器处理的。我们使用Mandiant根据MI T许可证发布并积极维护的Speakeasy[18]基于Python的仿真器。我们测试中使用的Speakeasy版本是1.5.9。它依赖于QEMU[6]CPU仿真框架。我们获得了108204个成功的仿真报告，每个报告的平均运行时间为<strong>12.23秒</strong>，仿真了来自训练和验证集的90857个样本，以及测试集中的17347个样本。</p>
<p>不幸的是，一些示例模拟是错误的，主要是由于写入汇编指令的无效内存读取。然而，模拟错误的另一个常见原因是<strong>调用了不受支持的API函数或反调试技术</strong>。图1显示了不同恶意软件系列的错误率。</p>
<p>据推测，模拟数据集的一个潜在缺点可能是，<strong>与沙箱中实时执行样本相比，其相对稀疏</strong>。然而，经验证据表明，我们的数据集比使用全Windows系统虚拟化执行类似数据采集的其他组报告的数据更为多样。<strong>例如，我们在训练和验证数据集报告中获得2822个唯一的API调用。</strong>与相关工作数据集相比，这种行为的异构性明显更高-Athiwaratkun和Stokes[4]总共有114个独特的API调用，Kolosnjaji等人[14]报告了60个独特的API调用，Yen等人[33]有286个不同的API调用。<strong>Rosenberg等人[23]有314个单独的API调用</strong>。这种观察部分可以通过我们数据集的更大容量来描述，因为唯一调用的数量与样本数量正相关。然而，我们强调这是仿真技术效率的证据。仿真报告生成丰富多样的遥测数据，其质量相当于用于动态分析目的的沙箱。</p>
<h3><span id="四-系统结构">四、系统结构</span></h3><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191552651.png" alt="image-20220918180837521" style="zoom:50%;"></p>
<p>图2显示了混合模型体系结构的总体概述𝜙, 它们被“融合”在一起，由元模型生成最终决策𝜓. 三种早期融合模型𝜙 是：</p>
<ul>
<li>file-path 1D convolutional neural network (CNN), 𝜙𝑓𝑝</li>
<li>emulated API call sequence 1D CNN, 𝜙𝑎𝑝𝑖</li>
<li>FFNN model processing Ember feature vector, 𝜙𝑒𝑚𝑏</li>
</ul>
<p>每个子模型从输入数据获取的128维表示向量。此外，所有三个模型的输出连接在一起形成384维向量。因此，给定输入样本𝑥, 由原始PE（字节）及其文件路径（字符串）组成，早期融合过程统称为：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191552576.png" alt="image-20220918180145232" style="zoom:50%;"></p>
<p>中间向量𝜙 (𝑥) 传递给元模型𝜓, 其产生最终预测：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191552409.png" alt="image-20220918180255048" style="zoom:50%;"></p>
<p><strong>所有早期融合网络都是单独预训练的</strong>，在第4.3节中详细定义了模型配置和训练过程。我们想强调，构建具有多个单独预训练组件的模块化系统而不是构建单个端到端可训练架构的决策是经过深思熟虑的。首先，Yang等人[32]表明，具有高概率的复合神经网络优于单个预训练组件的性能。</p>
<p><strong>「 然而，主要原因是通过仅重新训练元模型，扩展具有互补模块的混合决策启发式的巨大潜力」</strong>。恶意活动分类问题依赖于原始PE字节以外的高度异构的信息源，这种体系结构保留了添加依赖于系统日志记录的启发式的能力。在本次发布之时，我们已经合并了文件路径信息，例如可以从<strong>Sysmon2</strong>遥测中获取。然而，可以进一步提取Sysmon数据或<strong>Speakeasy</strong>报告中的知识。</p>
<h4><span id="41-api-调用处理">4.1 API 调用处理</span></h4><blockquote>
<p><strong>Json_normalize</strong>：json to pandas</p>
</blockquote>
<p>为了获取API调用序列的数值，我们根据可变词汇表大小选择最常见的调用𝑉 . 保留的API调用是标签编码的，不属于词汇表的调用将替换为专用标签。使用填充标签将最终序列截断或填充到固定长度𝑁. </p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191552244.png" alt="image-20220918181408143" style="zoom:50%;"></p>
<p>表2显示了数据集多样性和各自模型性能背后的统计数据。尽管100个最常见的调用在一个数据集中包含95%以上的API调用，<strong>但实验表明该模型仍然受益于相对较大的词汇表大小值，因此我们选择𝑉 = 600用于我们的最终配置。</strong>这种现象可以通过每个样本的API调用分布来解释。具有数百个调用的冗长可执行文件会影响系统调用频率，而具有适度API序列的可执行文件则执行更独特的函数组合。</p>
<h4><span id="42-路径处理">4.2 路径处理</span></h4><p>路径预处理的第一部分包括<strong>路径规范化</strong>，因为文件路径语义的某些部分具有与通过深度学习模型进行的安全分析无关的可变性。如果使用<strong>通用命名约定（UNC）格式</strong>，则包括特定的<strong>驱动器号或网络位置</strong>，以及单个用户名。因此，在规范化过程中，我们为这些<strong>路径组件引入了通用占位符</strong>，如下所示：</p>
<blockquote>
<p>[drive]\users[user]\desktop\04-ca\8853.vbs<br>[drive]\users[user]\appdata\local\file.tmp<br>[net]\company\priv\timesheets\april2021.xlsm</p>
</blockquote>
<p>此外，有必要分析Windows环境变量，使其类似于实际的文件路径，而不是用作变量名的环境别名。因此，<strong>我们构建了一个由大约30个环境变量组成的变量映射</strong>，这些变量表示系统上的特定路径，并在当代和传统Windows系统中使用。变量映射的一些示例如下所示:</p>
<blockquote>
<p>r”%systemdrive%”:    r”[drive]”,<br>r”%systemroot%”:    r”[drive]\windows”,<br>r”%userprofile%”:    r”[drive]\users[user]”</p>
</blockquote>
<p><strong>分别使用100和150个最频繁的UTF-8字节。频率阈值以下的罕见字符将被丢弃，并由单个专用标签替换。</strong></p>
<h4><span id="43-早期融合模型架构">4.3 早期融合模型架构</span></h4><p>如第3节所述，我们不能依赖于公开发布的恶意软件集合，因为该模型在执行时需要文件路径形式的上下文信息，而这些信息不是公开的。因此，与现有研究相比，为了保持评估模型性能的能力，我们只能依赖其他研究小组发布的带有预训练参数的恶意软件分类模型，并进一步评估我们数据集上的模型。不幸的是，没有任何混合或动态分析出版物[14、17、23、27、33]提供这样的伪影。</p>
<p>幸运的是，多个静态分析出版物伴随着代码库形式的工件[3，21，24]。例如，可以直接使用Ember LightGBM[12]模型，该模型是在Endgame3于2019年发布的Ember数据集[3]上预训练的。<strong>然而，我们不将该模型包括在我们的复合解决方案中，因为决策树模型不学习元模型可以使用的表示，只提供标量形式的最终预测。</strong>我们仍然依赖Ember特征提取方案[3]，但使用Rudd等人[24]发布的FFNN，分别具有三个隐藏层或512、512和128个隐藏神经元，均使用ELU[9]非线性，具有层归一化[5]和丢失率[30]𝑝 = 0.05。我们在来自Ember训练集[3]的600k个特征向量和来自我们训练集的72k个样本上重新训练了200个周期的FFNN。</p>
<p><strong>「 文件路径和API调用序列的分析可以被表述为相关的优化问题，即一维（1D）序列的分类 」</strong>。我们在受Kyadige和Rudd等人[15]影响的两种模型中使用了类似的神经结构，即嵌入层，具有用于表示提取的一维卷积神经网络（CNN）和完全连接的神经网络学习分类器功能。我们知道，有多种选择可以用交替结构（如递归神经网络（RNN）[8，10]）来建模序列分类问题。<strong>然而，如API调用序列分类的相关工作所示，两种模型体系结构都报告了类似的性能[14，23]，但表明1D CNN的计算要求明显降低[34]。</strong></p>
<p>编码输入向量𝑥 定长𝑁 为嵌入层提供尺寸𝐻 词汇量𝑉 . 这些参数受到超参数优化的影响。通过验证集上的超参数优化获得的<strong>文件路径模型</strong>的最佳值为：输入向量𝑥𝑓𝑝 长𝑁 = 100，嵌入维数𝐻 = 64、词汇量𝑉 = 150.模拟<strong>API调用序列模型</strong>的相应值为：输入向量𝑥𝑒𝑚 长𝑁 = 150，嵌入尺寸𝐻 = 96和词汇量𝑉 = 600.<strong>文件路径模型的词汇表由最常见的UTF-8字节形成，对于API调用序列模型，选择最常见的系统调用</strong>。<strong>两个词汇表都有两个用于填充和稀有字符的标签</strong>。<strong>「 嵌入层的输出被传递到四个单独的1D 卷积层 」</strong>，内核大小为2、3、4和5个字符，输出通道的数量𝐶 = 128.带较低𝐶 价值模型表现不佳。例如𝐶 = 64文件路径的模块验证集F1分数低至0.962，而𝐶 ∈ {100，128，160}得分在0.966左右。</p>
<p>所有四个卷积层的输出连接到大小为 4×𝐶 并传递到具有四个隐藏层的FFNN，其中包含1024、512、256和128个神经元。FFNN的隐藏层使用整流线性单元（ReLU）[1]激活。最后一层使用S形激活。在ReLU激活之前，将批量归一化[11]应用于FFNN的隐藏层。此外，为了防止过度装配，使用 𝑝 = 0.5的速率。</p>
<p>使用二进制交叉熵损失函数拟合所有早期融合网络：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191552850.png" alt="image-20220918183415081" style="zoom:50%;"></p>
<p>𝜙 (𝑥; 𝜃) 表示由深度学习模型给定参数近似的函数𝜃, 和𝑦 ∈ ｛0，1｝是地面真相标签。使用Adam优化器[13]进行优化，学习率为0.001，批量大小为1024个样本。<strong>我们使用PyTorch[19]深度学习库构建了一维卷积网络、EMBER FFNN和训练例程</strong>。</p>
<h4><span id="44-元模型">4.4 元模型</span></h4><p>早期融合模型的输出𝜙 (𝑥) 用于训练元模型𝜓. <strong>评估了三种不同的架构类型，使用scikit学习库[20]实现了逻辑回归和FFNN，并基于xgboost[7]实现了梯度提升决策树分类器</strong>。</p>
<p>自元模型𝜓 执行相对复杂的非线性映射 [0，1]^384→ [0，1]，基于表3中的性能指标，我们得出结论，融合的分类表面不平滑，并呈现了利用所有三种特征提取方法的表示进行最终决策的组合，而逻辑回归等简单模型无法学习。<strong>我们选择具有384、128、64和16个神经元的四层FFNN作为最终评估的元模型，因为它具有接近最佳的分数。</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191552051.png" alt="image-20220918183736944" style="zoom:50%;"></p>
<h3><span id="五-实验结果">五、实验结果</span></h3><p>实验表明，同时利用静态、动态和上下文信息的检测率明显高于单个模型的性能，特别是在<strong>低误报要求下</strong>。这种需求通常表达为安全行业中的机器学习解决方案。不符合低误报需求的解决方案通常不允许为人类分析师生成警报[22]。图3显示了样本外测试集给定固定假阳性率（FPR）的检测率（%）。</p>
<p>在100000个案例中，在FPR仅为100个错误分类的情况下设置警报阈值，Ember FFNN、filepath和仿真模型的个体模型检测率分别为56.86%、33.31%和33.89%。 <strong>然而，通过结合从所有三种处理技术中学习到的表示，混合解决方案可以从训练三个月后收集的测试集中的所有样本中正确分类86.28%。</strong></p>
<p>令人惊讶的观察结果产生了文件路径和仿真模型。两种模型各自表现相对较差，尤其是与余烬FFNN相比。这一观察结果背后的一个潜在解释是由原始ember出版物[3]中的600k个特征向量组成的ember FFNN训练集。这种训练语料库比我们的100k样本更好地概括了“真实”恶意PE分布，反映了特定时间窗口中的威胁情况。</p>
<p>然而，在低误报要求下，仅文件路径和仿真模型的组合，忽略静态分析，优于在更广泛的数据集上训练的最先进的余烬特征提取方案，在10000个样本中的一个样本的FPR下，检测率为77.36%对55.86%。</p>
<p>此外，将这两种模型结合在一起会导致检测率高于单个模型的累积能力，从而更加突出了混合元模型相对于狭义解决方案的优势。例如，虽然单个文件路径和仿真模型在105个错误警报的FPR中仅检测到33.31%和10.24%的样本，但它们的组合产生了77.36%的检测率。</p>
<p><strong>这一观察结果适用于从不同系统和不同时间框架收集的样本内验证和样本外测试集</strong>，使我们能够得出结论，这是具有元模型的混合检测启发式的一般属性，而不是特定数据集的伪影。对于文件路径、仿真和组合启发式，在105种情况下，给定一次错误分类的样本内验证集的值分别为34.46%、13.52%和97.25%</p>
<p>这一观察结果可以得出结论，元模型可以学习不同分析技术产生的恶意样本表示外组合的典型模式，例如组合特定的API调用序列和Trizna文件路径n-gram。这些表示中的每一个单独产生的证据都不足以将样本归类为恶意，因为它也发生在良性应用中。因此，只有当良性和恶意样本都被标记时，检测才会解除假阳性要求。然而，来自文件路径和API调用的表示的组合允许元模型在384维空间中构建决策边界以隔离此类情况，从而产生超过40%的检测率。</p>
<p>因此，我们看到，静态、动态和上下文数据的综合利用解决了独立方法的弱点，允许最小化FPR和假阴性率（FNR）。<br>表4中报告了所有集合的F1得分、精确度、召回率、精确度和AUC得分，<strong>元模型决策阈值为0.98，类似于FPR≈ 验证集为0.25%</strong>。虽然样本</p>
<p><strong>内验证集的报告结果允许得出模型几乎没有过度拟合的结论，但我们仍然观察到样本外测试集F1和AUC得分下降了≈ 4.− 4.5%. 尽管恶意软件家族的比例相同，但检测分数仍会下降，我们认为这种现象的因果关系源自恶意逻辑的进化本质。</strong></p>
<h3><span id="六-结论">六、结论</span></h3><p>我们已经证明，ML算法受益于混合分析，从而提高了性能，特别是在低误报要求下。我们确实报告了基于余烬特征向量[3]的当前最先进的恶意软件建模方案的异常性能，该方案报告的检测率明显高于单独的文件路径或仿真模型，如图3所示。然而，将余烬模型与文件路径或模拟或两种模型相结合，可以显著提高检测能力，在某些情况下，如低误报要求，几乎30%。</p>
<p>此外，我们报告说，混合解决方案可以检测恶意样本，即使没有一个单独的组件表示足够的信心来将输入分类为恶意。例如，对于100000个错误分类案例的FPR，单个文件路径和仿真模型仅检测到33.31%和10.24%的样本。它们的组合产生77.36%的检测率，如果两个模型一起使用但独立使用，则检测能力提高了40%以上。</p>
<p>我们得出结论，<strong>元模型</strong>可以从不同分析技术产生的表示组合中学习恶意样本的典型模式。此外，这一结论得到了数据集大小的支持，数据集大小明显大于行为恶意软件分析的相关工作。</p>
<p>我们认为，动态和语境分析的积极特征可以进一步扩展。虽然我们用API调用序列表示系统上的PE行为，<strong>但并非所有可执行功能都是通过API调用表示的</strong>。==<strong>额外的可见性源对于最小化模型决策启发式中的模糊性可能至关重要，我们认为扩展组合解决方案的模块性是一个有前途的研究方向。我们的分析中省略了大部分仿真遥测。</strong>==</p>
<p>我们公开发布了108204个样本的模拟报告，并期望在这方面进一步开展工作。<strong>文件系统和注册表修改、网络连接和内存分配可能为检测提供关键信息。</strong>我们解决方案的架构允许我们通过只重新训练<strong>元模型</strong>的参数，以最小的努力扩展决策启发式的模块性。</p>
<h2><span id="代码结构">代码结构</span></h2><blockquote>
<p>data/ </p>
<ul>
<li><p>adversarial.emulated</p>
</li>
<li><p>emulation.dataset</p>
</li>
</ul>
<ul>
<li><p>path.dataset</p>
</li>
<li><p>pe.dataset</p>
</li>
<li><p>train_val_test_sets</p>
</li>
</ul>
<p>evaluation/【评估】</p>
<p>img/【图片】</p>
<p>modules/【模型保存】</p>
<p>preprocessing/【数据预处理】</p>
<ul>
<li>arrary</li>
</ul>
<p>utils/</p>
<p>models.py 【模型结构】【CompositeClassifier 类】</p>
<p>example.py 【代码样例】</p>
</blockquote>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（10）Dos and Don&#39;ts of Machine Learning in Computer Security</title>
    <url>/posts/21HN90E/</url>
    <content><![CDATA[<h2><span id="dos-and-donts-of-machine-learning-in-computer-security">Dos and Don’ts of Machine Learning in Computer Security</span></h2><blockquote>
<p>改github的主页信息</p>
<p>USENIX Security ‘22：<a href="https://www.usenix.org/conference/usenixsecurity22/presentation/arp">https://www.usenix.org/conference/usenixsecurity22/presentation/arp</a></p>
<p><strong>Why not（ ? ）Use AI in Cyber Security</strong> </p>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p><strong>随着计算系统处理能力的不断增强和海量数据集的日益可用，机器学习算法在许多不同领域取得了重大突破</strong>。这一发展影响了计算机安全，催生了一系列基于机器学习的安全系统的工作，例如<strong>恶意软件检测、漏洞发现和二进制代码分析</strong>。尽管机器学习在安全领域有着巨大的潜力，但它容易出现一些微妙的缺陷，这些缺陷会破坏其性能，并使基于学习的系统可能不适合安全任务和实际部署。</p>
<p>在本文中，我们用批判的眼光看待这个问题。首先，我们确定了基于学习的安全系统的设计、实现和评估中的常见陷阱。我们对过去10年中顶级安全会议上的30篇论文进行了研究，确认这些缺陷在当前安全文献中普遍存在。在实证分析中，我们进一步证明了个别陷阱如何导致不切实际的表现和解释，阻碍对当前安全问题的理解。作为补救措施，我们提出了可行的建议，以支持研究人员在可能的情况下避免或减轻陷阱。此外，我们发现了将机器学习应用于安全时存在的问题，并为进一步的研究提供了方向。</p>
<h3><span id="一-说明">一、说明</span></h3><p><strong>没有一天不阅读机器学习的成功故事</strong>。对专业计算资源和大型数据集的广泛访问，以及用于深度学习的新概念和架构，为机器学习在几个领域的突破铺平了道路，例如自然语言的翻译[13，31，125]和图像内容的识别[62，78，117]。这一发展自然影响了安全研究：虽然过去主要局限于特定应用[53、54、132]，但机器学习现在已成为研究和解决多个应用领域中普遍存在的安全相关问题的关键促成因素之一，包括<strong>入侵检测</strong>[43、93]、<strong>恶意软件分析</strong>[69、88]、<strong>漏洞发现</strong>[83、142]，和<strong>二进制代码分析</strong>[42、114、140]。</p>
<p>然而，机器学习没有洞察力，需要在相当精细的工作流程中对数据的统计特性进行推理：错误的假设和实验偏见可能会对这一过程产生怀疑，以至于不清楚我们是否可以信任使用学习算法得出的科学发现[56]。<strong>二十年前[11、119、126]开始尝试识别特定安全领域（如网络入侵检测）中的挑战和限制，最近扩展到其他领域，如恶意软件分析和网站指纹识别[3、72、104、112]</strong>。然而，与这项工作垂直的是，我们认为存在与机器学习相关的一般陷阱，这些陷阱影响所有安全领域，迄今为止几乎没有受到关注。<strong>这些缺陷可能导致结果过于乐观，甚至影响整个机器学习工作流，削弱假设、结论和经验教训</strong>。因此，人们感觉到一种虚假的成就感，阻碍了学术界和工业界采用研究进展。健全的科学方法是支持直觉和得出结论的基础。我们认为，这一需求在安全性方面尤其重要，在安全性领域，过程往往受到积极绕过分析并破坏系统的对手的破坏。</p>
<p><strong>在本文中，我们确定了十个常见但微妙的陷阱，这些陷阱对研究结果的有效性构成威胁，并阻碍了对其的解释。</strong>为了支持这一说法，我们分析了过去十年中30篇依靠机器学习解决不同问题的顶级安全论文中这些陷阱的普遍性。令我们惊讶的是，每篇论文至少有三个陷阱；更糟糕的是，有几个陷阱影响了大多数论文，这表明这个问题是多么普遍和微妙。尽管这些陷阱很普遍，但了解它们在多大程度上削弱了结果并导致过于乐观的结论可能更为重要。最后我们对四个不同安全领域中的陷阱进行了影响分析。这些发现支持了我们的假设，回应了社区更广泛的关注。</p>
<h4><span id="贡献">贡献：</span></h4><ul>
<li><strong>陷阱识别</strong>。我们确定了机器学习在安全性方面的十个陷阱，并提出了可行的建议，以支持研究人员尽可能避免这些陷阱。此外，要确定无法轻松缓解的开放性问题，需要进一步研究。</li>
<li><strong>流行率分析</strong>。我们分析了在过去十年中发表的30份具有代表性的顶级证券报纸中发现的陷阱的普遍性。此外，我们进行了一项广泛的调查，其中我们获得并评估了这些论文作者关于已识别缺陷的反馈。</li>
<li><strong>影响分析</strong>。在四个不同的安全领域，我们通过实验分析了此类缺陷在多大程度上导致了实验偏差，以及我们如何通过应用建议的建议来有效克服这些问题。</li>
</ul>
<blockquote>
<p><strong>评论</strong> : 这项工作不应被解释为指手画脚的练习。相反，这是一种反思性的努力，表明了微妙的陷阱会对安全研究的进展产生多大的负面影响，以及我们作为一个社区如何充分缓解它们。</p>
</blockquote>
<h3><span id="二-机器学习中的陷阱">二、机器学习中的陷阱</span></h3><p>尽管机器学习取得了巨大的成功，但在实践中应用机器学习通常并不简单，而且容易出现一些缺陷，从明显的缺陷到微小的瑕疵。<strong>忽视这些问题可能会导致实验偏差或错误结论，尤其是在计算机安全方面。</strong>在本节中，我们介绍了在security research中经常出现的十个常见陷阱。虽然这些陷阱乍一看似乎显而易见，但它们根源于安全研究中普遍存在的细微缺陷，甚至在ATOP会议上发表的论文中也是如此。</p>
<p>我们根据典型机器学习工作流的各个阶段对这些缺陷进行分类，如图1所示。对于每个缺陷，我们提供了简短的描述，讨论了其对安全领域的影响，并提出了建议。此外，彩色条显示了我们分析中遭受陷阱的论文比例，较暖的颜色表示存在陷阱（见图3）</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550030.png" alt="image-20220804131643885"></p>
<h3><span id="21-数据收集与标记">2.1 数据收集与标记</span></h3><p>基于学习的系统的设计和开发通常从获取代表性数据集开始。显然，使用不切实际的数据进行实验会导致错误估计方法的能力。以下两个陷阱经常导致这个问题，因此在开发基于学习的计算机安全系统时需要特别注意。</p>
<h4><span id="221-sampling-bias-样本偏差1">2.2.1 Sampling Bias ： 样本偏差（1）</span></h4><p>收集的数据不足以代表潜在安全问题的真实数据分布。</p>
<p><strong>描述</strong>：除了少数例外，研究人员开发基于学习的方法时没有确切了解输入空间的真实潜在分布。相反，他们需要依赖于包含固定数量样本的数据集，这些样本旨在与实际分布相似。虽然在大多数情况下不可避免地存在一些偏见，但理解特定问题固有的特定偏见对于限制其在实践中的影响至关重要。如果数据不能有效地表示输入空间，甚至不能遵循不同的分布，那么从训练数据中得出有意义的结论就变得很有挑战性。</p>
<p><strong>安全影响</strong>：<strong>采样偏差与安全高度相关，因为数据采集尤其具有挑战性</strong>，通常需要使用多个质量不同的源。例如，对于用于Android恶意软件检测的合适数据集的收集，只有少数公共来源可用于获取此类数据[6, 134]。因此，依赖合成数据或组合来自不同来源的数据是常见的做法，正如我们在第4节中通过最先进的入侵和恶意软件检测方法的例子所证明的那样，这两种方法都会引入偏差。</p>
<p><strong>建议</strong>：在许多安全应用程序中，从真实分布中采样极其困难，有时甚至不可能。因此，这种偏差通常只能得到缓解，但不能完全消除。在§4中，我们表明，<strong>在某些情况下，一个合理的策略是构造真实分布的差分集并对其进行单独分析。进一步的策略包括使用合成数据扩展数据集</strong>[例如，28，60，137]或使用转移学习[See99，135，145，147]。然而，应避免来自不兼容源的数据混合，因为这是额外偏差的常见原因。无论如何，应该公开讨论所用数据集的局限性，让其他研究人员更好地理解潜在采样偏差的安全含义。</p>
<h4><span id="221-label-inaccuracy标签不准确2">2.2.1 Label Inaccuracy：标签不准确（2）</span></h4><p>分类任务所需的地面真实值标签不准确、不稳定或错误，影响基于学习的系统的整体性能[85，144]</p>
<p><strong>描述</strong>：许多基于AI的安全系统是为分类任务而构建的。为了训练这些系统，每次观测都需要一个 <strong>ground-truth</strong> label。不幸的是，这种标记很少是完美的，研究人员必须考虑不确定性和噪声，以防止他们的模型受到固有偏差的影响。</p>
<p><strong>安全影响</strong>：对于许多相关的安全问题，如检测网络攻击或恶意软件，通常无法获得可靠的标签，从而导致鸡和蛋的问题。作为补救措施，研究人员通常采用启发式方法，例如使用无法提供可靠基础真相的外部来源。例如，像 virustotal 这样的服务通常用于获取恶意软件的标签信息，但这些服务并不总是一致的[144]。此外，随着时间的推移，对手行为的改变可能会改变不同等级之间的比例[3,92，144]，引入一种称为<strong>标签偏移的偏差</strong>[85]。无法适应这些变化的系统一旦部署，性能将下降。</p>
<p><strong>建议</strong>：通常，应尽可能验证标签，例如，通过手动调查假阳性或随机样本[例如，122]。如果不排除噪声，则可以通过（i）使用稳健模型或损失函数，（ii）<strong>在学习过程中积极建模标签噪声</strong>，或（iii）清除训练数据中的噪声标签来减少其对学习模型的影响[见55,67,84]。为了证明这些方法的适用性，我们主要采用附录a中的清理方法。请注意，标签不确定的实例不得从测试数据中删除。这代表了采样偏差（P1）和数据窥探（P3）的变化，这是我们在§2.2中详细讨论的一个陷阱。由于标签可能会随时间变化，因此有必要采取预防措施，防止标签移动[85]，例如延迟标签，直到获得稳定的地面真相为止[见144]。</p>
<h3><span id="22-系统设计和学习">2.2 系统设计和学习</span></h3><p>一旦收集到足够的数据，就可以训练基于学习的安全系统。<strong>这个过程包括从数据预处理到提取有意义的特征和建立有效的学习模型</strong>。不幸的是，这些步骤中的每一步都可能引入缺陷和弱点。</p>
<h4><span id="221-data-snooping数据窥探3">2.2.1 Data Snooping：数据窥探（3）</span></h4><p><strong>学习模型是用实践中通常不可用的数据训练的。数据窥探可以以多种方式发生，其中一些非常细微，难以识别[1]。</strong></p>
<p><strong>描述</strong>：在生成学习模型之前，通常将收集的数据拆分为单独的训练集和测试集。虽然分割数据似乎很简单，但测试数据或其他通常不可用的背景信息有许多微妙的方式可以影响训练过程，从而导致数据窥探。<strong>虽然附录中提供了数据监听示例的详细列表（见表8</strong>），但我们大致区分了三种类型的数据监听：测试、临时和选择性监听。<strong>当测试集用于最终评估之前的实验时，会发生测试窥探</strong>。这包括识别有用特征、参数和学习算法的准备工作。<strong>如果忽略数据中的时间依赖性，则会发生时间监听</strong>。这是一个常见的陷阱，<strong>因为许多与安全相关的问题的潜在分布都处于不断变化的状态</strong>[例如，87，104]。最后，选择性监听描述了基于实践中不可用的信息清理数据。一个例子是基于完整数据集（即，训练和测试）的统计数据删除外部LIER，这些数据集通常在训练时不可用。</p>
<p><strong>安全影响</strong>：在安全方面，由于新的攻击或技术，数据分布是非平稳的，并且不断变化。因此，窥探未来或外部数据源的数据是一种普遍现象，导致结果过于乐观。例如，一些研究人员已经在基于学习的恶意软件检测系统中发现了时间窥探[4,8,104]。在所有这些情况下，由于混合了过去和现在的样本，这些方法的能力被高估。同样，在安全研究中也存在测试和选择性窥探事件，导致结果出现无意偏差（见§3）。</p>
<p><strong>建议</strong>：虽然训练、验证和测试数据应该严格分开似乎很明显，但在预处理阶段，这种数据隔离往往会被无意中违反。<strong>例如，我们观察到，在整个数据集上计算tf-idf权重或神经嵌入是一个常见错误</strong>（见§3）。为了避免这个问题，测试数据应该在数据收集期间尽早分割，并单独存储，直到最终评估。<strong>此外，在创建数据时，应考虑数据内的时间依赖性,数据集被拆分</strong>[4，87，104]。然而，其他类型的数据窥探很难解决。例如，随着公开数据集的特征日益暴露，使用该数据开发的方法隐含地从测试数据中获取年龄知识[见1，90]。<strong>因此，对知名数据集的实验应与对来自考虑适应应用领域的较新数据的实验相补充。</strong></p>
<h4><span id="222-spurious-correlations假相关性4">2.2.2 Spurious Correlations：假相关性（4）</span></h4><p><strong>与安全问题无关的工件创建了用于分类的快捷模式。因此，学习模型适应这些工件，而不是解决实际任务</strong>；</p>
<p><strong>描述</strong>：伪相关性是由与要解决的任务相关但实际上与之无关的产生的，从而导致错误关联。考虑一个网络入侵检测系统的例子，其中数据集中的大部分攻击来自某个网络区域。该模型可以学习检测特定IP范围而不是一般攻击模式。请注意，虽然抽样偏差是产生虚假相关性的常见原因，但这些偏差也是由其他因素造成的，我们在附录a.对此进行了更详细的讨论</p>
<p><strong>安全影响</strong>：<strong>机器学习通常应用于安全领域的黑盒。因此，虚假的相关性往往无法确定。一旦结果被解释并用于得出一般结论，这些相关性就会带来问题</strong>。如果不知道虚假相关性，则存在高估方法能力和误判其实际局限性的高风险。例如，§4.2报告了我们对漏洞发现系统的分析，表明基础数据中存在明显的虚假相关性。</p>
<p><strong>建议</strong>：<strong>为了更好地了解基于学习的系统的能力，我们通常建议应用机器学习的解释技术</strong>[见59、79、133]。尽管存在一些限制[例如，66，75127]，这些技术可以揭示虚假的相关性，并允许从业者评估其对系统能力的影响。作为一个例子，我们在§4中展示了不同安全相关问题的可解释学习如何有助于识别该问题。请注意，根据基于学习的系统的目标，一种设置中的虚假相关性可能被视为另一种设置的有效信号。因此，我们建议提前明确定义该目标，并验证系统学习的相关性是否符合该目标。例如，一个强大的恶意软件检测系统应该检测与恶意活动相关的特征，而不是数据中存在的其他无关信息</p>
<h4><span id="223-biased-parameter-selection-有偏参数选择5">2.2.3 Biased Parameter Selection ：有偏参数选择（5）</span></h4><p><strong>基于学习的方法的最终参数在训练时并不完全固定。相反，它们间接依赖于测试集</strong>。</p>
<p><strong>描述</strong>：在整个学习过程中，通常通过改变参数生成不同的模型。选择性能最佳的模型，并给出其在测试集上的性能。虽然这种设置通常是合理的，但它仍然会受到参数选择偏差的影响。<strong>例如，通过调整超参数或校准测试数据（而不是训练数据）上的阈值，可以很容易产生过于乐观的结果。</strong></p>
<p><strong>安全影响</strong>：参数在训练时未完全校准的安全系统在现实环境中的性能可能会有所不同。<strong>虽然网络入侵检测系统的检测阈值可以使用测试集上获得的ROC曲线来确定，但由于现实世界流量的多样性，在实践中很难选择相同的操作点</strong>[119]。与原始实验设置相比，这可能导致系统性能下降。请注意，此陷阱与数据窥探（P3）有关，但应明确考虑，因为它很容易导致夸大的结果。</p>
<p><strong>建议</strong>：这种陷阱构成了数据窥探的一种特殊情况，因此适用相同的对策。然而，在实践中，通过使用分离的评估集进行模型选择和参数调整，可以很容易地固定有偏差的参数选择。与通常难以缓解的一般数据窥探相比，严格的数据隔离已经足以排除确定超参数和阈值时的问题。</p>
<h3><span id="23-performance-evaluation">2.3 Performance Evaluation</span></h3><p><strong>性能评估典</strong>型机器学习工作流的下一个阶段是系统性能评估。在下文中，我们将展示不同的陷阱如何在评估此类系统时导致不公平的比较和有偏见的结果。</p>
<h4><span id="231-inappropriate-baseline-不适当的基线6">2.3.1  Inappropriate  Baseline: 不适当的基线（6）</span></h4><p>评估不使用或使用有限的基线方法进行。结果是，不可能证明对现有技术和其他安全机制的改进;</p>
<p><strong>描述</strong>:为了说明一种新方法在多大程度上提高了技术水平，将其与先前提出的方法进行比较至关重要。在选择基线时，重要的是要记住，不存在优于一般[136]中所有其他方法的通用学习算法。因此，仅提供所提出方法的结果或与基本相同的学习模型进行比较，并没有提供足够的背景来评估其影响。</p>
<p><strong>安全影响</strong>：<strong>过于复杂的学习方法会增加过度拟合的可能性，还会增加运行时开销、攻击面以及部署的时间和成本</strong>。为了证明与传统方法相比，机器学习技术提供了显著的改进，因此有必要对这些系统进行并排比较。</p>
<p><strong>推荐</strong>：在整个评估过程中，应考虑简单模型，而不是仅仅关注复杂模型进行比较。<strong>这些方法易于解释，计算要求较低，并且在实践中证明是有效的和可扩展的</strong>。在第4节中，我们演示了如何使用易于理解的简单模型作为基线来解释不必要的复杂学习模型。类似地，我们表明自动机器学习（AutoML）框架工作[例如，48,70]有助于找到合适的基线。虽然这些自动化方法肯定不能取代经验丰富的数据分析师，但它们可以用来设定所提出方法应达到的下限。最后，检查非学习方法是否也适用于应用场景是至关重要的。<strong>例如，对于入侵和恶意软件检测，存在多种使用其他检测策略的方法[例如，45、102、111]</strong>。</p>
<h4><span id="232-inappropriate-performance-measures不适当的性能衡量标准7">2.3.2 Inappropriate Performance Measures：不适当的性能衡量标准（7）</span></h4><p><strong>精选的性能度量没有考虑到应用场景的限制，例如数据不平衡或需要保持较低的误报率。</strong></p>
<p><strong>描述</strong>：可提供范围广泛的性能指标，但并非所有这些指标都适用于安全环境。例如，在评估检测系统时，仅报告一个性能值（如精度）通常是不够的，因为真阳性和假阳性判断是不可观察的。然而，更先进的测量方法，如ROC曲线，在某些应用环境中可能会模糊实验结果。图2显示了不平衡数据集上的ROC曲线和精度召回曲线（分类比率1:100）。仅考虑ROC曲线，性能看起来非常出色，但低精度揭示了分类器的真实性能，这对于许多安全应用来说是不切实际的；此外，各种与安全相关的问题涉及两个以上的类，需要多类度量。这一套可能会引入更多的微妙陷阱。众所周知，常用的策略，如”macro-averagingormicro-averaging “会过度估计和低估小类[51]。</p>
<p><strong>安全影响</strong>：不适当的性能度量是安全研究中的一个长期问题，特别是在检测任务中。例如，虽然真阳性和假阳性可以更详细地描述系统的性能，但当攻击发生率较低时，它们也可以掩盖实际精度。在机器学习中，性能指标的选择非常具体。因此，我们无法提供一般指南。相反，我们建议考虑基于学习的系统的实际部署，并确定有助于实践评估其性能的措施。请注意，这些度量通常与标准度量（如精度或误差）不同，因为它们更符合系统的日常操作。为了给读者一种直觉，在§4.1中，我们展示了Android恶意软件检测器的不同性能测量如何导致对其性能的矛盾解释。</p>
<h4><span id="233-base-rate-fallacy基本利率谬误8">2.3.3 Base Rate Fallacy：基本利率谬误（8）</span></h4><p><strong>在解释性能指标时，忽略了较大的类别不平衡，导致对绩效的高估。</strong></p>
<p><strong>描述</strong>：如果不考虑负类的基本速率，类不平衡很容易导致对性能的错误预测。如果这一类占主导地位，即使是极低的假阳性率也会导致意外的高假阳性率。请注意与先前陷阱的不同之处：P7指的是对绩效的不恰当描述，而基本利率谬误则是对结果的错误解释。这种特殊情况在实践中很容易被忽视（见§3）。考虑图2中的示例，其中99%的真阳性可能为1%的假阳性。然而，如果我们考虑1:100的分类比率，这实际上对应于每99个真阳性对应100个假阳性。</p>
<p><strong>安全影响</strong>：基本速率谬误与各种安全问题有关，例如入侵检测和网站指纹识别[11, 72, 100]。<strong>因此，现实地量化攻击者构成的安全和先验威胁是一项挑战。类似地，安装恶意软件的概率通常远低于恶意软件检测实验[104]</strong>。</p>
<p><strong>建议</strong>：安全方面的几个问题围绕着检测罕见事件，如威胁和攻击。对于这些问题，我们提倡使用精度和召回以及相关措施，如精度召回曲线。与其他衡量标准不同，这些功能考虑了分类平衡，因此类似于可靠的性能指标,对于关注少数类的检测任务[38118]。然而，请注意，如果少数群体的流行率被夸大，例如，由于抽样偏差[104]，精确度和召回率可能会产生误导。在这些情况下，马修相关系数（MCC）等其他度量更适合评估分类器的性能[29]（见§4）。此外，ROC曲线及其AUC值是比较检测和分类方法的有用指标。为了更加关注实际约束，我们建议考虑仅将曲线调整到可处理的假阳性率，并计算有界AUC值。<strong>最后，我们还建议讨论与负类（白样本）的基本比率相关的误报类，这使读者能够了解误报决策导致的工作量。</strong></p>
<h3><span id="24-部署和操作">2.4 部署和操作</span></h3><p>在典型机器学习工作流的最后一个阶段，部署了开发的系统来解决实践中潜在的安全问题。</p>
<h4><span id="241-lab-only-evaluation仅实验室评估9">2.4.1  Lab-Only Evaluation：仅实验室评估（9）</span></h4><p>基于学习的系统在实验室环境中进行了简单的评估，没有讨论其实际局限性</p>
<p><strong>描述</strong>：与所有经验学科一样，在某些假设下进行实验以证明方法的有效性是很常见的。虽然执行受控实验是检验某一方法特定方面的合法方法，但应在现实环境中进行评估，以透明地评估其能力，并展示将促进进一步研究的开放挑战。</p>
<p><strong>安全影响</strong>：许多基于学习的安全系统仅在实验室环境中评估，夸大了其实际影响。<strong>一个常见的例子是仅在封闭世界设置中评估的检测方法，具有有限的多样性，不考虑非平稳性[15,71]</strong>。例如，大量网站指纹攻击仅在有限时间内的封闭环境中评估[72]。类似地，一些基于学习的恶意软件检测系统在现实环境中没有得到充分的研究[见5，104]</p>
<p><strong>建议</strong>：<strong>重要的是要尽可能准确地远离实验室设置和近似的真实世界设置</strong>。例如，应考虑数据的时间和空间关系，以解释野外遇到的典型动态[见104]。类似地，运行时和存储约束应在实际条件下进行分析[See 15，112，130]。理想情况下，所提议的系统应该被部署来发现在纯实验室环境中无法观察到的问题，例如真实世界网络流量的多样性[see119]，尽管由于道德和隐私限制，这并非总是可能的。</p>
<h4><span id="242-inappropriate-threat-model不恰当的威胁模型10">2.4.2  Inappropriate Threat Model：不恰当的威胁模型（10）</span></h4><p>没有考虑机器学习的安全性，使系统面临各种攻击，如中毒和逃避攻击；</p>
<p><strong>描述</strong>：基于学习的安全系统在一个恶劣环境中运行，在设计这些系统时应考虑到这一点。<strong>先前在对抗式学习方面的工作表明，在工作流程的各个阶段，机器学习本身都引入了相当大的攻击面</strong>[见18，101]。其广泛的攻击面使这些算法容易受到各种类型的攻击，例如对抗性预处理、中毒和逃避[19、20、25、105、108]。</p>
<p><strong>安全影响</strong>：在威胁模型和评估中包括对抗性影响通常至关重要，因为攻击的系统不能保证输出可信和有意义的结果。因此，除了传统的安全问题外，还必须考虑与机器学习相关的攻击。例如，与考虑到安全因素而设计的适当规范化模型相比，攻击者可能更容易避开仅依赖少数功能的模型[40]，尽管还应考虑特定领域的影响[105]。此外，机器学习工作流程中的语义漏洞可能会造成攻击盲点。例如，不精确的解析和特征提取可能会使对手隐藏恶意内容[131]。</p>
<p><strong>建议</strong>：在使用基于学习的系统的大多数安全领域中，我们在一个动态的环境中操作。因此，应准确定义威胁模型，并根据这些模型评估系统。在大多数情况下，有必要假设一个适应性强的对手专门针对拟议的系统，并将搜索和利用弱点进行规避或操纵。类似地，考虑机器学习工作流的所有阶段并调查可能的漏洞也是至关重要的[见18、26、39、101]。对于该分析，我们建议尽可能关注白盒策略，遵循Kerckhoff的原则[73]和安全最佳实践。最后，我们要强调的是，对对抗性方面的评估不是附加内容，而是安全研究中的一个强制性组成部分。</p>
<h3><span id="三-流行性分析">三、流行性分析</span></h3><p>一旦我们了解了基于学习的安全系统所面临的陷阱，就有必要评估其普遍性并调查其对科学进步的影响。为此，我们对过去十年在ACM CCS、IEEE S&amp;P、USENIX Security和NDSS上发表的30篇论文进行了研究，这是我们社区中与安全相关研究的四大会议。这些论文被选为我们研究的代表性例子，因为它们涉及大量不同的安全主题，并成功地将机器学习应用于相应的研究问题。</p>
<p>特别是，我们选择的顶级论文涵盖以下主题：</p>
<ul>
<li>恶意软件检测[9，34，88，104，121，138]；</li>
<li>网络入侵检测[43，93，113，115]；</li>
<li>漏洞发现[42、49、50、83]；</li>
<li>tacks网站指纹识别[44100110116]；</li>
<li>社交网络滥用[22,95,120]；</li>
<li>二进制代码分析[14，32，114]；</li>
<li>代码归属[2,23]；</li>
<li>隐写术[17]；</li>
<li>网上诈骗[74]；</li>
<li>游戏机器人[80]；</li>
<li>[68]</li>
</ul>
<p><strong>评估标准</strong>：对于每一篇论文，<strong>陷阱大致分为存在、不存在、文本不清楚或不适用</strong>。在没有补救（存在）的实验中，陷阱可能完全存在，也可能不存在。如果作者纠正了任何偏见或缩小了他们的主张范围以适应陷阱，这也被视为不存在。此外，我们还介绍了一个类别，以说明确实存在陷阱的实验，但其影响已经得到了特别处理。如果目前或部分出现了一个陷阱，但在文本中得到了承认，我们将按照讨论的方式对分类进行调整。如果审稿人无法排除由于信息缺失而出现的陷阱，我们会将出版物与文本区分开来。最后，在P10的特殊情况下，如果陷阱不适用于纸张的设置，则将其视为单独的类别；</p>
<p><strong>观察</strong>：<strong>普适性分析的汇总结果如图3所示。条形图的颜色表示存在陷阱的程度，其宽度表示具有该分类的论文的比例。受影响纸张的数量记录在条形图的中心</strong>。最普遍的缺陷是<strong>抽样偏差</strong>（P1）和<strong>数据窥探</strong>（P3），这两种情况至少部分出现在90%和73%的论文中。在超过50%的论文中，我们发现至少部分存在不适当的威胁模型（第10页）、仅实验室评估（第9页）和不适当的性能度量（第7页）。每一份报纸都会受到至少三个陷阱的影响，这突出了这些问题在最近的计算机安全研究中的普遍性。特别是，我们发现数据集的收集仍然非常具有挑战性：我们作为社区开发的一些最具权威性和扩展性的开放数据集仍然不完善（见§4.1）</p>
<p>&lt;img src=”pic/Dos and Don’ts of Machine Learning in Computer Security</p>
<blockquote>
<p>改github的主页信息</p>
<p>USENIX Security ‘22：<a href="https://www.usenix.org/conference/usenixsecurity22/presentation/arp">https://www.usenix.org/conference/usenixsecurity22/presentation/arp</a></p>
<p><strong>Why not（ ? ）Use AI in Cyber Security</strong> </p>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p><strong>随着计算系统处理能力的不断增强和海量数据集的日益可用，机器学习算法在许多不同领域取得了重大突破</strong>。这一发展影响了计算机安全，催生了一系列基于机器学习的安全系统的工作，例如<strong>恶意软件检测、漏洞发现和二进制代码分析</strong>。尽管机器学习在安全领域有着巨大的潜力，但它容易出现一些微妙的缺陷，这些缺陷会破坏其性能，并使基于学习的系统可能不适合安全任务和实际部署。</p>
<p>在本文中，我们用批判的眼光看待这个问题。首先，我们确定了基于学习的安全系统的设计、实现和评估中的常见陷阱。我们对过去10年中顶级安全会议上的30篇论文进行了研究，确认这些缺陷在当前安全文献中普遍存在。在实证分析中，我们进一步证明了个别陷阱如何导致不切实际的表现和解释，阻碍对当前安全问题的理解。作为补救措施，我们提出了可行的建议，以支持研究人员在可能的情况下避免或减轻陷阱。此外，我们发现了将机器学习应用于安全时存在的问题，并为进一步的研究提供了方向。</p>
<h3><span id="一-说明">一、说明</span></h3><p><strong>没有一天不阅读机器学习的成功故事</strong>。对专业计算资源和大型数据集的广泛访问，以及用于深度学习的新概念和架构，为机器学习在几个领域的突破铺平了道路，例如自然语言的翻译[13，31，125]和图像内容的识别[62，78，117]。这一发展自然影响了安全研究：虽然过去主要局限于特定应用[53、54、132]，但机器学习现在已成为研究和解决多个应用领域中普遍存在的安全相关问题的关键促成因素之一，包括<strong>入侵检测</strong>[43、93]、<strong>恶意软件分析</strong>[69、88]、<strong>漏洞发现</strong>[83、142]，和<strong>二进制代码分析</strong>[42、114、140]。</p>
<p>然而，机器学习没有洞察力，需要在相当精细的工作流程中对数据的统计特性进行推理：错误的假设和实验偏见可能会对这一过程产生怀疑，以至于不清楚我们是否可以信任使用学习算法得出的科学发现[56]。<strong>二十年前[11、119、126]开始尝试识别特定安全领域（如网络入侵检测）中的挑战和限制，最近扩展到其他领域，如恶意软件分析和网站指纹识别[3、72、104、112]</strong>。然而，与这项工作垂直的是，我们认为存在与机器学习相关的一般陷阱，这些陷阱影响所有安全领域，迄今为止几乎没有受到关注。<strong>这些缺陷可能导致结果过于乐观，甚至影响整个机器学习工作流，削弱假设、结论和经验教训</strong>。因此，人们感觉到一种虚假的成就感，阻碍了学术界和工业界采用研究进展。健全的科学方法是支持直觉和得出结论的基础。我们认为，这一需求在安全性方面尤其重要，在安全性领域，过程往往受到积极绕过分析并破坏系统的对手的破坏。</p>
<p><strong>在本文中，我们确定了十个常见但微妙的陷阱，这些陷阱对研究结果的有效性构成威胁，并阻碍了对其的解释。</strong>为了支持这一说法，我们分析了过去十年中30篇依靠机器学习解决不同问题的顶级安全论文中这些陷阱的普遍性。令我们惊讶的是，每篇论文至少有三个陷阱；更糟糕的是，有几个陷阱影响了大多数论文，这表明这个问题是多么普遍和微妙。尽管这些陷阱很普遍，但了解它们在多大程度上削弱了结果并导致过于乐观的结论可能更为重要。最后我们对四个不同安全领域中的陷阱进行了影响分析。这些发现支持了我们的假设，回应了社区更广泛的关注。</p>
<h4><span id="贡献">贡献：</span></h4><ul>
<li><strong>陷阱识别</strong>。我们确定了机器学习在安全性方面的十个陷阱，并提出了可行的建议，以支持研究人员尽可能避免这些陷阱。此外，要确定无法轻松缓解的开放性问题，需要进一步研究。</li>
<li><strong>流行率分析</strong>。我们分析了在过去十年中发表的30份具有代表性的顶级证券报纸中发现的陷阱的普遍性。此外，我们进行了一项广泛的调查，其中我们获得并评估了这些论文作者关于已识别缺陷的反馈。</li>
<li><strong>影响分析</strong>。在四个不同的安全领域，我们通过实验分析了此类缺陷在多大程度上导致了实验偏差，以及我们如何通过应用建议的建议来有效克服这些问题。</li>
</ul>
<blockquote>
<p><strong>评论</strong> : 这项工作不应被解释为指手画脚的练习。相反，这是一种反思性的努力，表明了微妙的陷阱会对安全研究的进展产生多大的负面影响，以及我们作为一个社区如何充分缓解它们。</p>
</blockquote>
<h3><span id="二-机器学习中的陷阱">二、机器学习中的陷阱</span></h3><p>尽管机器学习取得了巨大的成功，但在实践中应用机器学习通常并不简单，而且容易出现一些缺陷，从明显的缺陷到微小的瑕疵。<strong>忽视这些问题可能会导致实验偏差或错误结论，尤其是在计算机安全方面。</strong>在本节中，我们介绍了在security research中经常出现的十个常见陷阱。虽然这些陷阱乍一看似乎显而易见，但它们根源于安全研究中普遍存在的细微缺陷，甚至在ATOP会议上发表的论文中也是如此。</p>
<p>我们根据典型机器学习工作流的各个阶段对这些缺陷进行分类，如图1所示。对于每个缺陷，我们提供了简短的描述，讨论了其对安全领域的影响，并提出了建议。此外，彩色条显示了我们分析中遭受陷阱的论文比例，较暖的颜色表示存在陷阱（见图3）</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551371.png" alt="image-20220804131643885"></p>
<h3><span id="21-数据收集与标记">2.1 数据收集与标记</span></h3><p>基于学习的系统的设计和开发通常从获取代表性数据集开始。显然，使用不切实际的数据进行实验会导致错误估计方法的能力。以下两个陷阱经常导致这个问题，因此在开发基于学习的计算机安全系统时需要特别注意。</p>
<h4><span id="221-sampling-bias-样本偏差1">2.2.1 Sampling Bias ： 样本偏差（1）</span></h4><p>收集的数据不足以代表潜在安全问题的真实数据分布。</p>
<p><strong>描述</strong>：除了少数例外，研究人员开发基于学习的方法时没有确切了解输入空间的真实潜在分布。相反，他们需要依赖于包含固定数量样本的数据集，这些样本旨在与实际分布相似。虽然在大多数情况下不可避免地存在一些偏见，但理解特定问题固有的特定偏见对于限制其在实践中的影响至关重要。如果数据不能有效地表示输入空间，甚至不能遵循不同的分布，那么从训练数据中得出有意义的结论就变得很有挑战性。</p>
<p><strong>安全影响</strong>：<strong>采样偏差与安全高度相关，因为数据采集尤其具有挑战性</strong>，通常需要使用多个质量不同的源。例如，对于用于Android恶意软件检测的合适数据集的收集，只有少数公共来源可用于获取此类数据[6, 134]。因此，依赖合成数据或组合来自不同来源的数据是常见的做法，正如我们在第4节中通过最先进的入侵和恶意软件检测方法的例子所证明的那样，这两种方法都会引入偏差。</p>
<p><strong>建议</strong>：在许多安全应用程序中，从真实分布中采样极其困难，有时甚至不可能。因此，这种偏差通常只能得到缓解，但不能完全消除。在§4中，我们表明，<strong>在某些情况下，一个合理的策略是构造真实分布的差分集并对其进行单独分析。进一步的策略包括使用合成数据扩展数据集</strong>[例如，28，60，137]或使用转移学习[See99，135，145，147]。然而，应避免来自不兼容源的数据混合，因为这是额外偏差的常见原因。无论如何，应该公开讨论所用数据集的局限性，让其他研究人员更好地理解潜在采样偏差的安全含义。</p>
<h4><span id="221-label-inaccuracy标签不准确2">2.2.1 Label Inaccuracy：标签不准确（2）</span></h4><p>分类任务所需的地面真实值标签不准确、不稳定或错误，影响基于学习的系统的整体性能[85，144]</p>
<p><strong>描述</strong>：许多基于AI的安全系统是为分类任务而构建的。为了训练这些系统，每次观测都需要一个 <strong>ground-truth</strong> label。不幸的是，这种标记很少是完美的，研究人员必须考虑不确定性和噪声，以防止他们的模型受到固有偏差的影响。</p>
<p><strong>安全影响</strong>：对于许多相关的安全问题，如检测网络攻击或恶意软件，通常无法获得可靠的标签，从而导致鸡和蛋的问题。作为补救措施，研究人员通常采用启发式方法，例如使用无法提供可靠基础真相的外部来源。例如，像 virustotal 这样的服务通常用于获取恶意软件的标签信息，但这些服务并不总是一致的[144]。此外，随着时间的推移，对手行为的改变可能会改变不同等级之间的比例[3,92，144]，引入一种称为<strong>标签偏移的偏差</strong>[85]。无法适应这些变化的系统一旦部署，性能将下降。</p>
<p><strong>建议</strong>：通常，应尽可能验证标签，例如，通过手动调查假阳性或随机样本[例如，122]。如果不排除噪声，则可以通过（i）使用稳健模型或损失函数，（ii）<strong>在学习过程中积极建模标签噪声</strong>，或（iii）清除训练数据中的噪声标签来减少其对学习模型的影响[见55,67,84]。为了证明这些方法的适用性，我们主要采用附录a中的清理方法。请注意，标签不确定的实例不得从测试数据中删除。这代表了采样偏差（P1）和数据窥探（P3）的变化，这是我们在§2.2中详细讨论的一个陷阱。由于标签可能会随时间变化，因此有必要采取预防措施，防止标签移动[85]，例如延迟标签，直到获得稳定的地面真相为止[见144]。</p>
<h3><span id="22-系统设计和学习">2.2 系统设计和学习</span></h3><p>一旦收集到足够的数据，就可以训练基于学习的安全系统。<strong>这个过程包括从数据预处理到提取有意义的特征和建立有效的学习模型</strong>。不幸的是，这些步骤中的每一步都可能引入缺陷和弱点。</p>
<h4><span id="221-data-snooping数据窥探3">2.2.1 Data Snooping：数据窥探（3）</span></h4><p><strong>学习模型是用实践中通常不可用的数据训练的。数据窥探可以以多种方式发生，其中一些非常细微，难以识别[1]。</strong></p>
<p><strong>描述</strong>：在生成学习模型之前，通常将收集的数据拆分为单独的训练集和测试集。虽然分割数据似乎很简单，但测试数据或其他通常不可用的背景信息有许多微妙的方式可以影响训练过程，从而导致数据窥探。<strong>虽然附录中提供了数据监听示例的详细列表（见表8</strong>），但我们大致区分了三种类型的数据监听：测试、临时和选择性监听。<strong>当测试集用于最终评估之前的实验时，会发生测试窥探</strong>。这包括识别有用特征、参数和学习算法的准备工作。<strong>如果忽略数据中的时间依赖性，则会发生时间监听</strong>。这是一个常见的陷阱，<strong>因为许多与安全相关的问题的潜在分布都处于不断变化的状态</strong>[例如，87，104]。最后，选择性监听描述了基于实践中不可用的信息清理数据。一个例子是基于完整数据集（即，训练和测试）的统计数据删除外部LIER，这些数据集通常在训练时不可用。</p>
<p><strong>安全影响</strong>：在安全方面，由于新的攻击或技术，数据分布是非平稳的，并且不断变化。因此，窥探未来或外部数据源的数据是一种普遍现象，导致结果过于乐观。例如，一些研究人员已经在基于学习的恶意软件检测系统中发现了时间窥探[4,8,104]。在所有这些情况下，由于混合了过去和现在的样本，这些方法的能力被高估。同样，在安全研究中也存在测试和选择性窥探事件，导致结果出现无意偏差（见§3）。</p>
<p><strong>建议</strong>：虽然训练、验证和测试数据应该严格分开似乎很明显，但在预处理阶段，这种数据隔离往往会被无意中违反。<strong>例如，我们观察到，在整个数据集上计算tf-idf权重或神经嵌入是一个常见错误</strong>（见§3）。为了避免这个问题，测试数据应该在数据收集期间尽早分割，并单独存储，直到最终评估。<strong>此外，在创建数据时，应考虑数据内的时间依赖性,数据集被拆分</strong>[4，87，104]。然而，其他类型的数据窥探很难解决。例如，随着公开数据集的特征日益暴露，使用该数据开发的方法隐含地从测试数据中获取年龄知识[见1，90]。<strong>因此，对知名数据集的实验应与对来自考虑适应应用领域的较新数据的实验相补充。</strong></p>
<h4><span id="222-spurious-correlations假相关性4">2.2.2 Spurious Correlations：假相关性（4）</span></h4><p><strong>与安全问题无关的工件创建了用于分类的快捷模式。因此，学习模型适应这些工件，而不是解决实际任务</strong>；</p>
<p><strong>描述</strong>：伪相关性是由与要解决的任务相关但实际上与之无关的产生的，从而导致错误关联。考虑一个网络入侵检测系统的例子，其中数据集中的大部分攻击来自某个网络区域。该模型可以学习检测特定IP范围而不是一般攻击模式。请注意，虽然抽样偏差是产生虚假相关性的常见原因，但这些偏差也是由其他因素造成的，我们在附录a.对此进行了更详细的讨论</p>
<p><strong>安全影响</strong>：<strong>机器学习通常应用于安全领域的黑盒。因此，虚假的相关性往往无法确定。一旦结果被解释并用于得出一般结论，这些相关性就会带来问题</strong>。如果不知道虚假相关性，则存在高估方法能力和误判其实际局限性的高风险。例如，§4.2报告了我们对漏洞发现系统的分析，表明基础数据中存在明显的虚假相关性。</p>
<p><strong>建议</strong>：<strong>为了更好地了解基于学习的系统的能力，我们通常建议应用机器学习的解释技术</strong>[见59、79、133]。尽管存在一些限制[例如，66，75127]，这些技术可以揭示虚假的相关性，并允许从业者评估其对系统能力的影响。作为一个例子，我们在§4中展示了不同安全相关问题的可解释学习如何有助于识别该问题。请注意，根据基于学习的系统的目标，一种设置中的虚假相关性可能被视为另一种设置的有效信号。因此，我们建议提前明确定义该目标，并验证系统学习的相关性是否符合该目标。例如，一个强大的恶意软件检测系统应该检测与恶意活动相关的特征，而不是数据中存在的其他无关信息</p>
<h4><span id="223-biased-parameter-selection-有偏参数选择5">2.2.3 Biased Parameter Selection ：有偏参数选择（5）</span></h4><p><strong>基于学习的方法的最终参数在训练时并不完全固定。相反，它们间接依赖于测试集</strong>。</p>
<p><strong>描述</strong>：在整个学习过程中，通常通过改变参数生成不同的模型。选择性能最佳的模型，并给出其在测试集上的性能。虽然这种设置通常是合理的，但它仍然会受到参数选择偏差的影响。<strong>例如，通过调整超参数或校准测试数据（而不是训练数据）上的阈值，可以很容易产生过于乐观的结果。</strong></p>
<p><strong>安全影响</strong>：参数在训练时未完全校准的安全系统在现实环境中的性能可能会有所不同。<strong>虽然网络入侵检测系统的检测阈值可以使用测试集上获得的ROC曲线来确定，但由于现实世界流量的多样性，在实践中很难选择相同的操作点</strong>[119]。与原始实验设置相比，这可能导致系统性能下降。请注意，此陷阱与数据窥探（P3）有关，但应明确考虑，因为它很容易导致夸大的结果。</p>
<p><strong>建议</strong>：这种陷阱构成了数据窥探的一种特殊情况，因此适用相同的对策。然而，在实践中，通过使用分离的评估集进行模型选择和参数调整，可以很容易地固定有偏差的参数选择。与通常难以缓解的一般数据窥探相比，严格的数据隔离已经足以排除确定超参数和阈值时的问题。</p>
<h3><span id="23-performance-evaluation">2.3 Performance Evaluation</span></h3><p><strong>性能评估典</strong>型机器学习工作流的下一个阶段是系统性能评估。在下文中，我们将展示不同的陷阱如何在评估此类系统时导致不公平的比较和有偏见的结果。</p>
<h4><span id="231-inappropriate-baseline-不适当的基线6">2.3.1  Inappropriate  Baseline: 不适当的基线（6）</span></h4><p>评估不使用或使用有限的基线方法进行。结果是，不可能证明对现有技术和其他安全机制的改进;</p>
<p><strong>描述</strong>:为了说明一种新方法在多大程度上提高了技术水平，将其与先前提出的方法进行比较至关重要。在选择基线时，重要的是要记住，不存在优于一般[136]中所有其他方法的通用学习算法。因此，仅提供所提出方法的结果或与基本相同的学习模型进行比较，并没有提供足够的背景来评估其影响。</p>
<p><strong>安全影响</strong>：<strong>过于复杂的学习方法会增加过度拟合的可能性，还会增加运行时开销、攻击面以及部署的时间和成本</strong>。为了证明与传统方法相比，机器学习技术提供了显著的改进，因此有必要对这些系统进行并排比较。</p>
<p><strong>推荐</strong>：在整个评估过程中，应考虑简单模型，而不是仅仅关注复杂模型进行比较。<strong>这些方法易于解释，计算要求较低，并且在实践中证明是有效的和可扩展的</strong>。在第4节中，我们演示了如何使用易于理解的简单模型作为基线来解释不必要的复杂学习模型。类似地，我们表明自动机器学习（AutoML）框架工作[例如，48,70]有助于找到合适的基线。虽然这些自动化方法肯定不能取代经验丰富的数据分析师，但它们可以用来设定所提出方法应达到的下限。最后，检查非学习方法是否也适用于应用场景是至关重要的。<strong>例如，对于入侵和恶意软件检测，存在多种使用其他检测策略的方法[例如，45、102、111]</strong>。</p>
<h4><span id="232-inappropriate-performance-measures不适当的性能衡量标准7">2.3.2 Inappropriate Performance Measures：不适当的性能衡量标准（7）</span></h4><p><strong>精选的性能度量没有考虑到应用场景的限制，例如数据不平衡或需要保持较低的误报率。</strong></p>
<p><strong>描述</strong>：可提供范围广泛的性能指标，但并非所有这些指标都适用于安全环境。例如，在评估检测系统时，仅报告一个性能值（如精度）通常是不够的，因为真阳性和假阳性判断是不可观察的。然而，更先进的测量方法，如ROC曲线，在某些应用环境中可能会模糊实验结果。图2显示了不平衡数据集上的ROC曲线和精度召回曲线（分类比率1:100）。仅考虑ROC曲线，性能看起来非常出色，但低精度揭示了分类器的真实性能，这对于许多安全应用来说是不切实际的；此外，各种与安全相关的问题涉及两个以上的类，需要多类度量。这一套可能会引入更多的微妙陷阱。众所周知，常用的策略，如”macro-averagingormicro-averaging “会过度估计和低估小类[51]。</p>
<p><strong>安全影响</strong>：不适当的性能度量是安全研究中的一个长期问题，特别是在检测任务中。例如，虽然真阳性和假阳性可以更详细地描述系统的性能，但当攻击发生率较低时，它们也可以掩盖实际精度。在机器学习中，性能指标的选择非常具体。因此，我们无法提供一般指南。相反，我们建议考虑基于学习的系统的实际部署，并确定有助于实践评估其性能的措施。请注意，这些度量通常与标准度量（如精度或误差）不同，因为它们更符合系统的日常操作。为了给读者一种直觉，在§4.1中，我们展示了Android恶意软件检测器的不同性能测量如何导致对其性能的矛盾解释。</p>
<h4><span id="233-base-rate-fallacy基本利率谬误8">2.3.3 Base Rate Fallacy：基本利率谬误（8）</span></h4><p><strong>在解释性能指标时，忽略了较大的类别不平衡，导致对绩效的高估。</strong></p>
<p><strong>描述</strong>：如果不考虑负类的基本速率，类不平衡很容易导致对性能的错误预测。如果这一类占主导地位，即使是极低的假阳性率也会导致意外的高假阳性率。请注意与先前陷阱的不同之处：P7指的是对绩效的不恰当描述，而基本利率谬误则是对结果的错误解释。这种特殊情况在实践中很容易被忽视（见§3）。考虑图2中的示例，其中99%的真阳性可能为1%的假阳性。然而，如果我们考虑1:100的分类比率，这实际上对应于每99个真阳性对应100个假阳性。</p>
<p><strong>安全影响</strong>：基本速率谬误与各种安全问题有关，例如入侵检测和网站指纹识别[11, 72, 100]。<strong>因此，现实地量化攻击者构成的安全和先验威胁是一项挑战。类似地，安装恶意软件的概率通常远低于恶意软件检测实验[104]</strong>。</p>
<p><strong>建议</strong>：安全方面的几个问题围绕着检测罕见事件，如威胁和攻击。对于这些问题，我们提倡使用精度和召回以及相关措施，如精度召回曲线。与其他衡量标准不同，这些功能考虑了分类平衡，因此类似于可靠的性能指标,对于关注少数类的检测任务[38118]。然而，请注意，如果少数群体的流行率被夸大，例如，由于抽样偏差[104]，精确度和召回率可能会产生误导。在这些情况下，马修相关系数（MCC）等其他度量更适合评估分类器的性能[29]（见§4）。此外，ROC曲线及其AUC值是比较检测和分类方法的有用指标。为了更加关注实际约束，我们建议考虑仅将曲线调整到可处理的假阳性率，并计算有界AUC值。<strong>最后，我们还建议讨论与负类（白样本）的基本比率相关的误报类，这使读者能够了解误报决策导致的工作量。</strong></p>
<h3><span id="24-部署和操作">2.4 部署和操作</span></h3><p>在典型机器学习工作流的最后一个阶段，部署了开发的系统来解决实践中潜在的安全问题。</p>
<h4><span id="241-lab-only-evaluation仅实验室评估9">2.4.1  Lab-Only Evaluation：仅实验室评估（9）</span></h4><p>基于学习的系统在实验室环境中进行了简单的评估，没有讨论其实际局限性</p>
<p><strong>描述</strong>：与所有经验学科一样，在某些假设下进行实验以证明方法的有效性是很常见的。虽然执行受控实验是检验某一方法特定方面的合法方法，但应在现实环境中进行评估，以透明地评估其能力，并展示将促进进一步研究的开放挑战。</p>
<p><strong>安全影响</strong>：许多基于学习的安全系统仅在实验室环境中评估，夸大了其实际影响。<strong>一个常见的例子是仅在封闭世界设置中评估的检测方法，具有有限的多样性，不考虑非平稳性[15,71]</strong>。例如，大量网站指纹攻击仅在有限时间内的封闭环境中评估[72]。类似地，一些基于学习的恶意软件检测系统在现实环境中没有得到充分的研究[见5，104]</p>
<p><strong>建议</strong>：<strong>重要的是要尽可能准确地远离实验室设置和近似的真实世界设置</strong>。例如，应考虑数据的时间和空间关系，以解释野外遇到的典型动态[见104]。类似地，运行时和存储约束应在实际条件下进行分析[See 15，112，130]。理想情况下，所提议的系统应该被部署来发现在纯实验室环境中无法观察到的问题，例如真实世界网络流量的多样性[see119]，尽管由于道德和隐私限制，这并非总是可能的。</p>
<h4><span id="242-inappropriate-threat-model不恰当的威胁模型10">2.4.2  Inappropriate Threat Model：不恰当的威胁模型（10）</span></h4><p>没有考虑机器学习的安全性，使系统面临各种攻击，如中毒和逃避攻击；</p>
<p><strong>描述</strong>：基于学习的安全系统在一个恶劣环境中运行，在设计这些系统时应考虑到这一点。<strong>先前在对抗式学习方面的工作表明，在工作流程的各个阶段，机器学习本身都引入了相当大的攻击面</strong>[见18，101]。其广泛的攻击面使这些算法容易受到各种类型的攻击，例如对抗性预处理、中毒和逃避[19、20、25、105、108]。</p>
<p><strong>安全影响</strong>：在威胁模型和评估中包括对抗性影响通常至关重要，因为攻击的系统不能保证输出可信和有意义的结果。因此，除了传统的安全问题外，还必须考虑与机器学习相关的攻击。例如，与考虑到安全因素而设计的适当规范化模型相比，攻击者可能更容易避开仅依赖少数功能的模型[40]，尽管还应考虑特定领域的影响[105]。此外，机器学习工作流程中的语义漏洞可能会造成攻击盲点。例如，不精确的解析和特征提取可能会使对手隐藏恶意内容[131]。</p>
<p><strong>建议</strong>：在使用基于学习的系统的大多数安全领域中，我们在一个动态的环境中操作。因此，应准确定义威胁模型，并根据这些模型评估系统。在大多数情况下，有必要假设一个适应性强的对手专门针对拟议的系统，并将搜索和利用弱点进行规避或操纵。类似地，考虑机器学习工作流的所有阶段并调查可能的漏洞也是至关重要的[见18、26、39、101]。对于该分析，我们建议尽可能关注白盒策略，遵循Kerckhoff的原则[73]和安全最佳实践。最后，我们要强调的是，对对抗性方面的评估不是附加内容，而是安全研究中的一个强制性组成部分。</p>
<h3><span id="三-流行性分析">三、流行性分析</span></h3><p>一旦我们了解了基于学习的安全系统所面临的陷阱，就有必要评估其普遍性并调查其对科学进步的影响。为此，我们对过去十年在ACM CCS、IEEE S&amp;P、USENIX Security和NDSS上发表的30篇论文进行了研究，这是我们社区中与安全相关研究的四大会议。这些论文被选为我们研究的代表性例子，因为它们涉及大量不同的安全主题，并成功地将机器学习应用于相应的研究问题。</p>
<p>特别是，我们选择的顶级论文涵盖以下主题：</p>
<ul>
<li>恶意软件检测[9，34，88，104，121，138]；</li>
<li>网络入侵检测[43，93，113，115]；</li>
<li>漏洞发现[42、49、50、83]；</li>
<li>tacks网站指纹识别[44100110116]；</li>
<li>社交网络滥用[22,95,120]；</li>
<li>二进制代码分析[14，32，114]；</li>
<li>代码归属[2,23]；</li>
<li>隐写术[17]；</li>
<li>网上诈骗[74]；</li>
<li>游戏机器人[80]；</li>
<li>[68]</li>
</ul>
<p><strong>评估标准</strong>：对于每一篇论文，<strong>陷阱大致分为存在、不存在、文本不清楚或不适用</strong>。在没有补救（存在）的实验中，陷阱可能完全存在，也可能不存在。如果作者纠正了任何偏见或缩小了他们的主张范围以适应陷阱，这也被视为不存在。此外，我们还介绍了一个类别，以说明确实存在陷阱的实验，但其影响已经得到了特别处理。如果目前或部分出现了一个陷阱，但在文本中得到了承认，我们将按照讨论的方式对分类进行调整。如果审稿人无法排除由于信息缺失而出现的陷阱，我们会将出版物与文本区分开来。最后，在P10的特殊情况下，如果陷阱不适用于纸张的设置，则将其视为单独的类别；</p>
<p><strong>观察</strong>：<strong>普适性分析的汇总结果如图3所示。条形图的颜色表示存在陷阱的程度，其宽度表示具有该分类的论文的比例。受影响纸张的数量记录在条形图的中心</strong>。最普遍的缺陷是<strong>抽样偏差</strong>（P1）和<strong>数据窥探</strong>（P3），这两种情况至少部分出现在90%和73%的论文中。在超过50%的论文中，我们发现至少部分存在不适当的威胁模型（第10页）、仅实验室评估（第9页）和不适当的性能度量（第7页）。每一份报纸都会受到至少三个陷阱的影响，这突出了这些问题在最近的计算机安全研究中的普遍性。特别是，我们发现数据集的收集仍然非常具有挑战性：我们作为社区开发的一些最具权威性和扩展性的开放数据集仍然不完善（见§4.1）</p>
<p>此外，文本中存在的一些陷阱比其他陷阱更容易被忽略。我们观察到，当没有对参数的描述时，这种有偏参数选择（P5）,给出了超参数或调谐过程；对于伪相关（P4），当没有试图解释模型的决定时；以及当文本中未明确描述数据集分割或归一化过程时的数据窥探（P3）。这些问题还表明，由于缺乏信息，实验设置更难复现；</p>
<p><strong>作者的反馈</strong>：为了促进我们社区内的讨论，我们联系了所选论文的作者，并收集了对我们研究结果的反馈。我们对135位有联系方式的作者进行了调查。为了保护作者的隐私并鼓励公开讨论，所有回复均匿名。调查包括一系列关于已识别缺陷的一般和具体问题。首先，我们询问作者是否阅读过我们的作品，并认为它对社区有帮助。其次，对于每一个陷阱，我们收集反馈信息，说明他们是否同意（a）他们的出版可能受到影响，（b）安全文件中经常出现陷阱，以及（c）在大多数情况下很容易避免。为了量化评估回答，我们对每个问题使用五点Likert量表，范围从强烈不同意到强烈同意。此外，我们提供了一个不回答的选项，并允许作者省略问题。我们收到了49位作者的反馈，回复率为36%。这些作者对应于30篇选定论文中的13篇，因此占考虑研究的43%。关于一般性问题，46（95%）的作者阅读了我们的论文，48（98%）同意这有助于提高对已识别缺陷的认识。对于具体的陷阱问题，作者和我们的发现之间的总体一致性平均为63%，这取决于安全区域和陷阱。所有的作者都认为他们的论文至少有一个缺陷。平均而言，他们指出他们的工作中存在2.77个陷阱，标准偏差为1.53，涵盖了所有十个陷阱。在评估总体缺陷时，作者特别同意，仅<strong>实验室评估（92%）、基本比率谬误（77%）、不适当的绩效衡量（69%）和抽样偏差（69%）经常发生在安全人员中</strong>。此外，他们指出，<strong>不适当的性能测量（62%）、不适当的参数选择（62%）和基准利率谬误（46%）在实践中可以很容易地避免</strong>，而其他陷阱需要更多的努力。我们在附录B中提供了关于该调查的更多信息。总之，我们从该调查中得出了三个中心观察结果。首先，大多数作者都同意，我们的社区缺乏对已识别缺陷的意识。第二，他们确认，这些陷阱在安全文献中很普遍，有必要减轻它们。第三，仍然缺乏对已识别缺陷的一致理解。例如，几位作者（44%）既不同意也不反对数据窥探是否容易避免，强调了明确定义和建议的重要性。我们发现§2中引入的所有陷阱在安全研究中都很普遍，影响了17%到90%的选定论文。每篇论文至少有三个陷阱，只有22%的实例与本文中的讨论相关。虽然作者在某些情况下甚至可能故意省略了对陷阱的讨论，但我们的患病率分析结果总体上表明，我们的社区缺乏认识。虽然这些发现指出了研究中的一个严重问题，<strong>但我们想指出，所有分析的论文都提供了出色的贡献和宝贵的见解。我们的目标不是责怪研究人员陷入陷阱，而是提高安全领域机器学习研究的意识和实验质量。</strong></p>
<h3><span id="四-影响分析">四、影响分析</span></h3><p>在前几节中，我们介绍了计算机安全文献中普遍存在的缺陷。然而，到目前为止，尚不清楚单个陷阱会在多大程度上影响实验结果及其结论。在本节中，我们估计了机器学习在安全领域的流行应用中的一些缺陷的实验影响。同时，我们展示了§2中讨论的建议如何帮助识别和解决这些问题。在我们的讨论中，我们考虑了计算机安全领域的四个热门研究主题：</p>
<ul>
<li><strong>移动恶意软件检测</strong>： (P1, P4, and P7)</li>
<li>漏洞发现： (P2, P4, and P6)</li>
<li>源代码作者归属：（P1和P4）</li>
<li><strong>网络入侵检测</strong>：（P6和P9）</li>
</ul>
<blockquote>
<p><strong>评论对于该分析，我们考虑了每个安全域的最先进方法</strong>。我们注意到，本节中的结果并不意味着具体批评这些方法；我们选择它们是因为它们代表了陷阱如何影响不同领域。值得注意的是，我们能够复制这些方法的事实高度赞扬了他们的学术水平;</p>
</blockquote>
<h4><span id="41-移动恶意软件检测">4.1 移动恶意软件检测</span></h4><p>使用机器学习自动检测Android恶意软件是一个特别活跃的研究领域。这种方法的设计和评估是微妙的，可能会显示出一些先前讨论的缺陷。在下文中，我们<strong>讨论了采样偏差（P1）、伪相关性（P4）和不适当的性能度量（P7）对基于学习的检测的影响</strong>；</p>
<p><strong>数据集集合</strong>：最近移动数据的一个常见来源是AndroZoop项目[6]，该项目从各种来源收集Android应用程序，包括Official 谷歌市场和几个中国app软件市场。在撰写本文时，它包括来自18个不同来源的超过1100万个Android应用程序。<strong>除了样本本身，它还包括元信息，如抗病毒检测的数量</strong>。虽然AndroZoo是获取移动应用的优秀来源，但我们证明，如果不考虑数据集的特性，实验可能会出现严重的抽样偏差（P1）。请注意，以下讨论不限于AndroZoo数据，而是与Android数据集的组成相关。</p>
<p><strong>数据集分析</strong>：在第一步中，我们通过考虑应用程序的来源和Android应用程序的防病毒检测数量来分析AndroZoo的数据分布。为了进行分析，我们将各个市场大致分为四个不同的来源：谷歌游戏、中国市场、VirusShare和所有其他市场。图4显示了从特定来源随机采样的概率，这取决于应用程序的防病毒检测数量。例如，当选择一个对检测次数没有限制的样本时，从谷歌游戏中采样的概率大约为80%。如果我们考虑10次检测的结果，我们从中国市场随机选择应用的概率是70%。如果我们忽略数据分布，数据集中的大部分良性应用很可能来自GooglePlay，而大多数恶意应用来自中国市场。请注意，此采样偏差不限于Andro Zoo。我们确定了DredeBin数据集[9]的类似抽样偏差，该偏差通常用于评估基于学习的Android恶意软件检测方法的性能[9、58、146]。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551654.png" alt="image-20220807135625094" style="zoom: 67%;"></p>
<p><strong>实验装置</strong>：为了更好地理解这一发现，我们使用两个数据集进行了实验：</p>
<ul>
<li>对于第一个数据集（D1），我们将来自Google Play的10000个良性应用程序与来自中国市场的1000个恶性应用程序（安齐安达应用程序中国）合并。</li>
<li>然后，我们使用相同的10000个良性应用程序创建第二个数据集（D2），但将它们与Google Play独家提供的1000个软件样本相结合。所有合法的应用程序都至少被10个病毒扫描程序检测到。接下来，我们使用来自最先进分类器的两个特征集（DREBIN[9]和OPSEQS[91]），在这些数据集上训练线性支持向量机[47]。</li>
</ul>
<p><strong>实验结果</strong>：在数据集D1和D2之间，DREBIN 和 OPSDEQS的召回率（真阳性率）分别超过10%和15%，而准确性仅受到轻微影响（见表1）。因此，性能度量的选择至关重要（P7）。有趣的是，URLplay.google.Com 被证明是良性类的五个最具歧视性的特征之一，这表明分类者已经学会区分Android apps的起源，而不是恶意软件和benign apps 之间的区别（P4）。<strong>虽然我们的实验装置通过故意忽略时间相关性（P3）高估了分类器的性能，但我们仍然可以清楚地观察到陷阱的影响</strong>。请注意，在以前的工作[4, 104]中已经证明了在这种情况下时间窥探的效果。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551720.png" alt="image-20220807211200980" style="zoom: 67%;"></p>
<h3><span id="42-漏洞发现">4.2 漏洞发现</span></h3><p>源代码中的漏洞可能导致权限提升和远程代码执行，使其成为主要威胁。由于手动搜索漏洞复杂且耗时，近年来提出了基于机器学习的检测方法[57，83，141]。在下面的内容中，我们展示了用于漏洞检测的数据集包含仅在一个类（P4）中发生的事件。我们还发现，用于检测脆弱性的神经网络Vuldeepecker[83]使用伪影进行分类，并且简单的线性分类器在相同的数据集上获得更好的结果（P6）。最后，我们讨论了forVulDeePecker提出的预处理步骤如何使我们无法确定某些代码片段是否包含漏洞（P2）</p>
<p><strong>数据集集合</strong>：在我们的分析中，我们使用了Li等人[83]发布的数据集，其中包含来自国家漏洞数据库[36]和SARD项目[37]的源代码。我们关注与缓冲区（CWE-119）相关的漏洞，并获得了39757个源代码片段，其中10444（26%）被标记为包含漏洞。</p>
<h3><span id="43-源代码作者归属">4.3 源代码作者归属</span></h3><p>基于源代码识别开发人员的任务称为作者归属[23]。编程习惯具有多种风格模式，因此</p>
<h4><span id="44-网络入侵检测检测">4.4 网络入侵检测检测</span></h4><p>网络入侵是安全[41]中最古老的问题之一，毫不奇怪，异常网络流量的检测严重依赖于基于学习的方法[27,81,82,93]。<strong>然而，收集真实攻击数据的挑战[46]常常导致研究人员生成合成数据，用于实验室评估</strong>（P9）。在这里，我们演示了这些数据如何不足以证明使用复杂模型（如神经网络）的合理性，以及将一个更简单的模型作为基线将如何揭示这些缺点（P6）。</p>
<p><strong>数据集集合</strong>：我们考虑Mirsky等人[93]发布的数据集，其中包含物联网（IoT）网络流量的捕获，模拟Mirai僵尸网络恶意软件的初始激活和传播。该数据包覆盖了三台个人电脑和九台物联网设备的Wi-Fi网络上119分钟的流量.</p>
<p><strong>数据集分析</strong>：首先，我们分析捕获的网络流量的传输量。图8显示了捕获过程中良性和恶意数据包的频率，划分为10秒。这表明在分组频率中有一个强信号，这高度指示了一个持续攻击。<strong>此外，所有良性活动似乎都随着袭击的开始而停止。74分钟后，尽管网络上有很多设备。这表明个体观测可能已经合并，并可能进一步导致系统受益于虚假相关性</strong>（P4）</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551694.png" alt="image-20220807213124529" style="zoom:50%;"></p>
<blockquote>
<p>图:Mirai数据集中良性与恶意数据包的频率[93]。灰色虚线显示了定义使用简单基线计算的正常流量的阈值（箱线图法[129]）。用于校准的数据范围由浅蓝色阴影区域突出显示.</p>
</blockquote>
<p><strong>实验设置</strong>：为了说明这些缺陷的严重程度，我们考虑了<strong>Kitsune[93]</strong>，这是一种基于深度学习的最先进入侵检测器，构建在一组au-toencoders上。对于每个数据包，提取115个特征，输入到12个自动编码器，这些自动编码器本身反馈到另一个作为异常检测器运行的最终自动编码器。</p>
<blockquote>
<p>[93]:</p>
</blockquote>
<p><strong>作为与Kitsune进行比较的简单基线，我们选择了箱线图法[129]</strong>，这是一种识别异常值的常用方法。我们使用10秒滑动窗口处理分组，并使用每个窗口的分组频率作为唯一特征。接下来，我们从干净的校准分布推导出一个上下阈值：τlow=Q1−1.5·IQRand ，τ高＝Q3+1.5·IQR。<strong>在测试期间，如果滑动窗口的数据包频率在τ低和τ高之间，则数据包被标记为良性，否则为恶意。在图8中，这些阈值用灰色虚线表示。</strong></p>
<p><strong>后果</strong>：表5显示了与箱线图方法相比，自动编码器集成的分类性能。虽然两种方法在ROC AUC方面表现相似，但简单箱线图法在低误报率（FPR）下优于自动编码器集成。<strong>除了其优越的性能外，箱线图方法与集成的特征提取和测试程序相比，重量非常轻</strong>。这一点尤其重要，因为集成设计用于在低延迟的资源受限设备（如物联网设备）上运行。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551172.png" alt="image-20220807220205015" style="zoom:50%;"></p>
<blockquote>
<p>表5：比较Kitsune[93]，一种自动编码器集成NIDS，相比于一个简单基线，箱线图法[129]，用于检测Mirai感染。</p>
</blockquote>
<p><strong>注意</strong>：<strong>本实验的目的不是证明箱图法可以检测到野外操作的Mirai实例，也不是证明Kitsuneis无法检测到其他攻击，而是证明没有适当基线（P6）的实验不足以证明集成的复杂性和过度性</strong>。箱线图法的成功还表明，简单的方法可以揭示仅用于实验室评估的数据产生的问题（第9页）。在Mirai数据集中，感染过于明显；在野外进行的攻击很可能只占网络流量的一小部分。</p>
<h3><span id="五-总结">五、总结</span></h3><p>我们识别并系统地评估了机器学习在安全领域的应用中的十个细微缺陷。这些问题会影响研究的有效性，并导致高估安全系统的性能。我们发现这些陷阱在安全研究中非常普遍，并展示了这些陷阱在不同安全应用中的影响。为了支持研究人员避免这些问题，我们提供了适用于所有安全领域的建议，从入侵和恶意软件检测到漏洞发现。最终，我们努力提高安全领域机器学习实验工作的科学质量。在Sommer和Paxson[119]的开创性研究十年后，weagain鼓励社区深入封闭的世界，探索将机器学习嵌入现实世界安全系统的挑战和机遇。AI安全的十大缺陷/image-20220806171411086.png” alt=”image-20220806171411086” style=”zoom:50%;” /&gt;</p>
<p>此外，文本中存在的一些陷阱比其他陷阱更容易被忽略。我们观察到，当没有对参数的描述时，这种有偏参数选择（P5）,给出了超参数或调谐过程；对于伪相关（P4），当没有试图解释模型的决定时；以及当文本中未明确描述数据集分割或归一化过程时的数据窥探（P3）。这些问题还表明，由于缺乏信息，实验设置更难复现；</p>
<p><strong>作者的反馈</strong>：为了促进我们社区内的讨论，我们联系了所选论文的作者，并收集了对我们研究结果的反馈。我们对135位有联系方式的作者进行了调查。为了保护作者的隐私并鼓励公开讨论，所有回复均匿名。调查包括一系列关于已识别缺陷的一般和具体问题。首先，我们询问作者是否阅读过我们的作品，并认为它对社区有帮助。其次，对于每一个陷阱，我们收集反馈信息，说明他们是否同意（a）他们的出版可能受到影响，（b）安全文件中经常出现陷阱，以及（c）在大多数情况下很容易避免。为了量化评估回答，我们对每个问题使用五点Likert量表，范围从强烈不同意到强烈同意。此外，我们提供了一个不回答的选项，并允许作者省略问题。我们收到了49位作者的反馈，回复率为36%。这些作者对应于30篇选定论文中的13篇，因此占考虑研究的43%。关于一般性问题，46（95%）的作者阅读了我们的论文，48（98%）同意这有助于提高对已识别缺陷的认识。对于具体的陷阱问题，作者和我们的发现之间的总体一致性平均为63%，这取决于安全区域和陷阱。所有的作者都认为他们的论文至少有一个缺陷。平均而言，他们指出他们的工作中存在2.77个陷阱，标准偏差为1.53，涵盖了所有十个陷阱。在评估总体缺陷时，作者特别同意，仅<strong>实验室评估（92%）、基本比率谬误（77%）、不适当的绩效衡量（69%）和抽样偏差（69%）经常发生在安全人员中</strong>。此外，他们指出，<strong>不适当的性能测量（62%）、不适当的参数选择（62%）和基准利率谬误（46%）在实践中可以很容易地避免</strong>，而其他陷阱需要更多的努力。我们在附录B中提供了关于该调查的更多信息。总之，我们从该调查中得出了三个中心观察结果。首先，大多数作者都同意，我们的社区缺乏对已识别缺陷的意识。第二，他们确认，这些陷阱在安全文献中很普遍，有必要减轻它们。第三，仍然缺乏对已识别缺陷的一致理解。例如，几位作者（44%）既不同意也不反对数据窥探是否容易避免，强调了明确定义和建议的重要性。我们发现§2中引入的所有陷阱在安全研究中都很普遍，影响了17%到90%的选定论文。每篇论文至少有三个陷阱，只有22%的实例与本文中的讨论相关。虽然作者在某些情况下甚至可能故意省略了对陷阱的讨论，但我们的患病率分析结果总体上表明，我们的社区缺乏认识。虽然这些发现指出了研究中的一个严重问题，<strong>但我们想指出，所有分析的论文都提供了出色的贡献和宝贵的见解。我们的目标不是责怪研究人员陷入陷阱，而是提高安全领域机器学习研究的意识和实验质量。</strong></p>
<h3><span id="四-影响分析">四、影响分析</span></h3><p>在前几节中，我们介绍了计算机安全文献中普遍存在的缺陷。然而，到目前为止，尚不清楚单个陷阱会在多大程度上影响实验结果及其结论。在本节中，我们估计了机器学习在安全领域的流行应用中的一些缺陷的实验影响。同时，我们展示了§2中讨论的建议如何帮助识别和解决这些问题。在我们的讨论中，我们考虑了计算机安全领域的四个热门研究主题：</p>
<ul>
<li><strong>移动恶意软件检测</strong>： (P1, P4, and P7)</li>
<li>漏洞发现： (P2, P4, and P6)</li>
<li>源代码作者归属：（P1和P4）</li>
<li><strong>网络入侵检测</strong>：（P6和P9）</li>
</ul>
<blockquote>
<p><strong>评论对于该分析，我们考虑了每个安全域的最先进方法</strong>。我们注意到，本节中的结果并不意味着具体批评这些方法；我们选择它们是因为它们代表了陷阱如何影响不同领域。值得注意的是，我们能够复制这些方法的事实高度赞扬了他们的学术水平;</p>
</blockquote>
<h4><span id="41-移动恶意软件检测">4.1 移动恶意软件检测</span></h4><p>使用机器学习自动检测Android恶意软件是一个特别活跃的研究领域。这种方法的设计和评估是微妙的，可能会显示出一些先前讨论的缺陷。在下文中，我们<strong>讨论了采样偏差（P1）、伪相关性（P4）和不适当的性能度量（P7）对基于学习的检测的影响</strong>；</p>
<p><strong>数据集集合</strong>：最近移动数据的一个常见来源是AndroZoop项目[6]，该项目从各种来源收集Android应用程序，包括Official 谷歌市场和几个中国app软件市场。在撰写本文时，它包括来自18个不同来源的超过1100万个Android应用程序。<strong>除了样本本身，它还包括元信息，如抗病毒检测的数量</strong>。虽然AndroZoo是获取移动应用的优秀来源，但我们证明，如果不考虑数据集的特性，实验可能会出现严重的抽样偏差（P1）。请注意，以下讨论不限于AndroZoo数据，而是与Android数据集的组成相关。</p>
<p><strong>数据集分析</strong>：在第一步中，我们通过考虑应用程序的来源和Android应用程序的防病毒检测数量来分析AndroZoo的数据分布。为了进行分析，我们将各个市场大致分为四个不同的来源：谷歌游戏、中国市场、VirusShare和所有其他市场。图4显示了从特定来源随机采样的概率，这取决于应用程序的防病毒检测数量。例如，当选择一个对检测次数没有限制的样本时，从谷歌游戏中采样的概率大约为80%。如果我们考虑10次检测的结果，我们从中国市场随机选择应用的概率是70%。如果我们忽略数据分布，数据集中的大部分良性应用很可能来自GooglePlay，而大多数恶意应用来自中国市场。请注意，此采样偏差不限于Andro Zoo。我们确定了DredeBin数据集[9]的类似抽样偏差，该偏差通常用于评估基于学习的Android恶意软件检测方法的性能[9、58、146]。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551441.png" alt="image-20220807135625094" style="zoom: 67%;"></p>
<p><strong>实验装置</strong>：为了更好地理解这一发现，我们使用两个数据集进行了实验：</p>
<ul>
<li>对于第一个数据集（D1），我们将来自Google Play的10000个良性应用程序与来自中国市场的1000个恶性应用程序（安齐安达应用程序中国）合并。</li>
<li>然后，我们使用相同的10000个良性应用程序创建第二个数据集（D2），但将它们与Google Play独家提供的1000个软件样本相结合。所有合法的应用程序都至少被10个病毒扫描程序检测到。接下来，我们使用来自最先进分类器的两个特征集（DREBIN[9]和OPSEQS[91]），在这些数据集上训练线性支持向量机[47]。</li>
</ul>
<p><strong>实验结果</strong>：在数据集D1和D2之间，DREBIN 和 OPSDEQS的召回率（真阳性率）分别超过10%和15%，而准确性仅受到轻微影响（见表1）。因此，性能度量的选择至关重要（P7）。有趣的是，URLplay.google.Com 被证明是良性类的五个最具歧视性的特征之一，这表明分类者已经学会区分Android apps的起源，而不是恶意软件和benign apps 之间的区别（P4）。<strong>虽然我们的实验装置通过故意忽略时间相关性（P3）高估了分类器的性能，但我们仍然可以清楚地观察到陷阱的影响</strong>。请注意，在以前的工作[4, 104]中已经证明了在这种情况下时间窥探的效果。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551324.png" alt="image-20220807211200980" style="zoom: 67%;"></p>
<h3><span id="42-漏洞发现">4.2 漏洞发现</span></h3><p>源代码中的漏洞可能导致权限提升和远程代码执行，使其成为主要威胁。由于手动搜索漏洞复杂且耗时，近年来提出了基于机器学习的检测方法[57，83，141]。在下面的内容中，我们展示了用于漏洞检测的数据集包含仅在一个类（P4）中发生的事件。我们还发现，用于检测脆弱性的神经网络Vuldeepecker[83]使用伪影进行分类，并且简单的线性分类器在相同的数据集上获得更好的结果（P6）。最后，我们讨论了forVulDeePecker提出的预处理步骤如何使我们无法确定某些代码片段是否包含漏洞（P2）</p>
<p><strong>数据集集合</strong>：在我们的分析中，我们使用了Li等人[83]发布的数据集，其中包含来自国家漏洞数据库[36]和SARD项目[37]的源代码。我们关注与缓冲区（CWE-119）相关的漏洞，并获得了39757个源代码片段，其中10444（26%）被标记为包含漏洞。</p>
<h3><span id="43-源代码作者归属">4.3 源代码作者归属</span></h3><p>基于源代码识别开发人员的任务称为作者归属[23]。编程习惯具有多种风格模式，因此</p>
<h4><span id="44-网络入侵检测检测">4.4 网络入侵检测检测</span></h4><p>网络入侵是安全[41]中最古老的问题之一，毫不奇怪，异常网络流量的检测严重依赖于基于学习的方法[27,81,82,93]。<strong>然而，收集真实攻击数据的挑战[46]常常导致研究人员生成合成数据，用于实验室评估</strong>（P9）。在这里，我们演示了这些数据如何不足以证明使用复杂模型（如神经网络）的合理性，以及将一个更简单的模型作为基线将如何揭示这些缺点（P6）。</p>
<p><strong>数据集集合</strong>：我们考虑Mirsky等人[93]发布的数据集，其中包含物联网（IoT）网络流量的捕获，模拟Mirai僵尸网络恶意软件的初始激活和传播。该数据包覆盖了三台个人电脑和九台物联网设备的Wi-Fi网络上119分钟的流量.</p>
<p><strong>数据集分析</strong>：首先，我们分析捕获的网络流量的传输量。图8显示了捕获过程中良性和恶意数据包的频率，划分为10秒。这表明在分组频率中有一个强信号，这高度指示了一个持续攻击。<strong>此外，所有良性活动似乎都随着袭击的开始而停止。74分钟后，尽管网络上有很多设备。这表明个体观测可能已经合并，并可能进一步导致系统受益于虚假相关性</strong>（P4）</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551606.png" alt="image-20220807213124529" style="zoom:50%;"></p>
<blockquote>
<p>图:Mirai数据集中良性与恶意数据包的频率[93]。灰色虚线显示了定义使用简单基线计算的正常流量的阈值（箱线图法[129]）。用于校准的数据范围由浅蓝色阴影区域突出显示.</p>
</blockquote>
<p><strong>实验设置</strong>：为了说明这些缺陷的严重程度，我们考虑了<strong>Kitsune[93]</strong>，这是一种基于深度学习的最先进入侵检测器，构建在一组au-toencoders上。对于每个数据包，提取115个特征，输入到12个自动编码器，这些自动编码器本身反馈到另一个作为异常检测器运行的最终自动编码器。</p>
<blockquote>
<p>[93]:</p>
</blockquote>
<p><strong>作为与Kitsune进行比较的简单基线，我们选择了箱线图法[129]</strong>，这是一种识别异常值的常用方法。我们使用10秒滑动窗口处理分组，并使用每个窗口的分组频率作为唯一特征。接下来，我们从干净的校准分布推导出一个上下阈值：τlow=Q1−1.5·IQRand ，τ高＝Q3+1.5·IQR。<strong>在测试期间，如果滑动窗口的数据包频率在τ低和τ高之间，则数据包被标记为良性，否则为恶意。在图8中，这些阈值用灰色虚线表示。</strong></p>
<p><strong>后果</strong>：表5显示了与箱线图方法相比，自动编码器集成的分类性能。虽然两种方法在ROC AUC方面表现相似，但简单箱线图法在低误报率（FPR）下优于自动编码器集成。<strong>除了其优越的性能外，箱线图方法与集成的特征提取和测试程序相比，重量非常轻</strong>。这一点尤其重要，因为集成设计用于在低延迟的资源受限设备（如物联网设备）上运行。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191551961.png" alt="image-20220807220205015" style="zoom:50%;"></p>
<blockquote>
<p>表5：比较Kitsune[93]，一种自动编码器集成NIDS，相比于一个简单基线，箱线图法[129]，用于检测Mirai感染。</p>
</blockquote>
<p><strong>注意</strong>：<strong>本实验的目的不是证明箱图法可以检测到野外操作的Mirai实例，也不是证明Kitsuneis无法检测到其他攻击，而是证明没有适当基线（P6）的实验不足以证明集成的复杂性和过度性</strong>。箱线图法的成功还表明，简单的方法可以揭示仅用于实验室评估的数据产生的问题（第9页）。在Mirai数据集中，感染过于明显；在野外进行的攻击很可能只占网络流量的一小部分。</p>
<h3><span id="五-总结">五、总结</span></h3><p>我们识别并系统地评估了机器学习在安全领域的应用中的十个细微缺陷。这些问题会影响研究的有效性，并导致高估安全系统的性能。我们发现这些陷阱在安全研究中非常普遍，并展示了这些陷阱在不同安全应用中的影响。为了支持研究人员避免这些问题，我们提供了适用于所有安全领域的建议，从入侵和恶意软件检测到漏洞发现。最终，我们努力提高安全领域机器学习实验工作的科学质量。在Sommer和Paxson[119]的开创性研究十年后，weagain鼓励社区深入封闭的世界，探索将机器学习嵌入现实世界安全系统的挑战和机遇。</p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（12）Neurlux: Dynamic Malware Analysis Without Feature Engineering</title>
    <url>/posts/1EB7867/</url>
    <content><![CDATA[<h2><span id="neurlux-dynamic-malware-analysis-without-feature-engineering">Neurlux: Dynamic Malware Analysis Without Feature Engineering</span></h2><blockquote>
<ul>
<li>论文：<a href="https://dl.acm.org/doi/10.1145/3359789.3359835#:~:text=Neurlux%20does%20not%20rely%20on%20any%20feature%20engineering%2C,report%20is%20from%20a%20malicious%20binary%20or%20not">https://dl.acm.org/doi/10.1145/3359789.3359835#:~:text=Neurlux%20does%20not%20rely%20on%20any%20feature%20engineering%2C,report%20is%20from%20a%20malicious%20binary%20or%20not</a>.</li>
<li>项目：<a href="https://github.com/ucsb-seclab/Neurlux">https://github.com/ucsb-seclab/Neurlux</a></li>
</ul>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p>恶意软件检测在计算机安全中起着至关重要的作用。现代机器学习方法一直以提取恶意特征的领域知识为中心。然而，可以使用许多潜在的功能，手动识别最佳功能既耗时又困难，特别是考虑到恶意软件的多样性。在<strong>本文中，我们提出了Neurlux，一种用于恶意软件检测的神经网络。Neurlux不依赖任何特征工程，而是从详细描述行为信息的动态分析报告中自动学习。我们的模型借鉴了文档分类领域的思想，使用报告中的单词序列来预测报告是否来自恶意二进制文件。我</strong>们调查了我们模型的学习特征，并显示了它倾向于给予报告的哪些组成部分最高的重要性。然后，我们在两种不同的数据集和报告格式上评估了我们的方法，表明Neurlux改进了现有技术，可以有效地从动态分析报告中学习。此外，我们还表明，我们的方法可移植到其他恶意软件分析环境，并可推广到不同的数据集。</p>
<h3><span id="说明">说明</span></h3><p>随着恶意软件变得越来越复杂，恶意软件分析也需要不断发展。传统上，大多数反恶意软件使用基于签名的检测，将可执行文件与已知恶意软件签名列表进行交叉引用。然而，这种方法有局限性，因为对恶意软件的任何更改都可能更改签名，因此同一恶意软件的新版本通常可以通过加密、混淆、打包或重新编译原始样本来逃避基于签名的检测。VirusTotal报告称，每天分析超过68万个新样本[40]，其中可能有相当多的样本只是之前看到的样本的重新打包版本，正如Brosch等人[3]观察到的那样。超过50%的新恶意软件只是现有恶意软件的重新打包版本。</p>
<p>近年来，由于需要将技术推广到以前看不见的恶意软件样本，因此产生了利用机器学习技术的检测方法[25，26，38]。恶意软件分析可以大致分为两类：代码（静态）分析和行为（动态）分析。静态分析和动态分析都有其优点和缺点。尽管动态分析提供了可执行行为的清晰画面，但它在实践中面临着一些问题。例如，对不可信代码的动态分析需要一个复制目标主机的虚拟机，这需要大量的计算资源。此外，恶意软件可能不会表现出其恶意行为，或者虚拟化环境可能不会反映恶意软件所针对的环境[5，15，27，30]。为了避免这些限制，一些相关工作仅依赖于从静态分析中提取的特征来实现对大量恶意软件样本的快速检测。然而，可以采用各种加密和混淆技术来阻碍静态分析[19，24]。这对静态恶意软件检测器来说是一个更严重的问题，因为包装在今天的良性样本中也被广泛使用。样本[28]。尽管动态分析被证明容易受到规避技术的影响，但运行时行为很难混淆。动态分析二进制文件可以解包并记录其与操作系统的交互，这是恶意软件分析的一个有吸引力的选择。</p>
<p>无论使用静态分析还是动态分析，大多数基于机器学习的恶意软件检测器都严重依赖于相关的领域知识[12，13，34]。这些方法通常依赖于恶意软件专家手动调查的功能，这需要大量的功能工程。例如，Kolbitsch等人[13]捕获了PE可执行程序在为此目的设计的特定功能中的行为图。恶意软件不断被创建、更新和更改，这可能会使最初精心设计的功能不适用于较新的恶意软件或不同的恶意软件家族。在这种情况下，必须不断完善成本高昂的特征工程工作。因此，找到一种降低人工特征工程成本的方法来从原始数据中提取有用的信息是至关重要的。</p>
<p>最近有一些关于基于深度学习的恶意软件分类的工作，这不需要特征工程。然而，现有的深度学习方法并没有利用来自现有动态分析系统的信息，而是倾向于选择一种类型的动态特征[14]或使用静态特征[6]。这些解决方案遗漏了关于每个样本所采取的行动的完整信息。</p>
<p>在本文中，我们提出了Neurlux，这是一个使用神经网络分析动态分析报告的系统。Cuckoo[21]等服务通过在沙盒中跟踪可执行文件来提供对可执行文件的详细动态分析。此分析包含诸如网络活动、注册表更改、文件操作等信息。我们使用这些报告作为分析的基础。也就是说，给定一个动态分析报告，我们希望能够预测该报告是针对恶意软件样本还是良性可执行文件。</p>
<p>我们的直觉是，我们可以将这些报告视为文件。凭借这种直觉，我们提出了Neurlux，这是一种神经网络，它在不需要任何特征工程的情况下学习并操作（清洁的）动态分析报告。Neurlux借用了文档分类领域的概念，将报告视为一系列单词，这些单词构成了一系列句子，从而创建了一个有用的模型。Neurlux打算用学习这些行为伪像或启发式的神经网络来取代昂贵的手工启发式。</p>
<p>为了检查我们的方法是否偏向于特定的报告格式（即沙盒），我们在评估中包括了两个不同的沙盒，即杜鹃沙盒[21]、杜鹃沙盒和一个商业反恶意软件供应商的沙盒（我们将其称为VendorSandbox）。此外，我们使用了两个不同的数据集，一个由商业反恶意软件供应商VendorDataset提供，另一个是标记的基准数据集EMBER[2]EmberDataset。</p>
<p>为了证明Neurlux比特征工程方法做得更好，我们实现了三种这样的技术，并与稍后讨论的技术进行了比较。此外，我们实现并比较了由Karbab等人[11]提出的MalDy模型作为基线。MalDy将行为（动态）报告形式化为一个单词袋（BoW），其中特征是报告中的单词。总之，我们做出了以下贡献：</p>
<ul>
<li>我们提出了Neurlux，这是一种利用文档分类概念来检测恶意软件的方法，该方法基于沙箱生成的行为（动态）报告，而无需进行功能工程。唯一的预处理步骤是清理报告以提取单词，在此基础上，我们的模型学习相关的单词序列，这有助于其预测。Neurlux在我们的K倍验证中显示出高准确度，达到96.8%的测试准确度。</li>
<li>我们在动态分析报告中创建并测试了几种恶意软件分类方法，包括新方法，如集成功能的堆叠集合和功能计数模型。我们与这些方法进行了比较，表明Neurlux优于特征工程方法。</li>
<li>我们通过在新的数据集和新的报告格式（即由新的沙盒生成）中测试Neurlux来评估其泛化能力，并表明它的泛化能力比我们评估的方法更好。</li>
<li>可执行文件的源代码和数据集将在github上发布。</li>
</ul>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（13）Dynamic Malware Analysis</title>
    <url>/posts/GW7NEC/</url>
    <content><![CDATA[<h2><span id="dynamic-malware-analysis-in-the-modern-eraa-state-of-the-art-survey">Dynamic Malware Analysis in the Modern Era—A State of the Art Survey</span></h2><p>​        本次调查的目的是对用于动态分析恶意软件的现有方法进行全面和最新的概述，其中包括对每种方法的<strong>描述</strong>、其<strong>优缺点</strong>以及对<strong>恶意软件规避技术的适应性</strong>。此外，我们还概述了利用机器学习方法来增强动态恶意软件分析能力的一些重要研究，这些研究旨在检测、分类和分类。</p>
<h4><span id="学习内容">学习内容：</span></h4><ul>
<li>分析方法：<strong>易失性内存取证（volatile memory forensics）、侧通道分析（side-channel analysis）</strong></li>
</ul>
<h4><span id="动态分析的意义">动态分析的意义：</span></h4><ol>
<li>虽然恶意软件编写者可以使用各种技术（如代码混淆、动态代码加载、加密和打包）来逃避静态分析（包括基于签名的防病毒工具）态分析对这些技术是健壮的，并且可以提供关于所分析文件的更多理解，因此可以导致更好的检测能力。</li>
</ol>
<h4><span id="动态分析的局限性">动态分析的局限性：</span></h4><ul>
<li>只有执行的代码是可观察的。这意味着，如果没有精确地满足所需的条件，那么某些代码可能无法执行，从而无法进行分析。<strong>Deeplocker</strong></li>
<li>动态分析还需要计算开销，这可能会降低执行速度。</li>
<li>分析必须在恶意软件针对的特定操作系统和/或硬件上执行。</li>
</ul>
<h4><span id="说明">说明：</span></h4><ul>
<li>物联网设备是另一个可以受益于内存分析的平台示例，因为基于软件的新检测机制的设计和安装并不简单。此外，由于物联网设备具有有限的计算资源，现有的动态分析技术对于此类设备可能不太相关或有效。</li>
</ul>
<h4><span id="恶意软件分类">恶意软件分类</span></h4><ul>
<li>Classification of Malware by Type</li>
<li>Classification of Malware by Malicious Behavior</li>
<li>Classification of Malware by Privilege</li>
<li>About Behavior and Privilege</li>
</ul>
<h4><span id="analyzing-malware-behavior">ANALYZING MALWARE BEHAVIOR</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191623702.png" alt="image-20210529200406017"></p>
<h4><span id="动态恶意软件分析框架描述">动态恶意软件分析框架描述</span></h4><h4><span id="分析技术和学术工具">分析技术和学术工具</span></h4><ul>
<li><h5><span id="函数功能分析">函数功能分析</span></h5><ul>
<li>每个进程都依赖函数调用来执行其职责，不管这些函数是进程内部的还是外部的（例如，由其他进程导出的函数、系统调用）。通过跟踪恶意软件调用的各种函数以及与这些函数相关的参数，可以更好地了解所分析恶意软件的行为。调用函数时获取通知可以通过将一段代码<strong>Hooking</strong>到该函数来实现。<ul>
<li>hooking mechanism</li>
<li>code injection</li>
</ul>
</li>
<li><strong>TTAnalyze(LastLine)</strong>是一个分析工具，它用一个称为Inside the Matrix（insideetm）的客户机组件扩展了QEMU仿真器，该组件将虚拟地址转换为物理地址。</li>
<li><strong>CWSandbox</strong>基于将可信DLL的代码注入到分析的进程中，该进程通过覆盖导出地址表（EAT）中的条目并将执行重定向到CWSandbox来拦截函数调用。CWSandbox收集大量的信息（比如被调用函数的名称、参数、系统状态等），然后将这些信息呈现给用户。</li>
<li><strong>Capture</strong>使用三个监视器分析操作系统的状态：<strong>文件系统监视器</strong>（跟踪所有硬盘上的读/写事件）、<strong>注册表监视器</strong>（跟踪多个注册表事件，如OpenKey、CreateKey等）和<strong>进程监视器</strong>（跟踪进程的创建和终止）。所有监视器都提供其他数据，如触发事件的进程、完整路径和时间戳。使用内核驱动程序，捕获与上述每个监视器对应的内核事件。最终结果是恶意软件触发的事件列表，以及它们的时间戳和参数。</li>
<li><strong>MalTRAK</strong>是一个跟踪恶意软件行为并扭转其影响的框架。它使用内核模式组件并将自身与关键函数挂钩，以跟踪恶意软件的操作，同时提供撤消这些操作的机制。</li>
<li><strong>dAnubis</strong>是用来分析内核驱动程序和检测rootkit的。它使用一个in-guest组件来监视rootkit和系统其余部分之间的通信。触发器引擎调用各种windowsapi调用来显示<strong>rootkit（定义为一组在恶意软件中获得root访问权限、完全控制目标操作系统和其底层硬件的技术编码）</strong>的存在（例如，隐藏的进程或文件）。</li>
</ul>
</li>
<li><h5><span id="execution-control-执行控制">Execution Control 执行控制</span></h5><p>动态恶意软件分析应该包含一种机制，偶尔停止恶意软件的执行，并检查恶意进程和操作系统的状态。执行控制技术包括：</p>
<ul>
<li><strong>Debugging</strong>调试（也称为单步）是一种可靠的分析技术，最初是为了帮助程序员发现代码中的错误而开发的。使用CPU的陷阱标志在每个操作码指令后生成中断，调试器可以允许恶意软件在强制上下文切换回分析进程之前只运行一条操作码指令，然后分析进程可以检查恶意软件和操作系统的状态。</li>
<li>等</li>
</ul>
</li>
<li><p><strong>Flow Tracking 流量跟踪</strong></p>
<p>细粒度分析用于跟踪通过恶意软件执行代码的信息流（例如，当一个函数的结果用作调用另一个函数的参数时）。</p>
<ul>
<li><strong>Data Tainting</strong></li>
<li><strong>Vigilite</strong>是一种分析工具，它使用二进制工具实现数据污染。最初是为了检测和阻止蠕虫的传播而开发的，Vigilite寻找来自网络的受污染数据的执行。因此，污点源是网络，当指令指针（IP）指向污点数据时，污点接收器就到达了，这意味着一些来自网络的不可信代码正在被执行。</li>
<li><strong>Panorama</strong>最初是一个TEMU插件，用于对各种I/O设备（包括硬盘、键盘或鼠标）进行污染分析。后来成为一个独立的平台。全景图的输出是以图形的形式提供的，它允许用户跟踪进程和内存区域之间的数据流。</li>
<li><strong>Dytan</strong>是Pin仪器系统的一个扩展，为数据污染提供了一个易于使用的API。它的开发采用了灵活的设计，允许用户配置各种组件，如污染源、污染汇、数据流跟踪器和控制流跟踪器。Dytan可以配置为跟踪显式和隐式信息流。此外，它的功能可以通过使用回调函数来扩展，回调函数实现额外的污染源、标签、传播和接收器。</li>
<li><strong>TQana</strong>是一个构建在QEMU之上的框架，用于分析和检测安装在internetexplorer上的恶意浏览器扩展。它使用带有两个污点源的数据污点：（1）用户访问的页面的所有URL字符串和（2）浏览器收到的响应其请求的信息。污点接收器是文件系统、注册表和网络。当受污染的数据被写入文件或通过网络发送时，被分析的样本就被怀疑是间谍软件。</li>
</ul>
</li>
<li><p><strong>Tracing 追踪</strong></p>
<p>收集执行某些代码后留下的信息称为跟踪。网络连接和分配给恶意软件的内存会留下恶意软件行为的痕迹。分析这些痕迹可以提供有关恶意软件的见解，而无需使用客户端组件中的。</p>
<ul>
<li><strong>Volatile memory analysis 易失性内存分析分析</strong>从内存转储文件分析恶意软件的影响（见第6.5节）需要了解操作系统如何跟踪进程、文件、用户和配置。所有这些数据结构都以二进制形式存在于内存转储中。</li>
<li><strong>Network Tracing</strong> 网络跟踪由于恶意软件在大多数情况下需要连接Internet才能执行其操作，因此在没有Internet访问的情况下，可能无法显示恶意软件的确切性质。然而，允许恶意软件完全访问互联网有时是不可取或不可能的。通过恶意软件限制网络访问并分析网络连接可以揭示恶意软件的C&amp;C和从中收到的命令。恶意软件留下的网络痕迹有助于理解其呈现的通信模式。</li>
<li><strong>HookFinder</strong>是TEMU实现的另一部分，旨在通过分析易失性内存来检测和分析恶意钩子。在堆栈中找到的信息被转换为创建钩子图，这有助于识别钩子链。然后，HookFinder将属于恶意软件的内存段标记为污点接收器。为了验证钩子实际上是由恶意软件安装的，HookFinder调用各种函数调用，并通过检查指令指针（IP）来跟踪控制流。当IP指向受污染的内存时，恶意钩子就会被发现并验证。</li>
<li><strong>LiveDM</strong>利用QEMU来分析内核中新内存区域的分配。通过挂接操作系统实现的几个内存分配函数，它可以跟踪恶意软件的安装位置，并对恶意软件的二进制代码执行静态分析。使用模拟器对客户操作系统的控制，LiveDM能够获得客户操作系统的易失性内存并分析受感染的内核。</li>
<li>等</li>
</ul>
</li>
<li><p><strong>Side-channel Analysis</strong></p>
<ul>
<li>到目前为止提出的分析技术依赖于从操作系统、易失性存储器或仿真机器的状态中提取数据。然而，任何类型的计算设备都可能成为恶意软件的攻击目标。这些设备包括PCI卡、物联网设备、硬盘、医疗设备等。分析和检测这些设备上运行的恶意软件是困难的，因为大多数情况下，这些设备不包含一个操作系统，可以支持传统的分析技术。与从操作系统的角度（或二进制级别）跟踪系统的行为不同，<strong>可以通过物理组件的功耗、电磁辐射或内部CPU事件来分析它们的行为</strong>。获取的数据分为“正常行为”和“感染行为”。使用统计方法和机器学习算法，检测到偏离正常行为可能表明CPU行为异常（例如，存在cryptominer或rootkit）。侧通道分析无法提供有关操作系统、网络或正在修改的文件的内部事件的深入信息。没有向仅仅收到最终判决的用户提供报告（称为恶意或非恶意）。</li>
<li><strong>WattsUpDoc</strong>是一种分析工具，用于使用外部设备对医疗设备进行侧信道分析。它证明了侧通道分析可以用于分析没有操作系统的设备，而无需向分析的设备加载任何代码。</li>
</ul>
</li>
</ul>
<h4><span id="布局映射技术mapping-techniques-to-layouts">布局映射技术（MAPPING TECHNIQUES TO LAYOUTS）</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191624416.png" alt="image-20210531145449113"></p>
<p>​        恶意软件和分析工具之间的战争是一场军备竞赛。攻击者不断开发新的方法来规避和检测分析框架，同时负责检测恶意软件的分析框架和工具的能力也在不断提高。创建图13是为了帮助读者理解这场军备竞赛以及<strong>攻击者和分析人员使用的不同方法</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191624015.png" alt="image-20210531150025715"></p>
<h4><span id="综合比较研究">综合比较研究</span></h4><p>本部分比较了近年来动态恶意软件分析的研究，并讨论了与此相关的趋势和参数。进行此比较，我们可以发现一些重要的见解，我们也与读者分享。</p>
<ul>
<li><p>基于功能和实际方面的比较</p>
<ul>
<li><p>我们的比较基于以下几个关键方面：</p>
<p>（1）该工具的相关性（其对科学界的影响和贡献基于引文数量和是否开源），</p>
<p>（2）该工具提供的分析的多功能性，</p>
<p>（3）用于实现该工具的分析布局（见第6节），</p>
<p>（4）工具的限制（预分析要求、所依赖的附加软件、特殊要求的硬件）和</p>
<p>（5）工具提供的输出。请注意，这些工具是按分析技术分组的，在每种技术中，它们是按出版年份排序的。关于图14的详细说明如下：</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（2）Dynamic Malware Analysis with Feature Engineering and Feature Learning</title>
    <url>/posts/NFRYYZ/</url>
    <content><![CDATA[<h2><span id="dynamic-malware-analysis-with-feature-engineering-and-feature-learning">Dynamic Malware Analysis with Feature Engineering and Feature Learning</span></h2><ul>
<li>项目：<a href="https://github.com/joddiy/DynamicMalwareAnalysis">https://github.com/joddiy/DynamicMalwareAnalysis</a></li>
<li>文章：<a href="https://arxiv.org/abs/1907.07352">https://arxiv.org/abs/1907.07352</a></li>
</ul>
<h3><span id="摘要">摘要</span></h3><p>动态恶意软件分析在隔离的环境中执行程序，并监控其运行时行为（如系统<strong>API调用</strong>），以检测恶意软件。该技术已被证明对各种代码混淆技术和新发布的（“零日”）恶意软件有效。然而，现有的工作通常<strong>只考虑API名称而忽略参数</strong>，<strong>或者需要复杂的特征工程操作和专家知识来处理参数</strong>。在本文中，我们<strong>提出了一种新的、低成本的特征提取方法，以及一种有效的深度神经网络体系结构，用于准确、快速地检测恶意软件</strong>。具体而言，<strong>特征表示方法利用特征哈希技巧对与API名称关联的API调用参数进行编码</strong>。深度神经网络体系结构应用多个选通CNN（卷积神经网络）来转换每个API调用的提取特征。通过双向LSTM（长-短期内存网络）进一步处理输出，以<strong>了解API调用之间的顺序相关性</strong>。实验表明，我们的解决方案在大型真实数据集上的性能明显优于基线。从烧蚀研究中获得了有关特征工程和建筑设计的宝贵见解。</p>
<h3><span id="一-说明">一、说明</span></h3><p>网络安全给全世界带来了巨大的经济成本。美国政府的一份报告（CEA 2018）估计，2016年美国经济中恶意网络活动的成本在570亿美元至1090亿美元之间。恶意软件（或恶意软件）是快速发展的主要网络安全威胁之一。据报道，每年发现超过1.2亿个新的恶意软件样本（AV-TEST 2017）。因此，开发恶意软件检测技术是迫切而必要的。</p>
<p>数十年来，研究人员一直致力于恶意软件检测。主流解决方案包括静态分析和动态分析。静态分析方法扫描软件的二进制字节流以创建签名，如可打印字符串、n-gram、指令等（Kruegel等人，2005）。然而，基于签名的静态分析可能容易受到代码混淆的影响（Rhode、Burnap和Jones，2018；Gibert等人，2018），或者不足以检测新的（“zeroday”）恶意软件（Vinod等人，2009）。相反，动态分析算法在隔离的环境（例如沙盒）中执行每个软件，以收集其运行时行为信息。通过使用行为信息，动态分析的检测率更高，比静态分析更稳健（Damodaran et al.2017）。在本文中，我们主要关注动态分析。</p>
<p>在行为信息中，<strong>系统API调用序列</strong>是最常用的数据源，因为它捕获了软件执行的所有操作（包括<strong>网络访问、文件操作等</strong>）。序列中的每个API调用都包含两个重要部分，即<strong>API名称和参数</strong>。每个API可能有零个或多个参数，每个参数都表示为<strong>名称-值对</strong>。为了处理行为信息，提出了许多特征工程方法。例如，如果我们将API名称视为一个字符串，则可以提取最多N个（例如1000个）频繁的N-gram特征（N=1，2…..) 从序列中。然而，从异构类型的参数（包括字符串、整数、地址等）中提取特征并非易事。最近，研究人员将深度学习模型应用于动态分析。卷积神经网络（CNN）和递归神经网络（RNN）等深度学习模型可以直接从序列数据中学习特征，而无需进行特征工程。尽管如此，计算机视觉和自然语言处理等传统深度学习应用程序的数据是同质的，例如图像（或文本）。<strong>使用深度学习模型处理异构API参数仍然具有挑战性</strong>。因此，大多数现有方法都忽略了这些参数。有几种利用API参数的方法（Tian et al.2010；Fang et al.2017；Agrawal et al.2018）。然而，这些方法要么将所有参数视为字符串（Tian et al.2010；Agrawal et al.2018），要么只考虑参数的统计信息（Ahmed et al.2009；Tian et al.2010；Islam et al.2013）。因此，<strong>它们无法充分利用来自不同类型参数的异构信息</strong>。</p>
<p>在本文中，我们提出了一种<strong>新的特征工程方法</strong>和一种<strong>新的恶意软件检测深度学习体系结构</strong>。特别是，对于不同类型的参数，我们的特征工程方法<strong>利用哈希方法分别提取异构特征</strong>。<strong>从API名称、类别和参数中提取的特征将进一步串联并输入到深度学习模型中</strong>。我们使用多个选通CNN模型（Dauphin et al.2017）从每个API调用的高维哈希特征中学习抽象的低维特征。来自选通CNN模型的输出由双向LSTM处理，以提取所有API调用的顺序相关性。</p>
<p>我们的解决方案比所有基线都有很大的优势。通过广泛的烧蚀研究，我们发现特征工程和模型架构设计对于实现高泛化性能至关重要。本文的主要贡献包括：</p>
<ul>
<li>我们为系统API参数提出了一种新的特征表示。从我们的数据集中提取的特征将发布供公众访问。</li>
<li>我们设计了一种深度神经网络结构来处理提取的特征，它将多个门控CNN和一个双向LSTM相结合。它以巨大的利润超过了所有现有的解决方案。</li>
<li>我们在一个大型真实数据集1上进行了广泛的实验。通过消融研究，发现了有关特征和模型架构的宝贵见解。</li>
</ul>
<h3><span id="二-相关工作">二、相关工作</span></h3><p>在本节中，我们将从特征工程和深度学习的角度回顾动态恶意软件分析。</p>
<h4><span id="21-api调用的功能工程">2.1 API调用的功能工程</span></h4><p>（Trinius等人，2009）介绍了一种称为<strong>恶意软件指令集（MIST）</strong>的特征表示。MIST使用多个级别的功能来表示系统调用。第一级表示API调用的类别和名称。以下级别是为每个API调用手动指定的，以表示它们的参数。然而，对于不同的API，同一级别的特性可能表示不同类型的信息。这种不一致性给使用机器学习模型学习模式带来了挑战。</p>
<p>（Qiao等人，2013）扩展了MIST，提出了一种称为<strong>基于字节的行为指令集（BBIS）</strong>的表示。他们声称，只有MIST的第一级（API调用的类别和名称）是有效的。此外，他们还提出了一种算法<strong>CARL来处理连续重复的API调用</strong>。</p>
<p>统计特征是训练机器学习模型的常用方法。提取API调用名称及其参数中的字符串，以计算频率和分布，作为中的特征（Tian et al.2010；Islam et al.2010；2013）。（Ahmed et al.2009）还使用统计特征来捕获空间和时间信息。从参数中提取空间信息，如均值、方差和熵。时间信息来自ngram API调用，包括两个n-gram API调用之间的相关性和转换可能性。</p>
<p>（Salehi、Ghiasi和Sami 2012）提出了一种将API调用与其参数关联起来的特性表示。它将每个参数与其API调用的名称连接起来，形成一个新的序列，然而，这种方法会导致一个非常长的特征向量，并且可能会丢失API调用序列的模式。</p>
<p>（Hansen等人，2016）提出了另外两种特征表示法。这些表示包括前200个API调用及其“参数”。但是，此“参数”仅指示此API调用是否与后一个API调用连接，而忽略原始参数。</p>
<h4><span id="22-deep-learning-based-approaches">2.2 Deep Learning Based Approaches</span></h4><p>（David and Netanyahu 2015）将<strong>沙盒报告视为一个完整的文本字符串</strong>，然后用任何特殊字符拆分所有字符串。他们计算每个字符串的频率，并使用20000位向量来表示最频繁的20000个字符串。他们的模型是一个<strong>深度信念网络（DBN）</strong>，由八层组成（从20000个大小的向量到30个大小的向量）。利用交叉熵损失对模型进行训练。在一个包含600个测试样本的小数据集上，它们的准确率达到98.6%。</p>
<p>（Pascanu et al.2015）提出了两个阶段的方法，即特征学习阶段和分类阶段。在第一阶段，他们<strong>使用RNN根据之前的API调用序列预测下一个可能的API调用</strong>。在分类阶段，他们冻结RNN，并将输出输入最大池层，以转换特征进行分类。在75000个样本的数据集上，它们的召回率达到71.71%，假阳性率为0.1%。</p>
<p><strong>（Kolosnjaji et al.2016）提出了一种将CNN与LSTM相结合的方法。他们的方法堆叠两个CNN层，每个CNN层使用一个3大小的内核来模拟3-gram方法。在CNN之后，附加一个具有100大小隐藏向量的LSTM来处理时间序列。以前的论文通常忽略了论点。</strong></p>
<p><strong>（Huang和Stokes 2016）使用了一种包含三个部分的特征表示，即参数中存在可运行代码、API调用名称与其中一个参数的组合（手动选择）以及3-gram的API调用序列</strong>。通过随机投影（random projection），此特征表示从50000减少到4000。（Agrawal et al.2018）提出了一种特征表示方法，<strong>该方法使用来自API调用名称的一个one-hot和参数字符串的前N个频繁N-gram</strong>。该模型使用了几个堆叠的LSTM，其性能优于（Kolosnjaji等人，2016）。他们还声称，多个LSTM不能提高性能。</p>
<h3><span id="三-系统框架">三、系统框架</span></h3><p>为了收集运行时API调用，我们实现了图1所示的系统。该系统由体育档案采集、行为信息采集、特征提取和模型训练三部分组成。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191547460.png" alt="image-20220526165948643"></p>
<h4><span id="31-pe文件收集">3.1 PE文件收集</span></h4><p>我们系统的工作流程从可移植可执行文件（PE）集合开始。在本文中，我们重点检测Windows系统中可移植可执行文件（PE）格式的恶意软件，这是最流行的恶意软件文件格式（AV-TEST 2017）。此收集部分已由新加坡SecureAge Technology本地反病毒公司实施。此外，该公司还维护了一个包含12个防病毒引擎的平台，用于对PE文件进行分类。对分类结果进行聚合，以获得每个PE文件的标签，用于模型培训。一旦模型经过培训，它将作为第13个防病毒引擎添加到平台中。收集之后，将维护一个执行队列，以提交PE文件以供执行。它监视存储使用情况并决定是否执行更多PE文件。</p>
<h4><span id="32-行为信息收集">3.2 行为信息收集</span></h4><blockquote>
<p>  <a href="https://cuckoosandbox.org/">https://cuckoosandbox.org/</a></p>
<p>  json在线解析：<a href="http://www.jsons.cn/jsoncheck/">http://www.jsons.cn/jsoncheck/</a></p>
</blockquote>
<p><strong>布谷鸟</strong>是一款开源软件，用于运行PE文件和收集执行日志。它在虚拟机内执行PE文件，并使用API挂钩监视API调用跟踪（即行为信息）。此外，布谷鸟模拟了一些用户行为，例如单击按钮、键入一些文本等。<strong>在我们的系统中，我们在每台服务器上维护了数十台虚拟机。所有虚拟机都安装了64位Windows 7系统和一些日常使用的软件</strong>。我们利用虚拟机的快照功能在执行后回滚它。<strong>所有生成的日志都存储在本地的布谷鸟服务器上</strong>。</p>
<h4><span id="33-特征提取和模型训练">3.3 特征提取和模型训练</span></h4><p>沙盒生成的执行日志包含<strong>PE文件的详细运行时信息</strong>，PE文件的大小从几KB到数百GB不等。我们设计了一个可以<strong>并行运行的特征工程解决方案</strong>，<strong>以有效地从原始执行日志中提取特征</strong>。提取特征后，我们在带有GPU的模型服务器上训练我们的深度学习模型，以进行恶意软件分类。</p>
<h3><span id="4-方法">4、方法</span></h3><blockquote>
<h4><span id="cuckoo-json">cuckoo json:</span></h4><ul>
<li><p><strong>behavior</strong></p>
<ul>
<li><p>summary</p>
<ul>
<li>file_recreated</li>
<li>file_failed</li>
<li>dll_loaded</li>
<li>guid</li>
<li>file_opened</li>
<li>file_created</li>
<li>file_written</li>
</ul>
</li>
<li><p><strong>generic</strong></p>
<ul>
<li>process_path: 进程启动路径</li>
<li><strong>process_name</strong>: 进程执行程序名</li>
<li>pid: 进程id</li>
<li>first_seen: 进程启动时间戳</li>
<li>ppid: 父进程id</li>
</ul>
</li>
<li><strong>apistats</strong>：<strong>API名称的调用次数信息</strong></li>
<li><p><strong>processes</strong></p>
<ul>
<li>command_line</li>
<li><p><strong>calls</strong></p>
<blockquote>
<p>  {</p>
<pre><code>&quot;**api**&quot;: &quot;NtCreateFile&quot;, 
&quot;**category**&quot;: &quot;file&quot;, 
&quot;return_value&quot;: 0, 
&quot;stacktrace&quot;: [ ], 
&quot;flags&quot;: &#123;
        &quot;desired_access&quot;: &quot;FILE_READ_ATTRIBUTES|READ_CONTROL|SYNCHRONIZE&quot;, 
        &quot;share_access&quot;: &quot;FILE_SHARE_READ&quot;, 
        &quot;file_attributes&quot;: &quot;&quot;, 
        &quot;create_disposition&quot;: &quot;FILE_OPEN&quot;, 
        &quot;create_options&quot;: &quot;FILE_NON_DIRECTORY_FILE|FILE_SYNCHRONOUS_IO_NONALERT&quot;, 
        &quot;status_info&quot;: &quot;FILE_OPENED&quot;
 &#125;, 
 &quot;**arguments**&quot;: &#123;
         &quot;desired_access&quot;: &quot;0x00120080&quot;, 
         &quot;share_access&quot;: 1, 
         &quot;**filepath_r**&quot;: &quot;\\??\\C:\\Users\\xn\\AppData\\Local\\Temp\\0e823db8b1beaca62207e2a82a9cd691c7af44cd14b876cb9f823f29750dd008&quot;, 
          &quot;**filepath**&quot;: &quot;C:\\Users\\xn\\AppData\\Local\\Temp\\0e823db8b1beaca62207e2a82a9cd691c7af44cd14b876cb9f823f29750dd008&quot;, 
           &quot;file_attributes&quot;: 0, 
           &quot;create_disposition&quot;: 1, 
           &quot;file_handle&quot;: &quot;0x000004ac&quot;, 
           &quot;status_info&quot;: 1, 
           &quot;create_options&quot;: 96
  &#125;, 
 &quot;tid&quot;: 3564, 
 &quot;status&quot;: 1, 
 &quot;time&quot;: 1622640835.384851
</code></pre><p>  }, </p>
</blockquote>
</li>
<li><p><strong>modules</strong>: <strong>样本运行时调用的系统文件信息, 包括被调用文件名/路径/基地址及其大小</strong></p>
</li>
<li>time: 运行时间</li>
</ul>
</li>
<li><p><strong>processtree</strong></p>
<ul>
<li>children: 子进程列表</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<h4><span id="41-特征工程">4.1 特征工程</span></h4><blockquote>
<ul>
<li><p><strong>FeatureHasher</strong>: <a href="https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher">https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher</a></p>
</li>
<li><p><strong>Feature Hashing for Large Scale Multitask Learning</strong> （2013）</p>
</li>
</ul>
</blockquote>
<p>以前的大多数工作（Qiao et al.2013；Pascanu et al.2015；Kolosnjaji et al.2016）都<strong>忽略了API调用的参数</strong>，只考虑了API名称和类别。因此，一些重要（鉴别）信息丢失（Agrawal et al.2018）。例如，如果忽略文件路径参数，则两个写操作（API调用）的功能将完全相同。但是，当目标文件由程序本身创建时，写入操作可能是良性的，但如果目标文件是系统文件，则写入操作可能是恶意的。一些考虑到论点的著作（Trinius et al.2009；Agrawal et al.2018；Huang and Stokes 2016）未能利用不同类型论点的异质信息。我们建议采用（Weinberger et al.2009）中的哈希方法，<strong>分别对API的名称、类别和参数进行编码</strong>。</p>
<p>如下表所示，我们的特征表示由不同类型的信息组成。<strong>API名称有8维，API类别有4维。API参数部分有90个维，16个用于整数参数，74个用于字符串参数。对于字符串参数，将处理几种特定类型的字符串（文件路径、DLL等）。此外，从所可打印字符串中提取了10个统计特征</strong>。将所有这些特征串联起来，形成<strong>102维特征向量</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191547725.png" alt="image-20220526170451620" style="zoom:50%;"></p>
<ul>
<li><h4><span id="api名称和类别">API名称和类别</span></h4></li>
</ul>
<blockquote>
<p>  单词拆分？fastext？<strong>删除循环调用？</strong></p>
</blockquote>
<p><strong>布谷鸟沙盒总共跟踪312个API调用，它们属于17个类别</strong>。每个API名称由多个单词组成，每个单词的第一个字母大写，例如“<strong>GetFileSize</strong>”。我们<strong>将API名称拆分为单词</strong>，然后应用下面的<strong>特性哈希技巧处理这些单词</strong>。对于API类别，由于该类别通常是单个单词，例如“<strong>network</strong>”，我们将该单词拆分为字符并应用特征哈希技巧。此外，我们<strong>计算API名称、类别和参数的MD5值，以删除任何连续重复的API调用</strong>。</p>
<p>我们使用方程1中的<strong>特征哈希</strong>（Weinberger et al.2009）<strong>将字符串序列编码为固定长度的向量</strong>。随机变量x表示元素序列，其中每个元素要么是字符串，要么是字符。M表示维度数量，即8表示API名称，4表示API类别。第i个bin的值通过以下公式计算：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548610.png" alt="image-20220526172933876" style="zoom:50%;"></p>
<p>其中，h是将元素（例如xj）映射到自然数m的哈希函数∈ {1，…，M}作为bin索引；ξ是另一个将元素映射到{+-1} 。也就是说，对于x的每个元素xj，其bin索引h（xj）为i，我们将ξ（xj）添加到bin中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">APIName</span>(<span class="title class_ inherited__">FeatureType</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; api_name hash info &#x27;&#x27;&#x27;</span></span><br><span class="line">    name = <span class="string">&#x27;api_name&#x27;</span></span><br><span class="line">    dim = <span class="number">8</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeatureType, self).__init__()</span><br><span class="line">        self._name = re.<span class="built_in">compile</span>(<span class="string">&#x27;^[a-z]+|[A-Z][^A-Z]*&#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">raw_features</span>(<span class="params">self, input_dict</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        input_dict: string</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        tmp = self._name.findall(input_dict)</span><br><span class="line">        hasher = FeatureHasher(self.dim, input_type=<span class="string">&quot;string&quot;</span>).transform([tmp]).toarray()[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> hasher</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_raw_features</span>(<span class="params">self, raw_obj</span>):</span><br><span class="line">        <span class="keyword">return</span> raw_obj</span><br><span class="line">      </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">APICategory</span>(<span class="title class_ inherited__">FeatureType</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; api_category hash info &#x27;&#x27;&#x27;</span></span><br><span class="line">    name = <span class="string">&#x27;api_category&#x27;</span></span><br><span class="line">    dim = <span class="number">4</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeatureType, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">raw_features</span>(<span class="params">self, input_dict</span>):</span><br><span class="line">        hasher = FeatureHasher(self.dim, input_type=<span class="string">&quot;string&quot;</span>).transform([input_dict]).toarray()[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> hasher</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_raw_features</span>(<span class="params">self, raw_obj</span>):</span><br><span class="line">        <span class="keyword">return</span> raw_obj</span><br></pre></td></tr></table></figure>
<ul>
<li><h4><span id="api参数">API参数</span></h4></li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548031.png" alt="image-20230319160222991"></p>
<p>至于API参数，只有两种类型的值，即<strong>整数</strong>和<strong>字符串</strong>。整数的单个值没有意义。需要参数名称才能获取值的含义。<strong>相同的整数值可能表示具有不同参数名称的完全不同语义</strong>。例如，<strong>名为“port”的数字22与名为“size”的数字不同</strong>。我们采用前面的特征哈希方法<strong>对整数的参数名称及其值进行编码</strong>，如等式2所示。我们使用参数名称来定位哈希容器。特别是，我们使用名称哈希值为i的所有参数通过求和更新第i个bin。对于每个这样的参数，我们计算对bin的贡献，如等式2所示，其中ξ($x<em>{name_j}$)是参数名称上的哈希函数，$x</em>{value_j}$是整数参数的值。由于整数可能在某个范围内稀疏分布，因此我们使用对数对值进行规格化，以压缩该范围。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548958.png" alt="image-20220526174316254" style="zoom:50%;"></p>
<p>其中，h和ξ是与等式1中相同的哈希函数。<strong>对于API参数字符串，它们的值比整数更复杂。某些以“0x”开头的字符串包含某些对象的地址。还有一些可能包含文件路径、IP地址、URL或纯文本</strong>。此外，一些API参数甚至可能包含整个文件的内容。字符串的多样性使得处理它们具有挑战性。根据之前的工作（Tian et al.2010；Islam et al.2010；2013；Ahmed et al.2009），最重要的字符串是关于<strong>文件路径、DLL、注册表项、URL和IP地址的值</strong>。因此，我们使用<strong>方程1中的特征哈希方法【string】</strong>来提取这些字符串的特征。</p>
<p>为了捕获<strong>字符串中包含的层次信息</strong>，我们将整个字符串解析为几个子字符串，并分别对它们进行处理。例如，我们使用“C:\”来标识文件路径。所有这些子串都通过方程1进行处理</p>
<blockquote>
<p>  <strong>Path</strong>：<strong>对于像“C:\a\b\C”这样的路径，将生成四个子字符串，即“C:”、“C:\a”、“C:\a\b”和“C:\a\b\C”。</strong></p>
<p>  dll：以“.dll”结尾的字符串</p>
<p>  注册表：项通常以“HKEY”开头</p>
<p>  IP：由点分隔的四个数字（范围从0到255）组成的字符串</p>
<p>  URL：我们仅从URL的主机名生成子字符串。例如，对于“<a href="https://security.ai.cs.org/，将生成以下子字符串“org”、“cs.org”、“ai.cs.org”和“security.ai.cs.org”。">https://security.ai.cs.org/，将生成以下子字符串“org”、“cs.org”、“ai.cs.org”和“security.ai.cs.org”。</a></p>
</blockquote>
<p><strong>DLL、注册表项和IP也采用相同的处理方法</strong>。<strong>dll是以“.dll”结尾的字符串。注册表项通常以“HKEY”开头。IP是由点分隔的四个数字（范围从0到255）组成的字符串</strong>。URL略有不同，我们仅从URL的主机名生成子字符串。例如，对于“<a href="https://security.ai.cs.org/，将生成以下子字符串“org”、“cs.org”、“ai.cs.org”和“security.ai.cs.org”。这样，域和组织信息将对该功能贡献更多。">https://security.ai.cs.org/，将生成以下子字符串“org”、“cs.org”、“ai.cs.org”和“security.ai.cs.org”。这样，域和组织信息将对该功能贡献更多。</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PRUIInfo</span>(<span class="title class_ inherited__">FeatureType</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Path, Registry, Urls, IPs hash info &#x27;&#x27;&#x27;</span></span><br><span class="line">    name = <span class="string">&#x27;prui&#x27;</span></span><br><span class="line">    dim = <span class="number">16</span> + <span class="number">8</span> + <span class="number">12</span> + <span class="number">16</span> + <span class="number">12</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeatureType, self).__init__()</span><br><span class="line">        self._paths = re.<span class="built_in">compile</span>(<span class="string">&#x27;^c:\\\\&#x27;</span>, re.IGNORECASE)</span><br><span class="line">        self._dlls = re.<span class="built_in">compile</span>(<span class="string">&#x27;.+\.dll$&#x27;</span>, re.IGNORECASE)</span><br><span class="line">        self._urls = re.<span class="built_in">compile</span>(<span class="string">&#x27;^https?://(.+?)[/|\s|:]&#x27;</span>, re.IGNORECASE)</span><br><span class="line">        self._registry = re.<span class="built_in">compile</span>(<span class="string">&#x27;^HKEY_&#x27;</span>)</span><br><span class="line">        self._ips = re.<span class="built_in">compile</span>(<span class="string">&#x27;^\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">raw_features</span>(<span class="params">self, input_dict</span>):</span><br><span class="line">        paths = np.zeros((<span class="number">16</span>,), dtype=np.float32)</span><br><span class="line">        dlls = np.zeros((<span class="number">8</span>,), dtype=np.float32)</span><br><span class="line">        registry = np.zeros((<span class="number">12</span>,), dtype=np.float32)</span><br><span class="line">        urls = np.zeros((<span class="number">16</span>,), dtype=np.float32)</span><br><span class="line">        ips = np.zeros((<span class="number">12</span>,), dtype=np.float32)</span><br><span class="line">        <span class="keyword">for</span> str_name, str_value <span class="keyword">in</span> input_dict.items():</span><br><span class="line">            <span class="keyword">if</span> self._dlls.<span class="keyword">match</span>(str_value):</span><br><span class="line">                tmp = re.split(<span class="string">&#x27;//|\\\\|\.&#x27;</span>, str_value)[:-<span class="number">1</span>]</span><br><span class="line">                tmp = [<span class="string">&#x27;\\&#x27;</span>.join(tmp[:i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(tmp) + <span class="number">1</span>)]</span><br><span class="line">                dlls += FeatureHasher(<span class="number">8</span>, input_type=<span class="string">&quot;string&quot;</span>).transform([tmp]).toarray()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> self._paths.<span class="keyword">match</span>(str_value):</span><br><span class="line">                tmp = re.split(<span class="string">&#x27;//|\\\\|\.&#x27;</span>, str_value)[:-<span class="number">1</span>]</span><br><span class="line">                tmp = [<span class="string">&#x27;\\&#x27;</span>.join(tmp[:i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(tmp) + <span class="number">1</span>)]</span><br><span class="line">                paths += FeatureHasher(<span class="number">16</span>, input_type=<span class="string">&quot;string&quot;</span>).transform([tmp]).toarray()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> self._registry.<span class="keyword">match</span>(str_value):</span><br><span class="line">                tmp = str_value.split(<span class="string">&#x27;\\&#x27;</span>)[:<span class="number">6</span>]</span><br><span class="line">                tmp = [<span class="string">&#x27;\\&#x27;</span>.join(tmp[:i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(tmp) + <span class="number">1</span>)]</span><br><span class="line">                registry += FeatureHasher(<span class="number">12</span>, input_type=<span class="string">&quot;string&quot;</span>).transform([tmp]).toarray()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> self._urls.<span class="keyword">match</span>(str_value):</span><br><span class="line">                tmp = self._urls.split(str_value + <span class="string">&quot;/&quot;</span>)[<span class="number">1</span>]</span><br><span class="line">                tmp = tmp.split(<span class="string">&#x27;.&#x27;</span>)[::-<span class="number">1</span>]</span><br><span class="line">                tmp = [<span class="string">&#x27;.&#x27;</span>.join(tmp[:i][::-<span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(tmp) + <span class="number">1</span>)]</span><br><span class="line">                urls += FeatureHasher(<span class="number">16</span>, input_type=<span class="string">&quot;string&quot;</span>).transform([tmp]).toarray()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> self._ips.<span class="keyword">match</span>(str_value):</span><br><span class="line">                tmp = str_value.split(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">                tmp = [<span class="string">&#x27;.&#x27;</span>.join(tmp[:i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(tmp) + <span class="number">1</span>)]</span><br><span class="line">                ips += FeatureHasher(<span class="number">12</span>, input_type=<span class="string">&quot;string&quot;</span>).transform([tmp]).toarray()[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> np.hstack([paths, dlls, registry, urls, ips]).astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_raw_features</span>(<span class="params">self, raw_obj</span>):</span><br><span class="line">        <span class="keyword">return</span> raw_obj</span><br></pre></td></tr></table></figure>
<h4><span id="统计信息">统计信息</span></h4><p>对于许多其他类型的字符串，根据之前的工作（Ahmed et al.2009；Tian et al.2010；Islam et al.2010），我们<strong>从所有可打印字符串中提取统计信息</strong>。可打印字符串由0x20到0x7f的字符组成。<strong>因此，包括所有路径、注册表项、URL、IP和其他一些可打印字符串</strong>。</p>
<ul>
<li>以“MZ”开头的一类字符串通常是包含整个PE文件的缓冲区，通常出现在恶意PE文件中，如<strong>线程注入</strong>（Liu et al.2011）。因此，我们额外计算“MZ”字符串的出现次数。</li>
<li><strong>10维向量用于记录字符串的数量、平均长度、字符数、所有可打印字符串的字符熵，以及路径、DLL、URL、注册表项、IP和“MZ”字符串的数量。</strong>我们没有处理其他参数，如虚拟地址、结构等，与上述类型的参数相比，这些参数相对不太重要。</li>
</ul>
<p>虽然所提出的特征工程方法很容易使用额外的bins应用于它们，但我们期待着进行更有针对性的研究来探索这些论点。</p>
<h4><span id="42-模型结构">4.2 模型结构</span></h4><p>我们提出了一种深度神经网络架构，它利用了所提出的特征工程步骤中的特征。图2概述了我们提出的深度学习模型。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548514.png" alt="image-20220526184024769" style="zoom:50%;"></p>
<ul>
<li><h4><span id="输入模块">输入模块</span></h4></li>
</ul>
<p>在特征工程之后，我们得到大小为（N，d）的输入向量，<strong>其中N是API调用序列的长度</strong>，<strong>d（102位）是每个提取的API特征的维数</strong>。我们首先通过<strong>批量规范化层（BN）</strong>对输入进行规范化（Ioffe和Szegedy 2015）。该批次标准化层通过减去批次平均值并除以批次标准偏差来标准化输入值。<strong>它保证了特征向量的某些维数不会太大而影响训练；实验验证了该方法的正则化效果。</strong></p>
<ul>
<li><h4><span id="门控cnns模块">门控CNNs模块</span></h4></li>
</ul>
<p>输入模块后应用了多个<strong>门控-CNN</strong>（Dauphin et al.2017）。<strong>门控-CNN允许选择重要和相关的信息，使其在语言任务上与循环模型相比，但消耗更少的资源和时间</strong>。对于每个选通的CNN，输入分别馈入两个卷积层。设XA表示第一卷积层的输出，XB表示第二卷积层的输出；它们由<img src="image-20220526184352710.png" alt="image-20220526184352710" style="zoom:50%;">组成，它涉及到元素乘法运算。这里，σ是sigmoid函数。<strong>σ（XB）被视为控制从XA传递到模型中下一层的信息的门</strong>。<strong>按照（Shen et al.2014)中的想法，一维卷积滤波器被用作n-gram检测器。如图2所示，我们使用两个选通CNN，其过滤器大小分别为2和3。所有卷积层的滤波器个数为128，步长为1。</strong></p>
<ul>
<li><h4><span id="bi-lstm模块">BI-LSTM模块</span></h4></li>
</ul>
<p><strong>来自门CNN的所有输出连接在一起</strong>。对这些输出应用批处理规范化层，以减少过拟合。我们使用<strong>双向LSTM来学习序列模式</strong>。<strong>每个LSTM的单元数为100</strong>。LSTM是一种递归神经网络架构，其中设计了几个门来控制信息传输状态，以便能够捕获长期上下文信息（Pichotta和Mooney 2016）。双向LSTM是两个LSTM叠加在一起，但方向输入不同。与单向LSTM相比，双向LSTM能够同时整合过去和未来状态的信息。双向LSTM在恶意软件检测方面已被证明是有效的（Agrawal et al.2018）。</p>
<ul>
<li><h4><span id="分类模块">分类模块</span></h4></li>
</ul>
<p><strong>在Bi LSTM模块中学习序列模式后，应用全局最大池层从隐藏向量中提取抽象特征</strong>。全局最大池层不是使用Bi LSTM的最终激活，而是依赖于整个序列中观察到的每个信号，这有助于保留整个序列中学习到的相关信息。在全局最大池层之后，<strong>我们使用单元数为64的密集层将中间向量的维数减少到64</strong>。将<strong>ReLU激活应用于该致密层</strong>。然后，我们使用速率为0.5的<strong>衰减层（dropout）</strong>来减少过度拟合。最后，<strong>单位数为1的致密层将维数减少到1</strong>。在致密层之后附加一个<strong>Sigmoid激活以输出概率</strong>。我们的模型使用与每个输入向量相关的标签进行监督。为了测量用于训练模型的损失。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548788.png" alt="image-20220526185129696" style="zoom:50%;"></p>
<p>此外，我们采用的优化方法是Adam，学习率为0.001。</p>
<h2><span id="dynamic-malware-analysis-代码结构"><strong><font color="red"> Dynamic Malware Analysis - 代码结构</font></strong></span></h2><blockquote>
<p>  <strong>model.py</strong></p>
<ul>
<li>ClassifyGenerator: Generates data for Keras</li>
<li>Model: 模型结构定义、模型训练</li>
<li>Cuckoo2DMDS：</li>
<li>DMDS</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（14）MALWARE综述</title>
    <url>/posts/2GBZRTM/</url>
    <content><![CDATA[<h1><span id="malware">MALWARE</span></h1><p>[TOC]</p>
<h2><span id="0补充">0.补充</span></h2><blockquote>
<p>  综述：</p>
<p>  Malconv</p>
<p>  数据集</p>
</blockquote>
<h2><span id="1-survey-overview">1. Survey Overview</span></h2><blockquote>
<p>  Period :2014-2021</p>
</blockquote>
<ul>
<li><h5><span id="platform">Platform</span></h5><ul>
<li>Windows [13,32]</li>
<li>Android [1-3,11,14,16,18,23,25,33,35-37,40]</li>
<li>Linux</li>
</ul>
</li>
<li><h5><span id="direction">Direction</span></h5><ul>
<li>Malware features from various aspects [1,9,24,28,31,40]</li>
<li>Malware propagation(传播) [2,25]</li>
<li>System mechanisms or services against malware [3,37]</li>
<li>Malware behaviors [5,15]<ul>
<li>Obfuscation [8,37]</li>
<li>Packing [8]</li>
<li>Stealth technologies [3,6]</li>
<li>Hook</li>
<li>Evasion from dynamic analysis [10,17,20,31,62]</li>
</ul>
</li>
<li>Dataset challenges, such as aging problem [21,23,41,51]</li>
<li>Performance metrics[14,23]</li>
<li>Specific malware：such as IoT malware[25,26,39], fileless[30] and PDF malware[43,54]</li>
<li>Visualization [15]</li>
<li>Graph representation [22]</li>
<li><strong>Detection Methods</strong> [3-4,8,9,11,12,14,16,19,31,33,36]<ul>
<li>ML based techniques [13,18,21,29,38,40]</li>
<li>DL based techniques [22,29,35]</li>
</ul>
</li>
<li><strong>APT</strong>(Advanced Persistent Threats) [20]</li>
<li><strong>Adversarial malware example generation</strong> [27,32]</li>
<li>ML/DL flaws [7,28]</li>
<li>ML/DL interpretability [34]</li>
</ul>
</li>
</ul>
<h2><span id="2-android-malware-detection">2. Android Malware detection</span></h2><h3><span id="21-behavior-detection-6364">2.1 Behavior detection [63,64]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Malton:Towards On-Device Non-Invasive Mobile Malware Analysis for ART</td>
<td>2017</td>
<td>Toprovide a comprehensive view of malware’s behaviors</td>
<td>Detectingeffectively</td>
<td>multi-layermonitoring &amp; information flow tracking</td>
</tr>
<tr>
<td>CopperDroid:Automatic Reconstruction of Android Malware Behaviors</td>
<td>2015</td>
<td>Toidentify OS- and high-level Android-specific behaviors.</td>
<td>Toreconstruct the behaviors of Android malware</td>
<td>VMI-baseddynamic analysis</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="22-signature-based-6566">2.2 Signature based [65,66]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>EnMobile: Entity-based Characterization and Analysis of Mobile</td>
<td>2018</td>
<td>Tocharacaterize malware comprehensively</td>
<td>Detectingeffectively</td>
<td>entity-based characterization and static analysis; signature based approach</td>
</tr>
<tr>
<td>Screening smartphone applications using malware family signatures</td>
<td>2015</td>
<td>Toimprove the robustness of signature matching</td>
<td>Toautomaticly extract family signature and matching</td>
<td>family signature</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="23-rule-based6768">2.3 Rule based[67,68]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Toward a more dependable hybrid analysis of android malware using aspect-oriented programming</td>
<td>2018</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>dataflowanalysis, detection of resource abuse;rule based</td>
</tr>
<tr>
<td>DroidNative: Automating and optimizing detection of Android native code malware variants</td>
<td>2017</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>specific control flow patterns;rule based</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="24-similarity-based">2.4 Similarity based</span></h3><h4><span id="241-model-similarity69-73">2.4.1 Model similarity[69-73]</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>An HMM and structural entropy based detector for Android malware: An empirical study</td>
<td>2016</td>
<td>Todefeat hiding</td>
<td>Detectingeffectively</td>
<td>HiddenMarkov Model, structural entropy.</td>
</tr>
<tr>
<td>Scalable and robust unsupervised android malware fingerprinting using community-based network partitioning</td>
<td>2020</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>maliciouscommunity</td>
</tr>
<tr>
<td>On the use of artificial malicious patterns for android malware detection</td>
<td>2020</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>malwarepatterns; Genetic Algorithm (GA); Apriori algorithm</td>
</tr>
<tr>
<td>Andro-Dumpsys: Anti-malware system based on the similarity of malware creator and malware centric information</td>
<td>2016</td>
<td>Todefeat packing, dynamic loading etc.</td>
<td>Detectingeffectively</td>
<td>similarity matching of malware creator-centric</td>
</tr>
<tr>
<td>Bayesian Active Malware Analysis</td>
<td>2020</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>the Markov chain models</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="242-graph-similarity74-79">2.4.2 Graph similarity[74-79]</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>PermPair: Android Malware Detection Using Permission Pairs</td>
<td>2020</td>
<td>Tomake use of permission information</td>
<td>Todetect Android malware</td>
<td>The comparasion of the graph of permission pairs.</td>
</tr>
<tr>
<td>Apposcopy: Semantics-Based Detection of Android Malware through Static Analysis</td>
<td>2014</td>
<td>Toimprove signature based methods</td>
<td>Detectingeffectively</td>
<td>combination of static taint analysis and program representation called Inter-Component Call Graph</td>
</tr>
<tr>
<td>Profiling user-trigger dependence for Android malware detection</td>
<td>2015</td>
<td>Tocapture stealthily launch operation</td>
<td>Detectingeffectively</td>
<td>Graphcomparision</td>
</tr>
<tr>
<td>Identifying Android Malware Using Network-Based Approaches</td>
<td>2019</td>
<td>Tomake use of network information</td>
<td>Detectingeffectively</td>
<td>aweighted network to compare closeness</td>
</tr>
<tr>
<td>Cypider: Building Community-Based Cyber-Defense Infrastructure for Android Malware Detection</td>
<td>2016</td>
<td>Todeal with endless new malware</td>
<td>Detectingeffectively</td>
<td>scalablesimilarity network infrastructure;malicious community</td>
</tr>
<tr>
<td>Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs</td>
<td>2014</td>
<td>Tocharacaterize malware from program semantics</td>
<td>Detectingeffectively</td>
<td>a weighted contextual API dependency graph as program semantics;graphsimilarity metrics</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="25-ml-based-6080-101">2.5 ML based [60,80-101]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MAMADROID:Detecting Android Malware by Building Markov Chains of Behavioral Models</strong></td>
<td>2017</td>
<td>Todesign robust malware mitigation techniques</td>
<td>Constructinga classifier</td>
<td>BuildingMarkov Chains of Behavioral Models;Random Forests , Nearest Neighbor (1-NN) ,3-Nearest Neighbor (3-NN) ,and Support Vector Machines (SVM)</td>
</tr>
<tr>
<td><strong>Drebin:Effective and Explainable Detection of Android Malware in Your Pocket</strong></td>
<td>2014</td>
<td>Tomitigate the influence on limited resources in Android platform</td>
<td>To propose a lightweight method to detect malware at run-time</td>
<td>Staticanalysis and SVM</td>
</tr>
<tr>
<td>MakeEvasion Harder: An Intelligent Android Malware Detection System</td>
<td>2018</td>
<td>Todetect evolving Android malware</td>
<td>Higherdetection rate</td>
<td>APIcalls and higher-level semantics; SVM</td>
</tr>
<tr>
<td>UsingLoops For Malware Classification Resilient to Feature-unaware Perturbations</td>
<td>2018</td>
<td>Tosolve feature-unaware perturbation</td>
<td>Todetect malware resilient to feature-unaware perturbation</td>
<td>Looplocating and random forest</td>
</tr>
<tr>
<td>SemanticModelling of Android Malware for Effective Malware Comprehension, Detection,and Classification</td>
<td>2016</td>
<td>Tomake use of semantic information</td>
<td>Todetect Android malware</td>
<td>Semanticmodel; Random forest</td>
</tr>
<tr>
<td>Detecting Android Malware Leveraging Text Semantics of Network Flows</td>
<td>2018</td>
<td>Tomake use of network information</td>
<td>Todetect Android malware</td>
<td>Usingthe text semantics of network traffic; SVM</td>
</tr>
<tr>
<td>Improving Accuracy of Android Malware Detection with Lightweight Contextual Awareness</td>
<td>2018</td>
<td>Toreduce redundant metadata in modeling</td>
<td>ImprovingAccuracy of Android Malware Detection</td>
<td>KNN;RF;MLP</td>
</tr>
<tr>
<td>MalScan: Fast Market-Wide Mobile Malware Scanning by Social-Network Centrality Analysis</td>
<td>2019</td>
<td>Toreduce the cost of semantic analysis</td>
<td>To propose a lightweight method to detect malware</td>
<td>social-network-basedcentrality analysis; kNN and random forest</td>
</tr>
<tr>
<td>PIndroid: A novel Android malware detection system using ensemble learning methods</td>
<td>2017</td>
<td>Tofight against covert technique of malware</td>
<td>Detectingeffectively</td>
<td>Permissionsand Intents based framework supplemented with Ensemble methods:Nave Bayesian,Decision Tree, Decision Table, Random Forest, Sequential Minimal Optimization and Multi Lateral Perceptron(MLP)</td>
</tr>
<tr>
<td>A pragmatic android malware detection procedure</td>
<td>2017</td>
<td>Todesign a new ML model</td>
<td>Detectingeffectively</td>
<td>Atomic Naive Bayes classifiers used as inputs for the Support Vector Machine ensemble.</td>
</tr>
<tr>
<td>ICCDetector: ICC-Based Malware Detection on Android</td>
<td>2016</td>
<td>Tocapture communication among components or cross boundaries to supplymentfeatures</td>
<td>Detectingeffectively</td>
<td>SVM</td>
</tr>
<tr>
<td>A Probabilistic Discriminative Model for Android Malware Detection with Decompiled Source Code</td>
<td>2015</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>the 2-class Naive Bayes with Prior (2-PNB) and a discriminative model,the regularized logistic regression</td>
</tr>
<tr>
<td>DroidCat: Effective Android Malware Detection and Categorization via App-Level Profiling</td>
<td>2019</td>
<td>Tofight against systemcall obfuscation</td>
<td>Detectingeffectively</td>
<td>Dynamicanalysis based on method calls and inter-component communication; RandomForest</td>
</tr>
<tr>
<td>MADAM: Effective and Efficient Behavior-based Android Malware Detection and Prevention</td>
<td>2018</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>KNN</td>
</tr>
<tr>
<td>==Android Malware Detection via (Somewhat) Robust Irreversible Feature Transformations==</td>
<td>2020</td>
<td>Toavoid ML classifier evading</td>
<td>Transferingfeatures to a new feature domain</td>
<td>Classifiers used:(1) Bernoulli Naive Bayes, (2) Random Forest, (3) NearestNeighbors, (4) Logistic Regression, (5) Gaussian Naive Bayes, (6) AdaBoost Classifier, (7) Gradient Boosting Decision Tree, (8) XGB Classifier and (9)SVM.</td>
</tr>
<tr>
<td>Leveraging ontologies and machine-learning techniques for malware analysis into Android permissions ecosystems</td>
<td>2018</td>
<td>None.</td>
<td>Detectingeffectively</td>
<td>ontology-basedframework;random forest</td>
</tr>
<tr>
<td>Lightweight, Obfuscation-Resilient Detection and Family Identification of Android Malware</td>
<td>2018</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>familyidentification;linear SVM</td>
</tr>
<tr>
<td>A multi-view context-aware approach to Android malware detection and malicious code localization</td>
<td>2018</td>
<td>To characaterize malware comprehensively</td>
<td>Detectingeffectively</td>
<td>multipleviews of apps;SVM</td>
</tr>
<tr>
<td>DroidFusion: A Novel Multilevel Classifier Fusion Approach for Android Malware Detection</td>
<td>2019</td>
<td>Toimprove classifier</td>
<td>Detectingeffectively</td>
<td>CLASSIFIER FUSION:J48, REPTree, voted perceptron, and random tree</td>
</tr>
<tr>
<td>DL-Droid: Deep learning based android malware detection using real devices</td>
<td>2020</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>input generation;MLP</td>
</tr>
<tr>
<td>JOWMDroid: Android malware detection based on feature weighting with joint optimization of weight-mapping and classifier parameters</td>
<td>2021</td>
<td>Tocharacaterize malware from feature importance</td>
<td>Detectingeffectively</td>
<td>featureweighting with the joint optimization of weight-mapping;SVM, LR, MLP</td>
</tr>
<tr>
<td>Towards using unstructured user input request for malware detection</td>
<td>2020</td>
<td>Todefeat privacy analysis evading</td>
<td>Detectingeffectively</td>
<td>decision tree</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="26-dl-based-102-109">2.6 DL based [102-109]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Motivation</th>
<th>Goal</th>
<th>Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>Toward s an interpretable deep learning model for mobile malware detection and family identification</td>
<td>2021</td>
<td>Topropose a interpretable DL model</td>
<td>Detectingreasonablely</td>
<td>DL:Grad-CAM</td>
</tr>
<tr>
<td>AMalNet: A deep learning framework based on graph convolutional networks for malware detection</td>
<td>2020</td>
<td>Tohave a lower cost</td>
<td>Detectingeffectively</td>
<td>DL:GCNsand IndRNN</td>
</tr>
<tr>
<td>Disentangled Representation Learning in Heterogeneous Information Network for Large-scale Android Malware Detection in the COVID-19 Era and Beyond</td>
<td>2021</td>
<td>Tosolve the problem that society relys on the complex cyberspace</td>
<td>Detectingeffectively</td>
<td>heterogeneousinformation network (HIN);DNN</td>
</tr>
<tr>
<td>A Multimodal Deep Learning Method for Android Malware Detection Using Various Features</td>
<td>2019</td>
<td>Tocharacaterize malware comprehensively</td>
<td>Detectingeffectively</td>
<td>multimodaldeep learning method;DNN</td>
</tr>
<tr>
<td>Android Fragmentation in Malware Detection</td>
<td>2019</td>
<td>Todeal with multiple Android version</td>
<td>Detectingeffectively</td>
<td>Deep Neural Network</td>
</tr>
<tr>
<td>An Image-inspired and CNN-based Android Malware Detection Approach</td>
<td>2019</td>
<td>Todefeat obfuscation</td>
<td>Detectingeffectively</td>
<td>CNN</td>
</tr>
<tr>
<td>A Performance-Sensitive Malware Detection System Using Deep Learning on Mobile Devices</td>
<td>2021</td>
<td>Toreduce time cost of download and upload</td>
<td>Detectingfastly</td>
<td>customized DNN</td>
</tr>
<tr>
<td>Byte-level malware classification based on markov images and deep learning</td>
<td>2020</td>
<td>Toimprove the accuracy of gray image based methods</td>
<td>Detectingeffectively</td>
<td>deep convolutional neural network</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="3-windows-malware-detection">3 Windows Malware detection</span></h2><h3><span id="31-behavior-detection-110111">3.1 Behavior detection [110,111]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>API Chaser: Anti-analysis Resistant Malware Analyzer</td>
<td>2013</td>
<td>API call feature capture</td>
</tr>
<tr>
<td>MalViz: An Interactive Visualization Tool for Tracing Malware</td>
<td>2018</td>
<td>Behavior visualization</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="32-signature-based-112">3.2 Signature based [112]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>CloudEyes: Cloud-based malware detection with reversible sketch for resource-constrained internet of things (IoT) devices</td>
<td>2017</td>
<td>Based on cloud</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="33-rule-based113">3.3 Rule based[113]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>A fast malware detection algorithm based on objective-oriented association mining</td>
<td>2013</td>
<td>API selection</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="34-similarity-based">3.4 Similarity based</span></h3><h4><span id="341-model-similarity114-122">3.4.1 Model similarity[114-122]</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>PoMMaDe: Pushdown Model-checking for Malware Detection</td>
<td>2013</td>
<td>model checking</td>
</tr>
<tr>
<td>Growing Grapes in Your Computer to Defend Against Malware</td>
<td>2014</td>
<td>clustering and template matching</td>
</tr>
<tr>
<td>Hypervisor-based malware protection with AccessMiner</td>
<td>2015</td>
<td>system-centric behavioral detector</td>
</tr>
<tr>
<td>Probabilistic Inference on Integrity for Access Behavior Based Malware Detection</td>
<td>2015</td>
<td>probabilistic model of integrity</td>
</tr>
<tr>
<td>Probabilistic analysis of dynamic malware traces</td>
<td>2018</td>
<td>1.Features of system interaction 2. interpretability</td>
</tr>
<tr>
<td>A malware detection method based on family behavior graph</td>
<td>2018</td>
<td>common behavior graph</td>
</tr>
<tr>
<td>Malware classification using self organising feature maps and machine activity data</td>
<td>2018</td>
<td>1.The improvement of ML. to reduce over-fitting 2. Self Organizing Feature Maps</td>
</tr>
<tr>
<td>Volatile memory analysis using the MinHash method for efficient and secured detection of malware in private cloud</td>
<td>2019</td>
<td>Based on memory features</td>
</tr>
<tr>
<td>A dynamic Windows malware detection and prediction method based on contextual understanding of API call sequence</td>
<td>2020</td>
<td>1.Contextual relationship between API call features 2. Marcovchain</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="342-graph-similarity123-127">3.4.2 Graph similarity[123-127]</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deriving common malware behavior through graph clustering</td>
<td>2013</td>
<td>common behavior graph</td>
</tr>
<tr>
<td>Enhancing the detection of metamorphic malware using call graphs</td>
<td>2014</td>
<td>API call graph matching</td>
</tr>
<tr>
<td>Minimal contrast frequent pattern mining for malware detection</td>
<td>2016</td>
<td>Graph matching</td>
</tr>
<tr>
<td><strong>Heterogeneous Graph Matching Networks for Unknown Malware Detection</strong></td>
<td>2019</td>
<td>Graph matching similarity of benign software</td>
</tr>
<tr>
<td>Random CapsNet for est model for imbalanced malware type classification task</td>
<td>2021</td>
<td>The improvement of the Model</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="35-ml-based-128-143">3.5 ML based [128-143]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>A Scalable Approach for Malware Detection through Bounded Feature Space Behavior Modeling</td>
<td>2013</td>
<td>Scalable feature space</td>
</tr>
<tr>
<td>SigMal: A Static Signal Processing Based Malware Triage</td>
<td>2013</td>
<td>noise-resistant similarity signatures</td>
</tr>
<tr>
<td>Unsupervised Anomaly-Based Malware Detection Using Hardware Features</td>
<td>2014</td>
<td>hardware supported lower-level features</td>
</tr>
<tr>
<td>Control flow-based opcode behavior analysis for Malware detection</td>
<td>2014</td>
<td>Based on control flow method features</td>
</tr>
<tr>
<td>Employing Program Semantics for Malware Detection</td>
<td>20152021</td>
<td>Extracting information-rich call sequence based on AEPThe improvement of the Model</td>
</tr>
<tr>
<td>AMAL: High-fidelity, behavior-based automated malware analysis and classification</td>
<td>2015</td>
<td>Based on behavior analysis</td>
</tr>
<tr>
<td>Optimized Invariant Representation of Network Traffic for Detecting Unseen Malware Variants</td>
<td>2016</td>
<td>Network features</td>
</tr>
<tr>
<td>DYNAMINER: Leveraging Offline Infection Analytics for On-the-Wire Malware Detection</td>
<td>2017</td>
<td>Network features</td>
</tr>
<tr>
<td>Security importance assessment for system objects and malware detection</td>
<td>2017</td>
<td>Based on importance of system objects</td>
</tr>
<tr>
<td>From big data to knowledge: A spatiotemporal approach to malware detection</td>
<td>2018</td>
<td>cloud based security service features</td>
</tr>
<tr>
<td>From big data to knowledge: A spatiotemporal approach to malware detection</td>
<td>2018</td>
<td>cloud based security service features</td>
</tr>
<tr>
<td>MalDAE: Detecting and explaining malware based on correlation and fusion of static and dynamic characteristics</td>
<td>2019</td>
<td>fusion of static and dynamic API sequence features</td>
</tr>
<tr>
<td>Leveraging Compression-Based Graph Mining for Behavior-Based Malware Detection</td>
<td>2019</td>
<td>Based on data flow graph</td>
</tr>
<tr>
<td>Advanced Windows Methods on Malware Detection and Classification</td>
<td>2020</td>
<td>API based Features extraction.</td>
</tr>
<tr>
<td>Sub-curve HMM: A malware detection approach based on partial analysis of API call sequences</td>
<td>2020</td>
<td>1.Subset of API call feature 2. HMM</td>
</tr>
<tr>
<td>Multiclass malware classification via first- and second-order texture statistics</td>
<td>2020</td>
<td>visualization</td>
</tr>
<tr>
<td>Catch them alive: A malware detection approach through memory forensics, manifoldlearning and computer vision</td>
<td>2021</td>
<td>Visualization</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="36-dl-based-144-156">3.6 DL based [144-156]</span></h3><div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Year</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Auto-detection of sophisticated malware using lazy-binding control flow graph and deep learning</td>
<td>2018</td>
<td>1.The improvement of CFG 2. Visualizaiton</td>
</tr>
<tr>
<td>Malware identification using visualization images and deep learning</td>
<td>2018</td>
<td>1.SimHash of features 2. Visualization</td>
</tr>
<tr>
<td>Classification of Malware by Using Structural Entropy on Convolutional Neural Networks</td>
<td>2018</td>
<td>visual similarity</td>
</tr>
<tr>
<td>Classifying Malware Represented as Control Flow Graphs using Deep Graph Convolutional Neural Network</td>
<td>2019</td>
<td>The improvement of CFG</td>
</tr>
<tr>
<td><strong>Neurlux: Dynamic Malware Analysis Without Feature Engineering</strong></td>
<td>2019</td>
<td>Based on dynamic analysis reports</td>
</tr>
<tr>
<td>A feature-hybrid malware variants detection using CNN based opcode embedding and BPNN based API embedding</td>
<td>2019</td>
<td>Hybrid features</td>
</tr>
<tr>
<td>Effective analysis of malware detection in cloud computing</td>
<td>2019</td>
<td>The improvement of the DL.</td>
</tr>
<tr>
<td>Recurrent neural network for detecting malware</td>
<td>2020</td>
<td>The improvement of RNN</td>
</tr>
<tr>
<td><strong>Dynamic Malware Analysis with Feature Engineering and Feature Learning</strong></td>
<td>2020</td>
<td>Feature hashing to encode API call info.</td>
</tr>
<tr>
<td>An improved two-hidden-layer extreme learning machine for malware hunting</td>
<td>2020</td>
<td>Improvement of the DL.</td>
</tr>
<tr>
<td>HYDRA: A multimodal deep learning framework for malware classification</td>
<td>2020</td>
<td>Hybrid features</td>
</tr>
<tr>
<td>A novel method for malware detection on ML-based visualization technique</td>
<td>2020</td>
<td>visualization</td>
</tr>
<tr>
<td>Image-Based malware classification using ensemble of CNN architectures (IMCEC)</td>
<td>2020</td>
<td>visualization</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="4-mldl-flaws-overview">4. ML/DL flaws Overview</span></h2><ul>
<li>Ensemble classifier evasion [42]</li>
<li>Performance degradation [42,46,53,54]</li>
<li>Adversarial example generation [43,44,45,48,55,56,57,58]</li>
<li>Poisoning Attack [47]</li>
<li>Feature weights [49]</li>
<li>Cost analysis [50]</li>
<li>ML bias from dataset [51]</li>
<li>Influence of packing [52]</li>
<li>Methods reproduction [59]</li>
</ul>
<h2><span id="5-references">5. References</span></h2><ol>
<li><p>2014 A Survey of Android Malware Characterisitics and Mitigation Techniques</p>
</li>
<li><p>2014 Smartphone Malware and Its Propagation Modeling:A Survey</p>
</li>
<li><p>2015 Android Security: A Survey of Issues, Malware Penetration, and Defenses</p>
</li>
<li><p>2014 Evolution and Detection of Polymorphic and Metamorphic Malwares: A Survey</p>
</li>
<li><p>2015 Kernel Malware Core Implementation: A Survey </p>
</li>
<li><p>2016 A Survey of Stealth Malware Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions</p>
</li>
<li><p>2016 On the Security of Machine Learning in Malware C&amp;C Detection: A Survey</p>
</li>
<li><p>2017 Malware Methodologies and Its Future: A Survey</p>
</li>
<li><p>2017 A Survey on Malware Detection Using Data Mining Techniques</p>
</li>
<li><p>2018 Malware Dynamic Analysis Evasion Techniques: A Survey</p>
</li>
<li><p>2018 Android Malware Detection: A Survey</p>
</li>
<li>2018 A Survey on Metamorphic Malware Detection based on Hidden Markov Model</li>
<li>2018 Machine Learning Aided Static Malware Analysis: A Survey and Tutorial</li>
<li>2018 A survey on dynamic mobile malware detection</li>
<li>2018 A survey of malware behavior description and analysis</li>
<li>2019 A Survey on Android Malware Detection Techniques Using Machine Learning Algorithms</li>
<li>2019 Dynamic Malware Analysis in the Modern Era—A State of the Art Survey</li>
<li>2019 Data-Driven Android Malware Intelligence: A Survey</li>
<li>2019 A survey of zero-day malware attacks and itsdetection methodology</li>
<li>2019 A Survey on malware analysis and mitigation techniques</li>
<li><strong>2019 Survey of machine learning techniques for malware analysis</strong></li>
<li>2020 Deep Learning and Open Set Malware Classification: A Survey</li>
<li>2020 A Comprehensive Survey on Machine Learning Techniques for Android Malware Detection</li>
<li>2015 A Survey on Mining Program-Graph Features for Malware Analysis</li>
<li>2020 Stochastic Modeling of IoT Botnet Spread: A Short Survey on Mobile Malware Spread Modeling</li>
<li>2020 A survey of IoT malware and detection methods based on static features</li>
<li>2020 A survey on practical adversarial examples for malware classifiers</li>
<li>2020 A Survey of Machine Learning Methods and Challenges for Windows Malware Classification</li>
<li>2020 A Survey on Malware Detection with Deep Learning</li>
<li>2020 An emerging threat Fileless malware: a survey and research challenges</li>
<li>2021 Malware classification and composition analysis: A survey of recent developments</li>
<li><strong>2021 Adversarial EXEmples: A Survey and Experimental Evaluation of Practical Attacks on Machine Learning for Windows Malware Detection</strong></li>
<li>2020 A Survey on Mobile Malware Detection Techniques</li>
<li>2021 Towards interpreting ML-based automated malware detection models: a survey</li>
<li>2021 A Survey of Android Malware Detection with Deep Neural Models</li>
<li>2021 A survey of malware detection in Android apps: Recommendations and perspectives for future research</li>
<li>2021 A survey of android application and malware hardening</li>
<li>2021 A survey on machine learning-based malware detection in executable files</li>
<li>2021 The evolution of IoT Malwares, from 2008 to 2019: Survey, taxonomy, process simulator and perspectives</li>
<li>2021 A Survey of Android Malware Static Detection Technology Based on Machine Learning</li>
<li>2016 Empirical assessment of machine learning-based malware detectors for Android Measuring the gap between in-the-lab and in-the-wild validation scenarios </li>
<li>2016 When a Tree Falls: Using Diversity in Ensemble Classifiers to Identify Evasion in Malware Detectors</li>
<li><strong>2016 Automatically Evading Classifiers A Case Study on PDF Malware Classifiers</strong></li>
<li>2017 SecureDroid: Enhancing Security of Machine Learning-based Detection against Adversarial Android Malware Attacks</li>
<li>2017 How to defend against adversarial attack</li>
<li><strong>2017 Transcend: Detecting Concept Drift in Malware Classification Models</strong></li>
<li><strong>2018 Automated poisoning attacks and defenses in malware detection systems: An adversarial machine learning approach</strong></li>
<li><strong>2018 Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers</strong></li>
<li><strong>==2019 Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection==</strong></li>
<li>2019 A cost analysis of machine learning using dynamic runtime opcodes for malware detection</li>
<li><strong>2019 TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time</strong></li>
<li>2020 When Malware is Packin’ Heat; Limits of Machine Learning Classifiers Based on Static Analysis Features</li>
<li>2020 Assessing and Improving Malware Detection Sustainability through App Evolution Studies</li>
<li>2020 On Training Robust PDF Malware Classifiers</li>
<li>2020 Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection</li>
<li>2020 Intriguing Properties of Adversarial ML Attacks in the Problem Space Fabio</li>
<li>2020 Query-Efficient Black-Box Attack Against Sequence-Based Malware Classifiers</li>
<li>2020 Enhancing State-of-the-art Classifiers with API Semantics to Detect Evolved Android Malware</li>
<li>2021 Lessons Learnt on Reproducibility in Machine Learning Based Android Malware Detection</li>
<li>2016 Semantics-Based Online Malware Detection: Towards Efficient Real-Time Protection Against Malware</li>
<li>2018 Understanding Linux Malware</li>
<li>2017 Droid-AntiRM: Taming Control Flow Anti-analysis to Support Automated Dynamic Analysis of Android Malware</li>
<li>2017 Malton: Towards On-Device Non-Invasive Mobile Malware Analysis for ART</li>
<li>2015 CopperDroid: Automatic Reconstruction of Android Malware Behaviors</li>
<li>2018 EnMobile: Entity-based Characterization and Analysis of Mobile</li>
<li>2015 Screening smartphone applications using malware family signatures</li>
<li>2018 Toward a more dependable hybrid analysis of android malware using aspect-oriented programming</li>
<li>2017 DroidNative: Automating and optimizing detection of Android native code malware variants</li>
<li>2016 An HMM and structural entropy based detector for Android malware: An empirical study</li>
<li>2020 Scalable and robust unsupervised android malware fingerprinting using community-based network partitioning </li>
<li>2020 On the use of artificial malicious patterns for android malware detection </li>
<li>2016 Andro-Dumpsys: Anti-malware system based on the similarity of malware creator and malware centric information</li>
<li>2020 Bayesian Active Malware Analysis </li>
<li>2020 PermPair: Android Malware Detection Using Permission Pairs</li>
<li>2014 Apposcopy: Semantics-Based Detection of Android Malware through Static Analysis</li>
<li>2015 Profiling user-trigger dependence for Android malware detection</li>
<li>2019 Identifying Android Malware Using Network-Based Approaches</li>
<li>2016 Cypider: Building Community-Based Cyber-Defense Infrastructure for Android Malware Detection</li>
<li>2014 Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs </li>
<li>2017 MAMADROID: Detecting Android Malware by Building Markov Chains of Behavioral Models</li>
<li>2014 Drebin: Effective and Explainable Detection of Android Malware in Your Pocket</li>
<li>2018 Make Evasion Harder: An Intelligent Android Malware Detection System</li>
<li>2018 Using Loops For Malware Classification Resilient to Feature-unaware Perturbations</li>
<li>2016 Semantic Modelling of Android Malware for Effective Malware Comprehension, Detection, and Classification</li>
<li>2018 Detecting Android Malware Leveraging Text Semantics of Network Flows</li>
<li>2018 Improving Accuracy of Android Malware Detection with Lightweight Contextual Awareness</li>
<li>2019 MalScan: Fast Market-Wide Mobile Malware Scanning by Social-Network Centrality Analysis</li>
<li>2017    PIndroid: A novel Android malware detection system using ensemble learning methods</li>
<li>2017    A pragmatic android malware detection procedure</li>
<li>2016    ICCDetector: ICC-Based Malware Detection on Android</li>
<li>2015    A Probabilistic Discriminative Model for Android Malware Detection with Decompiled Source Code</li>
<li>2019    DroidCat: Effective Android Malware Detection and Categorization via App-Level Profiling</li>
<li>2018    MADAM: Effective and Efficient Behavior-based Android Malware Detection and Prevention</li>
<li>2020    Android Malware Detection via (Somewhat) Robust Irreversible Feature Transformations</li>
<li>2018    Leveraging ontologies and machine-learning techniques for malware analysis into Android permissions ecosystems </li>
<li>2018    Lightweight, Obfuscation-Resilient Detection and Family Identification of Android Malware</li>
<li>2018    A multi-view context-aware approach to Android malware detection and malicious code localization</li>
<li>2019    DroidFusion: A Novel Multilevel Classifier Fusion Approach for Android Malware Detection</li>
<li>2020    DL-Droid: Deep learning based android malware detection using real devices </li>
<li>2021    JOWMDroid: Android malware detection based on feature weighting with joint optimization of weight-mapping and classifier parameters</li>
<li>2020    Towards using unstructured user input request for malware detection</li>
<li>2021 Toward s an interpretable deep learning model for mobile malware detection and family identification</li>
<li>2020 AMalNet: A deep learning framework based on graph convolutional networks for malware detection </li>
<li>2021 Disentangled Representation Learning in Heterogeneous Information Network for Large-scale Android Malware Detection in the COVID-19 Era and Beyond</li>
<li>2019 A Multimodal Deep Learning Method for Android Malware Detection Using Various Features</li>
<li>2019 Android Fragmentation in Malware Detection</li>
<li>2019 An Image-inspired and CNN-based Android Malware Detection Approach</li>
<li>2021 A Performance-Sensitive Malware Detection System Using Deep Learning on Mobile Devices</li>
<li>2020 Byte-level malware classification based on markov images and deep learning</li>
<li>2013 API Chaser: Anti-analysis Resistant Malware Analyzer</li>
<li>2018 MalViz: An Interactive Visualization Tool for Tracing Malware</li>
<li>2017 CloudEyes: Cloud-based malware detection with reversible sketch for resource-constrained internet of things (IoT) devices</li>
<li>2013 A fast malware detection algorithm based on objective-oriented association mining</li>
<li>2013 PoMMaDe: Pushdown Model-checking for Malware Detection</li>
<li>2014 Growing Grapes in Your Computer to Defend Against Malware</li>
<li>2015 Hypervisor-based malware protection with AccessMiner</li>
<li>2015 Probabilistic Inference on Integrity for Access Behavior Based Malware Detection</li>
<li>2018 Probabilistic analysis of dynamic malware traces</li>
<li>2018 A malware detection method based on family behavior graph</li>
<li>2018 Malware classification using self organising feature maps and machine activity data</li>
<li>2019 Volatile memory analysis using the MinHash method for efficient and secured detection of malware in private cloud</li>
<li>2020 A dynamic Windows malware detection and prediction method based on contextual understanding of API call sequence</li>
<li>2013 Deriving common malware behavior through graph clustering</li>
<li>2014 Enhancing the detection of metamorphic malware using call graphs</li>
<li>2016 Minimal contrast frequent pattern mining for malware detection</li>
<li>2019 Heterogeneous Graph Matching Networks for Unknown Malware Detection</li>
<li>2021 Random CapsNet for est model for imbalanced malware type classification task</li>
<li>2013 A Scalable Approach for Malware Detection through Bounded Feature Space Behavior Modeling</li>
<li>2013 SigMal: A Static Signal Processing Based Malware Triage</li>
<li>2014 Unsupervised Anomaly-Based Malware Detection Using Hardware Features</li>
<li>2014 Control flow-based opcode behavior analysis for Malware detection</li>
<li>2015 Employing Program Semantics for Malware Detection</li>
<li>2015 AMAL: High-fidelity, behavior-based automated malware analysis and classification</li>
<li>2016 Optimized Invariant Representation of Network Traffic for Detecting Unseen Malware Variants</li>
<li>2017 DYNAMINER: Leveraging Offline Infection Analytics for On-the-Wire Malware Detection</li>
<li>2017 Security importance assessment for system objects and malware detection</li>
<li>2018 From big data to knowledge: A spatiotemporal approach to malware detection</li>
<li>2019 MalDAE: Detecting and explaining malware based on correlation and fusion of static and dynamic characteristics</li>
<li>2019    Leveraging Compression-Based Graph Mining for Behavior-Based Malware Detection</li>
<li>2020    Advanced Windows Methods on Malware Detection and Classification</li>
<li>2020    Sub-curve HMM: A malware detection approach based on partial analysis of API call sequences </li>
<li>2020 Multiclass malware classification via first- and second-order texture statistics </li>
<li>2021 Catch them alive: A malware detection approach through memory forensics, manifold learning and computer vision</li>
<li>2018 Auto-detection of sophisticated malware using lazy-binding control flow graph and deep learning</li>
<li>2018 Malware identification using visualization images and deep learning</li>
<li>2018 Classification of Malware by Using Structural Entropy on Convolutional Neural Networks</li>
<li>2019 Classifying Malware Represented as Control Flow Graphs using Deep Graph Convolutional Neural Network</li>
<li>2019 Neurlux: Dynamic Malware Analysis Without Feature Engineering</li>
<li>2019 A feature-hybrid malware variants detection using CNN based opcode embedding and BPNN based API embedding </li>
<li>2019 Effective analysis of malware detection in cloud computing </li>
<li>2020 Recurrent neural network for detecting malware</li>
<li>2020 Dynamic Malware Analysis with Feature Engineering and Feature Learning</li>
<li>2020 An improved two-hidden-layer extreme learning machine for malware hunting</li>
<li>2020 HYDRA: A multimodal deep learning framework for malware classification</li>
<li>2020 A novel method for malware detection on ML-based visualization technique</li>
<li>2020 Image-Based malware classification using ensemble of CNN architectures (IMCEC)</li>
</ol>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（4）【draft】You are What you Do</title>
    <url>/posts/7X1P6X/</url>
    <content><![CDATA[<h2><span id="you-are-what-you-do-hunting-stealthy-malware-via-data-provenance-analysis">You Are What You Do: Hunting Stealthy Malware via Data Provenance Analysis</span></h2><p><a href="https://blog.csdn.net/ll14856lk/article/details/122151992">https://blog.csdn.net/ll14856lk/article/details/122151992</a></p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（1）SeqNet: An Efficient Neural Network for Automatic Malware Detection</title>
    <url>/posts/JFCMNX/</url>
    <content><![CDATA[<h2><span id="seqnet-an-efficient-neural-network-for-automatic-malware-detection">SeqNet: An Efficient Neural Network for Automatic Malware Detection</span></h2><ul>
<li><a href="https://github.com/Darren-8/SeqNet.git">https://github.com/Darren-8/SeqNet.git</a>.</li>
</ul>
<h3><span id="摘要">摘要</span></h3><p>恶意软件继续快速发展，每天捕获超过45万个新样本，这使得手动恶意软件分析变得不切实际。然而，现有的深度学习检测模型需要人工特征工程，或者需要很高的计算开销来进行长时间的训练，这可能会很难选择特征空间，并且很难再训练来缓解模型老化。因此，对探测器的一个关键要求是实现自动、高效的检测。在本文中，我们<strong>提出了一种轻量级的恶意软件检测模型SeqNet</strong>，该模型可以在原始二进制文件上以低内存进行高速训练。通过避免上下文混淆和减少语义丢失，SeqNet在将参数数量减少到仅136K时保持了检测精度。在我们的实验中，我们证明了我们的方法的有效性和SeqNet的低训练成本要求。此外，我们还公开了我们的数据集和代码，以促进进一步的学术研究。</p>
<h3><span id="一-说明">一、说明</span></h3><p>恶意软件是一种严重的网络安全威胁，可能会对个人和公司系统造成严重损害，例如，急剧减速或崩溃、严重数据丢失或泄漏，以及灾难性的硬件故障。<strong>AVTest报告称，平均每天检测到超过45万个新的恶意程序和可能不需要的应用程序</strong>。[1]. 大量新的恶意软件变体使得手动恶意软件分析效率低下且耗时。为了更有效地检测恶意软件，许多研究人员提出了用于恶意软件分析和检测的高级工具[2,5,9,14]。这些工具通过对分析员的行为进行部分工作，帮助他们更高效地完成任务。然而，在处理如此大量的恶意软件时，这些解决方案无法从根本上减少工作量。为了解决这个问题，许多专家和学者将机器学习算法，尤其是深度学习应用于恶意软件检测和分类[7,12,15,16,18,22,26,28,34,37,45,50,51,53,55,58-60,62,63]。他们的努力为恶意软件分析神经网络的研究和实用的恶意软件自动检测做出了很大贡献。</p>
<p>然而，这些模型通常需要各种特征工程来帮助神经网络做出判断，这可能很费劲，并且容易丢失一些关键信息。为了实现更加友好和自动的检测，提出了基于二进制的方法[31,37,41,42]。两种流行的原始二进制处理方法是文件剪切和二进制图像转换。然而，这两种方法可能会遇到<strong>语境混乱和语义丢失</strong>，这将在后面讨论。</p>
<p><strong>此外，模型老化是神经网络的一个关键问题</strong>[25,40]。与计算机视觉和自然语言处理不同，恶意软件正在不断快速发展。恶意软件检测是攻击者和检测器之间的斗争。随着恶意软件的不断发展，深度学习模型可能已经过时。例如，五年前训练的模型在今天的恶意软件检测中可能非常薄弱。神经网络很难识别看不见的恶意行为，这可能会导致较低的检测精度和更容易的规避。模型不可能预测未来恶意软件的特征，但快速学习检测新恶意软件的知识是可行的。因此，再培训模式成为缓解老龄化问题的少数方法之一。我们可以让神经网络快速重新训练，学习新恶意软件的新特征，以便识别新的攻击方法。</p>
<p><strong>由于模型的结构和规模，再培训可能会耗费时间和计算量，而且如此高的成本可能会导致模型更新困难</strong>。此外，恶意软件检测是几乎所有电子系统中的常见操作，对于计算能力较低的设备来说，进行检测是必要的。例如，笔记本电脑或其他移动设备很难运行一个庞大的模型来扫描所有文件以检测恶意软件。这些要求表明，检测模型应该足够小和有效，使其更实用，以便我们能够快速重新培训或执行检测。此外，无需复杂特征工程的自动检测对于各种场景中使用的模型至关重要。</p>
<p>总的来说，我们的工作面临两个挑战：<strong>自动化和高效</strong>。检测模型应该足够自动化，并且几乎不需要人工特征工程。该模型的规模应足够小，以降低训练和检测成本。</p>
<p>在本文中，我们提出了一个有效的恶意软件自动检测模型，该模型只有大约136K个参数，我们称之为SeqNet。在没有人工特征选择的情况下，SeqNet可以仅基于原始二进制文件自动分析样本并找出恶意程序和良性程序之间的差异。较小的神经网络通常具有较少的参数，这可能会导致较低的学习能力。这可能是因为较小的模型往往更难适应从原始二进制文件到恶意域的复杂映射。这个问题可能会导致小型深度学习模型中恶意软件检测的准确性较低。</p>
<p>为了在减少参数数量时保持准确性，我们<strong>提出了一种新的二进制代码表示方法</strong>，以减少语义损失并避免上下文混淆。根据我们的方法，我们使SeqNet在无需特征工程的情况下，在可移植可执行（PE）恶意软件检测方面表现良好。基于这种表示方法，我们创建了一种新的卷积方法，称为<strong>序列深度可分离卷积（SDSC）</strong>，以进一步压缩检测模型的规模。我们在一个大型PE数据集上对SeqNet进行训练，发现与许多现有的基于二进制的方法和模型相比，它具有很好的性能。我们还在进一步的实验中证明了我们的模型收缩方法的有效性。此外，我们还公开了我们的代码和数据集以供进一步研究，我们希望深度学习算法能够更好地应用于恶意软件检测。</p>
<p>我们在一个大型PE数据集上对SeqNet进行训练，发现与许多现有的基于二进制的方法和模型相比，它具有很好的性能。我们还在进一步的实验中证明了我们的模型收缩方法的有效性。此外，我们还公开了我们的代码和数据集以供进一步研究，我们希望深度学习算法能够更好地应用于恶意软件检测。</p>
<h4><span id="本文的主要贡献包括">本文的主要贡献包括：</span></h4><ul>
<li>我们提出了一种<strong>新的方法来表示二进制代码</strong>，同时减少语义损失，避免上下文混淆。</li>
<li>基于上述新的表示方法，我们提出了一种<strong>新的卷积压缩恶意软件检测模型的方法SDSC</strong>。</li>
<li>我们<strong>设计了一个深度卷积神经网络（CNN），称为SeqNet</strong>，它具有更小的规模和更短的训练过程。</li>
<li>我们将数据集和代码公开以供进一步研究。</li>
</ul>
<p>这是本文的布局。第2节介绍了深层恶意软件检测的主要方法和几个问题。第3节描述了我们应用于SeqNet的主要方法。第四部分阐述了我们的实验和相应的结果。</p>
<h3><span id="二-背景">二、背景</span></h3><p>在本节中，我们将介绍使用深度学习进行恶意软件检测的背景。首先，我们列举了这方面的两种主要方法。然后进一步讨论了目前流行的二进制表示方法的几个问题。最后，我们解释了其中一种方法所基于的深度可分离卷积。</p>
<h4><span id="21-深度恶意软件检测">2.1 深度恶意软件检测</span></h4><p>神经网络具有强大的学习能力，在计算机视觉和自然语言处理中得到了广泛的应用。深度学习算法已经被许多研究人员应用于恶意软件检测。据我们所知，我们认为有两种主流思想，类似于[47]。</p>
<p><strong>基于特征的方法</strong>。在早期作品中，深度学习模型是从精心制作的恶意软件功能中训练出来的[7、15、16、18、26、28、34、58、63]。在检查可疑样本时，模型需要提取特定特征，以特定方式处理它们，然后检测恶意代码以给出结果。所选功能可以是API调用、控制流图（CFG）或任何其他能够反映程序操作的信息。的确，人工特征学习是神经网络识别恶意样本和良性样本之间主要差异的有效方法。然而，特定的领域特征只能从一个角度很好地描述样本的关键信息。它不能完全覆盖二进制代码的语义，甚至会引发严重的信息丢失。例如，仅使用API调用作为特性会导致模型忽略控制流。此外，精心设计的功能需要足够的先验知识，这需要专家仔细选择。因此，耗时的手动特征提取可能会限制基于特征的模型的使用，并使其难以对抗恶意软件的持续演化。</p>
<p><strong>基于二进制的方法</strong>。与传统的特征工程相比，自动特征提取是神经网络的发展趋势之一，人工干预更少，性能更好。我们读取文件的二进制文件，直接将其发送到检测模型，无需或几乎不需要预处理。该模型将自动找到可疑部分，并将二进制文件识别为恶意或良性。这种方法可以更有效地避免人们分析恶意软件的需要，并更好地减少分析人员的工作量。<strong>此外，通过减少手动特征工程造成的损失，直接从原始二进制文件学习可能会更好地保留语义和上下文信息。</strong></p>
<p>为了使我们的模型更加自动化，避免信息丢失，<strong>我们将重点放在基于二进制的模型上</strong>，SeqNet将原始二进制文件用作输入。此外，较低的计算开销可以使模型更好地适应恶意软件的演变，并扩展应用场景，例如在物联网环境中。我们将一种新颖但简单的二进制代码表示方法应用于SeqNet，并在减少参数时保持其性能。</p>
<h4><span id="22-二进制代码表示法">2.2 二进制代码表示法</span></h4><p>在这一部分中，我们将介绍几种主要的二进制代码表示方法，它们适用于基于二进制的模型。将样本转换为神经网络的输入会显著影响模型的性能。因此，<strong>正确的二进制代码表示方法是基于二进制的恶意软件检测神经网络的重要组成部分</strong>。目前，人们提出了两种主要的方法来完全表示原始二进制码。</p>
<p><strong>文件剪切</strong>。<strong>由于内存限制的限制，许多作品对最大文件大小设置了人为限制，这种方法是从二进制程序的开头提取一段固定长度的代码片段</strong>。如果二进制程序的长度小于所需代码段的长度，则该代码段的末尾将填充零。文件剪切会导致语义信息丢失，因为如果二进制文件的结尾比固定长度长得多，就会忽略它。然而，恶意代码通常位于二进制文件的末尾。例如，嵌入的病毒通常嵌入在受感染文件的尾部，这可能有助于它们逃避基于这种方法的模型的检测。<strong>为了缓解这个问题，Mal-ConvGCT[42]通过扩展代码段大小限制来提高MalConv[41]的性能。</strong></p>
<p><strong>二值图像转换</strong>。第二种方法将所有二进制代码转换为图像，并利用图像分类解决方案执行恶意软件检测。所有<strong>图像都可以使用双线性插值算法重新采样到相同的大小</strong>。然而，图像与序列不同，这可能会导致几个问题。我们认为这种方法会导致上下文信息混乱，下面列举了三个例子。</p>
<ul>
<li><strong>边缘丢失</strong>：如果二进制指令位于图像边缘，换行符可能会将指令分成两部分，如图1（a）所示。此问题可能会导致模型难以识别多条长指令。此外，由于<strong>强相关指令的中断，上下文信息可能在边缘被破坏</strong>。</li>
<li><strong>重采样噪声</strong>：如果我们重塑图像大小，不同行中不相关的指令会导致上下文信息混淆，如图1（b）所示。这个问题很容易使原始序列中的指令彼此相距很远，而被迫集成到相应的图像中，这可能会混淆神经网络。</li>
<li><strong>填充问题</strong>：填充操作可能会使模型难以识别原始序列的开始和结束，如图1（c）所示。为了确保卷积层输入和输出的一致性，我们通常在输入的边缘填充一些零，神经网络可能会根据填充得到空间信息[27]。与图像处理不同，识别出的空间信息可能会误导模型。</li>
</ul>
<p><strong>文件剪切导致的语义丢失</strong>和<strong>二值图像转换导致的上下文混乱</strong>阻碍了恶意软件检测模型的性能。这些问题可能会混淆神经网络，甚至误导它们做出截然相反的决定。通过缓解这些问题，我们可以帮助我们的模型在减少参数时保持其性能。</p>
<h4><span id="23-卷积法">2.3 卷积法</span></h4><p>传统的卷积算法模拟动物视觉，在计算机视觉中具有很好的性能。这个简单的操作可以有效地提取图像中的视觉特征。低级卷积层检测图像中的纹理和简单特征，高级卷积层可以识别内容和整体语义[61]。这就是为什么计算机可以通过多个卷积层的叠加来识别复杂的物体。</p>
<p>然而，传统卷积所需的参数数量往往使得深度学习模型太大，无法应用于计算能力较低的设备。此外，参数过多的模型可能需要很长的训练过程。例如，VGG有超过1.3亿个参数，已经接受了2年的培训  3周[49]。如此庞大的模型不适合在普通设备上运行。</p>
<p>因此，我们将CNN架构应用于SeqNet，并将DSC的输入形式从2D调整为1D，从而更好地减少了训练参数的数量。因此，训练时间成本和新生成的模型的大小都进一步减小。我们的方法的细节在第3节中描述。</p>
<h3><span id="三-方法">三、 方法</span></h3><p>在本节中，我们将介绍SeqNet的详细信息。首先，我们概述了我们的方法。其次，我们引入了可以减少语义损失和避免语境混淆的序列表征。第三部分描述了SDSC如何压缩模型的规模。最后，我们详细介绍了SeqNet的体系结构。</p>
<h4><span id="31概述">3.1概述</span></h4><p>SeqNet的目标是以较低的培训成本实现高效、自动的恶意软件检测。在整个培训和检测过程中，操作员不需要专业的恶意软件分析知识来执行手动领域特定的功能工程。实际上，我们直接将原始二进制文件输入SeqNet，SeqNet将自动分析序列并提取特征。SeqNet的输出是可疑样本的恶意可能性，输入样本是否为恶意软件取决于模型给出的可能性。</p>
<p>准确检测是恶意软件检测模型的基本要求。我们认为恶意软件检测不同于图像分类。恶意软件检测可能需要更多地关注几个<strong>关键的恶意代码</strong>，而图像分类可能更关注整体。根据这一理论，我们对SeqNet的主要设计要点之一是减少上下文混淆和语义丢失。我们使用<strong>原始二进制序列作为SeqNet的输入</strong>，这样可以避免上下文混淆，减少语义损失。</p>
<p>轻量级模型通常具有更广泛的应用场景和更快的检测性能。显然，小型模型的培训成本也很低。因此，压缩SeqNet的规模是必要的。新的卷积方法，称为序列深度可分离卷积（SDSC），有助于SeqNet满足这一要求。</p>
<h4><span id="32-序列表征">3.2 序列表征</span></h4><p>输入格式对神经网络性能和模型大小至关重要。较大的输入往往导致较大的模型，适当的输入格式可以有效地提高神经网络的学习效果。SeqNet的输入是原始二进制序列，这些序列通过线性插值算法调整到相同的长度。原始二进制序列输入几乎不需要人工干预。在不转换为图像的情况下，很明显，我们可以避免上下文信息混淆，减少语义损失，如图所示</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182201637.png" alt="image-20220510151305157"></p>
<blockquote>
<p>  一个例子表明，序列表征可以解决上下文混淆和语义丢失的问题。在图像中，二进制指令“BF 01 00”在边缘被切断，但它仍保留序列中的形状。插值后，我们可以看到，图像强制加强了“56”与“00”之间的关系，其中“56”表示“推esi”，而“00”在“0F 85 93 00 00 00 00 00”中表示“jnz loc 1000DF90”，但它忽略了具有更强关系的指令，例如“推edi”和“mov edi，1”。在序列中，指令“56”与前端结合，而不是“00”，并且在物理上与“57”保持接近，后者表示“推edi”。在图像中，输入到卷积层之前添加的填充提供了不正确的位置信息，但在序列中，它标记了开始和结束。</p>
</blockquote>
<ul>
<li><strong>避免边缘丢失</strong>。由于序列只有两条边，即开始和结束，因此可以避免边缘丢失。序列格式符合代码的空间结构，因此任何指令都没有中断，这使得所有指令在输入模型时都完好无损。中断的消失也有效地保护了原始二进制序列的语义，因为相邻的指令是不分离的。</li>
<li><strong>重采样降噪</strong>。当我们调整序列的大小时，元素只会受到前向和后向上下文的影响，因此重采样噪声可以减少。此外，图像中两条无关指令之间的强制关系消失。通过使用序列特征，远程指令不能相互影响，这使得模型能够更清楚地识别指令之间的关系。此外，通过使用线性插值算法，我们可以确保输入序列的长度是相同的。</li>
<li><strong>避免问题</strong>。填充问题可以解决，因为在卷积之前，我们只需要在序列的两端填充。此外，与向图像中添加不正确的信息相比，序列中的填充将有效地标记相应程序的开始和结束。因此，该模型可以根据填充来识别指令的正确位置。</li>
<li><strong>语义损失减少</strong>。由于我们输入的是整个二进制文件，而不仅仅是一个片段，因此语义损失可能会减少。通过线性插值算法，我们可以压缩语义，而不是忽略它。在这种情况下，嵌入式病毒也可能包括在内，因为程序中的所有指令都被输入到模型中。</li>
</ul>
<p>在缩放到相同的长度之前，我们首先规范化整个序列，使元素的值介于-1和1之间，并以浮点格式存储。由于实数字段的连续性，浮点格式可以比整数格式代表更多的信息。因此，这个操作对于减少线性插值算法造成的语义损失是必要的，如图4所示。通过对数据集进行统计，<strong>我们发现大多数文件的大小约为256KB。因此，我们将所有输入序列扩展到2^18字节。</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191547964.png" alt="image-20220510152040055" style="zoom:50%;"></p>
<blockquote>
<p>  图4：如果我们在规范化之前调整序列的大小，结果不能代表原始二进制代码（a）中的所有信息。<strong>相反，如果我们在规范化后调整以浮点格式存储的序列的大小，我们可以有效地减少语义损失</strong>（b）。</p>
</blockquote>
<p>在序列格式中，指令之间的所有信息都将得到正确和更好的保留。两条指令之间的物理距离反映了关系的真实强度。这种表示方法还利用了代码中的空间局部性，因为模型将更多地关注附近的指令，而不是那些相关性较弱的指令。因此，该模型在学习和检测时接收到的干扰较小。</p>
<p><strong>使用序列特征而不是二进制图像转换的另一个原因是，可执行文件的前后相关性比平面相关性更明显。这就是为什么序列可以更好地代表程序。</strong></p>
<h4><span id="33-序列深度可分离卷积">3.3 序列深度可分离卷积</span></h4><p><strong>序列输入格式不仅解决了语义丢失和上下文混淆的问题，还压缩了SeqNet的规模</strong>。SeqNet的卷积核只需要在只有一维的序列上提取特征。与处理一维输入相比，提取二维特征需要更大的卷积核和更多的计算。例如，如图2（c）所示，a 3 3用于图像的内核至少需要10个参数（包括偏差），而3 1序列中使用的内核只需要至少四个参数（包括偏差）。</p>
<p><strong>基于序列输入和深度可分离卷积</strong>[21]，我们提出了一种称为序列深度可分离卷积（SDSC）的方法，它需要更少的参数和更少的计算。在SDSC中，我们使用3 1个内核替换DSC中的2D深度卷积内核。通过使用SDSC层，SeqNet的尺寸比现有模型小得多。<strong>在下文中，我们分析了与DSC相比计算量的减少</strong>。</p>
<p>此外，SDSC的输入是一维数据，因此与DSC相比，它不太容易受到无关指令的影响。在实验中，我们发现SDSC具有良好的性能，并成功地保持了SeqNet的性能。基于SDSC，我们在SeqNet中使用了以下两种主要的卷积块架构。</p>
<ul>
<li><strong>标准SDSC块</strong>。如图5（a）所示，标准SDSC块由三部分组成。我们使用批量归一化层[23]来帮助模型更好地了解训练样本的概率分布。ReLU[38]激活函数可以通过使模型快速收敛来加速训练过程</li>
<li><strong>残差SDSC块</strong>。如图5（b）所示，剩余SDSC块结合了ResNet[19]中使用的方法。通过跳过SDSC块，我们可以有效地防止梯度消失，并构建更深层的架构。</li>
</ul>
<h4><span id="34-模型架构">3.4 模型架构</span></h4><p>SeqNet的构建主要基于SDSC，图6解释了该架构。为了减少参数的数量，我们使用<strong>更小的核和更深的结构</strong>，这也可以扩大感受野。标准SDSC块用于在对序列进行下采样时提取特征。对于第一个卷积层，我们使用单个公共卷积层嵌入原始输入，内核大小为3<em>1.我们<strong>设置大小是因为经常使用的CPU指令的长度通常是三个字节</strong>。对于高层特征，我们使用五个剩余的SDSC块进行分析，与完全连接的层相比，这也可以更好地保留上下文和空间信息。此外，<em>*残差SDSC块可以防止梯度消失，使模型快速收敛</em></em>。最后两层是完全连接层和softmax层。全连通层用于对模型前端给出的分析结果进行分类。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191547603.png" alt="image-20220510152817964" style="zoom:50%;"></p>
<p>在这个公式中，P表示转换后的结果，x表示softmax层的输入。对于池层，我们使用平均池。<strong>在我们的实验中，我们发现当剩余SDSC块的数量为5个，输入128个通道，并且完全连接的层的数量仅为1个时，该模型的性能最好</strong>。SeqNet输出样本的恶意可能性，如果可能性超过50%，模型将视其为恶意软件。根据输出，对于损失函数，我们使用交叉熵函数。SeqNet总共只有大约136K个参数，几乎是MalConv的十分之一，我们将在第4节中讨论。</p>
<h3><span id="四-实验">四、实验</span></h3><h4><span id="41培训数据集">4.1培训数据集</span></h4><p>建立良好的培训数据集对于评估SeqNet的性能至关重要。正确的标签和足够的样本是展示SeqNet学习能力的必要条件。对于样本类型，我们认为PE恶意软件是电子系统的主要威胁之一。此外，有很多PE恶意样本，很容易获得足够的PE样本。</p>
<p>因此，以下实验适用于一组PE文件，因为它们很普遍。在这项工作中，所有恶意样本都来自VirusShare[4]。<strong>齐安信公司提供了约10000份良性样本。我们还从真实的个人电脑中收集了许多良性样本，以模拟我们日常生活中的真实环境。为了确保良性样本中没有混合病毒，我们使用VirusTotal[6]检测所有文件。如果在VirusTotal报告中没有AV引擎将其视为恶意软件，我们将其视为良性样本。操作系统文件和恶意软件通常具有类似的行为，这可能会混淆检测模型，甚至会混淆训练有素的分析师[8]</strong>。因此，为了帮助SeqNet观察恶意程序和良性程序之间的一般差异，并使SeqNet更加健壮，<strong>我们添加了大约10000个系统文件作为良性数据。</strong>VirusTotal[6]也会检查系统文件，以确保它们是良性的。我们总共获得了72329个二进制文件的训练数据集，其中37501个恶意文件和34828个良性文件，以及24110个二进制文件的验证数据集，其中12501个恶意文件和11609个良性文件。</p>
<p>数据集中的所有文件都是PE文件，我们<strong>通过比较SHA256值来消除重复</strong>。我们将恶意和良性样本的比例设置为1左右，以确保结果可靠。例如，如果数据集只有恶意样本，模型可能会通过识别“4D 5A”来检测所有恶意软件。相反，如果我们向数据集中添加足够多的良性样本，模型就可以了解恶意样本和良性样本之间的真正区别。</p>
<h4><span id="42-测量">4.2 测量</span></h4><p>在我们的实验中，我们从训练成本和准确性两个方面测量SeqNet。对于培训成本测量，我们使用参数的数量来表示模型的规模。更大的模型包含更多的神经元，需要更多的参数来构建。在训练和预测过程中，每个参数都会占用恒定的内存。因此，参数的数量显著影响模型训练和预测所需的记忆。为了准确测量模型推理的计算开销，我们通过输入一个随机二进制来计算每个模型上的浮点运算（Flops）。我们还通过记录一个历元所需的时间来测量模型的速度，包括训练和验证过程。</p>
<h4><span id="43-培训">4.3 培训</span></h4><p>设置模型和培训设置会显著影响培训过程和结果。所有模型都经过70个阶段的训练，我们选择了过去30个阶段的验证结果，以获得平均精度和其他指标。<strong>我们将批量大小设置为32，并选择Adam[29]作为所有模型的优化器。为了保证训练的公平性，我们将交叉熵损失应用于所有模型。</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191547084.png" alt="image-20220510153236041" style="zoom:50%;"></p>
<h4><span id="44-模型评估">4.4 模型评估</span></h4><p>我们选择几种最先进的基于二进制的方法作为基线。<strong>为了反映基于图像转换的模型的总体性能，我们选择著名的MobileNet[21]作为代表性模型</strong>。我们将程序转换为RGB图像作为MobileNet的输入。在转换过程中，一个字节映射一个通道中的一个像素。<strong>对于基于文件剪切的模型，我们选择MalConv[41]和MalConvGCT[42]</strong>。<strong>ResNet和MobileNet都是由Pytorh[39]实现的，我们使用ResNet18作为ResNet，而MobileNet v2作为MobileNet</strong>。我们使用MalConv和MalConvGCT作者提供的源代码，并将它们应用到我们的实验中。我们在每个模型的末尾添加一个softmax层，以便在验证时将结果转换为可能性格式。根据表1中的参数数量，我们可以发现SeqNet的最小参数仅为MalConv和Mal-ConvGCT的十分之一。此外，SeqNet在恶意软件检测方面保持了良好的性能。图7是精度和模型尺寸对比图，左上角的模型位置表明模型尺寸较小，精度较高。</p>
<p>精度还表明，SeqNet误解良性样本的可能性很低。此次召回意味着SeqNet可能有能力防止逃税。在训练过程中，我们发现大多数模型在第一个历元后达到90%的准确率。在我们的训练中，我们发现SeqNet只需要大约两分钟半就可以完成一个历元，而Mal-Conv大约需要一个小时。因此，我们可以看到，SeqNet的微小尺寸导致了较低的计算开销，基于卷积的体系结构加速了训练和推理。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191547005.png" alt="image-20220510153439695" style="zoom:50%;"></p>
<h4><span id="45-进一步评估和消融研究">4.5 进一步评估和消融研究</span></h4><p>接下来，我们将描述进一步的实验，以讨论SeqNet中的上下文混淆避免、模型收缩和架构设计。我们对SeqNet进行了几个简单的更改，以测试我们的假设。</p>
<ul>
<li><strong>语境混乱</strong>。为了证明上下文混淆的存在，我们将SeqNet更改为SeqNet2D，可以输入512 <em> 512张图片。我们将图像大小设置为512 </em> 512，因为它可以提供与序列相同的信息量。<strong>SeqNet2D使用深度可分离卷积层的架构与SeqNet非常相似</strong>。表2确保了图像转换方法会导致混淆，在从程序学习功能时可能会混淆网络。MobileNet使用图像作为输入，因此表1还通过SeqNet和ResNet之间的比较反映了上下文混淆的存在。</li>
<li><strong>模型收缩</strong>。SDSC层通过降低输入维数和分解卷积来减少参数量。我们用SeqNet中的公共卷积层替换所有SDSC层，并将新模型命名为SeqNetConv，以反映因子分解卷积的效果。为了验证尺寸减小在模型收缩中的作用，我们使用SeqNet2D作为实验对象。我们还将公共卷积层应用于SeqNet2D，并调用新模型SeqNet2DConv来展示这两种方法的缩减效果。如表2所示，我们可以看到维数的有效降低减少了参数的数量，卷积分解进一步减少。通过结果可以看出，SDSC层在保持性能的情况下有效地缩小了模型。我们还发现，参数越少，模型在训练时收敛速度越快。</li>
<li><strong>结构设计</strong>。在我们的实验中，我们还发现，更深层次的结构可能不会表现得更好。我们调整SeqNet的深度。具有更深层次架构的模型称为SeqNetDeep，而更浅层次的架构称为SeqNetShal。与我们的直觉类似，较浅的体系结构会显著降低模型的性能。然而，表2中的结果表明，更深层次的体系结构并不能明显提高性能，而是扩大了模型的规模。<strong>我们假设这种现象可能是因为从二进制到恶意软件的简单映射关系，而不需要复杂的神经网络来拟合。</strong>另一个可能的原因是，更深层次的结构使网络难以训练，这可能会导致精确度较低。我们发现的另一个现象是，扩张卷积不能有效地提高性能。我们认为这是因为与输入的长度相比，使用或不使用扩张卷积对模型的感受野没有太大影响。</li>
</ul>
<h4><span id="46-稳健性评估">4.6 稳健性评估</span></h4><p>我们还检查了SeqNet的健壮性，并将其与MalConv进行了比较。关于攻击深度模型有大量的研究[24,32,36,44,48,54,56]。<strong>然而，与针对图像相关任务的神经网络的传统攻击不同，我们不能直接在二进制文件上添加扰动，因为这可能会使二进制文件不可执行</strong>。此外，我们很难根据提取的特征[47]调整攻击策略以适应原始的二进制模型。因此，对于攻击策略，我们采用[30]中的方法，在输入端的填充部分注入一个短的有毒二进制文件。</p>
<p>由于Mal-Conv和SeqNet之间的输入格式不同，我们等效地采用了有毒二进制生成方法。与沿梯度选择最近的嵌入向量相比，SeqNet的毒药生成过程如下所示。</p>
<p>为了使攻击策略在有限的二进制长度下有效，我们从验证恶意数据集中随机选择了500个可用样本。我们将毒药的长度设置为32000字节，MalConv的整个二进制文件固定为16000000字节，并逐步增加毒药生成迭代次数。我们测试了SeqNet和MalConv错误分类的样本数量。结果如图9所示。我们看到SeqNet对有毒二进制攻击有很强的防御能力。我们假设这种现象是由于文件剪切方法中的填充部分造成的漏洞。<strong>基于文件剪切的模型在训练时经常会看到不完整的二进制文件。</strong>因此，文件剪切中的填充使攻击者有机会混淆模型，而模型不确定有毒二进制文件是否是样本的一部分。与文件剪切相比，我们的方法可以通过输入整个二进制文件来缓解这个问题。然而，我们认为这一理论仍需进一步验证，我们可能会在未来的工作中对其进行研究。</p>
<h4><span id="47-案例研究">4.7 案例研究</span></h4><p>为了更好地理解SeqNet所学到的知识，我们随机选择了四个样本，并使用Grad <strong>CAM</strong>[46]解释技术生成热图，以便我们能够可视化哪个部分对结果影响最大。此外，我们还手动分析相应的样本，以验证SeqNet是否找到了正确的恶意代码。在手动分析中，我们通过IDA Pro[3]分解样本，精确定位恶意功能或代码。为了更好地绘制结果，我们提取了热图的关键部分，并对片段应用以下归一化公式。</p>
<p>其中X表示代码段。用于热图的激活图是由SeqNet的最后一个卷积层和ReLU层生成的，因为剩余的空间信息由卷积层编码。我们还在原始二进制文件上标记手动定位结果，以便更好地进行比较。图10显示了手动定位和基于梯度凸轮的解释之间的比较。我们看到，本地激活位置与分析人员定位的恶意部件接近，这反映了SeqNet可能会发现恶意代码并进行可靠的检测。在我们的解释中，我们发现在整个热图中有许多噪音。我们认为这可能是因为数据集中可能存在潜在的异常统计[10]和一些错误标签[43]。然而，遗憾的是，由于学术界对良性档案的忽视，我们很难收集到更多的良性样本。我们希望在未来的工作中能够探索这一现象。此外，我们还发现PE头通常会对SeqNet产生很大影响。这可能意味着PE头包含恶意软件中的恶意信息。更多细节见附录。</p>
<h3><span id="5讨论">5.讨论</span></h3><p>在这一部分中，我们将讨论我们的工作的局限性，并提出一些未来需要进一步研究的工作。</p>
<h4><span id="51局限性">5.1局限性</span></h4><p>尽管SeqNet表现良好，但我们的工作仍有一些局限性。仍然是<strong>语义缺失</strong>。虽然我们有效地减少了语义损失，但SeqNet的输入仍然不能包含所有语义。如果序列太长，在插值过程中会对序列进行压缩，压缩后的序列不能代表所有原始信息。此外，如果序列太短，序列将被扩展，这可能会混淆SeqNet。SeqNet的体系结构决定了输入必须具有相同的大小，这是SeqNet的一个限制。</p>
<p><strong>缺乏良性样本</strong>。我们面临的主要困难是缺乏良性样本。我们可以获得大量恶意软件收集网站，但很难找到权威的良性样本提供商。为了均匀采样，仅通过添加恶意样本来扩展训练数据集是不合适的，这可能会降低SeqNet的性能，并使实验结果不可靠。因此，很难在具有足够样本的更大数据集上训练神经网络。</p>
<p><strong>标签的质量</strong>。除了缺乏良性样本，标签的质量可能是一个潜在问题。由于权威供应商寥寥无几，我们无法保证培训和验证数据集中的所有良性样本都正确标记。我们数据集中的所有恶意样本都是从VirusShare收集的，无需人工确认。几家报纸检查了恶意软件标签的质量，发现它可能达不到我们的预期[43]。虽然这些限制可能会对SeqNet的训练过程产生一些影响，但我们假设，几个不正确的标记样本不会显著影响整体性能。</p>
<p><strong>可能存在的漏洞</strong>。对抗性攻击是大多数神经网络的风险，我们也不例外。有动机的对手可能会污染训练数据集，并逃避SeqNet的检测。<strong>此外，基于梯度的攻击是混淆深度学习模型的有效方法[17,30,32]。相反，这个问题也有很多解决方案[7,12,20,33,57]。</strong>虽然SeqNet可以抵御多次攻击，但我们仍然无法完全保证SeqNet的安全。此外，SeqNet的健壮性原则需要我们进一步探索。</p>
<h4><span id="52-未来工作">5.2 未来工作</span></h4><p>我们提出了一种高效的恶意软件自动检测神经网络SeqNet。SeqNet的主要目标是实现自动、高效的检测，可以以较低的原始二进制文件培训成本快速进行培训。尽管如此，未来仍有许多工作需要完成。基于深度学习的恶意软件检测研究的最大障碍之一是<strong>缺乏工业规模的公共可用数据集</strong>。研究人员需要权威可靠的数据集，这些数据集不仅包含恶意特征，还包含原始二进制序列。我们将建立一个更大的数据集，以进一步评估SeqNet的性能。此外，还需要足够的良性样本进行进一步研究。<strong>我们认为，当使用深度学习模型进行检测时，恶意软件分析不仅应该关注恶意样本，还应该关注良性样本</strong>。由于神经网络是黑盒模型，恶意软件检测神经网络的可靠性可能会受到怀疑。虽然SeqNet给了我们很好的结果，但我们仍然无法完全解释原因。因此，在实践中使用深度学习算法检测恶意软件仍需进一步研究。通过我们的实验，<strong>我们认为神经网络在恶意软件检测方面可能具有巨大的潜力</strong>，我们期待着神经网络在这一领域取得重大突破。SeqNet的健壮性仍需进一步研究。我们仍然缺乏这方面的实验和研究。在未来的工作中，我们将更深入地探索该模型的稳健性，并对其进行更多的改进和分析。</p>
<blockquote>
<p>  基于二进制的模型。与传统的特征工程相比，自动特征提取是神经网络的发展趋势之一，人工干预更少，性能更好。Raff等人设计了一种称为MalConv的架构，它可以直接从原始PE二进制样本中学习，而无需手动选择特征[41,42]。Krc al等人设计了一个简单的CNN，可以从PE原始字节序列中学习，而无需特定领域的特征选择，这项工作获得了较高的AUC分数，尤其是在小型PE恶意软件样本上[31]。</p>
</blockquote>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（3）Living-Off-The-Land 恶意软件系统分析</title>
    <url>/posts/3403A74/</url>
    <content><![CDATA[<h2><span id="living-off-the-land-恶意软件系统分析">Living-Off-The-Land 恶意软件系统分析</span></h2><blockquote>
<p>  [][AI安全论文] <strong>1、<a href="https://mp.weixin.qq.com/s?__biz=Mzg5MTM5ODU2Mg&amp;mid=2247496069&amp;idx=1&amp;sn=f5aecaae5494b900b29f12078a2d632e&amp;chksm=cfcf4148f8b8c85ee56fbab09252bb4dc90f936cfb6decaa860b5e91457c9838cfb35179041a&amp;scene=178&amp;cur_album_id=1776483007625822210#rd">21.S&amp;P21 Survivalism经典离地攻击（Living-Off-The-Land）恶意软件系统分析</a>:S&amp;P21的离地攻击（Living-Off-The-Land）系统分析，这是一篇非常经典的论文，并且系统性分析文章是另一种讲故事的方式。</strong></p>
<p>  2、2021 RAID <strong>Living-Off-The-Land Command Detection Using Active Learning</strong>：<a href="https://dl.acm.org/doi/pdf/10.1145/3471621.3471858">https://dl.acm.org/doi/pdf/10.1145/3471621.3471858</a></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548226.png" alt="图片" style="zoom:67%;"></p>
<blockquote>
<p>  原文作者：Frederick Barr-Smith, Xabier Ugarte-Pedrero, et al.<br>  <strong>原文标题</strong>：Survivalism: Systematic Analysis of Windows Malware  Living-Off-The-Land<br>  <strong>原文链接</strong>：<a href="https://ieeexplore.ieee.org/document/9519480">https://ieeexplore.ieee.org/document/9519480</a><br>  <strong>发表会议</strong>：2021 IEEE Symposium on Security and Privacy (SP)</p>
</blockquote>
<p><strong>文章目录：</strong></p>
<ul>
<li><p><strong>摘要</strong></p>
</li>
<li><p><strong>一.引言</strong></p>
<p>1.什么是离地攻击</p>
<p>2.APT中的离地攻击</p>
<p>3.提出五个关键问题</p>
<p>4.贡献（Contribution）</p>
</li>
<li><p><strong>二.背景和相关工作</strong></p>
<p>A.LotL Binaries</p>
<p>B.Scope of our Study</p>
<p>C.Related Work</p>
</li>
<li><p><strong>三.MOTIVATION: 杀毒软件产品 vs 离地攻击技术</strong></p>
</li>
<li><p><strong>四.离地攻击流行性评估</strong></p>
<p>A.Dataset Composition</p>
<p>B.Analysis Pipeline</p>
<p>C.LotL Technique Identification</p>
<p>D.Parameter Analysis to Identify Execution Purpose</p>
</li>
<li><p><strong>五.评估结果</strong></p>
<p>A.商用恶意软件中LotL技术的流行性（Prevalence）</p>
<p>B.Comparison of Benign and Malicious Samples</p>
<p>C.Prevalence of LotL techniques in APT Malware</p>
</li>
<li><p><strong>六.案例分析</strong></p>
</li>
<li><p><strong>七.要点和讨论</strong></p>
</li>
<li><p><strong>八.局限性和未来工作</strong></p>
</li>
<li><p><strong>九.个人感受</strong></p>
</li>
</ul>
<h3><span id="摘要">摘要</span></h3><p>随着恶意软件检测算法和方法变得越来越复杂（sophisticated），恶意软件作者也采用（adopt）同样复杂的逃避机制（evasion mechansims）来对抗（defeat）它们。<strong>民间证据表明离地攻击技术（Living-Off-The-Land，LotL）是许多恶意软件攻击中最主要的逃避技术之一。这些技术利用（leverage）系统中已经存在的二进制文件来执行（conduct）恶意操作。</strong>基于此，我们首次对Windows系统上使用这些技术的恶意软件进行大规模系统地调查。</p>
<p>在本文中，我们分析了这些本地系统的二进制文件在多个恶意软件数据集上的使用情况，这些数据集共包含31,805,549个样本。我们发现平均流行率（prevalence）为9.41%。实验结果表明，LotL技术被大量的使用，特别是在高级持久性威胁（Advanced Persistent Threat ，APT）恶意软件样本中，离地攻击占比为26.26%，是社区恶意软件的两倍多。</p>
<p><strong>为了验证（illustrate）LotL技术的逃逸潜力，我们在本地沙箱环境（sandboxed environment）中对几个完全打补丁的Windows系统进行了离地攻击技术的测试，其结果表明在10个最流行的反病毒产品（anti-virus）中存在明显的gap。</strong></p>
<h3><span id="一-引言">一、引言</span></h3><h4><span id="11-什么是离地攻击">1.1 什么是离地攻击</span></h4><p>恶意软件开发和检测是猫和老鼠的游戏，恶意软件作者不断开发新技术来绕过（bypass）检测系统。像AV杀毒软件（anti-virus）这样的安全产品通过静态和启发式分析（heuristic analysis）技术，以检测、分类和防止恶意软件有效执行。</p>
<p>在过去，许多解决方案严重依赖于基于签名的检测，但不幸的是，由于使用了<strong>多态性（polymorphism）和加壳程序（packers）</strong>，这些方法变得不再那么有效。相反，许多产品开始开发启发式分析解决方案，包括检测恶意行为的算法。这些算法已成为AV引擎的重要组成部分。随着时间的推移，这些算法越来越复杂，因此需要更多创新性的逃避技术。</p>
<p>恶意软件作者和红队经常研究和发现新方法来绕过安全解决方案。虽然它们的潜在目标本质上可能有所不同，但这两种类型的攻击者通常都利用（leverage）最先进（state-of-the-art）的逃避技术来实现目标。<strong>从防守者的角度来看，为了及时作出响应，了解这些攻击和研究它们的趋势是至关重要的（crucial）</strong>。其中，在红队和恶意软件作者中都流行的规避策略就是使用离地攻击（LotL）技术。</p>
<p><strong>离地攻击（LotL）技术是指使用系统中已经存在或易于安装的二进制文件（如已签名的合法管理工具）来执行后渗透活动（post-exploitation activity）。</strong></p>
<ul>
<li>通过利用这些工具，攻击者可以<strong>实现注册表修改、持久化、网络或系统侦察，或执行其他恶意代码</strong>。它们甚至可以用来减少由恶意活动产生的事件日志，而不需要将其他文件下载到本地的系统中。</li>
</ul>
<h4><span id="12-apt中的离地攻击">1.2 APT中的离地攻击</span></h4><p>离地攻击并不是隐蔽的技术，它们在互联网上公开记录着。许多开源的攻击安全工具利用了LotL技术，并且经常被攻击者所使用，从合法的红队到业余的网络攻击者，以及有组织的APT团队。</p>
<ul>
<li><code>PoshSpy[15]</code>：是一个俄罗斯APT29攻击模块，它是第一个被检测到的APT组织使用的LotL技术，特别是在PowerShell和Windows Management中。</li>
<li>伊朗威胁组织[1]、APT33、APT34和其他组织也以使用本地Windows二进制文件和其它签名工具而闻名，特别是PowerShell[8]。</li>
</ul>
<p>尽管“离地攻击”在信息安全界是一个相对知名的术语，但有时很难找到一个精确的定义。此外，据我们所知，没有任何研究包含了对LotL技术在恶意软件样本中的流行程度的系统分析。关于LotL技术的文档大多以博客的形式出现，并记录着某些恶意软件家族的在野发现，或者攻击者在远程访问受损系统中所使用技术的描述。</p>
<ul>
<li>例如，<code>Emotet</code> 和 <code>Trickbot</code>，两个最常见的远程访问木马（Remote Access Trojans，RAT），据称是使用链接的LotL二进制文件来实现持久化。</li>
<li>作为一种对策，微软描述了对抗使用LotL技术商用RAT的基本步骤。高度逃逸的远程访问木马 <code>Astaroth</code>，<code>TA505</code> 组织的一些恶意软件库，<code>Dexphot cryptominer</code> 和 <code>Nodersok</code> 同期使用的多个LotL二进制文件。</li>
</ul>
<h4><span id="13-提出5个关键性问题">1.3 提出5个关键性问题</span></h4><p>在本文中，我们分析了LotL现象，即商用恶意软件中与离地攻击二进制文件利用相关的文件。我们首先描述了什么是LotL binary以及它如何被恶意软件利用来实施恶意行为的。</p>
<p>本文的研究重点是以Windows为主导的操作系统下流行且恶意软件最常针对的目标。<strong>许多基于离地攻击的AV逃逸行为已被记录下来。因此（As a consequence），安全界很大程度上认为，LotL技术（如代理执行恶意软件）实际上对安全解决方案是有效的。</strong></p>
<p>首先，我们提出了第一个假设以及第一个研究问题：</p>
<blockquote>
<h4><span id="问题1can-lotl-techniques-effectively-evade-commercial-av"><strong>问题1：Can LotL techniques effectively evade commercial AV?</strong></span></h4><h4><span id="lotl技术能有效地逃避目前大部分安全厂商的杀毒软件检测吗">LotL技术能有效地逃避目前大部分安全厂商的杀毒软件检测吗？</span></h4></blockquote>
<p>为了回答这个问题，我们评估了一组具有代表性的安全产品，并展示了其中的一些技术，虽然这是攻击者和防御者所熟知的，但仍然是绕过安全解决方案的有效方法，因此对安全行业来说这仍是一个开放的挑战。</p>
<p><strong>事实上，LotL二进制文件经常被系统管理员和高级计算机用户使用来执行（perform）系统管理任务，这使得即使是对于训练有素的分析人员来说，区分（distinguish）合法行为和恶意行为也非常困难</strong>。我们负责任地向受影响的供应商披露了我们的发现并进行跟进，因此提高了他们的检测能力。</p>
<p>尽管现有的文档提供了这些技术使用的可靠证据，但仍然不清楚这种现象在恶意软件样本中有多普遍。因此（In this way），我们就提出了第二个研究问题：</p>
<blockquote>
<h4><span id="问题2how-prevalent-is-the-use-of-lotl-binaries-in-malware"><strong>问题2：How prevalent is the use of LotL binaries in malware?</strong></span></h4><h4><span id="在恶意软件中使用lotl二进制文件的情况有多普遍">在恶意软件中使用LotL二进制文件的情况有多普遍？</span></h4></blockquote>
<p>在此基础上，我们试图阐明当前威胁情景中的一些趋势，以确定（identify）：</p>
<blockquote>
<h4><span id="问题3what-purposes-do-malware-binaries-use-lotl-techniques-for"><strong>问题3：What purposes do malware binaries use LotL techniques for?</strong></span></h4><h4><span id="恶意软件的二进制文件使用lotl技术的目的是什么">恶意软件的二进制文件使用LotL技术的目的是什么？</span></h4><h4><span id="问题4which-malware-families-and-types-use-lotl-binaries-most-prolifically-and-how-does-their-usage-differ"><strong>问题4：Which malware families and types use LotL binaries most prolifically and how does their usage differ?</strong></span></h4><h4><span id="哪些恶意软件家族和类型使用lotl二进制文件最多它们的使用情况又有何不同">哪些恶意软件家族和类型使用LotL二进制文件最多，它们的使用情况又有何不同？</span></h4></blockquote>
<p>此外，我们还调查（investigate）了为什么这些技术难以检测。部分杀毒软件公司参与了我们的披露，即将恶意攻击与系统管理员执行完全合法的管理任务区分开来是困难的。这就给我们带来了另一个问题：</p>
<blockquote>
<p>  <strong>问题5：What are the overlaps and differences in the behavior of legitimate and malicious binaries with respect to the usage of LotL binaries? How would this affect detection by heuristic AV engines?</strong></p>
<h4><span id="在使用lotl二进制文件方面合法和恶意二进制文件的行为有哪些重叠和差异呢这将如何影响启发式av引擎的检测呢">在使用LotL二进制文件方面，合法和恶意二进制文件的行为有哪些重叠和差异呢？这将如何影响启发式AV引擎的检测呢？</span></h4></blockquote>
<p>虽然恶意样本和良性样本之间的LotL二进制使用频率（prevalence）有一些明显的差异，但我们也注意到一些类别存在某些相似性，如<strong>代理执行（proxied execution）</strong>。</p>
<p>最后，<strong>我们将注意力集中在高逃逸和高级持续威胁的恶意软件上，我们发现它利用离地攻击技术是商用恶意软件的两倍</strong>。在表1中列出了一些使用LotL技术进行攻击的APT组织。</p>
<h4><span id="14-贡献">1.4 贡献</span></h4><p>据我们所知，本文提出了迄今为止对商用和APT恶意软件使用LotL技术最大规模的系统分析。本文的核心（core ）贡献：</p>
<ul>
<li>我们通过测试一组最流行的AV引擎来对抗基于LotL技术部署的恶意载荷，以评估LotL技术的可行性，并展示了离地攻击检测的复杂性对行业仍是一个挑战。<strong>即使在披露9个月后，这些技术仍没有被发现</strong>。</li>
<li>我们对代表现代商用恶意软件的几个数据集进行了大规模的评估，并确定了LotL技术的流行程度，以及在不同恶意软件家族和类型之间的差异。我们还评估了LotL技术由于假阳性风险可能对行业产生的影响。</li>
<li>我们评估了一个APT恶意软件数据集，并将其公开以促进（facilitate）后续的研究，并确定它执行LotL技术的频率是商用恶意软件的两倍。此外，我们还确定了哪些APT组织最多地使用LotL技术。</li>
</ul>
<h3><span id="二-背景和相关工作">二、背景和相关工作</span></h3><p>我们首先定义LotL二进制文件，并枚举恶意软件使用这些二进制文件的目的。</p>
<h4><span id="21-lotl-binaries">2.1 LotL Binaries</span></h4><p>近年来，“<code>Living-Off-The-Land binary（LOLbin）</code>”已经成为一个常用词，用来指在网络攻击中广泛使用的二进制文件。历史上，“Living-Off-The-Land”一直被用来表示可以为农业或狩猎提供喂养土地或离地的概念。<strong>转换为恶意软件和入侵领域，攻击者可能利用那些已经可以使用的文件（即系统上已经存在或易于安装的）来发起攻击并躲避检测。</strong></p>
<p>在本文中，我们将LotL二进制定义为：</p>
<ul>
<li><strong>任何具有公认合法用途的二进制文件，在攻击期间利用它直接执行恶意行为，或间接协助一系列恶意行动，从而达到恶意结果。</strong></li>
</ul>
<blockquote>
<p>  In this paper, we define a LotL binary as any binary with a recognised legitimate use, that is leveraged during an attack to directly perform a malicious action; or to assist indirectly, in a sequence of actions that have a final malicious outcome.</p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548420.png" alt="图片" style="zoom:50%;"></p>
<h4><span id="举例">举例：</span></h4><ul>
<li>在Windows系统上默认安装的二进制文件（binaries installed），如 <code>Reg.exe</code> 、<code>Sc.exe</code> 和 <code>Wmic.exe</code> 是最常被恶意软件执行的文件。</li>
<li>大多数默认安装的二进制文件都是由微软认证码签名的。认证码签名证明二进制文件没有在编译中被篡改或修改，这些二进制文件甚至可能被列为白名单。<strong>利用可信的LotL二进制文件的恶意软件可能因此避开杀毒软件</strong>。在Windows系统上使用系统二进制文件可以作为恶意软件操作的一部分，更重要的是，许多LotL技术使用系统二进制文件来实现这些二进制文件的目的。</li>
<li>此外，可以使用外部签名二进制文件（external signed binaries），如 <code>PsExec.exe</code> 或其他系统内部二进制文件。虽然它们使用频率不高，但本文的分析也囊括了这些文件。<strong>如APT组织在 <code>SoftCell</code> 和 <code>Havex</code> 中都使用 <code>PsExec.exe</code> 来秘密执行远程命令，从而实现网络中的横向移动。</strong></li>
<li>某些罕见情况，脆弱的（已签名）驱动程序被用来升级系统上的权限。这是 <code>RobbinHood</code> 勒索软件和各种 <code>APT wiper</code> 恶意软件样本所使用的一种技术，针对 <code>Saudi Arabian</code> 系统，包括 <code>Dustman</code> 、<code>Shamoon</code> 和 <code>Zerocleare</code>。</li>
</ul>
<h4><span id="可追溯性traceability"><strong>可追溯性（Traceability）：</strong></span></h4><ul>
<li>某些LotL二进制文件可能会比其他文件留下更多的系统日志，安全工具或取证分析人员可以利用这些日志来检测恶意操作。<strong>例如，可以将Powershell配置为具有全面的日志记录</strong>。</li>
<li>微软甚至建议<strong>阻止在系统上执行一些本机的二进制文件</strong>，除非有充分的理由。</li>
</ul>
<h4><span id="22-scope-of-our-study">2.2 Scope of our Study</span></h4><p>在本文中，我们关注的是Windows恶意软件执行系统二进制文件的目的。这些目的通常包括沿着 <strong>kill chain</strong>的进展或逃避AV的检测。所有这些技术都被部署在系统的用户空间中。</p>
<p><code>hollowing</code> 和 <code>injection（注入）</code> 不在我们的研究范围内，尽管这是无文件恶意软件部署的常见技术。因为根据我们早期的定义，它们不是LotL技术。</p>
<h4><span id="23-related-work">2.3 Related Work</span></h4><blockquote>
<p>  离地攻击相关工作较少，并且都非常经典，因此下面罗列了详细的相关研究，仅供自己后续深入，也希望对您有所帮助。</p>
</blockquote>
<ul>
<li>LotL恶意软件及其别名，“advanced volatile threat”或“无文件”恶意软件在当前的学术文献中很少被提及。这主要受限于介绍分析少或描述为一个新兴的高逃逸恶意软件变体。</li>
<li>Li等[31]对恶意PowerShell脚本进行了分析，其中有一个小节专门描述了LotL攻击和无文件攻击作为近年来网络攻击的趋势。（<strong>作者第17篇博客详细介绍过PS经典</strong>）</li>
<li>Wang等[72]最近发表的一篇关于数据来源分析的论文指出，Living-Off-The-Land 是一种新兴的、突出的逃避型恶意软件子类（evasive malware subtype）。（<strong>经典的You Are What You Do后续即将分享</strong>）</li>
<li>先前的工作[64]进行了介绍性分析，然而LotL恶意软件还没有受到详细的学术分析。（An emerging threat Fileless malware: a survey and research challenges）</li>
<li><strong>赛门铁克</strong>[73,66]和<strong>思科Talos</strong>的[65]白皮书介绍了这个主题，并对多个数据集的流行性进行了分析。目前，没有论文对包含多个使用LotL技术的Windows恶意软件数据集进行大规模地系统分析。（<strong>经典</strong>）<ul>
<li><a href="https://www.symantec.com/content/dam/symantec/docs/security-center/white-papers/istr-living-off-the-land-and-fileless-attack-techniques-en.pdf">https://www.symantec.com/content/dam/symantec/docs/security-center/white-papers/istr-living-off-the-land-and-fileless-attack-techniques-en.pdf</a></li>
<li><a href="https://www.symantec.com/content/dam/symantec/docs/white-papers/living-off-the-land-turning-your-infrastructure-against-you-en.pdf">https://www.symantec.com/content/dam/symantec/docs/white-papers/living-off-the-land-turning-your-infrastructure-against-you-en.pdf</a></li>
<li><a href="https://blog.talosintelligence.com/2019/11/hunting-for-lolbins.html">https://blog.talosintelligence.com/2019/11/hunting-for-lolbins.html</a></li>
</ul>
</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548029.png" alt="图片" style="zoom:50%;"></p>
<p>在一些论文中提到了LotL技术，强调了高隐蔽（stealthiness）和APT恶意软件曾使用。</p>
<ul>
<li><p>在一篇关于恶意软件分析工具Yara的论文中，Cohen[9]将LotL描述 “ LotL as a trend that has been recently observed in the tactics used by elite threat actors”，我们的分析结果进一步证实了该说法。</p>
</li>
<li><p>Hassan等[21]的研究表明，APT恶意软件使用LotL攻击策略来实现持续攻击并分析了两个活动，他们的工作还利用了MITRE ATT&amp;CK框架[45]，通过MITRE定义了一个描述和分类知名攻击的分类方法。<strong>许多LotL技术在MITRE ATT&amp;CK框架内被索引</strong>。Mitre公司及其常见CVE漏洞是安全领域的既定权威，他们囊括并描述许多LotL技术，这样表明离地攻击是一个值得深入分析的课题。</p>
</li>
<li><ul>
<li><strong>W. U. Hassan, A. Bates, and D. Marino, “Tactical Provenance Analysis for Endpoint Detection and Response Systems,” IEEE Symposium on Security and Privacy, 2020.</strong></li>
</ul>
</li>
</ul>
<p><strong>强烈推荐一个包含LotL二进制和ATT&amp;CK功能映射的资源</strong>：</p>
<ul>
<li><a href="https://github.com/LOLBAS-Project/LOLBAS">https://github.com/LOLBAS-Project/LOLBAS</a></li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548854.png" alt="图片" style="zoom:50%;"></p>
<p>与我们研究相关的是对基于脚本的恶意软件分析和去混淆。使用LotL技术的恶意软件经常使用恶意脚本作为有效负载。（<strong>下列论文在作者第16篇PowerShell总结博客中详细介绍过</strong>）</p>
<ul>
<li>Ugarte等[67]通过识别可疑行为模式，测试了经 <code>Powershell.exe</code> 二进制调用的恶意脚本。</li>
<li><strong>Rubin等[61]将机器学习应用于检测PowerShell恶意软件（微软团队）</strong>。</li>
<li>Curtsinger[11]等人提出了恶意Javascript攻击的检测机制——ZOZZLE。</li>
</ul>
<p><strong>虽然这些论文提出了有效的检测方法，但是他们都是为狭隘的恶意载荷（payload）所用，他们没有分析更广泛的恶意软件生态系统和这些有效载荷是如何被LotL二进制文件触发的。</strong></p>
<h3><span id="三-动机">三、动机</span></h3><p><strong>安全研究人员已经记录了许多使用LotL技术成功躲避安全产品的案例</strong>。在许多情况下，这些<strong>LotL二进制文件被用来代理恶意载荷的执行，使其在一个合法的进程上下文中执行，或者作为一个合法系统进程的子进程生成一个新进程</strong>。在某些情况下，这些有效载荷作为LotL二进制调用的副作用被执行，而在其他情况下，它只是其主要记录行为的结果。此外，许多杀毒产品未能正确检测到这些技术。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191548198.png" alt="图片" style="zoom:50%;"></p>
<p><strong>为了回答第一个问题，我们首先分析了当前AV产品是否将LotL技术作为恶意行为的指标。</strong></p>
<p>为此，我们首先选择了10个具有代表性的AV产品（详见附录C），并利用常见<strong>基于LotL的代理执行技术来实施反弹Shell的模拟攻击</strong>。此外，本研究的目的不是测试任何特定AV产品的检测能力或将它们相互比较，而是确定是否存在普遍的检测差距。</p>
<ul>
<li>实验在联网的Windows 10虚拟机执行，并将最新的本地AV产品连接到它们的云组件。</li>
<li>利用一个反弹Shell来评估AV系统在部署LotL技术的恶意软件中有多脆弱。本文认为能够允许远程执行命令的reverse shell是成功执行代码的证明，这与许多远程访问木马（RAT）功能相同。</li>
<li>通过从不同LotL二进制文件中运行这个反弹shell来进行实验，以测试AV产品是否检测到离地攻击技术是恶意的。</li>
<li>我们在必要时混淆了反弹shell的有效载荷，并使用各种有效载荷类型来测试AV检测传递机制本身的能力，而不是通过静态签名传递的特定有效载荷（详见附录D）。</li>
</ul>
<h4><span id="实验结果如表2所示">实验结果如表2所示：</span></h4><ul>
<li><strong>可以发现大部分的AV引擎允许我们建立一个反弹Shell并执行命令，它们并没有检测出利用LotL技术的恶意软件，60个中只检测出4个。</strong></li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191549745.png" alt="图片" style="zoom:50%;"></p>
<h4><span id="负责任的披露和回应">负责任的披露和回应：</span></h4><p>此后，我们向相关的AV供应商发布了一份文件，包含我们检查的结果并协助补救。9个月后，我们在Windows 10机器上重复了类似的测试，这允许我们测试AV供应商是否在他们的产品中包含了新的启发式规则来检测LotL二进制的使用。其结果如下：</p>
<ul>
<li><strong>可以发现在60个相同的有效载荷中检测到了25个</strong></li>
<li>在检测到的反弹shell测试中，我们修改了载荷（利用混淆或运行不同的载荷），同时为LotL二进制文件保持了完全相同的命令行参数，通过利用这些混淆和修改的有效载荷，我们成功地在这25个被拦截的实例中的19个执行了一个反向shell。</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191549539.png" alt="图片" style="zoom:50%;"></p>
<p><strong>实验结果表明，LotL技术仍然是杀毒软件供应商面临的一个重大挑战。合法用户通常以不可预知的方式使用这些工具，而安全公司很难在没有误报的情况下部署有效的检测策略。</strong></p>
<p>接下来将展示这些技术如何在商用恶意软件中是普遍存在的，以及离地攻击是不应该被安全社区忽视的问题。</p>
<h3><span id="四-离地攻击流行性评估">四、离地攻击流行性评估</span></h3><p>在本节中，我们测量了恶意软件中LotL技术的流行程度，并试图回答所提出的研究问题。</p>
<h4><span id="41-数据集描述">4.1 数据集描述</span></h4><p>评估工作是在9个独立的子数据集上进行的。我们总共收集了31,805,549个样本，其中我们从VirusTotal（VT）中获得了16,048,202份行为报告。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191549136.png" alt="图片" style="zoom:50%;"></p>
<p><strong>Public Datasets</strong></p>
<p>公共恶意软件数据集，包括商用恶意软件、VirusShare语料库的二进制文件、窗口恶意PE文件、佐治亚理工学院发布的可执行文件、VX-Mumbal和MalShare共享的样本（两个重点的共有数据集）。</p>
<ul>
<li><a href="https://impactcybertrust.org/dataset{">https://impactcybertrust.org/dataset{</a> }view?idDataset=1143</li>
<li><a href="https://vx-underground.org/samples.html">https://vx-underground.org/samples.html</a></li>
<li><a href="https://malshare.com">https://malshare.com</a></li>
</ul>
<p><strong>VirusTotal Balanced Dataset</strong></p>
<p>从VT中收集了237,288个hash值，利用 <code>AVClass</code> 预处理代码和打标签（家族分类），并平衡数据集中每个族。</p>
<p><strong>APT Malware</strong></p>
<p>我们根据一种类似于数据集论文dAPTaset[59]的方法收集了一个APT恶意软件的数据集。我们处理了HTML页面和pdf文件（<code>APTnotes</code>），并提取了这些文件中包含的所有恶意软件的hash值。</p>
<ul>
<li><a href="https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-rezaeirad.pdf">https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-rezaeirad.pdf</a></li>
<li><a href="https://github.com/aptnotes/data">https://github.com/aptnotes/data</a></li>
</ul>
<p><strong>Yara Rule Match Malware</strong></p>
<p>部署3个Yara规则来检测LotL二进制文件，并使用Livehunte来识别上传到VT的新的恶意软件hash，并使用LotL技术匹配恶意软件的行为特征。</p>
<h4><span id="42-analysis-pipeline">4.2 Analysis Pipeline</span></h4><p>当收集了由Windows PE二进制文件组成的不同数据集，我们就分析样本的行为。包括三个阶段：</p>
<ul>
<li>data collection</li>
<li>data augmentation</li>
<li>data analysis</li>
</ul>
<blockquote>
<p>  First Seen：首次发现病毒样本的时间戳<br>  AVClass Family：某恶意软件样本所属家族<br>  <strong>Behavioural Report：恶意行为报告，由特定恶意软件样本执行的进程和Shell命令的列表</strong></p>
</blockquote>
<h4><span id="43-lotl-technique-identification">4.3 LotL Technique Identification</span></h4><p><strong>数据准备就绪，那么如何识别是否使用了LotL技术呢？</strong></p>
<p>我们使用<strong>模式匹配</strong>来识别恶意软件执行过程中对LotL二进制文件调用的情况，从而处理所有收集到的行为报告（<code>behavioural reports</code>）。行为报告包括两个指标：</p>
<ul>
<li><p><strong>Shell Commands（Shell命令）</strong></p>
<p><strong>恶意二进制文件在主机操作系统中执行的Shell命令</strong>，Shell命令日志可以通过引用系统二进制文件的绝对路径来显示它的执行情况。<strong>同时，Windows的命令提示符还包括许多别名，例如Reg.exe的reg</strong>。</p>
</li>
<li><p><strong>Processes（进程）</strong></p>
<p><strong>进程日志明确由恶意软件样本执行的系统二进制文件</strong>。执行的参数也包含在行为报告中的进程日志中。</p>
</li>
</ul>
<p>在我们的分析中，如果一个样本的行为报告包含至少一个LotL二进制文件的执行，那么它使用了LotL技术。<strong>我们记录了每一个LotL的执行及其参数细节，并将它们插入到数据库中。然后，我们分析了这些恶意软件样本的参数，以确定每个数据集中最常见的参数类型和执行目的。</strong></p>
<p>具体而言，我们确定了这两种独立类型的二进制文件：</p>
<ul>
<li><strong>Default System Binaries</strong></li>
<li><strong>Installed Signed Binaries</strong></li>
</ul>
<p>模式匹配优化：模式匹配方法在不断改进，直到所有识别的LotL命令被正确分类和映射到执行目的，并进行了数据清洗处理。</p>
<ul>
<li>不带参数的二进制执行移除</li>
<li>沙箱产物删除（如Explorer.exe和sha256），Web访问不处理</li>
<li>删除Verclsid.exe的实例</li>
</ul>
<h4><span id="44-parameter-analysis-to-identify-execution-purpose">4.4 Parameter Analysis to Identify Execution Purpose</span></h4><p><strong>为了确定LotL技术的执行目的，我们观察了恶意软件样本提供的参数。</strong></p>
<p>图1说明了四个进程执行的映射。该映射通过识别单独的执行目的来在所有数据集上实施，<strong>例如执行Net.exe时使用stop参数表示任务停止。在将单个命令映射到执行目的之后，我们将为该二进制文件选择所有匹配的执行</strong>。我们在所有系统二进制执行中重复该步骤，直到每次执行被分类为属于特定的执行目的或被错误分类。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191549986.png" alt="图片" style="zoom:50%;"></p>
<p>按照这种方法，我们按目的将参数分为9个独立的类别。</p>
<p><strong>首先是三种与执行有关的类型：</strong></p>
<ul>
<li><strong>Proxied Execution</strong>：代理执行，如Mshta.exe执行.hta文件，Rundll32.exe执行.dll文件</li>
<li><strong>Persistence</strong>：持久化：如果恶意代码配置或修改系统以在未来某个时间点执行命令或存储的作业，那么它就实现了持久性，比如Sc.exe带有创建参数的Bitsadmin.exe，或带有日期时间参数的Schtasks.exe/At.exe</li>
<li><strong>Delayed Execution</strong>：延迟执行，比如 Ping.exe执行-n</li>
</ul>
<p><strong>接着是三类与底层系统组件的修改有关。恶意软件通常从事这种行为，以便在机器上对目标进行进一步的传播或行动。</strong></p>
<ul>
<li><strong>Firewall Modification</strong>：防火墙修改，如Netsh.exe</li>
<li><strong>Registry Modification</strong>：注册表修改，如Reg.exe</li>
<li><strong>Permissions Modification</strong>：权限修改，如Cacls.exe修改文件权限</li>
</ul>
<p><strong>最后是与执行或系统修改无关的三类。</strong></p>
<ul>
<li><strong>File Opening</strong>：打开文件，如Explorer.exe</li>
<li><strong>Reconnaissance</strong>：侦察，触发本地或远程配置的横向移动，如Net.exe</li>
<li><strong>Task Stopping</strong>：使用LotL二进制文件秘密停止另一个进程或服务，如Taskkill.exe</li>
</ul>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（5）BODMAS-An Open Dataset for Learning based Temporal Analysis of PE Malware</title>
    <url>/posts/1KY76QV/</url>
    <content><![CDATA[<h3><span id="bodmas-an-open-dataset-for-learning-based-temporal-analysis-of-pe-malware">BODMAS: An Open Dataset for Learning based Temporal Analysis of PE Malware</span></h3><blockquote>
<p>  2021 <a href="https://dblp.uni-trier.de/db/conf/sp/sp2021w.html#YangCLA021">SP Workshops</a></p>
<p>   On training robust PDF malware classifiers. (2020 USENIX)</p>
</blockquote>
<h3><span id="一-摘要">一、摘要</span></h3><p>我们描述并发布了一个名为BODMAS的开放PE恶意软件数据集，以促进基于机器学习的恶意软件分析的研究工作。通过仔细检查现有的open PE恶意软件数据集，我们发现了两个缺失的功能（即<strong>最近/时间戳</strong>的恶意软件样本和精心策划的<strong>家族信息</strong>），这限制了研究人员研究<strong>概念漂移和恶意软件系列演化</strong>等紧迫问题的能力。出于这些原因，我们发布了一个新的数据集来填补空白。<strong>BODMAS数据集包含从2019年8月至2020年9月收集的57293个恶意软件样本和77142个良性样本，以及精心策划的家族信息（581个家族）</strong>。我们还进行了初步分析，以说明概念漂移的影响，并讨论该数据集如何有助于促进现有和未来的研究工作。</p>
<h3><span id="二-说明">二、说明</span></h3><p>如今，研究人员[30]、[5]、[11]、[6]和反病毒供应商[1]将机器学习模型（包括深度神经网络）广泛应用于恶意软件分析任务中。在这一工作领域，拥有公共数据集和开放基准是非常可取的。一方面，这些数据集将有助于促进解决开放性挑战的新工作（例如，对抗性机器学习、可解释技术[28]、[10]）。另一方面，公共基准和数据集可以帮助研究人员轻松地比较他们的模型，并跟踪整个社区的进展。然而，创建开放式恶意软件数据集是一项极具挑战性的工作。例如，[5]的作者讨论了许多此类挑战，包括<strong>法律限制、标记恶意软件样本的成本和难度，以及潜在的安全责任</strong>。除了这些因素外，另一个关键挑战是恶意软件（以及良性软件）的动态演化性质[20]。随着时间的推移，新的恶意软件系列和变种不断出现，它们不断地对底层数据分布进行更改。因此，随着时间的推移，不断需要发布新的数据集和基准。在过去的十年中，只有少数公开的PE恶意软件数据集发布到研究社区[30]。值得注意的例子包括Microsoft恶意软件分类挑战数据集[24]、Ember[5]、<strong>UCSB打包恶意软件数据集[2]</strong>和最近的SOREL-20M数据集[11]。我们在表一中总结了它们的主要特征。</p>
<blockquote>
<p>[30] Survey of machine learning techniques for malware analysis. (2019 C&amp;S)</p>
<p>[5] Ember: an open dataset for training static pe malware machine learning models</p>
<p>[11] SOREL-20M: A Large Scale Benchmark Dataset for Malicious PE Detection</p>
<p><strong>[6] Scalable, behavior-based malware clustering (2009 NDSS)</strong></p>
<p><strong>[28] Exploring backdoor poisoning attacks against malware classifiers</strong></p>
<p><strong>[10] Maldae: Detecting and explaining malware based on correlation and fusion of static and dynamic characteristics. (2019 C&amp;S)</strong> </p>
<p>[20] </p>
</blockquote>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（6）DeepReflect：通过二进制重构发现恶意行为</title>
    <url>/posts/2Y43CXR/</url>
    <content><![CDATA[<h2><span id="usenixsec21-deepreflect通过二进制重构发现恶意行为经典">USENIXSec21 DeepReflect：通过二进制重构发现恶意行为（经典）</span></h2><blockquote>
<p>  <a href="https://mp.weixin.qq.com/s?__biz=Mzg5MTM5ODU2Mg&amp;mid=2247495981&amp;idx=1&amp;sn=fa34f5211e67a7d6c144019424657d22&amp;chksm=cfcf41e0f8b8c8f6e91705e142147a6803ca1af45aa8173727e7858788b1473da5b00db25d7b&amp;scene=178&amp;cur_album_id=1776483007625822210#rd">参考材料</a></p>
<p>  原文作者：Evan Downing, Yisroel Mirsky, Kyuhong Park, Wenke Lee<br>  原文标题：DeepReflect: Discovering Malicious Functionality through<br>  Binary Reconstruction<br>  原文链接：<a href="https://www.usenix.org/conference/usenixsecurity21/presentation/downing">https://www.usenix.org/conference/usenixsecurity21/presentation/downing</a><br>  发表会议：USENIXSec 2021<br>  <strong>代码地址</strong>：<a href="https://github.com/evandowning/deepreflect">https://github.com/evandowning/deepreflect</a></p>
</blockquote>
<h3><span id="一-摘要">一、摘要</span></h3><p>深度学习已在恶意软件分类任务中表现出良好的结果。然而：</p>
<ul>
<li><strong>人工分析效率低</strong>：对于未知恶意软件的binary，分析人员仍要花大量时间来利用静态分析工具逆向整个binary，从而识别关键的恶意行为</li>
<li><strong>监督学习开销大</strong>：尽管机器学习可用来帮助识别二进制的重要部分，但由于获取足够大的标记数据集开销很大，因此监督学习方法是不切实际的</li>
</ul>
<p><strong>为了提高静态（或手动）逆向工程的生产力，我们提出了DeepReflect：一种用于定位（localize）和识别（identify）恶意二进制文件中恶意软件组件的工具。</strong></p>
<ul>
<li>为了定位恶意软件组件，我们以一种新型（novel）方式，即首先使用一个<strong>无监督的深度神经网络l来定位恶意软件中恶意组件（函数）的位置</strong></li>
<li><strong>其次，通过半监督聚类分析对恶意组件进行分类，根据恶意行为分类确定恶意函数的行为</strong>，其中分析人员在他们的日常工作流程中逐步提供标签</li>
<li>该工具是实用的，因为它不需要数据标记（require no data labeling）来训练定位模型，也不需要最小/非侵入性标记来增量地训练分类器</li>
</ul>
<h4><span id="11-企业界对比capa">1.1 <strong>企业界对比：CAPA</strong></span></h4><p>我们通过5个恶意软件分析人员对超过26k个恶意软件样本进行评估。<strong>实验发现，DeepReflect让每个分析人员需要逆向工程的函数数量平均减少了85%</strong>。本文方法还可以检测到80%的恶意软件组件，而当使用基于签名的工具CAPA时，该值仅为43%。</p>
<h4><span id="12-学术界对比shap">1.2 <strong>学术界对比：Shap</strong></span></h4><p>此外，DeepReflect提出的自动编码器（autoencoder）比Shap（一种人工智能解释工具）表现得更好。这一点很重要，因为<strong>Shap是一种最先进（state-of-the-art）的方法，需要一个标记的数据集</strong>，而我们的自动编码器不需要。</p>
<h3><span id="二-引言">二、引言</span></h3><h4><span id="21-背景引出挑战">2.1 背景引出挑战</span></h4><p>静态逆向工程恶意软件可能是一个手动且乏味的过程。公司每周可以收到多达 500 万个PE样本。虽然大多数组织提前对这些样本进行分类（triage），以减少要分析的恶意软件数量（即，检查 VirusTotal来获取反病毒 (AV) 引擎结果、在受控沙箱中执行样本、提取静态和动态签名等） ，但最终仍然需要静态逆向工程的恶意软件样本。这是因为<strong>总会有新的恶意软件样本</strong>，没有被反病毒公司分析过，或者缺乏签名来识别这些新样本。最终，该样本有可能会拒绝在分析人员的动态沙箱（sandbox）中执行。</p>
<p>当前的解决方案以为恶意软件样本创建签名、分类和聚类的形式存在。然而，这些解决方案只能预测样本的类别（例如，良性与恶意，或特定的恶意软件家族）。<strong>他们无法定位或解释恶意软件样本本身内部的行为（定位恶意函数位置、解释恶意函数行为），而分析师需要执行（perform）这些行为来生成报告并改进他们公司的恶意软件检测产品</strong>。事实上，由于工作量过大，该领域已呈现了倦怠。</p>
<p>为了确定他们的需求，我们咨询了四名逆向工程恶意软件分析师（一名来自AV公司，三名来自政府部门）。本文发现，如果恶意软件分析师有一个工具可以：</p>
<ul>
<li><strong>识别恶意软件中恶意函数的位置</strong></li>
<li><strong>标记这些恶意函数的行为</strong></li>
</ul>
<p>那么，他们的工作将更有效率。开发这样一种工具的挑战在于：</p>
<ul>
<li><strong>需要能够区分什么是良性的（benign），什么是恶意的（malicious）</strong></li>
<li><strong>理解识别出的恶意行为的语义</strong></li>
</ul>
<p>对于第一个挑战，区分良性和恶意是困难的，因为恶意软件和良性软件的行为通常在高层次上重叠。对于第二个挑战，自动标记和验证这些行为是很困难的，因为没有单独标记的恶意软件函数的数据集（与使用反病毒标签的开放数据集的恶意软件检测和分类系统不同）。</p>
<h4><span id="22-如何解决挑战">2.2 如何解决挑战</span></h4><p>为了解决这些挑战，我们开发了DEEPREFLECT，它使用：</p>
<ul>
<li><font color="red">**一个无监督的深度学习模型来定位二进制中的恶意函数【异常检测】**</font></li>
<li><font color="red">**一个半监督聚类模型，它使用从分析人员的日常工作流程中获得的少量标签对识别的函数进行分类**</font>

</li>
</ul>
<p><strong>为了定位（locate）二进制文件中的恶意软件组件，我们使用自动编码器(autoencoder，AE)</strong>。AE是一种基于神经网络的机器学习模型，<strong>其任务是将其输入重构为输出（编码还原）</strong>。由于网络内层存在压缩，AE被迫学习训练分布中的关键概念。我们的直觉是，如果在良性二进制文件上训练AE，它将很难重建恶意二进制文件（即我们没有训练它的样本）。自然地，AE将无法重建（reconstruct）包含恶意行为的二进制数据区域（在良性样本中是不可见或罕见的）。<font color="red"><strong>因此（Thus），重构错误可以用来识别恶意软件中的恶意组件</strong></font>。此外，由于AE是以无监督的方式训练的，我们不需要数百万标记的样本，公司可以利用自己的恶意软件二进制数据集。</p>
<p><strong>为了对定位的恶意软件组件进行分类</strong>，我们：</p>
<ul>
<li>对恶意软件样本中所有已识别的函数进行聚类</li>
<li>使用分析人员在日常工作流程中所做的注释（即少量人工分析的函数行为标签）来标记聚类结果</li>
</ul>
<p><strong>这种方法是半监督的，因为每个类簇（cluster）只需要少数函数的行为标签（如三个）即可将大多数标签分配给整个集群</strong>。随着时间推移，我们可以将AE识别的函数映射到聚类模型来预测函数的类别（如，C&amp;C、特权升级等），即认为函数和最接近的类簇有相同的行为标签。这反过来又节省了分析人员的时间，因为他们不必一次又一次地对相同的代码进行逆向工程。</p>
<p>注意，无监督 AE 为恶意软件分析人员提供了即时实用程序，无需训练或使用半监督聚类模型。这是因为它：</p>
<ul>
<li><strong>通过对最相关的函数进行排序（重构误差）来吸引分析师的注意力</strong></li>
<li>过滤掉可能需要花费分析师数小时或数天时间来解释的函数</li>
</ul>
<blockquote>
<p>  DEEPREFLECT根据我们是为恶意软件分析人员的反馈进行设计和修改的，并评估其有效性和实用性。</p>
<p>  <strong>我们评估了DEEPREFLECT的性能，包括五个工作：</strong></p>
<ul>
<li>识别恶意软件中的恶意活动</li>
<li>聚类相关的恶意软件组件</li>
<li>将分析人员的注意力集中在重要事情上</li>
<li>揭示不同恶意软件家族之间的共享行为</li>
<li>处理涉及混淆的对抗性攻击</li>
</ul>
</blockquote>
<h4><span id="23-创新contribution">2.3 创新（Contribution）</span></h4><p><strong>我们的贡献如下：</strong></p>
<ul>
<li><strong>提出了一个新颖的工具，它可以帮助恶意软件分析师：(1) 在静态恶意软件样本中自动定位和识别恶意行为，(2) 洞察分析不同恶意软件家族之间的功能关系。</strong></li>
<li><strong>提出一种在静态分析中使用机器学习的新颖实用方法</strong>：（1) AE训练是在一种无监督方式下进行的，<strong>无需为系统标注任何样本</strong>，就可以产生突出显示恶意软件组件的实用程序，(2) 分类是以半监督方式完成，<strong>具有最小的干预</strong>：分析人员的常规工作流的注释用作标签，群集中的大多数标签用于对相关的恶意软件组件进行分类。</li>
<li>本文提出了一种解释框架（如我们提出的 AE 或 SHAP）定位恶意软件重要部分的方法，该方法可以<strong>映射回原始二进制或控制流图的特征</strong>。</li>
</ul>
<h3><span id="3-scope-amp-overview">3 <strong>Scope &amp; Overview</strong></span></h3><h4><span id="31-motivation">3.1 Motivation</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550446.png" alt="图片" style="zoom: 67%;"></p>
<font color="red">**图1展示了一个典型的恶意软件分析师Molly的工作流程** </font>。当给定一个恶意软件样本，Molly的任务是了解该样本在做什么，以便她写一份技术报告并改进公司的检测系统，从而在未来识别该类样本。

1.  **首先查询VT（virtotul）和其他组织**，以确定他们以前是否见过这个特定的样本，然而并没有
2.  在一个自定义的**沙箱中执行样本以了解其动态行为**，然而没有显示任何恶意行为或拒绝执行；运行一些内部工具，诱使恶意软件执行其隐藏的行为，但仍无效时；
3.  尝试**脱壳（unpacking）**和**静态逆向分析恶意样本**，以了解其潜在行为
4.  <font color="red">**在反汇编程序（IDA Pro 或 BinaryNinja）中打开脱壳后的样本，被数千个函数淹没，接着运行各种静态签名检测工具来识别恶意软件的某些特定恶意组件，但仍无效**</font>
5.  逐个**查看每个函数（可能通过 API 调用和字符串过滤）以尝试了解它们的行为**
6.  **在分析样本的行为后，撰写分析报告（包含基本信息、IOC、静态签名等）**

然而，当新的样本出现时，Molly需要重复同样的任务。由于这种重复的体力劳动，这项工作对Molly来说变得单调乏味和耗时。<font color="red"> **DEEPREFLECT旨在减轻恶意分析师的分析工作，能逆向一个未知的恶意软件样本，从而减轻他们繁重的任务，并为相似的函数标注行为标签。**</font>

<h4><span id="32-proposed-solution">3.2 Proposed Solution</span></h4><p>我们提出了<strong>DEEPREFLECT</strong>，该工具能：</p>
<ul>
<li><p><strong>定位恶意软件binary中的恶意函数</strong></p>
<blockquote>
<p>  locates malicious functions within a malware binary</p>
</blockquote>
</li>
<li><p><strong>描述这些函数的行为</strong></p>
<blockquote>
<p>  describes the behaviors of those functions</p>
</blockquote>
</li>
</ul>
<p>虽然分析人员可能首先尝试通过搜索特定的字符串和API调用来静态地识别行为，但这些行为很容易被分析人员混淆或隐藏（ obfuscated or hidden）。<strong>DEEPREFLECT没有做出这样的假设，并试图通过控制流图(control-flow graph，CFG)特性和API调用（API calls）的组合来识别这些相同的行为</strong>。</p>
<font color="red"> **DEEPREFLECT通过学习正常情况下良性的二进制函数来工作**</font>。因此，任何异常都表明这些函数不会出现在良性二进制文件中，而可能被用于恶意行为中。这些异常函数更可能是恶意函数，分析师可以只分析它们，从而缩小工作范围。如图5所示，DEEPREFLECT将分析师必须分析的函数数量平均减少了 85%。此外，实验表明我们的方法优于旨在实现相同目标的基于签名的技术。

<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550151.png" alt="图片" style="zoom:67%;">

#### 3.3 Research Goals

本文有四个主要目标：

+   准确地识别恶意软件样本中的恶意活动
+   帮助分析人员在静态分析恶意软件样本时集中注意力
+   **处理新的（不可见的）恶意软件家族**
+   **深入了解恶意软件家族的关系和趋势**

### 4、模型设计

#### 4.1 总体框架

**DEEPREFLECT的目标是识别恶意软件二进制中的恶意函数**。在实践中，<font color="red">**它通过定位异常基本块（感兴趣区域 regions of interest，RoI)来识别可能是恶意的函数**</font>。然后，分析人员必须确定这些函数是恶意行为还是良性行为。DEEPREFLECT有两个主要步骤，如图2所示：

-   **RoI检测（RoI detection）**：通过AE（AutoEncoder）来执行的
-   **RoI注释（RoI annotation）**：通过对每个函数的所有RoI聚类，并将标记聚类结果来执行注释。注意，一个函数可能有多个ROI，用每个函数自己的ROI的均值表示该函数，然后对函数聚类

![图片](https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550631.png)

##### （1）术语 Terminology

**首先定义恶意行为（malicious behaviors）的含义**。我们根据识别恶意软件源代码的**核心组件**（例如，拒绝服务功能、垃圾邮件功能、键盘记录器功能、命令和控制C&C功能、利用远程服务等）来生成真实情况（ground-truth）。<font color="red">**通过MITRE ATT&CK框架描述**</font>，如表3所示。

![图片](https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550843.png)

**然而，当静态逆向工程评估恶意软件二进制文件时（即在野生恶意软件二进制 in-the-wild malware binaries），我们有时无法肯定地将观察到的低级函数归因于更高级别的描述**。

例如，恶意软件可能会因为许多不同的原因修改注册表项，但有时确定哪个注册表项因什么原因而被修改是很困难的，因此只能粗略地标记为“<font color="red">`防御逃避：修改注册表（Defense Evasion: Modify Registry）`” </font>。即使是像CAPA这样的现代工具，也能识别出这些类型的模糊标签。**因此，在我们的评估中，我们将“恶意行为”表示为可由MITRE ATT&CK框架描述的函数。**

##### （2） **RoI Detection**

**检测的目标是自动识别恶意软件二进制文件中的恶意区域**。例如，我们希望检测C&C逻辑的位置，而不是检测该逻辑的特定组件（例如，网络API调用connect()、send() 和 recv()）。**RoI检测的优点是分析人员可以快速定位启动和操作恶意行为的特定代码区域**。先前的工作只关注于创建临时签名，简单地将二进制文件标识为恶意软件或仅基于API调用的某些函数。这对于分析人员扩大他们的工作特别有用（即不仅仅依赖手动逆向工程和领域专业知识）。

##### **(3) RoI Annotation**

**注释的目标是自动标记包含RoI的函数的行为，即识别恶意函数在做什么**。由于分析人员为标记集群所执行的初始工作是一个长尾分布。也就是说，只需要前期做比较重要的工作，随着时间推移，工作量会减少。这个过程的优点很简单：它为分析人员提供了一种自动生成未知样本的报告及见解的方法。例如，如果恶意软件示例的变体包含与之前的恶意软件示例相似的逻辑（但对于分析人员来说看起来不同以至于不熟悉），我们的工具为他们提供了一种更快实现这一点的方法。

#### 4.2 RoI Detection

首先介绍了AutoEncode（AE）神经网络。此外，先前的工作已经证明，当自动编码器在良性分布上进行训练时，AE可以检测到恶意（异常）行为。我们的假设是，与良性二进制文件相比，恶意软件二进制文件将包含相似但独特的功能。

当使用大量良性样本训练AE后，给定一个随机的样本，可以利用公式(2)计算，超过**MSE**的即认为是恶意区域，突出显示ROI异常基本块。与先前识别整个样本为恶意区域的工作相比，我们识别了每个样本中的恶意区域。具体而言，我们计算的 `localized MSE` 定义如下：
$$
\operatorname{LMSE}(x, \hat{x})=\left(x^{(i)}-\hat{x}^{(i)}\right)^{2}
$$

#### <font color="red"> （1）**Features**</font>

<p>为了在二进制样本中定位恶意行为的位置，编码使用的特征必须一对一的映射回原样本。<strong>因此，作者将每个二进制文件表示为一个 m×c 的矩阵，该矩阵使用c个静态特征捕获前m个基本块以总结样本的behavior</strong>。<strong>m设置为20k个基本块，是因为95%的数据集样本具有20k或者更少的基本块， c设置为18个特征</strong>。<strong>基本块</strong>通常是以控制传输指令结尾的一系列指令。当然，根据反汇编程序的不同，基本块可能会有不同的表示，因此这种严格的定义可能不适用于所有静态恶意软件分析系统。</p>
<p>我们特征（c）的灵感来自于先前工作中发现的特征，即<strong>属性控制流图（attributed control flow graph，ACFG）</strong>特征<strong>[23,75]</strong>。在这些工作中，ACFG特征被选择来执行二进制相似性，因为它们假设这些特征(由结构和数字CFG特征组成)将在多个平台和编译器上是一致的。</p>
<blockquote>
<p>  <strong>[23] Scalable graph-based bug search for firmware images.  2016 CCS</strong></p>
<p>  <strong>[75] Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection. 2017 CCS</strong></p>
</blockquote>
<p>虽然可以说我们的目标是相似的（即识别二进制文件之间的异同），但我们专门为研究恶意软件定制了这些功能。特别是，我们选择了autoencoder要使用的功能，以捕获更高级别的行为。我们的特征包括每个<strong>基本块中的指令类型计数</strong>（为ACFG特征提取的指令类型的更详细形式）、<strong>CFG的结构特征</strong>和<strong>API调用类别</strong>（用于总结恶意软件程序行为[18]），将每个基本块总结如下：</p>
<h5><span id="a-structural-characteristics"><font color="red">(a) <strong>Structural Characteristics</strong> </font></span></h5><p><strong>结构特征2个</strong>，每个<strong>基本块的后代（offspring）数量</strong>和<strong>betweenness score</strong>，可以描述不同功能的<strong>控制流结构</strong>，比如网络通信（connect, send, recv）或文件加密（findfile, open, read, encrypt, write, close）。如图所示:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550517.png" alt="image-20220602164403223" style="zoom:67%;"></p>
<blockquote>
<p>  该恶意软件通过InternetOpenUrlA() 访问URL，通过CreateFileA() 创建文件，并通过InternetReadFile() 和WriteFile() 将从连接接收的数据写入文件。</p>
</blockquote>
<p><strong>(b) Arithmetic Instructions</strong></p>
<p><strong>算术指令3个</strong>，每个<strong>基本块基本数学、逻辑运算、位移指令的数量</strong>（“basic math”, “logic operation”, and “bit shifting”）。这些算术指令特征可以用来表示如何对更高层次的行为执行数学运算，以及数字如何与函数交互。例如，加密函数可能包含大量的xor指令，混淆函数可能包含逻辑和位移操作的组合等。<strong>我们从《英特尔体系结构软件开发人员手册》[26]中检索到这些说明</strong>。此外，我们还提供了一个恶意软件示例，在图中展示了这些类型的功能。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550537.png" alt="image-20220602164519015" style="zoom:67%;"></p>
<blockquote>
<p>  此函数对数据执行各种按位操作。类似这样的复杂逻辑可以解释为执行某种除臭或解码，以隐藏恶意软件解释或收集的数据。</p>
</blockquote>
<p><strong>(c) Transfer Instructions</strong></p>
<p><strong>转移指令</strong>3个，每个基本块<strong>内堆栈操作，寄存器操作和端口操作的数量</strong>（“stack operation”, “register operation”, and “port operation”）。这些底层特征可描述更高级别函数的<strong>传输操作</strong>，比如函数的参数和返回值是如何与函数内其余数据交互的，从而描述更复杂的逻辑和数据操作。<strong>例如去混淆、解密函数可能设计更多move-related指令，C&amp;C逻辑设计更多堆栈相关指令</strong>。因为它调用了更多的内部/外部函数。我们同样从《英特尔体系结构软件开发人员手册》中检索到了这些说明</p>
<p><strong>(d) API Call Categories</strong></p>
<p><strong>API类别10个</strong>，我们使用的API调用特性是每个基本块中与<strong>“文件系统”、“注册表”、“网络”、“DLL”、“对象”、“进程”、“服务”、“同步”、“系统信息”和“时间”相关的API调用的数量</strong>。这些类别的灵感来自<strong>priorwork for malware clustering</strong>[18]。这些特性可用于表示执行恶意活动（如网络通信和文件系统、注册表和进程操作）所需的高级库操作。由于这些直接表示高级行为，因此它们对于理解函数的总体行为至关重要。<strong>下图显示了利用这些不同调用类型执行不同行为的恶意软件功能的示例。</strong></p>
<blockquote>
<p>  <strong>[18] Scalable, Behavior-Based Malware Clustering. NDSS 2009.</strong></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550690.png" alt="image-20220602164417162" style="zoom:67%;"></p>
<blockquote>
<p>  此函数用于搜索具有特定扩展名（即doc、jpg等）的各种文件。然后将这些文件复制到单独的位置。此行为可能是针对其他恶意行为的设置，如数据外泄或勒索。</p>
</blockquote>
<p><strong>我们认为，与经典的ACFG功能相比，这些功能更适合恶意软件</strong>，因为（1）它们包括在priorwork中用于恶意软件检测的API调用（2）<strong>指令类别更细粒度，允许每个基本块中有更多的上下文</strong>（如前所述）以及（3）<font color="red"><strong>它们不依赖太容易规避攻击的字符串</strong></font>[77]。当然，如果有一个有动机的对手，任何机器学习模型都可能受到攻击和欺骗，从而产生错误和意外的输出。虽然我们的功能和模型也不例外，但我们认为它们足以产生可靠的模型（即，其行为符合预期），并使其变得足够困难，以至于对手必须广泛地工作以产生误导性的输入（如中所示 4.7). 有关对我们系统的潜在攻击的讨论，请参阅 5.</p>
<h4><span id="2模型">（2）模型</span></h4><p><strong>Autoencoder使用U-Net模型，U-Net的优点是其在编码器和解码器之间有跳过连接（skip connections），对样本x可以跳过某些特征的压缩以在重构的x’中保持更高的保真度</strong>。</p>
<p>首先收集大量的良性样本，对每个binary抽取上述18个静态特征用于表示该binary。设有用feature表示的样本x，AE重构后得到x’，训练的目标是最小化重构损失，即输入x和输出x’之间的损失。</p>
<p><strong>RoI Detection会在m个基本块中检测出一些异常基本块</strong>。这些基本块分别属于不同的函数，使用例如BinaryNinja的工具就可以确定ROI属于哪些函数，即认为这些函数可能是恶意函数，也就完成了恶意函数定位的任务。<strong>后续RoI Annotation就是对这些函数聚类，完成恶意函数行为标记（分类）的任务。</strong></p>
<h4><span id="43-roi-annotation">4.3 RoI Annotation</span></h4><p><strong>给定一个新样本x，我们希望识别其每个函数的行为（类别），并将其报告给Molly</strong>。由于标记所有的函数都是不实用的，所以我们只注释了少量的函数，并使用聚类分析来传播结果。</p>
<h5><span id="1clustering-features">（1）<strong>Clustering Features</strong></span></h5><p>假设一组脱壳恶意软件，按上述特征提取方式（18种特征）得到每个binary的特征表示，其中一个binary为x。</p>
<p> <strong>（2）Clustering Model</strong></p>
<p>使用PCA将特征数从18降维至5，然后使用HDBSCAN算法对5维特征聚类。</p>
<h3><span id="五-实现">五、实现</span></h3><p>接下来，我们将描述如何部署和使用它。</p>
<p><strong>(1) Initialization</strong></p>
<ul>
<li>首先对良性和恶意binaries脱壳</li>
<li>提取binary静态特征，形成20×18的矩阵</li>
<li><strong>用良性样本训练AutoEncoder</strong></li>
<li><strong>使用训练好的AE从恶意样本中提取ROIs，即恶意基本块位置</strong></li>
<li>计算恶意二进制中恶意函数的行为表示，加入聚类的训练集D</li>
<li>PCA降维并聚类生成C</li>
</ul>
<p>人工分析恶意软件手动打标，这些label注释到聚类训练集中，从而评估实验结果。换句话说，每个cluster只需要其中几个函数的label，就可确定整个cluster的label，即确定整个cluster中函数的恶意行为。</p>
<p><strong>(2) Execution</strong></p>
<p>当Molly收到一个新的样本x，DeepReflect会自动定位恶意函数并标注恶意行为。</p>
<ul>
<li><strong>对样本x执行脱壳（unpack）</strong></li>
<li>通过AutoEncoder获取ROIs</li>
<li>使用BinaryNinja以及ROIs确定恶意函数集合，然后计算恶意函数的行为表示</li>
<li>PCA模型降维</li>
<li>计算每个恶意函数最相近的集群，通过计算和聚类中心的距离实现</li>
<li>分配大数据集群注释给函数</li>
</ul>
<p>接下来，Molly分析highlighted functions，从而实现：</p>
<ul>
<li>obtains a better perspective on what the malware is doing</li>
<li>annotates any function labeled “unknown” with the corresponding MITRE category (dynamically updating D)</li>
<li>observe shared relationships between other malware samples and families by their shared clusters（共享关系，分析恶意软件家族的相关性）</li>
</ul>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（7）【draft】CADE: Detecting and Explaining Concept Drift Samples for Security Applications</title>
    <url>/posts/3JZF773/</url>
    <content><![CDATA[<h2><span id="cade-detecting-and-explaining-concept-drift-samples-for-security-applications">CADE: Detecting and Explaining Concept Drift Samples for Security Applications</span></h2><p>原文作者：Limin Yang, <em>University of Illinois at Urbana-Champaign</em> </p>
<p>原文链接：<a href="https://www.usenix.org/conference/usenixsecurity21/presentation/yang-limin">https://www.usenix.org/conference/usenixsecurity21/presentation/yang-limin</a></p>
<p>发表会议：USENIXSec 2021</p>
<p><strong>代码地址</strong>：<a href="https://github.com/whyisyoung/CADE">https://github.com/whyisyoung/CADE</a></p>
<h3><span id="摘要">摘要</span></h3><p>概念漂移对部署机器学习模型来解决实际的安全问题提出了严峻的挑战。<strong>由于攻击者（和/或良性对手）的动态行为变化，随着时间的推移，测试数据分布往往会从原始的训练数据转移，从而导致部署的模型出现重大故障</strong>。</p>
<p>为了对抗概念漂移，我们提出了一种新的系统CADE，旨在（1）<strong>检测偏离现有类别的漂移样本</strong>；（2）<strong>解释检测到漂移的原因</strong>。与传统方法不同（需要大量新标签来统计确定概念漂移），我们的目标是在单个漂移样本到达时识别它们。认识到高维离群空间带来的挑战，我们建议将数据样本映射到低维空间，并自动学习距离函数来度量样本之间的相异性。通过对比学习，我们可以充分利用训练数据集中现有的标签来学习如何对样本进行比较和对比。<strong>为了解释检测到的漂移的意义，我们开发了一种基于距离的解释方法</strong>。我们表明，在这个问题背景下，解释“距离”比传统方法更有效，传统方法侧重于解释“决策边界”。我们通过两个案例来评估CADE：Android恶意软件分类和网络入侵检测。我们进一步与一家安全公司合作，在其恶意软件数据库上测试CADE。我们的结果表明，CADE可以有效地检测漂移样本，并提供语义上有意义的解释。</p>
<h3><span id="一-说明">一、说明</span></h3><p>由于概念漂移，部署基于机器学习的安全应用程序可能非常具有挑战性。无论是恶意软件分类、入侵检测还是在线滥用检测[6、12、17、42、48]，基于学习的模型都是在“封闭世界”假设下工作的，期望测试数据分布与训练数据大致匹配。然而，部署模型的环境通常会随着时间的推移而动态变化。这种变化可能既包括良性玩家的有机行为变化，也包括攻击者的恶意突变和适应。因此，测试数据分布从原始训练数据转移，这可能会导致模型出现严重故障[23]。</p>
<blockquote>
<p>  [23] A survey on concept drift adaptation. ACM computing surveys (CSUR), 2014.</p>
</blockquote>
<p>为了解决概念漂移问题，大多数基于学习的模型需要<strong>定期重新培训</strong>[36、39、52]。然而，再培训通常需要标记大量新样本（昂贵）。更重要的是，还很难确定何时应该对模型进行再培训。延迟的再培训会使过时的模型容易受到新的攻击。</p>
<font color="red">**我们设想，对抗概念漂移需要建立一个监控系统来检查传入数据流和训练数据（和/或当前分类器）之间的关系**</font>。图1说明了高级思想。当原始分类器在生产空间中工作时，另一个系统应定期检查分类器对传入数据样本做出决策的能力。**A检测模块(1) 可以过滤正在远离训练空间的漂移样本**。更重要的是，为了**解释漂移的原因（例如，攻击者突变、有机行为变化、以前未知的系统错误）**，我们需要一种解释方法(2) 将检测决策与语义上有意义的特征联系起来。这两项功能对于为开放世界环境准备基于学习的安全应用程序至关重要。

<img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550835.png" alt="image-20220605202728095" style="zoom: 67%;">

之前的工作已经探索了通过直接检查原始分类器（0）的预测置信度来检测漂移样本的方法 **[32]**。置信度较低可能表明传入样本是漂移样本。然而，该置信度得分是基于所有类别已知（封闭世界）的假设计算的概率（总和为1.0）。不属于任何现有类别的漂移样本可能会被分配到错误的类别，并具有很高的置信度（已通过现有工作验证[25、32、37]）。最近的一项工作提出了计算传入样本和每个现有类之间的不一致性度量的想法，以确定适合度[38]。**该不合格度量基于距离函数计算，以量化样本之间的不相似性**。**然而，我们发现这种距离函数很容易失效，尤其是当数据稀疏且维数较高时。**

>   **[32] A baseline for detecting misclassified and out-of-distribution examples in neural networks.**

**我们的方法**。在本文中，我们提出了一种检测漂移样本的新方法，并结合一种解释检测决策的新方法。我们共同构建了一个称为CADE的系统，它是“用于漂移检测和解释的对比自动编码器 (“**Contrastive Autoencoder for Drifting detection and Explanation**)”的缩写关键的挑战是**推导一个有效的距离函数来衡量样本的相异性**。我们没有随意选取距离函数，而是利用对比学习的思想[29]，根据现有的标签，从现有的训练数据中学习距离函数。给定原始分类器的训练数据（多个类别），我们将训练样本映射到低维潜在空间。映射函数通过对比样本来学习，以扩大不同类样本之间的距离，同时减少同一类样本之间的距离。**我们证明了在潜在空间中得到的距离函数可以有效地检测和排序漂移样本。**

评价我们使用两个数据集评估我们的方法，包括**Android恶意软件数据集[7]和2018年发布的入侵检测数据集[57]**。我们的评估表明，我们的漂移检测方法具有很高的准确性，F1平均得分为0.96或更高，优于各种基线和现有方法。我们的分析还表明，使用对比学习可以减少检测决策的模糊性。对于解释模型，我们进行了定量和定性评估。案例研究还表明，所选特征与漂移样本的语义行为相匹配。

此外，我们还与一家安全公司的合作伙伴合作，在其内部恶意软件数据库上测试CADE。作为初步测试，我们从395个家庭中获得了2019年8月至2020年2月出现的20613个Windows PE恶意软件样本。这使我们能够在不同的环境中测试更多恶意软件系列的系统性能。结果很有希望。<font color="red">**例如，CADE在10个家庭中进行训练并在160个以前未见过的家庭中进行测试时，F1成绩达到0.95分。这使得人们有兴趣在生产系统中进一步测试和部署CADE。** </font>

<h4><span id="贡献">贡献：</span></h4><p>本文有三个主要贡献。</p>
<ul>
<li>我们提出CADE来补充现有的基于监督学习的安全应用程序，以对抗概念漂移。提出了<strong>一种基于对比表征学习的漂移样本检测方法</strong>。</li>
<li>我们说明了监督解释方法在解释异常样本方面的局限性，并<strong>介绍了一种基于距离的解释方法</strong>。</li>
<li>我们通过两个应用对所提出的方法进行了广泛的评估。我们与一家安保公司的初步测试表明，CADE是有效的。我们在此处发布了CADE代码1，以支持未来的研究。</li>
</ul>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（8）【draft】Heterogeneous Temporal Graph Transformer: An Intelligent System for Evolving Android Malware Detection</title>
    <url>/posts/2MYAND2/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（9）Forecasting Malware Capabilities From Cyber Attack Memory Images</title>
    <url>/posts/1J98PCJ/</url>
    <content><![CDATA[<h2><span id="forecasting-malware-capabilities-from-cyber-attack-memory-images">Forecasting Malware Capabilities From Cyber Attack Memory Images</span></h2><blockquote>
<ul>
<li>项目：<a href="https://www.usenix.org/conference/usenixsecurity21/presentation/alrawi-forecasting">https://www.usenix.org/conference/usenixsecurity21/presentation/alrawi-forecasting</a></li>
<li>文章：<a href="https://arxiv.org/abs/1907.07352">https://arxiv.org/abs/1907.07352</a></li>
</ul>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p>正在进行的网络攻击的补救依赖于及时的恶意软件分析，其目的是发现尚未执行的恶意功能。不幸的是，这需要在不同工具之间重复切换上下文，并且会给分析员带来很高的认知负荷，从而减慢调查速度，并给攻击者带来优势。<strong>我们提出了Forecast，这是一种事后检测技术，可使事件响应者自动预测恶意软件的执行能力。预测基于概率模型，该模型允许预测发现能力，并根据其相对执行可能性（即预测）对每个能力进行权衡</strong>。<strong><font color="red"> Forecast利用当前攻击的执行上下文（来自恶意软件的内存映像）来指导恶意软件代码的符号分析。</font></strong>我们进行了广泛的评估，有6727个真实世界的恶意软件和旨在颠覆预测的未来主义攻击，显示了预测恶意软件能力的准确性和鲁棒性。</p>
<blockquote>
<p>  Forecast is based on a probabilistic model that allows Forecast to discover capabilities and also weigh each capability according to its relative likelihood of execution</p>
</blockquote>
<h3><span id="一-说明">一、说明</span></h3><p>网络攻击响应需要对抗阶段性恶意软件功能（即尚未执行的恶意功能），以防止进一步的损害[1]、[2]。不幸的是，检测后预测恶意软件功能仍然是手动的、繁琐的和容易出错的。目前，分析师必须重复执行多个分类步骤。例如，分析员通常会将二进制文件加载到静态反汇编程序中，并执行内存取证，以组合静态和动态工件。这一艰苦的过程需要在二进制分析和取证工具之间进行上下文切换。因此，它给分析员带来了很高的认知负荷，减慢了调查速度，给攻击者带来了优势。</p>
<p><strong>为了自动化事件响应，符号执行很有可能用于恶意软件代码探测，但缺少先前的攻击执行状态，而这种状态在事后可能无法重新实现（例如，来自C&amp;C活动的具体输入）</strong>。特定于环境的条件（如预期的C&amp;C命令）限制了动态和协同技术（如[3]–[14]）预测无法访问的能力。此外，这些技术依赖于剖析独立恶意软件二进制文件或在沙箱中运行它。然而，已知恶意软件会删除其二进制文件或将自身锁定为仅在受感染的计算机上运行（硬件锁定）。<strong>更糟糕的是，研究人员发现，无文件恶意软件事件（即，仅驻留在内存中）继续增加[1]、[15]、[16]</strong>。</p>
<p>有权访问正确的执行上下文对于引导恶意软件显示其功能是必要的。恶意软件在内部收集来自特定环境源的输入，例如注册表、网络和环境变量，以便做出行为决策【11】、【17】、【18】。因此，恶意软件的理想和实用输入公式可以根据内存中的该内部执行状态进行调整，其中包含已收集的输入工件。<strong>事实证明，在检测到恶意进程后，反病毒和IDS已经收集了该进程的内存映像。恶意软件内存映像包含被调查的特定攻击实例特有的这种内部具体执行状态。</strong></p>
<p><strong><font color="red"> 在我们的研究过程中，我们注意到，如果我们可以在内存图像中设置代码和数据页的动画，并根据捕获的快照执行向前的代码探索，那么我们就可以重用这些早期的具体执行数据来推断恶意软件的下一步。</font></strong>此外，通过分析这些具体输入如何在代码探索期间诱导路径，我们可以根据恶意软件捕获的执行状态预测哪些路径更有可能执行功能。基于这一思想，<strong><font color="red"> 我们提出用通过内存图像取证获得的具体执行状态对恶意软件的预阶段路径进行符号探索。</font></strong>通过这一点，我们克服了分析员之前必须进行的艰苦而繁重的认知过程。</p>
<p><strong>我们介绍了Forecast，这是一种后检测技术，使事件响应者能够从捕获的内存图像中预测可能的功能。Forecast根据发现的每个功能的执行概率（即预测）对其进行排序，以使分析师能够确定其修复工作流的优先级。</strong>为了计算这种概率，Forecast会衡量每条路径对具体数据的相对使用情况。这种方法基于内存映像执行状态的具体程度（或DC）的形式化模型。Forecast从内存映像中的最后一个指令指针（IP）值开始，通过象征性地执行每条指令的CPU语义来探索每条路径。在此探索过程中，预测建模符号和具体数据的混合如何影响路径生成和选择。基于这种混合，沿路径计算每个状态的“具体性”分数，以得出每个发现能力的预测百分比。DC（s）还通过动态调整循环边界、处理符号控制流和修剪路径来优化符号分析，以减少路径爆炸。</p>
<p>为了自动识别每个功能，我们开发了几个模块化的功能分析插件：<strong>代码注入、文件过滤、滴管、持久性、键和屏幕监视、反分析和C&amp;C URL连接</strong>。每个插件根据API序列、它们的参数以及它们的输入和输出约束如何连接每个API来定义一个给定的功能。Forecast插件是可移植的，可以很容易地扩展以捕获基于目标系统API的其他功能。值得注意的是，Forecast的分析只需要一个法医记忆图像，允许它对无文件恶意软件工作，使其非常适合事件响应。</p>
<p>我们使用涵盖274个家族的6727个真实恶意软件（包括打包和解包）的内存图像评估Forecast。与人工专家手动生成的报告相比，Forecast可提供准确的能力预测。此外，我们还表明，Forecast对旨在颠覆Forecast的未来主义攻击具有鲁棒性。我们表明，预测的检测后预测是由早期的具体输入准确诱导的。我们将预测与S2E【6】、angr【22】和Triton【23】进行了经验比较，发现预测在识别能力和减少路径爆炸方面优于它们。预测可在线访问：<a href="https://cyfi.ece.gatech.edu/">https://cyfi.ece.gatech.edu/</a>.</p>
<blockquote>
<p>  “S2E: A platform for in-vivo multi-path analysis of software systems,” 2011</p>
<p>  “SoK: (State of) The Art of War: Offensive Techniques in Binary Analysis,” 2016</p>
<p>  “Triton: A Dynamic Symbolic Execution Framework” 2015</p>
</blockquote>
<h3><span id="二-概述">二、概述</span></h3><p>本节介绍了结合<strong>记忆图像取证和符号分析技术</strong>的挑战和好处。以<strong>DarkHotel事件</strong>[2]为例，我们将展示事件响应者如何利用预测加快调查并补救网络攻击。</p>
<p><strong><font color="red"> 运行示例：DarkHotel APT。</font></strong>DarkHotel是一种APT，通过矛式网络钓鱼攻击C&amp;C服务器【2】。感染后，DarkHotel会从受害者的文件系统中删除其二进制文件，与C&amp;C服务器通信，将线程注入Windows资源管理器，并最终过滤侦察数据。当IDS检测到受感染主机上的异常活动时，终端主机代理会捕获可疑进程内存（即DarkHotel内存），终止其执行，并生成通知。<strong>此时，事件响应者必须从不同的可用取证来源（网络日志、事件日志、内存快照等）快速了解DarkHotel的能力，以防止进一步的损坏。</strong></p>
<p>动态技术【11】–【14】可能需要一个激活的C&amp;C以诱使恶意软件二进制文件显示其功能，然而该C&amp;C可能已被取下。由于DarkHotel只驻留在内存中，这些通过在沙盒中运行恶意软件而起作用的技术无法应用。</p>
<ul>
<li><strong>只有内存图像，分析员可以使用取证工具，如Volatility</strong>【24】，来“切割”内存图像代码和数据页。基于提取的代码页，符号分析可以模拟恶意软件的执行，以探索所有潜在的路径。</li>
<li>不幸的是，现有的符号工具需要一个格式正确的二进制文件，并且没有针对内存图像进行优化[7]、[22]、[23]。</li>
</ul>
<p><strong>理想情况下，分析员可以手动将这些代码片段投影到符号分析中，并从数据页中获取具体值，以告诉哪一个代码分支会导致某个功能</strong>。然而，这种用提取的内存工件“缝合”代码的来回过程涉及符号执行和取证工具之间的上下文切换。这给分析员带来了很高的认知负担。<strong>分析员还必须处理路径爆炸、API调用模拟[4]、[22]、[25]–[27]和具体化API参数（例如攻击者的URL）等挑战，这些挑战可能无法在内存映像中静态访问</strong>。最后，分析师必须沿每条路径手动检查API，以推断高级功能。</p>
<h4><span id="21混合事件响应">2.1混合事件响应</span></h4><p>事件响应者依靠内存取证来识别内存图像中的攻击瑕疵。然而，仅仅是基于签名的记忆取证，由于高漏报率，会遗漏重要的数据结构【21】。另一方面，符号执行可以正向探索代码，但会遇到路径爆炸等问题[22]。为了解决这些限制，Forecast通过反馈循环将符号执行和内存取证结合起来，以解决这两种技术的缺点。</p>
<p><strong>上下文感知记忆取证</strong>。符号分析提供代码探索上下文，以准确识别内存取证遗漏的数据构件。例如，DarkHotel内存图像的传统法医解析遗漏了C&amp;C URL字符串，因为它们通过自定义编码方案进行了混淆。然而，对引用这些字节作为参数的指令（如strncpy API）进行的后续符号分析允许Forecast正确识别和利用内存映像中的这些数据瑕疵。此外，有针对性的恶意软件可能会采用旨在颠覆预测的策略，使用反取证和反符号分析，这是我们在设计和评估中仔细考虑的。</p>
<p>内存图像取证提供了具体的输入，可以帮助符号分析执行地址具体化、控制流解析和循环边界。此外，内存取证可以识别内存中加载的库地址，从而允许Forecast执行库函数模拟。</p>
<p>路径概率。给定一个内存映像，目标是利用可用的具体数据来探索潜在的代码路径，并预测沿途的功能。通过分析具体内存图像数据是如何产生不同路径的，Forecast可以得出一条路径相对于其他路径达到某种能力的概率。Forecast基于建模具体和符号数据操作如何影响路径生成和选择来计算此概率。Forecast还利用此概率度量作为一种启发式方法来修剪具有最少具体数据的路径。</p>
<h4><span id="22-incident-response-with-forecast">2.2 Incident Response with Forecast</span></h4><p><strong>Forecast可识别来自自动管道中恶意软件内存映像的功能</strong>。为了证明这一点，我们模拟了DarkHotel的攻击和内存捕获，其中包括使用DarkHotel的网络签名设置IDS并执行高级持久性威胁（APT）。检测完成后，IDS向终端host agent发出信号，以捕获DarkHotel进程内存。然后，我们输入该内存图像进行预测以进行分析。在459秒内，<strong><font color="red"> Forecast显示了DarkHotel的功能：C&amp;C通信（即mse.vmmnat.com）、文件过滤（即主机信息）和代码注入（即注入Windows Explorer）。</font></strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191550504.png" alt="image-20220624145017568"></p>
<p>处理法医记忆图像有六个阶段，如图1所示。1 Forecast以取证方式解析内存映像，并通过将最后一个CPU和内存状态加载到符号环境中进行分析来重建先前的执行上下文。在分析内存映像时，Forecast检查加载的库，以识别导出的函数名和地址。接下来，2 Forecast继续探索可能的路径，利用内存映像中可用的具体数据来具体化路径约束。3预测模型和权重每个路径如何由具体数据归纳，并为每个生成的路径分配概率。4 Forecast然后使用此概率作为权重来调整循环边界并修剪错误路径，从而允许Forecast缩小诱导能力相关路径的范围。5 Forecast将已识别的API与功能分析插件库相匹配，以向分析师报告功能。最后，6 Forecast确定了三种能力，并从路径概率中得出了它们的预测百分比，分别为31%、15%和54%。</p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>高级威胁发现（3）SLEUTH: Real-time Attack Scenario Reconstruction from COTS Audit Data</title>
    <url>/posts/M5Q53E/</url>
    <content><![CDATA[<h2><span id="sleuth-real-time-attack-scenario-reconstruction-from-cots-audit-data">SLEUTH: Real-time Attack Scenario Reconstruction from COTS Audit Data</span></h2><ul>
<li><a href="https://blog.csdn.net/Sc0fie1d/article/details/104273798">https://blog.csdn.net/Sc0fie1d/article/details/104273798</a></li>
</ul>
<h3><span id="摘要">摘要</span></h3><p>本文提出了一种实时重构企业主机上攻击场景的方法和系统。为了满足问题的<strong>可伸缩性</strong>和<strong>实时需求</strong>，我们开发了一个平台中立的、基于主存的、审计日志数据的依赖图抽象方法。然后，我们提出了高效的、基于标签的攻击检测和重建技术，包括源识别和影响分析。我们还开发了一些方法，通过构建紧凑的攻击步骤的可视化图来揭示攻击的大局。我们的系统参与了由DARPA组织的红色团队评估，并能够成功地检测并重建了红色团队对运行Windows、FreeBSD和Linux的主机的攻击细节</p>
<h3><span id="一-说明">一、说明</span></h3><p>我们正在目睹由熟练的对手进行的有针对性的网络攻击(“企业高级和持续威胁(APTs))[1]的迅速升级。通过将社会工程技术（例如，鱼叉式网络钓鱼）与先进的开发技术相结合，这些对手通常会绕过广泛部署的软件保护系统，如ASLR、DEP和沙箱。因此，企业越来越依赖于二线防御，例如，<strong>入侵检测系统(IDS)、安全信息和事件管理(SIEM)工具、身份和访问管理工具，以及应用程序防火墙。虽然这些工具通常很有用，但它们通常会生成大量的信息，这使得安全分析师很难区分真正重要的攻击——众所周知的“大海捞针”——从背景噪音</strong>。此外，分析人员缺乏“连接这些点”的工具，即，将跨越多个应用程序或主机并在长时间扩展的攻击活动的碎片拼凑起来。相反，需要大量的手工努力和专业知识来整理由多个安全工具发出的众多警报。因此，许多攻击活动被错过了数周甚至数月的[7,40]。</p>
<p>为了有效地控制高级攻击活动，分析人员需要新一代的工具，不仅帮助检测，而且还生成一个总结攻击的因果链的紧凑总结。这样的摘要将使分析人员能够快速确定是否存在重大入侵，了解攻击者最初是如何违反安全规则的，并确定攻击的影响。</p>
<p>将导致攻击的事件的因果链拼接在一起的问题首先在反向跟踪器[25,26]中进行了探索。随后的研究[31,37]提高了由反向跟踪器构建的依赖链的精度。然而，这些工作在一个纯粹的法医环境中运行，因此不处理实时进行分析的挑战。相比之下，本文提出了侦探（<strong>SLEUTH</strong>），一个系统，该系统可以实时提醒分析师一个正在进行的活动，并在攻击后的几秒钟或几分钟内为他们提供一个紧凑的、直观的活动摘要。这将使在对受害者企业造成巨大损害之前及时作出反应。实时攻击检测和场景重构提出以下几点：</p>
<ul>
<li><strong>事件存储和分析</strong>：我们如何有效地存储来自事件流的数百万条记录，并让算法在几秒钟内筛选这些数据？</li>
<li><strong>确定分析实体的优先级</strong>：我们如何帮助被数据量淹没的分析师，优先排序并快速“放大”最有可能的攻击场景？</li>
<li><strong>场景重构</strong>：如何从攻击者的入口点开始，简洁地总结攻击场景，识别整个活动对系统的影响？</li>
<li><strong>处理常见的使用场景</strong>：如何应对正常的、良性的活动，这可能类似于在攻击期间观察到的常见活动，例如，软件下载？</li>
<li><strong>快速、交互式推理</strong>：我们如何为分析人员提供通过数据进行有效推理的能力，比如说，用另一种假设？</li>
</ul>
<p>下面，我们将简要介绍侦探调查，并总结我们的贡献。侦探假设攻击最初来自企业外部。例如，对手可以通过外部提供的恶意输入劫持web浏览器、插入受感染的u盘或向企业内运行的网络服务器提供零日攻击来启动攻击。我们假设对手在侦探开始监视系统之前并没有在主机上植入持续的恶意软件。我们还假设操作系统内核和审计系统是值得信赖的</p>
<h4><span id="11方法概述和贡献">1.1方法概述和贡献</span></h4><p>图1提供了我们的方法的概述。侦探是操作系统中立的，目前支持微软的Windows、Linux和FreeBSD。来自这些操作系统的审计数据被处理成平台中立的图形表示，其中顶点表示主题（<strong>进程</strong>）和对象（<strong>文件、套接字</strong>），边表示审计事件（例如，读、写、执行和连接等操作）。该图可作为攻击检测、因果关系分析和场景重建的基础。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191554457.png" alt="image-20220513135637994"></p>
<ul>
<li><p>本文的第一个贡献是<strong>针对高效事件存储依赖图表示的开发和紧凑的事件存储和分析（第2节）的挑战</strong>。主存表示上的图形算法可以比磁盘上的表示快几个数量级，这是实现实时分析能力的一个重要因素。<strong>在我们的实验中，我们能够在14秒内处理来自FreeBSD系统的79小时的审计数据，主存使用量为84MB。这种性能表示的分析速率比生成数据的速率快2万倍。</strong></p>
</li>
<li><p>本文的第二个主要贡献是<strong>开发了一种基于标签的方法</strong>，<strong>用以识别最有可能参与攻击的主题、对象和事件</strong>。标签使我们能够确定分析的优先级和重点，从而解决上面提到的第二个挑战。标签编码对数据（即对象）以及过程（主题）的可信度和敏感性进行评估。此评估是基于来自审计日志的数据来源。从这个意义上说，<strong>从审计数据中衍生出的标签类似于粗粒度信息流标签</strong>。我们的分析也可以很自然地支持更细粒度的标记，例如，细粒度的污染标记[42,58]，如果它们可用的话。在第3节中更详细地描述了标签，以及它们在攻击检测中的应用。</p>
</li>
<li><p>本文的第三个贡献是<strong>开发了利用标签进行根源识别和影响分析的新算法</strong>（第5节）。从图1中所示的攻击检测组件产生的警报开始。我们的反向分析算法遵循图中的依赖关系来识别攻击的来源。从源代码开始，我们使用前向搜索对对手的行动进行全面的影响分析。我们提出了几个标准，以生成一个紧凑的图。我们还给出了一些转换，进一步简化了这个图，并生成了一个图，以一种简洁和语义上有意义的方式直观地捕获攻击，例如，图中的图。 4.实验表明，我们基于标记的方法是非常有效的：例如，侦探可以分析3850万个事件，并生成一个只有130个事件的攻击场景图，代表事件量减少了5个数量级。</p>
</li>
<li><p>本文的第四个贡献，旨在解决上面提到的最后两个挑战，是一个用于<strong>标记初始化和传播的可定制策略框架</strong>(第4节)。我们的框架提供了合理的默认值，但是可以覆盖它们以适应特定于操作系统或应用程序的行为。这使我们能够调整检测和分析技术，以避免在良性应用程序表现出类似攻击行为的情况下出现误报。(参见第6.6节了解的尾部。)策略还使分析人员能够测试攻击的“备选假设”，方法是重新对认为为可信或敏感的内容进行分类，并重新运行分析。如果分析人员怀疑某些行为是攻击的结果，他们还可以使用策略捕获这些行为，并重新运行分析以发现其原因和影响。由于我们处理和分析审计数据的速度比它生成的速度快数万倍，因此可以有效地、并行地、实时地测试alternate假设。</p>
</li>
</ul>
<p>本文的最后贡献是一个实验评估（第6节），主要基于由<strong>DARPA组织</strong>的一个红色团队评估，作为其透明计算项目的一部分。在这项评估中，<strong>在Windows、FreeBSD和Linux主机</strong>上进行了类似现代apt的攻击活动。在这项评估中，侦探能够：</p>
<ul>
<li>在几秒钟内处理包含参与期间产生的数千万事件的审计日志；</li>
<li>成功地检测和重建这些攻击的细节，包括它们的入口点、系统中的活动和过滤点；</li>
<li>过滤无关事件，实现数据中非常高的减少率(高达100K次)，从而提供这些攻击的清晰语义表示，其中几乎不包含系统中其他活动的噪声；</li>
<li>并实现较低的假阳性和假阴性率。</li>
</ul>
<p>我们的评估并不是为了表明我们发现了最复杂的对手；相反，我们的观点是，给定几种未知的可能性，我们系统的优先级结果可以实时到位，没有任何人类的帮助。因此，它确实填补了今天存在的一个空白，即法医分析似乎主要是手动启动的。</p>
<h3><span id="二-主内存依赖性图">二、 主内存依赖性图</span></h3><p>为了支持快速检测和实时分析，我们将依赖关系存储在图形数据结构中。存储此图的一个可能的选择是图形数据库。然而，诸如Neo4J[4]或Titan[6]等流行数据库的性能[39]在许多图形算法中都是有限的，除非主内存足够大，可以容纳大部分数据。此外，一般图数据库的内存使用太高，适合我们的问题。即使是毒刺[16]和NetworkX[5]，两个为主存性能而优化的图形数据库，每个图边[39]分别使用约250字节和3KB。<strong>在企业网络上报告的审计事件的数量每天很容易达到数十亿到数百亿亿美元之间，这将需要几tb范围内的主内存。相比之下，我们提出了一个更有效的空间依赖图设计，每条边只使用大约10个字节。在一个实验中，我们能够在329mb的主存中存储38m个事件</strong>。</p>
<p><strong>Subjects</strong>：</p>
<ul>
<li>表示进程；</li>
<li>属性值包括：process id（pid）、命令行、所有者（owner）以及代码和数据的标签</li>
</ul>
<p><strong>Objects</strong>：</p>
<ul>
<li>表示实体，例如文件、pipes、网络连接</li>
<li>属性值包括：名称、类型（文件、pipe、socket等）、所有者和标签</li>
</ul>
<p><strong>事件</strong>：subjects和objects之间或者两个subjects之间带标签的边，用read 、connect、 execveread、connect、execveread、connect、execve来表示。<br><strong>事件存储在subjects中</strong>，从而消除了subject-event的指针、事件标识符（event id）的需求。他们的表示采用<strong>可变长编码</strong>，在通常情况下可以采用4 bytes，当需要时可以扩展到8、12或者16 bytes。</p>
<h3><span id="三-标签和攻击检测">三、 标签和攻击检测</span></h3><p>我们使用标签来描述<strong>objects和subjects的可信度和敏感度</strong>。对可信度和敏感度的评估基于以下三个因素：</p>
<ul>
<li><strong>起源（Provenance）</strong>：依赖图中，subject或object直接祖先的标记</li>
<li><strong>系统先验知识</strong>：我们对一些重要应用行为的了解，比如远程接入服务器、软件安装程序和重要的文件（/etc/passwd 和 /dev/audio）</li>
<li><strong>行为</strong>：观察subject的行为，并将其与预期行为进行比较</li>
</ul>
<p>一个默认的策略被用从从input到output传播标签：为output分配input的可信度标签中的最低值，以及机密性标签的最大值（也就是说，入口点的行为是危险的，出口点的行为也被标注为危险；入口点的数据是机密的，出口点的数据也被标注为是机密的）。这是一种保守的策略，该策略可能会导致一些良性事件被错误地识别为恶意事件（over-tainting），但绝不会漏掉攻击。</p>
<p><strong>标签在SLEUTH中扮演了核心角色</strong>。它为攻击检测提供了重要的上下文信息，每个事件都在这些标记组成的上下文中进行解释，以确定其导致攻击的可能性。此外，标签对我们的前向和回溯分析的速度也很有用。最后，标签为消除大量与攻击无关的审计数据也起到了关键作用。</p>
<h4><span id="31-标签设计">3.1 标签设计</span></h4><p>如下定义<strong>可信度标签（trustworthiness tags，t-tags）</strong>，可信度依次降低：</p>
<ul>
<li><strong>良性可信标签（Benign authentic tag）</strong>：为<strong>数据和代码</strong>分配该标签，其来源（source）为<strong>良性</strong>且可靠性<strong>可被验证</strong>的</li>
<li><p><strong>良性标签（Benign tag）</strong>：为<strong>数据和代码</strong>分配该标签，其来源为<strong>良性</strong>，但是来源可靠性<strong>未被充分验证</strong></p>
</li>
<li><p><strong>未知标签（Unknown tag）</strong>：为<strong>数据和代码</strong>分配该标签，但是其来源未知</p>
</li>
</ul>
<p><strong>策略（policy）定义了那些来源是良性的</strong>，哪些来源验证时充分的；策略的最简单情况是白名单。如果对于某个源，没有策略应用在它上面，那么这个源则被打上未知标签。如下定义<strong>机密性标签（confidentiality tags，c-tags）</strong>，机密性依次降低：</p>
<ul>
<li><strong>Secret</strong>：高度敏感的信息，例如登陆凭证、私钥</li>
<li><strong>Sensitive</strong>：数据的披露可能会对安全产生重大影响，例如，披露了系统中的漏洞，但没有为攻击者提供访问系统的直接途径。</li>
<li><strong>Private</strong>：资料的披露涉及私隐，但未必构成安全威胁。</li>
<li><strong>Public</strong>：可以被公开的数据</li>
</ul>
<p>我们设计的一个重要方面是分离<strong>代码和数据的t-tag</strong>。具体而言，即一个subject给定两个t-tag，一个表示其代码可信度（code trustworthiness，code t-tag），另一个表示其数据可信度（data trustworthiness，data t-tag）。这样的设计可以削减重建场景的规模，加快取证分析的速度。而机密性标签仅仅与数据相关联。</p>
<p>已经存在的objects和subjects使用<strong>标签初始化策略</strong>分配初始标签。在系统执行过程中还会产生新的objects和subjects，它们由<strong>标签传播策略</strong>分配标签。最后，<strong>基于行为的检测策略</strong>来检测攻击。</p>
<h4><span id="32-基于标签的攻击检测">3.2 基于标签的攻击检测</span></h4><p>检测方法不应该要求知晓特定应用的一些细节，因为这需要有关应用程序的专家知识，而在动态环境中，<strong>应用程序可能会频繁更新</strong>。</p>
<p>我们不把着眼点放在变化的应用行为上，而是着眼于攻击者的高级别目标，比如后门插入和信息窃取。具体而言，我们结合了攻击者的动机和手段的推理，注意到我们提出的标签就是用来捕获攻击者的手段：如果一段数据或代码有未 知 标 签 未知标签未知标签，那么它就是由不受信任的源产生的。</p>
<p>根据攻击者的攻击步骤，我们<strong>定义</strong>了下面包含<strong>攻击者目标和手段的策略</strong>（Detection Policy）：</p>
<ul>
<li><p><strong>不受信任的代码执行</strong>：当一个拥有高code t-tag的subject执行或<strong>加载拥有低t-tag的object</strong>时，便会引发警报</p>
</li>
<li><p><strong>被拥有低code t-tag的subject修改</strong>：当拥有低code t-tag的subject修改一个拥有高t-tag的object时，便会引发警报。修改的可能是文件内容或者文件名、文件权限等。</p>
</li>
<li><p><strong>机密文件泄露</strong>：当不可信的subjects泄漏敏感数据时，将触发警报。具体地说，也就是具有sensitive c-tag 和 unkonwn code t-tag的subject在网络中执行写操作时会触发警报。</p>
</li>
<li><strong>为执行准备不可信的数据</strong>：该策略由一个拥有unknown t-tag的subject的操作触发，该操作使一个object可执行。这样的操作会包含chmod和mprotect</li>
</ul>
<blockquote>
<p>  <strong>一点优势</strong>：值得注意的是，攻击者的手段并不会因为数据或代码经过了多个中间媒介之后而被“稀释”。啥意思呢？举个栗子：对于不受信任的代码执行策略来说，如果直接从未知网站加载数据的话，当然会触发警报。但是，当这些数据是被下载、提取、解压缩，甚至有可能是编译之后再加载的，在经过了重重转化之后，只要数据被加载，该策略仍然能够被触发。随后再进行回溯分析，就可以找到漏洞利用的第一步。</p>
<p>  （与其它探测器合作的能力）另外，<strong>其它检测器的输入可以很容易地被集成到SLEUTH中</strong>。比如说，某个外部的检测器将一个subject标为可疑，这个时候再SLEUTH中可以将该subject的code t-tag标为unknown，从而后面的分析都会受益。此外该操作也会保留图节点之间的依赖关系。</p>
</blockquote>
<p>被不受信的代码执行所触发的策略，不应该被认为工作在静态环境中（需要动态匹配策略），静态环境意味着不允许新代码产生。实际上，我们期望可以连续地更新和升级，但在企业环境中，我们不希望用户下载未知代码。因此，下面会叙述如何支持标准化的软件更新机制。</p>
<h3><span id="四-策略框架">四、策略框架</span></h3><p>本文开发了一个灵活的<strong>策略框架（policy framework）</strong>，用于标签的分配、传播和攻击检测。我们使用<strong>基于规则</strong>的记法来描述策略，例如：</p>
<p><code>exec(s,o):o.ttag&lt;benign→alert(&quot;UntrustedExec&quot;)</code></p>
<p>这条规则被触发的条件是：当一个subject s 执行了一个object o(比如文件），而o的t-tag要小于良性。<br>在该策略框架中，<strong>规则通常与事件关联</strong>，并且包含objects或subjects的<strong>属性的一些条件</strong>，这些属性包括：</p>
<ul>
<li><strong>name</strong>：使用Perl正则表达式来匹配<strong>object name</strong>和<strong>subject命令行</strong></li>
<li><p><strong>tags</strong>: 条件中可以放置objects或subjects的t-tags或者c-tags。对于subjects来说，代码和数据的t-tag可以分别使用</p>
</li>
<li><p><strong>所有权和权限</strong>：条件中可以放置objects和subjects的所有权，或者objects和事件权限</p>
</li>
</ul>
<p>不同类型的策略有不同的作用：</p>
<ul>
<li>检测策略：引发警报</li>
<li>标签初始化和传播策略：修改标签</li>
</ul>
<p><strong>触发点（trigger points）</strong>：为了更好地控制不同类型策略的匹配，我们将策略与触发点联系起来。此外，触发点允许有相似目的的不同事件共享策略。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191554914.png" alt="在这里插入图片描述"></p>
<p>上图展示了策略框架中定义的触发点，define表示一个新的object，比如一个新网络连接的建立、首次提及一个已经存在的文件、新文件的创建等。<br>（检测策略匹配过程）当事件出现时，检测策略就会被执行。后面，除非手动配置，否则仅当<strong>目标subject或object</strong>（某个信息流的终点，Target）发生变化时，检测策略才会被再次执行。</p>
<p>（标签策略）然后，标签策略按照指定的顺序进行尝试，一旦规则匹配，被规则指定的标签将会被分配给<strong>目标事件（Target，也就是subject/object）</strong></p>
<h3><span id="五-基于标签的双向分析">五、基于标签的双向分析</span></h3><h4><span id="51-回溯分析">5.1 回溯分析</span></h4><p>回溯分析的目标是识别攻击的入口点，入口点是图中入度为0的节点，并且被标记为untrusted。通常是网络连接，但有时也会是其他形式，比如U盘中的文件。</p>
<p><strong>回溯分析的起点是检测策略产生警报的地方</strong>。每个警报都与一个或多个实体相关，这些实体在图中被标记为可疑节点。反向搜索涉及对图的反向遍历，从而识别由可疑节点连接到入口点的路径。我们注意到，在这样的遍历和接下来的讨论中，依赖关系边的方向是相反的。反向搜索带来了几个重大<strong>挑战</strong>:</p>
<ul>
<li><p><strong>性能</strong>：依赖图可能包含数亿条边。警报数可以达到数千。在这么大的图上执行反向搜索，会消耗大量的计算资源。</p>
</li>
<li><p><strong>多路径</strong>：通常，从可疑节点向后可访问多个入口点。然而，在APT攻击中，通常只有一个真正的入口点。因此，简单的反向搜索可能会导致大量的误报</p>
</li>
</ul>
<p>标签可以用来解决这两个挑战。一方面，标签的计算和传播本来就是一种简洁的路径计算。另一方面，如果节点的标签值是unknown，那么该节点很有可能会构成攻击路径。如果节点A的标签是unknown，这意味着至少存在一条路径，从不受信任的入口点指向节点A，这样节点A就比其他拥有良性标签的邻居节点更有可能是攻击的一部分。使用标签来进行反向搜索，消除许多无关节点，极大地减少了搜索空间。</p>
<p>基于此，我们将反向分析当作<strong>最短路径问题</strong>的一个实例，<strong>标签</strong>被用来定义边的代价<strong>（cost）</strong>。一方面，标签能够“引导”搜索沿着攻击相关的路径，并远离不相关的路径。这使得搜索可以在不必遍历整个图的情况下完成，从而解决了性能方面的挑战。另一方面，最短路径算法通过选择最接近可疑节点的入口点（以路径成本衡量）来解决多个路径的挑战。<br>计算最短路径使用<strong>Dijkstra算法</strong>，当入口点被加入到路径中时，算法就停止。</p>
<p><strong>代价函数设计</strong>：对于那些表示节点依赖关系的<strong>边</strong>，如果其标签是<strong>“未知”</strong>，则为其分配<strong>较低的开销</strong>；其它节点分配较高的开胶，<strong>具体地说：</strong></p>
<ul>
<li>从一个“未知”数据/代码 t-tag 的节点，到一个“良性”代码/数据 t-tag 节点的边，为其分配<strong>代价为0</strong></li>
<li>从一个“良性”代码/数据 t-tag 的节点引出的边，为其分配一个<strong>较高的代价</strong></li>
<li>从已有“未知” tag 的节点之间引入边，为其分配<strong>代价为1</strong></li>
</ul>
<p>与未知 subject/object 直接相关的良性 subject/object 表示图中恶意和良性部分之间的边界。因此，它们必须包含在搜索中，因此这些边的代价是0。</p>
<p>良性实体之间的信息流动不是攻击的一部分，因此我们将它们的代价设置得非常高，以便将它们排除在搜索之外。</p>
<p>不可信节点之间的信息流可能是攻击的一部分，因此我们将它们的代价设置为一个较低的值。它们将被包括在搜索结果中，除非由较少边组成的可选路径可用。</p>
<h4><span id="52-向前分析">5.2 向前分析</span></h4><p>前向分析的目的是为了评估攻击的影响。通过从一个入口点开始，发现所有依赖于入口点的可能影响。与反向分析类似，主要的挑战是图的大小。一种简单的方法是，标记所有从入口点可到达的 subject/object，这些 subject/object 是通过反向分析得到的。不幸的是，这种方法将导致影响图太大。<br>在实验中，利用这种方法得到的影响图包含数百万条边，利用我们的简化算法可以降低100到500倍。</p>
<p>一个降低其大小的方法是使用距离阈值dth ，来排除那些距离可疑节点太远的点，分析人员可以调节该阈值。我们使用在回溯分析时使用到的 cost 。</p>
<p>（为什么回溯分析不考虑？？）不同于回溯分析的是，我们考虑机密性。特别的，一条边两端的节点，一个由高机密性标签，另外一个具有低代码 integrity（可信度？？） 标签（如未知进程）或者低数据 integrity 标签（如未知socket），那么为这条边分配代价为0；而当另一个节点由良性标签时，为其分配较高代价值。</p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>高级威胁发现（4）【Nan】RAID-Cyber Threat Intelligence Modeling Based on GCN</title>
    <url>/posts/22KBA6T/</url>
    <content><![CDATA[<h2><span id="raid-cyber-threat-intelligence-modeling-based-on-gcn">RAID-Cyber Threat Intelligence Modeling Based on GCN</span></h2><blockquote>
<p>  <a href="https://mp.weixin.qq.com/s?__biz=Mzg5MTM5ODU2Mg==&amp;mid=2247490941&amp;idx=1&amp;sn=30cf491cf7455bbd075f8285d292a9d8&amp;chksm=cfccadb0f8bb24a6c4e87c5c95aecda92578ee634f2b8642bece665efde54417655ed79936c7&amp;scene=178&amp;cur_album_id=1776483007625822210#rd">[AI安全论文] 05.RAID-Cyber Threat Intelligence Modeling Based on GCN</a></p>
<p>  这篇文章将详细介绍北航老师发表在RAID 2020上的论文《Cyber Threat Intelligence Modeling Based on Heterogeneous Graph Convolutional Network》</p>
<p>  原文作者：Jun Zhao, Qiben Yan, Xudong Liu, Bo Li, Guangsheng Zuo<br>  原文链接：<a href="https://www.usenix.org/system/files/raid20-zhao.pdf">https://www.usenix.org/system/files/raid20-zhao.pdf</a><br>  论文来源：RAID 2020/CCF B</p>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p>网络威胁情报（CTI，Cyber Threat Intelligence）已在业界被广泛用于抵御流行的网络攻击，CTI通常被看作将威胁参与者形式化的妥协指标（IOC）。然而当前的网络威胁情报（CTI）存在三个主要局限性：</p>
<ul>
<li>IOC提取的准确性低</li>
<li><strong>孤立的IOC几乎无法描述威胁事件的全面情况</strong></li>
<li><strong>异构IOC之间的相互依存关系尚未得到开发，无法利用它们来挖掘深层次安全知识</strong></li>
</ul>
<p>本文提出了基于异构信息网络（HIN， Heterogeneous Information Network）的网络威胁情报框架——HINTI，旨在建模异构IOCs之间的相互依赖关系，以量化其相关性，对CTI进行建模和分析。</p>
<p>本文的主要贡献如下：</p>
<ul>
<li><strong>提出了基于多粒度注意力机制（ multi-granular attention）的IOC识别方法，可以从非结构化威胁描述中自动提取网络威胁对象，并提高准确性。</strong></li>
<li>构建一个异构信息网络（HIN）来建模IOCs之间的依赖关系</li>
<li>提出一个基于图卷积网络（Graph Convolutional Networks）的威胁情报计算框架来发现知识</li>
<li>实现了网络威胁情报（CTI）原型系统</li>
</ul>
<p>实验结果表明，本文提出的IOC提取方法优于现有方法，HINTI可以建模和量化异构IOCs之间的潜在关系，为不断变化的威胁环境提供了新的线索。</p>
<blockquote>
<p>  IOC（Indicator of Compromise）是MANDIANT在长期的数字取证实践中定义的可以反映主机或网络行为的技术指示器。</p>
</blockquote>
<h3><span id="一-hinti工作步骤">一、HINTI工作步骤</span></h3><ul>
<li>通过B-I-O序列标注方法对安全相关帖子进行标注，用于构建IOC提取模型。</li>
<li>将标记的训练样本输入神经网络，训练<strong>IOC提取模型</strong>。</li>
<li><strong>HINTI利用句法依赖性解析器</strong>（e.g.，主-谓-宾，定语从句等）提取IOC之间的关联关系，每个IOC均表示为三元组（IOCi，relation，IOCj）</li>
<li>最后，HINTI集成了基于异构图卷积网络的CTI计算框架以有效地量化IOC之间的关系进行知识发现。</li>
</ul>
<h4><span id="11-hinti总体架构">1.1 HINTI总体架构</span></h4><p>HINTI由四个主要部分组成：（a）收集与安全相关的数据并提取即IOC；（b）将IOC之间的相互依存关系建模为异构信息网络；（c）使用基于权重学习的相似性度量将节点嵌入到低维向量空间中；（d）基于图卷积网络和知识挖掘来计算威胁情报。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555337.jpeg" alt="图片"></p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>高级威胁发现（5）Log2vec: A Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats within Enterprise</title>
    <url>/posts/31GH3TV/</url>
    <content><![CDATA[<h2><span id="log2vec-a-heterogeneous-graph-embedding-based-approach-for-detecting-cyber-threats-within-enterprise"><strong>Log2vec: A Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats within Enterprise</strong></span></h2><blockquote>
<p>  【顶会论文解读】Log2vec 基于异构图Heterogeneous Graph 检测网络空间威胁 - 笑个不停的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/275952146">https://zhuanlan.zhihu.com/p/275952146</a></p>
<p>  <strong>AIops博客</strong>：<a href="https://blog.csdn.net/markaustralia/category_11284226.html">https://blog.csdn.net/markaustralia/category_11284226.html</a></p>
<p>  <a href="https://randy.blog.csdn.net/article/details/111773951?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-111773951-blog-118577347.pc_relevant_multi_platform_whitelistv1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-111773951-blog-118577347.pc_relevant_multi_platform_whitelistv1&amp;utm_relevant_index=2">基于深度学习的日志数据异常检测</a></p>
</blockquote>
<p>发表在<strong>CCS2019</strong>会议的一篇应用异质图embedding进行企业内部网络空间威胁检测的文章。</p>
<h3><span id="摘要">摘要</span></h3><p>内部人员的攻击以及APT攻击是组织常见的攻击类型，现有的检测算法多基于行为检测，大部分方法考虑log日志的序列关系以及用户的行为序列，忽略了其他的关系导致不使用于丰富多样的攻击场景。<strong>本文提出的Log2vec模型，将日志转化为异质图，将日志学习为低纬度的embedding并使用检测算法进行攻击检测（由于攻击样本较少，因此使用聚类算法进行检测）。</strong></p>
<h3><span id="一-引言">一、引言</span></h3><p>现有方法一般会转换用户的各种操作（也包括日志条目）分成序列，这些序列可以保存信息，例如 日志条目之间的顺序关系，然后使用序列处理技术，例如 深度学习从过去的事件中学习并预测下一个事件。<strong>本质上，这些日志条目级别的方法可以模拟用户的正常行为行为并将其偏离标记为异常。</strong></p>
<p>但是，这种方法忽略了其他关系。 例如，<strong>比较用户的日常行为是常规内部威胁检测的一种常用方法</strong>。 此检测基于以下前提：用户的日常行为在一段时间内（几天之间的逻辑关系）相对规则。 上述预测方法忽略了这种关系，并且会降低其性能。 此外，他们需要正常的日志条目，甚至需要大量标记数据来进行模型训练。 但是，在现实世界中，存在罕见的攻击动作，从而限制了其正确预测的能力。</p>
<p><strong>对于检测内部威胁以及APT攻击来说，我们面临三个问题：</strong></p>
<p>（1）<strong>如何同时检测上述两种攻击情形，特别是考虑到检测系统中提到的所有三种关系（日志之间序列关系 sequantial relationship among log entries、几天之内的逻辑关系logical relationship among days以及交互关系interactive relationship among hosts）；</strong></p>
<p><strong>解决办法</strong>：构建异构图来表示前面提到的三种关系；</p>
<p>（2）<strong>如何在APT场景中进行细粒度的检测，尤其是深入挖掘和分析主机内日志条目之间的关系；</strong></p>
<p><strong>解决办法</strong>：将日志条目分为五个属性。根据这些属性，我们深入考虑了主机内日志之间的关系，并设计了精细的规则来关联它们。这种设计使正常和异常日志条目可以在这种图中拥有不同的拓扑；</p>
<p>（3）<strong>如何针对训练模型进行无攻击样本的检测。【内部威胁】</strong></p>
<p><strong>解决办法</strong>：log2vec的图嵌入和检测算法将日志条目表示并分组到不同的群集中，而没有攻击样本，适用于数据不平衡的情况（针对问题3）。此外，图形嵌入本身可以自动学习每个操作的表示形式（矢量），而无需手动提取特定于领域的特征，从而独立于专家的知识。我们的改进版本可以进一步差分提取并表示来自上述异构图的操作之间的多个关系。</p>
<h3><span id="二-文章的设计">二、<strong>文章的设计</strong></span></h3><p><strong>Log2vec包含以下三个组件：</strong></p>
<ul>
<li><p><strong>图的构造。</strong> Log2vec构造一个异构图以集成日志之间的多个关系条目;</p>
</li>
<li><p><strong>图嵌入</strong>（也是图表示学习）。这是一种强大的图形处理方法，可用于了解每个操作的</p>
<p>表示（向量）基于它们在这种图中的关系。通过矢量化用户操作，可以直接比较他们的操作</p>
<p>找出异常的相似之处；</p>
</li>
<li><p><strong>检测算法，</strong>无监督，要有效将恶意操作分组为单个群集</p>
</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555029.png" alt="image-20220703183858975" style="zoom:50%;"></p>
<h3><span id="三-概述">三、概述</span></h3><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555735.png" alt="image-20220703184011385" style="zoom:50%;"></p>
<p>如图2（a）所示，<strong>日志记录了用户的操作，如登录操作，设备之间移动操作以及网络操作</strong>。图2（b）描述日志的属性，<strong>主体（user），操作类型（visit或者send），客体（网址或者是邮件）以及时间和主机（设备ID）</strong>。实际上，日志的属性能够反映用户的行为，比如，第一个登录的时间和日志结束的时间能够反映用户的工作时长。系统管理员会频繁的登录服务器和操作对系统进行维护。</p>
<p>图2（c）将a中显示的<strong>日志组成序列</strong>，然后使用如LSTM的方法学习日志序列信息进行预测。此类模型能够捕捉日志的因果信息和序列关系。但是，这样的方法忽视了其他关系，比如（a）中day3序列，有大量的设备连接和文件复制的操作，远比以前多（这意味着数据泄露）。 可以通过直接比较用户的日常行为来检测出这种差异。进行比较的前提是，在一段时间内（几天之间的逻辑关系），用户的日常行为相对规则且相似。 虽然是深度学习，例如 LSTM可以记住序列的长期依赖关系（多天），它没有明确比较用户的日常行为，也无法获得令人满意的性能。 类似地，它们不能保持图2d中主机之间的另一种关系，交互关系，并且不能在APT检测中正常工作。 另外，其中一些需要大量标记数据进行培训。 但是，在我们的检测方案中，存在罕见的攻击行为。</p>
<p>图2d展示了图2a中的登录（红色字体）的图表，<strong>该图表指示用户在主机之间的行为</strong>。 我们可以分析主机之间的这些交互关系，以发现异常登录。 例如，管理员可以定期登录到一组主机以进行系统维护，而APT实施者只能访问他可以访问的主机。 登录跟踪的功能可以捕获这种差异。例如，良性跟踪中涉及的主机数量（1或3，实线）通常不同于APT（2，虚线）。在分析了这些功能之后，可以识别出受感染的主机。 但是，这些主机包含许多良性操作，并且手动提取的特定于域的特征显然无法应用于图2c中的攻击。</p>
<p>本文构建的模型用于检测以下类型的攻击： 第一种情况是<strong>内部人员滥用职权执行恶意操作，例如访问数据库或应用程序服务器，然后破坏系统或窃取知识产权以谋取个人利益</strong>。 其次，<strong>恶意内部人员通过窥视或密钥记录器获取其他合法用户的凭据，并利用此新身份来寻找机密信息或在公司中造成混乱</strong>。 这两种情况属于内部员工的典型攻击。 第三种攻击是<strong>APT参与者破坏了系统中的主机，并从该主机中持续破坏了多个主机，以提升其特权并窃取机密文件。</strong></p>
<h3><span id="四-详细论述"><strong>四、详细论述</strong></span></h3><h4><span id="41-图构建">4.1 图构建</span></h4><p><strong><font color="red"> 本文构建的图的节点是日志，只有一种节点类型，边是通过规则来建立联系的，本文提出10条规则，代表着图有10种边类型。因此，本文构建的异构图只是边类型不同，节点类型是相同的。</font></strong></p>
<p><strong>定义：<sub,obj,a,t,h> 提取日志的五个主要属性：subject, object, operation type, time and host，称为元属性</sub,obj,a,t,h></strong></p>
<ul>
<li>sub表示用户集合；</li>
<li>obj表示客体集合（文件、移动存储设备、网站）；</li>
<li>A是操作类型集合（文件操作和网页利用）；</li>
<li>T表示时间</li>
<li>H是主机（计算机或者是服务器）。</li>
</ul>
<p><strong>sub,obj,A,H有自己的子属性</strong>。比如，用户在服务器中写入文件，sub的属性包括用户角色（系统管理员等）和所属单位；obj属性包括文件类型和大小；H属性包括是文件服务器还是邮件服务器；对于登录操作，A的属性包括身份验证。对于用户登录，可以表示为user (sub) logs in to (A) a destination host (obj) in a source one (H),</p>
<h4><span id="42-构建图的规则">4.2 <strong>构建图的规则</strong></span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555928.png" alt="image-20220703193951973" style="zoom:50%;"></p>
<p>上图，R1-3描述一天中的因果关系和序列关系；R4-6描述多天之间的逻辑关系；R7 R9描述用户登录和web浏览行为序列。R8 R10按照逻辑关系进行关联。</p>
<p><strong>log2vec考虑三种关系：</strong></p>
<ul>
<li>causal and sequential relationships within a day; 一天内的因果关系和顺序关系；</li>
<li>logical relationships among days；日之间的逻辑关系</li>
<li>logical relationships among objects；对象之间的逻辑关系。</li>
</ul>
<p>在设计有关这三种关系的规则时，我们考虑这些元属性的不同组合，以关联较少的日志条目，并将更精细的日志关系映射到图中。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555332.png" alt="image-20220703194402679" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>高级威胁发现（6）UNICORN: Provenance-Based Detector for APTs</title>
    <url>/posts/1WZGAE8/</url>
    <content><![CDATA[<h2><span id="ndss20-unicorn-provenance-based-detector-for-apts">NDSS20 UNICORN: Provenance-Based Detector for APTs</span></h2><blockquote>
<p>  <a href="https://mp.weixin.qq.com/s?__biz=Mzg5MTM5ODU2Mg==&amp;mid=2247492967&amp;idx=1&amp;sn=60f14977758cc7d1e8504446422e5ec0&amp;chksm=cfcf55aaf8b8dcbc86935b76e7f36961202174ecba1df597fee9d44428c607e80d41add1b2f9&amp;scene=178&amp;cur_album_id=1776483007625822210#rd">[AI安全论文] 06.NDSS20 UNICORN: Provenance-Based Detector for APTs</a></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555641.png" alt="图片"></p>
<blockquote>
<p>  原文作者：Xueyuan Han, Thomas Pasquier, Adam Bates, James Mickens and Margo Seltzer<br>  原文标题：UNICORN: Runtime Provenance-Based Detector for Advanced Persistent Threats<br>  原文链接：<a href="https://arxiv.org/pdf/2001.01525.pdf">https://arxiv.org/pdf/2001.01525.pdf</a><br>  发表会议：NDSS 2020参考文献：感谢两位老师  <a href="https://blog.csdn.net/Sc0fie1d/article/details/104868847">https://blog.csdn.net/Sc0fie1d/article/details/104868847</a>  <a href="https://blog.csdn.net/xjxtx1985/article/details/106473928">https://blog.csdn.net/xjxtx1985/article/details/106473928</a></p>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p>本文提出的<strong>UNICORN是一种基于异常的APT检测器</strong>，可以有效利用数据<strong>Provenance进行分析</strong>。通过广泛且快速的图分析，使用<strong>graph sketching技术</strong>，UNICORN可以在长期运行的系统中分析Provenance Graph，从而识别未知慢速攻击。其中，Provenance graph提供了丰富的上下文和历史信息，实验证明了其先进性和较高准确率。</p>
<p>由于APT（Advanced Persistent Threats）攻击具有缓慢可持续的攻击模式以及频繁使用0-day漏洞的高级特性使其很难被检测到。<strong>本文利用数据来源分析（provenance）提出了一种基于异常的APT检测方法，称为UNICORN。</strong></p>
<ul>
<li>从建模到检测，UNICORN专门针对APT的独有特性（low-and-slow、0-Days）设计。</li>
<li>UNICONRN利用高效的图分析方法结合溯源图丰富的上下文语义和历史信息，在没有预先设定攻击特征情况下识别隐蔽异常行为。</li>
<li>通过图概要（graph sketching）技术，它有效概括了长时间系统运行来对抗长时间缓慢攻击。</li>
<li>UNICONRN使用一种新的建模方法来更好地捕捉长期行为规律，以提高其检测能力。</li>
</ul>
<p>最后通过大量实验评估表明，本文提出的方法优于现有最先进的APT检测系统，并且在真实APT环境中有较高的检测精度。</p>
<h3><span id="一-引言">一、引言</span></h3><p>APT攻击现在变得越来越普遍。这种攻击的时间跨度长，且与传统攻击行为有着本质的区别。APT攻击者的目的是获取特定系统的访问控制，并且能够长期潜伏而不被发现。攻击者通常使用0-day漏洞来获取受害者系统的访问控制。</p>
<p><strong>传统检测系统通常无法检测到APT攻击。</strong></p>
<ul>
<li>依赖恶意软件签名的检测器对利用新漏洞的攻击无效。</li>
<li>基于异常检测的系统通常分析一系列的系统调用或日志系统事件，其中大部分方法无法对长期行为进行建模。</li>
<li>由于基于异常检测的方法只能检测系统调用和事件的短序列很容易被绕过。</li>
</ul>
<p>综上，当前针对APT攻击的检测方法很少能成功。攻击者一旦使用0-Day漏洞，防御者便无计可施；而基于系统调用和系统事件的检测方法，由于数据过于密集，这些方法难以对长时间的行为模式进行建模。<strong>因此，数据溯源（data provenance）是一种检测APT更合适的数据。</strong></p>
<p>最近的研究成果表明数据溯源是一个很好的APT检测数据源。数据溯源将系统执行表示成一个有向无环图（DAG），该图描述了系统主体（如进程）和对象（文件或sockets）之间的信息流。即使跨了很时间，在图中也把因果相关的事件关联到一起。因此，即使遭受APT攻击的系统与正常系统比较类似，但是溯源图中丰富的上下文语义信息中也可以很好地区分正常行为与恶意行为。</p>
<p><strong>然而，基于数据溯源的实时APT检测依然具有挑战。</strong><br>随着APT攻击的渗透的进行，数据溯源图的规模会不断增大。其中必要的上下文分析需要处理大量图中的元素，而图上的分析通常复杂度比较高。当前基于数据溯源的APT检测方法根据已有的攻击知识通过简单的边匹配实现APT检测，无法处理未知的APT攻击。基于溯源的异常检测系统主要是基于图模型的邻域搜索，利用动态或静态模型识别正常行为模式。理论上关联的上下文越丰富越好，但是实际中由于图分析的复杂性较高限制了其可行性。</p>
<ul>
<li>Provenance Graph的分析是相当耗费计算资源，因为APT是可持续攻击，图的规模也会越来越大</li>
</ul>
<p><strong><font color="red"> 当前APT检测系统面临如下三种问题：</font></strong></p>
<ul>
<li>静态模型难以捕获长时间的系统行为；</li>
<li>low-and-slow APT投毒攻击：由于APT高级可持续的特性可以在系统中潜伏很长时间，相关的行为会被认为是正常行为，这样的攻击会影响检测模型；</li>
<li>在主存内进行计算的方法，应对长期运行的攻击表现不佳。</li>
</ul>
<p>基于此，本文提出了UNICORN，使用graph sketching来建立一个增量更新、固定大小的纵向图数据结构。这种纵向性质允许进行广泛的图探索，使得UNICORN可以追踪隐蔽的入侵行为。而固定大小和增量更新可以避免在内存中来表示provenance graph，因此UNICORN具有可扩展性，且计算和存储开销较低。UNICORN在训练过程中直接对系统的行为进行建模，但此后不会更新模型，从而防止模型的投毒攻击。</p>
<p><strong><font color="red"> 本文的主要贡献如下：</font></strong></p>
<ul>
<li>针对APT攻击特性提出一种<strong>基于Provenance的异常检测系统</strong>。</li>
<li><strong>引入一种新的基于概要的（sketch-based）、时间加权的（time-weighted）溯源编码</strong>，该编码非常紧凑且可处理长时间的溯源图。</li>
<li>通过模拟和真实的APT攻击来评估UNICORN，证明其能高精度检测APT活动。</li>
<li><strong><font color="red"> 实现代码开源。</font></strong></li>
</ul>
<h3><span id="二-背景">二、背景</span></h3><h4><span id="21-系统调用追踪的挑战"><strong>2.1 系统调用追踪的挑战</strong></span></h4><p><strong>系统调用抽象提供了一个简单的接口，用户级应用程序可以通过这个接口请求操作系统的服务</strong>。作为调用系统服务的机制，系统调用接口通常也是攻击者入侵的入口点。因此，系统调用跟踪一直被认为是入侵检测的实际信息源。然而：</p>
<ul>
<li>当前的攻击检测系统是对非结构化的系统调用的审记日志进行分析，但捕获的系统调用杂乱分散，传统基于异常检测的思路无法处理APT。因此需要将其关联成data provenance，基于溯源的方法是将历史上下文数据都编码到因果关系图中。</li>
<li>数据溯源方法已经被应用到攻击调查中，已经有一些方法能够根据审计数据构建系统溯源图用以实现对系统执行过程的建模。然而这些方法依然存在一些局限：(1) 这种事后构建很难保证溯源图的正确性，由于系统调用问题存大量并发，溯源图的完整性与可靠性无法保证；(2) 容易被绕过；(3) 时空复杂度较高。</li>
<li>由于一些内核线程不使用系统调用，因此<strong>基于Syscall生成的Provenance是一些分散的图，而不是一张系统运行状况的完整图</strong></li>
</ul>
<h4><span id="22-全系统追踪溯源">2.2 <strong>全系统追踪溯源</strong></span></h4><p><strong>全系统溯源运行在操作系统层面，捕获的是所有系统行为和它们之间的交互</strong>。通过捕获信息流和因果关系，即使攻击者通过操作内核对象来隐藏自己的行踪也无济于事。</p>
<p>本文使用CamFlow，采用了Linux安全模块（Linux Security Modules，LSM）框架来确保高效可靠的信息流记录。LSM可以消除race condition。</p>
<blockquote>
<p>  <strong>CamFlow：溯源搜集系统</strong>，参考官网 <a href="https://camflow.org/。">https://camflow.org/。</a></p>
<p>  <strong>CamFlow 将系统的执行表示为有向无环图</strong>。图中的顶点表示内核对象（例如线程、文件、套接字等）的状态，关系表示这些状态之间的信息流。</p>
<p>  <img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555346.png" alt="CamFlow 图表概览" style="zoom: 33%;"></p>
<p>  在上面的示例中<code>process 1</code>克隆<code>process 2</code>。 <code>process 2</code>写到一个<code>pipe</code>。 <code>process 1</code>同读<code>pipe</code>。创建版本是为了保证非周期性并代表信息的正确排序（有关详细信息，请参阅我们的<a href="http://camflow.org/publications/ccs-2018.pdf">CCS’18 论文</a>）。</p>
</blockquote>
<h3><span id="23-问题描述">2.3 问题描述</span></h3><p>现有基于数据溯源的APT攻击检测方法主要存在如下缺陷：</p>
<ul>
<li>预定义的边匹配规则过于敏感，很难检测到APT攻击中的0-Day漏洞；</li>
<li>溯源图的近邻约束导致其只能提供局部上下文信息（而非whole-system），然而这会影响相关异常检测精度；</li>
<li>系统行为模型难以检测APT：静态模型无法捕获长期运行的系统的行为；动态模型容易遭受中毒攻击；</li>
<li>溯源图的存储与计算都是在内存中，在执行长期检测上有局限性。</li>
</ul>
<p><strong>UNICORN可以解决如上问题，其本质是把APT检测问题看成大规模、带有属性的实时溯源图异常检测问题。在任何时间，从系统启动到其当前状态捕获的溯源图都将与已知正常行为的溯源图进行比较。如果有明显差别，那么就认为该系统正在遭受攻击。</strong></p>
<p>对于APT检测来说，理想基于溯源的IDS应该如下：</p>
<ul>
<li>充分利用溯源图的丰富上下文，以时间与空间有效的方法持续分析溯源图；</li>
<li>在不假设攻击行为的基础上，应考虑系统执行的整个持续时间；</li>
<li>只学习正常行为的变化，而不是学习攻击者指示的变化。</li>
</ul>
<h3><span id="三-威胁模型">三、威胁模型</span></h3><p>假设主机入侵检测有适当的场景：攻击者非法获得对系统的访问权限，并计划在不被检测的情况下驻留在系统中很长一段时间。<strong>攻击者可能分阶段执行攻击，在每个阶段还会使用大量的攻击技术</strong>。UNICORN的目标是通过解决主机生成的溯源来实现在所有阶段对APT攻击进行检测。本文假设，我们假设在受到攻击之前，UNICORN在正常运行期间会完全观察主机系统，并且在此初始建模期间不会发生攻击。</p>
<p>数据收集框架的完整性是UNICORN正确性的核心，因此<strong>我们假定所使用的CamFlow中，LSM完整性是可信的。同时，本文假设内核、溯源数据和分析引擎的正确性，我们重点关注UNICORN的分析能力。</strong></p>
<h3><span id="四-系统设计">四、系统设计</span></h3><p>独角兽是一个基于主机的入侵检测系统，能够同时检测在网络主机集合上的入侵。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555615.png" alt="图片" style="zoom: 67%;"></p>
<ol>
<li><strong>以一个带标签的流式溯源图作为输入</strong>。该图由CamFlow生成，每条边是带属性的。溯源系统构建一个具有偏序关系的DAG溯源图，能实现有效的流式计算和上下文分析。</li>
<li><strong>建立一个运行时的内存直方图</strong>。UNICORN有效构建一个流式直方图，该直方图表示系统执行的历史，如果有新边产生则实时更新直方图的计数结果。通过迭代的探索大规模图的近邻关系，发现了在上下文环境中系统实体的因果关系。该工作是UNICORN的第一步，具体来说，<strong>直方图中每个元素描述了图中唯一的一个子结构</strong>，同时考虑了子结构中的顶点与边上的异构标签，以及这些边的时间顺序。APT攻击缓慢的渗透攻击目标系统，希望基于的异常检测方法最终忘记这一行为，把其当成正常的系统行为，但是APT攻击并不能破坏攻击成功的相关信息流依赖关系。</li>
<li><strong>定期计算固定大小的概要图（graph sketch）</strong>。在纯流式环境，当UNICORN对整个溯源进行汇总时，唯一直方图元素的数量可能会任意增长。这种动态变化导致两个直方图之间的相似计算变得非常有挑战，从而使得基于直方图相似计算的建模以及检测算法变的不可行。<strong>UNICORN采用相似度保存的hash技术把直方图转换成概要图。概要图可以增量维护，也意味着UNICORN并不需要将整个溯源图都保存在内存中。</strong>另外，<strong>概要图保存了两个直方图之间的jaccard相似性，这在后续图聚类分析中特别有效。</strong></li>
<li><strong>将简略图聚类为模型</strong>。UNICORN可以在没有攻击知识的前提下实现APT攻击检测。与传统的聚类方法不同，UNICORN利用它的流处理能力生成一个动态演化模型。该模型通过在其运行的各个阶段对系统活动进行聚类捕获单个执行中的行为改变，但是UNICORN无法在攻击者破坏系统时动态实时修改模型。因此，它更适合APT攻击这类长期运行的攻击。</li>
</ol>
<h4><span id="41-溯源图">4.1 溯源图</span></h4><p>最近几年溯源图在攻击分析中越来越流行，并且本身固有的特别可以有效的用于APT检测。溯源图挖掘事件之间的因果关系，因果关系有助于对时间跨度较远的事件进行推理分析，因此有助于在检测APT相关攻击。</p>
<p>UNICORN根据两个系统执行的溯源图的相似性还判定两个系统的行为相似性。而且UNICORN总是考虑整个溯源来检测长期持续的攻击行为。<strong>当前已经有许多图相似度计算方法，然而这些算法大部分是NPC的，即使多项式时间复杂度的算法也无法满足整个溯源图快速增涨的需求。</strong></p>
<h4><span id="42-构建graph直方图">4.2 构建Graph直方图</span></h4><p>本文方法的目标是有效对溯源图进行比较分析，同时容忍正常执行中的微小变化。对于算法，我们有两个标准：</p>
<ul>
<li>图表示应考虑长期的因果关系；</li>
<li>必须能够在<strong>实时流图数据</strong>上实现该算法，以便能够在入侵发生时阻止入侵（不仅仅是检测到入侵）。</li>
</ul>
<p><strong><font color="red"> 本文基于一维WL同构检验，采用了线性时间的、快速的Weisfeiler-Lehman（WL）子树图核算法。该算法的使用依赖于构造的顶点直方图的能力，需要直方图能捕捉每个顶点周围的结构信息。</font></strong>根据扩充的顶点标签对顶点进行分类，这些标签完全描述了顶点的领域，并且<strong>通过迭代的标签传播来构造这些扩展的顶点标签</strong>。</p>
<p>同构性的WL检验及其子树kernel变化，以其对多种图的判别能力而闻名，超越了许多最新的图学习算法（例如，图神经网络）。对Weisfeiler-Lehman（WL）子树图核的使用取决于我们构建顶点直方图的能力，捕获围绕每个顶点的图结构。我们根据增强顶点标签对顶点进行分类，标签描述了顶点的R-hop邻居。</p>
<p>为了简单说明，假设有一个完整静态图，重标记对所有的输入标签的聚合。对每个顶点都重复执行这个过程来实现对n跳邻居的描述。一旦为图中的每个顶点都构建了扩展标签，那么就可以基于此生成一个直方图，其中每个bucket表示一个标签。两个图的相似性比较是基于以下假设：两个图如果相似那么在相似的标签上会有相似的分布。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555022.png" alt="图片" style="zoom: 67%;"></p>
<p>我们的目标是构建一个直方图，图中的每个元素对应一个唯一的顶点标签，用于捕获顶点的R-hop的in-coming邻居。</p>
<p><strong>信息流的多样性与复杂性（Streaming Variant and Complexity）</strong>。算法1只有新顶点出现或是新边出现对其邻顶点有影响时才会执行。本文方法只需要为每条新边更新其目标顶点的邻域。UNICORN采用这种偏序关系来最小化计算代价。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191555986.png" alt="图片" style="zoom: 67%;"></p>
<p><strong>直方图元素的概念漂移问题</strong>。APT攻击场景需要模型必须能够处理长期运行行为分析能力，而系统行为的动态变化会导致溯源图的统计信息也随之变化，这种现象就叫概念漂移（concept drift）。</p>
<p>UNICORN通过对直方图元素计数使用指数权重衰减来逐渐消除过时的数据（逐渐忘记机制），从而解决了系统行为中的此类变化。它分配的权重与数据的年龄成反比。</p>
<script type="math/tex; mode=display">
H_h=\sum_t \mathbb{1}_{x_t=h}</script><p><strong>入侵检测场景中的适用性</strong>。上述“逐渐忘记”的方法，使得UNICORN可以着眼于当前的系统执行动态，而且那些与先前的object/activity有关系的事件不会被忘记。</p>
<h4><span id="43-生成概要图graph-sketches">4.3 生成概要图（Graph Sketches）</span></h4><p>Graph直方图是描述系统执行的简单向量空间图统计量。然而，与传统的基于直方图的相似性分析不同，UNICORN会随着新边的到来不断更新直方图。另外，UNCORN会根据图特征的分布来计算相似性，而不是利用绝对统计值。</p>
<p><strong><font color="red"> 本文采用locality sensitive hashing，也称作similarity-preserving data sketching。UNICORN的部署采用了前人的研究成果HistoSketch，该方法是一种基于一致加权采样的方法，且时间得性是常数。</font></strong></p>
<h4><span id="44-学习进化模型">4.4 学习进化模型</span></h4><p>在给定graph sketch和相似性度量的情况下，聚类是检测离群点常用的数据挖掘手段。<strong>然而传统的聚类方法无法捕获系统不断发展的行为</strong>。UNICORN利用其流处理的能力，创建了进化模型，可以捕获系统正常行为的变化。更重要的是，模型的建立是在训练阶段完成的，而不是在部署阶段，因为部署阶段训练模型可能会遭受中毒攻击。</p>
<p><strong><font color="red"> UNICORN在训练期间创建一个时序sketches，然后使用著名的K-medods算法从单个服务器对该概要序列进行聚类，使用轮廓系数（silhouette coefficient）确定最佳K值。</font></strong>每个簇表示系统执行的元状态（meta-states），如启动、初始化、稳定状态。然后UNICORN使用所有簇中sketches的时间顺序和每个簇的统计量（如直径、medoid），来生成系统进化的模型。</p>
<blockquote>
<ul>
<li><p>更新C中的每一列，即类中心 <img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]"> ,对于第j类，<strong>中心 <img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]"> 需要通过遍历所有该类中的样本，取与该类所有样本距离和最小的样本为该中心。</strong></p>
<p>K-means 模型: $\min <em>{G, C} \sum</em>{i=1}^n \sum<em>{j=1}^k g</em>{i j}\left|x_i-c_j\right|_2$<br>算法流程：</p>
</li>
</ul>
<ul>
<li>固定C,更新 G<ul>
<li>更新C中的每一列, 即类中心 $c_j$,其通过计算第j类中样本的平均值得到</li>
</ul>
</li>
</ul>
<p>K-mediods:模型： $\min <em>{G, C \subseteq X} \sum</em>{i=1}^n \sum<em>{j=1}^k g</em>{i j}\left|x_i-c_j\right|_1 \quad$ 可以是曼哈顿距离或其它距离度量;由于类中心的更新规 则，该方法较之于K-means更鲁棒。<br>  算法流程:</p>
<ul>
<li>固定C,更新 G</li>
<li>更新C中的每一列，即类中心$c<em>{j}$,对于第j类，**中心$c</em>{j}$需要通过遍历所有该类中的样本，取与该类所有样本距离和最小的样本为该中心。**</li>
</ul>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191556090.png" alt="图片" style="zoom: 67%;"></p>
<p><strong>对于每个训练实例，UNICORN创建一个模型，该模型捕获系统运行时执行状态的更新。直观地说，这类似于跟踪系统执行状态的自动机。</strong>最终的模型由训练数据中所有种源图的多个子模型组成。</p>
<h4><span id="45-异常检测">4.5 异常检测</span></h4><p>在部署期间，异常检测遵循前面章节中描述的相同流模式。UNICORN周期性地创建graph sketch，因为直方图从流式溯源图演变而来。给定一个概要图，UNICORN将该概要与建模期间学习的所有子模型进行比较，将其拟合到每个子模型中的一个聚类中。</p>
<p><strong>UNICORN假设监视从系统启动开始，并跟踪每个子模型中的系统状态转换。要在任何子模型中为有效，概要必须适合当前状态或下一个状态；否则，被视为异常。因此，我们检测到两种形式的异常行为：</strong></p>
<ul>
<li>不符合现有聚类的概要</li>
<li>聚类之间的无效转换</li>
</ul>
]]></content>
      <categories>
        <category>学术前沿</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（11）青藤云安全-安全狩猎</title>
    <url>/posts/285K7D8/</url>
    <content><![CDATA[<h2><span id="青藤云安全">青藤云安全</span></h2><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181517940.png" alt="image-20221105221126284"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181517783.png" alt="image-20221105221329660"></p>
<ul>
<li>护网检测的角度：交叉验证？？？</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181517592.png" alt="image-20221105232013125"></p>
<ul>
<li>索引的效率，压缩率</li>
<li>上下文，攻击树</li>
<li><strong>开源的解决方案？？？</strong></li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181518268.png" alt="image-20221105232149561"></p>
<h4><span id="可视化">可视化</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181518127.png" alt="image-20221105233353542"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181518088.png" alt="image-20221105235633129"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181518279.png" alt="image-20221105235813988"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181518619.png" alt="image-20221106150043663"></p>
<h4><span id="attck实战书">ATTCK，实战书</span></h4><h4><span id="attampck-数据源">ATT&amp;CK 数据源</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519913.png" alt="image-20221106001624464"></p>
<h4><span id="attampck-12-中一个套动作">ATT&amp;CK 12 中：一个套动作</span></h4><h4><span id="攻击链路的弱点">攻击链路的弱点</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519458.png" alt="image-20221106001713487"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519329.png" alt="image-20221106002243403"></p>
<h4><span id="告警确认">告警确认</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519141.png" alt="image-20221106150625666"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519976.png" alt="image-20221106150704168"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519449.png" alt="image-20221106002701972"></p>
<h3><span id="二-威胁搜猎案例分析">二、威胁搜猎案例分析</span></h3><p>在发现切入点之后，怎么把整个告警链路描绘出来？</p>
<ul>
<li>起点一般是？？</li>
<li>无文件攻击内存马；</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519752.png" alt="image-20221106151139571"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519579.png" alt="image-20221106151211979"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519132.png" alt="image-20221106151330278"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519605.png" alt="image-20221106151451348"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519493.png" alt="image-20221106151557997"></p>
<h4><span id="案例二">案例二</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519890.png" alt="image-20221106151644322"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519379.png" alt="image-20221106151759583"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519323.png" alt="image-20221106151841034"> </p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181519370.png" alt="image-20221106152030094"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520074.png" alt="image-20221106152054691"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520806.png" alt="image-20221106152157892"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520002.png" alt="image-20221106152227941"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520733.png" alt="image-20221106152258258"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520422.png" alt="image-20221106152429497"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520270.png" alt="image-20221106153100491"></p>
<p>  <img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520679.png" alt="image-20221106153126202"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520651.png" alt="image-20221106153149463"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520518.png" alt="image-20221106153209441"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520470.png" alt="image-20221106153342379"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520137.png" alt="image-20221106153413544"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520587.png" alt="image-20221106153521553"></p>
<p> <img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520102.png" alt="image-20221106153714241"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520726.png" alt="image-20221106153749604"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520328.png" alt="image-20221106153855615"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181520996.png" alt="image-20221106153825021"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521998.png" alt="image-20221106154040458"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521246.png" alt="image-20221106154854666"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521644.png" alt="image-20221106154914480"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521152.png" alt="image-20221106154934078"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521895.png" alt="image-20221106155005831"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521565.png" alt="image-20221106155018946"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521464.png" alt="image-20221106155119929">  </p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521605.png" alt="image-20221106155856100"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521537.png" alt="image-20221106160005755"></p>
<ul>
<li>sigma 社区；<ul>
<li>情报转换；</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（1）kaspersky《Machine Learning for Malware Detection》解析</title>
    <url>/posts/NVF6XT/</url>
    <content><![CDATA[<h2><span id="machine-learning-for-malware-detection">Machine Learning for Malware Detection</span></h2><blockquote>
<p>本文总结了 kaspersky 利用机器学习为客户建立高级保护的丰富经验。</p>
</blockquote>
<h3><span id="一-基本方法检测恶意软件">一、基本方法检测恶意软件</span></h3><p>一个高效、健壮、可扩展的恶意软件识别模块是每个网络安全产品的关键组成部分。恶意软件识别模块会根据其在该对象上所收集的数据来决定该对象是否构成威胁。这些数据可以在不同的阶段进行收集：</p>
<ul>
<li><p><strong>需要有代表性的大型数据集</strong></p>
<blockquote>
<p>必须强调这种方法的数据驱动特性。一个创建的模型在很大程度上依赖于它在训练阶段看到的数据，以确定哪些特征与预测正确的标签有统计关联。让我们来看看为什么制作一个具有代表性的数据集如此重要。</p>
</blockquote>
</li>
<li><p><strong>训练的模型必须是可解释的</strong></p>
<blockquote>
<p>目前使用的大多数模型家族，如深度神经网络，都被称为黑盒模型。黑盒子模型被给予输入X，它们将通过一个难以被人类解释的复杂操作序列产生Y。这可能会在现实生活中的应用程序中带来一个问题。例如，当出现错误警报时，我们想理解为什么会发生它，我们会问这是训练集还是模型本身的问题。模型的可解释性决定了我们管理它、评估其质量和纠正其操作的容易程度。</p>
</blockquote>
</li>
<li><p><strong><font color="red"> 假阳性率必须非常低</font></strong></p>
<blockquote>
<p><strong>当算法将良性文件错误地标记恶意标签时，就会发生误报。我们的目标是使假阳性率尽可能低，或为零</strong>。这在机器学习应用程序中并不常见。这是很重要的，因为即使在一百万个良性文件中出现一个误报，也会给用户带来严重的后果。这是复杂的事实，有许多干净的文件在世界上，他们不断出现。</p>
<p>为了解决这个问题，重要的是要对机器学习模型和指标施加高要求，并在<strong>训练期间进行优化，并明确关注低假阳性率(FPR)【FP/(FP+TN)】模型</strong>。</p>
<p>这还不够，因为之前看不见的新良性文件可能会被错误检测到。我们考虑到这一点，并实现了一个<strong>灵活的模型设计，允许我们动态地修复假阳性，而不需要完全重新训练模型</strong>。这些示例在我们的执行前和执行后的模型中实现，这将在以下章节中描述。</p>
</blockquote>
</li>
<li><p><strong><font color="red"> 算法必须允许我们快速调整它们以适应恶意软件作者的反击</font></strong></p>
<blockquote>
<p>在恶意软件检测领域之外，机器学习算法经常在<strong>固定数据分布的假设下工作，这意味着它不随时间变化</strong>。当我们有一个足够大的训练集时，我们可以训练模型，以便它将有效地推理测试集中的任何新样本。随着时间的推移，该模型将继续按预期进行工作。</p>
<p>在将机器学习应用于恶意软件检测后，我们必须面对我们的数据分布没有固定的事实：</p>
<ul>
<li>活跃的对手（恶意软件编写者）不断努力避免检测和发布新版本的恶意软件文件，这与在培训阶段看到的文件有显著不同。</li>
<li>成千上万的软件公司生产的新型良性可执行文件与以前已知的可执行文件显著不同。在训练集中缺乏关于这些类型的数据，但该模型需要识别出它们是良性的。</li>
</ul>
<p>这就导致了数据分布的严重变化，并提出了在任何机器学习实现中检测率随着时间的推移而下降的问题。在其反恶意软件解决方案中实现机器学习的网络安全供应商面临着这个问题，并需要克服它。<strong>该体系结构需要灵活，并且必须允许在再训练（retrain）之间“动态”更新模型</strong>。供应商还必须有<strong>有效的流程来收集和标记新样本，丰富训练数据集和定期的再训练模型。</strong></p>
</blockquote>
</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181501281.png" alt="image-20220429233456832" style="zoom:50%;"></p>
<h3><span id="二-卡巴斯基实验室机器学习应用">二、卡巴斯基实验室机器学习应用</span></h3><blockquote>
<p>上述真实世界恶意软件检测的特性使机器学习技术的直接应用成为一项具有挑战性的任务。<strong>在将机器学习方法应用于信息安全应用方面，卡巴斯基实验室拥有近十年的经验</strong>。</p>
<p>  在执行前检测新的恶意软件的<strong>相似性哈希</strong></p>
</blockquote>
<p>在杀毒行业的初期，计算机上的恶意软件检测是<strong>基于启发式特征</strong>，识别特定的恶意软件文件:</p>
<ul>
<li>代码段</li>
<li>代码片段或整个文件的哈希值</li>
<li>文件属性</li>
<li>以及这些特征的组合</li>
</ul>
<p>我们的主要目标是创建一个可靠的恶意文件指纹——一个功能的组合，可以快速检查。在此之前，这个工作流需要通过仔细选择指示恶意软件的代表性字节序列或其他特征来手动创建检测规则。在检测过程中，产品中的抗病毒引擎针对存储在防病毒数据库中的已知恶意软件指纹，检查文件中是否存在恶意软件指纹。</p>
<p>然而，恶意软件编写者发明了像服务器端多态性这样的技术。这导致每天都有成千十万的恶意样本被发现。同时，所使用的指纹对文件中的微小变化也很敏感。现有恶意软件的微小变化使它失去了雷达注意。前一种方法很快就变得无效了，因为：</p>
<ul>
<li>手动创建检测规则无法跟上新出现的恶意软件流。</li>
<li>针对已知恶意软件库检查每个文件的指纹意味着，在分析人员手动创建检测规则之前，您才能检测到新的恶意软件。</li>
</ul>
<p>我们感兴趣的是那些对文件中的小变化具有<strong>鲁棒性</strong>的特性。这些特性将检测到恶意软件的新修改，但不需要更多的资源来进行计算。性能和可伸缩性是反恶意软件引擎处理的第一阶段的关键优先事项。</p>
<p>为了解决这个问题，我们专注于提取以下特性：</p>
<ul>
<li>快速计算，如从<strong>文件字节内容</strong>或代码反汇编导出的统计数据</li>
<li>直接从可执行文件的结构中检索，比如<strong>文件格式描述</strong></li>
</ul>
<p><strong><font color="red"> 使用这些数据，我们计算了一种特定类型的哈希函数，称为局部敏感哈希(LSH)。</font></strong></p>
<blockquote>
<p>  ssdeep, TLSN 局部哈希</p>
</blockquote>
<p>两个几乎相同的文件的常规加密哈希和两个非常不同的文件的哈希差异一样大。文件的相似性和哈希值之间没有联系。然而，几乎相同的文件的LSHs映射到相同的二进制桶——它们的LSHs非常相似——而且概率非常高。两个不同文件的LSHs有很大差异。</p>
<blockquote>
<p><strong>LSH的基本思想是</strong>：将原始数据空间中的两个相邻数据点通过相同的映射或投影变换（projection）后，这两个数据点在新的数据空间中仍然相邻的概率很大，而不相邻的数据点被映射到同一个桶的概率很小。</p>
<ul>
<li><a href="https://colobu.com/2018/08/16/locality-sensitive-hashing/">https://colobu.com/2018/08/16/locality-sensitive-hashing/</a></li>
</ul>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181502195.png" alt="image-20220429233512068" style="zoom: 33%;"></p>
<p>但我们走得更远。LSH的计算是无监督的。它没有考虑到我们对每个样本都是恶意软件或良性的额外知识。</p>
<p>有一个相似和非相似对象的数据集，我们通过引入一个训练阶段来增强了这种方法。我们实现了一种相似性哈希方法。<strong>它类似于LSH，但它是==有监督==的，并且能够利用关于相似和非相似对象对的信息</strong>。在这种情况下：</p>
<ul>
<li>我们的训练数据X将是一对文件特征表示[X1, X2]</li>
<li>Y将是一个可以告诉我们这些物体在语义上是否真的相似的标签。</li>
<li>在训练过程中，该算法拟合哈希映射h(X)的参数，以最大化训练集中的对数，其中h(X1)和h(X2)对于相似的对象是相同的，而对于其他的对象是不同的。</li>
</ul>
<p>该算法正在应用于可执行文件特性，它提供了具有有用检测功能的特定相似性哈希映射。事实上，我们训练了这种映射的几个版本，它们对不同特征集的局部变化的敏感性不同。例如，一个版本的相似性散列映射可以更专注于捕获<strong>可执行文件结构</strong>，而较少关注实际内容。另一种方法可能更专注于捕获文件中的<strong>ascii字符串</strong>。</p>
<p>这就抓住了这样一种观点，即不同的特性子集可能或多或少可以区分不同类型的恶意软件文件。对于其中一个，文件内容统计数据可能显示未知恶意包装器的存在。对于其他方面，关于潜在行为的最重要信息集中在表示已使用的OSAPI、已创建的文件名、已访问的url或其他特性子集的字符串中。<strong>为了更精确的产品检测，将相似度哈希算法的结果与其他基于机器学习的检测方法相结合。</strong></p>
<blockquote>
<p>  <strong>基于局部敏感哈希（有监督）+ 决策树集成的用户计算机两阶段预执行检测</strong></p>
</blockquote>
<p>为了分析在预执行阶段的文件，我们的产品将相似性哈希方法与其他训练过的算法结合在一个两阶段的方案中。为了训练这个模型，我们使用了大量我们知道是恶意软件和良性的文件。</p>
<p><strong>两阶段分析设计解决了减少用户系统的==计算负载==和==防止误报==的问题。</strong>一些对检测很重要的文件特性需要更大的计算资源来计算它们。这些功能被称为“重的”。为了避免对所有扫描文件的计算，我们引入了一个称为预检测的初步阶段。当使用“轻量级”特性分析文件，并在系统上没有大量负荷的情况下提取文件时，就会发生==预检测==。<strong>在许多情况下，预检测为我们提供了足够的信息，以知道一个文件是否是良性的，并结束了文件扫描</strong>。有时它甚至会检测到一个文件是恶意软件。如果第一阶段不够，则文件将进入第二阶段的分析，即提取“重”特征以进行精确检测。</p>
<p>在我们的产品中，两阶段分析的工作方式如下。在<strong>预检测阶</strong>段，对扫描文件的<strong>轻量级特征</strong>计算学习到的相似度<strong>哈希映射</strong>。然后，检查是否有其他文件具有相同的哈希映射，以及它们是恶意软件还是良性的。一组具有类似哈希映射值的文件被称为哈希桶。根据扫描文件所属的散列桶，可能会出现以下结果：</p>
<ul>
<li><p>在一个简单的区域案例中，文件落入一个只包含一种对象的桶中：恶意软件或良性的。如果一个文件落入一个“纯恶意软件桶”，我们会检测到它是恶意软件。如果它落入一个“纯良性的桶”，我们不会扫描它更深。在这两种情况下，我们都不提取任何新的“重”特性。</p>
</li>
<li><p>在硬区域中，哈希桶同时包含恶意软件和良性文件。这是系统唯一可以从扫描文件中提取“重”特征以进行精确检测的情况。<strong>对于每个硬区域，都有一个单独的特定区域的分类器训练。</strong>目前，我们使用<strong>决策树集成</strong>或基于“<strong>重”特征的相似性哈希</strong>，这取决于哪个队硬区域更有效。</p>
</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181502256.png" alt="image-20220429233534859" style="zoom: 33%;"></p>
<blockquote>
<p><strong>对象空间的分割</strong>：</p>
<p>使用相似性散列映射创建的对象空间的分割的示意图表示。为简单起见，该插图只有两个维度。每个单元格的一个索引对应于特定的<strong>相似度哈希映射值</strong>。网格中的每个单元格都说明了一个具有相同相似性哈希映射值的对象区域，也称为<strong>哈希桶</strong>。点颜色：恶意（<strong>红色</strong>）和良性/未知（<strong>绿色</strong>）。有两种选项可用的：将一个区域的散列添加到恶意软件数据库（简单区域）中，或者将其作为两阶段检测器的第一部分，并与特定区域的分类器（硬区域）结合使用。</p>
</blockquote>
<p>在现实中，有一些困难的区域不适合用这种两阶段的技术进行进一步的分析，因为它们<strong>包含了太多流行的良性文件</strong>。用这种方法处理它们会产生假阳性和性能下降的高风险。对于这种情况，我们不训练特定区域的分类器，也不通过该模型扫描该区域中的文件。为了在这样的区域进行正确的分析，我们使用了<strong>其他的检测技术</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181502023.png" alt="image-20220429233555509" style="zoom:50%;"></p>
<p>预检测阶段的实现大大减少了在第二步中被大量扫描的文件的数量。这个过程提高了性能，因为在预检测阶段通过相似哈希映射查找可以快速完成。</p>
<p>我们的两阶段设计也降低了假阳性的风险：</p>
<ul>
<li>在第一个（预检测）阶段，我们不能在假阳性风险较高的区域使用特定区域的分类器进行检测。正因为如此，传递到第二阶段的对象的分布偏向于“恶意软件”类。这也降低了假阳性率。</li>
<li>在第二阶段，<strong>每个硬区域的分类器只从一个桶上对恶意软件进行训练，但在训练集的所有桶中可用的所有干净对象上。这使得区域分类器能够更精确地检测特定硬区域桶的恶意软件</strong>。当模型在具有真实数据的产品中工作时，它还可以防止任何意外的假阳性。</li>
</ul>
<p>两阶段模型的可解释性来自于数据库中的每个散列都与训练中的一些恶意软件样本子集相关联。<strong>整个模型可以通过添加检测来适应一个新的恶意软件流，包括散列映射和一个以前未观察到的区域的树集成模型。这允许我们撤销和重新训练特定区域的分类器，而不显著降低整个产品的检测率</strong>。如果没有这个，我们将需要对整个模型重新培训所有我们知道的恶意软件，我们想要做的每一个改变。话虽如此，两阶段恶意软件检测适用于在介绍中讨论的机器学习的细节。</p>
<h3><span id="三-针对罕见攻击的深度学习">三、针对罕见攻击的深度学习</span></h3><p>通常，当恶意和良性样本在训练集中大量表示时，机器学习会面对任务。但是有些攻击是如此罕见，以至于我们只有一个恶意软件进行训练的例子。这是针对性高调的有针对性攻击的典型情况。在这种情况下，我们使用了一个非常特定的基于深度学习的模型架构。我们将这种方法称为<strong>exemplar network( ExNet)</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181502308.png" alt="image-20220429233615467"></p>
<p>这里的想法是，我们训练模型来构建<strong>输入特征的紧凑表示</strong>。然后，我们使用它们来同时训练多个<strong>单范例的分类器（per-exemplar classifiers）</strong>——这些都是检测特定类型的恶意软件的算法。深度学习允许我们将这些多个步骤（对象特征提取、紧凑的特征表示和局部的，或每个范例的模型创建）结合到一个神经网络管道中，它可以提取各种类型的恶意软件的鉴别特征。</p>
<p>该模型可以有效地<strong>推广关于单个恶意软件样本</strong>和大量干净样本收集的知识。然后，它可以检测到相应的恶意软件的新修改。</p>
<h3><span id="四-在执行后行为检测中的深度学习">四、在执行后行为检测中的深度学习</span></h3><p>前面描述的方法是在静态分析的框架中考虑的，即在真实用户环境中执行对象之前提取和分析对象描述。</p>
<p>执行阶段的静态分析有许多显著的优势。其主要优点是它对用户来说是安全的。一个对象可以在它开始作用于真实用户的机器之前被检测到。但它面临着高级加密、混淆技术以及使用各种高级脚本语言、容器和无文件攻击场景的问题。这些情况都是当执行后的行为检测开始发挥作用的情况。</p>
<p>我们还使用深度学习方法来解决<strong>行为检测的任务</strong>。在执行后阶段，我们正在使用<strong>威胁行为引擎</strong>提供的<strong>行为日志</strong>。行为日志是在进程执行过程中发生的系统事件的序列，以及相应的参数。为了检测观察到的日志数据中的恶意活动，我们的模型将得到的事件序列压缩为一组二进制向量。然后，它训练一个深度神经网络来区分干净的和恶意的日志。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181502002.png" alt="image-20220429233634048"></p>
<p><strong>日志的压缩阶段</strong>包括以下几个步骤：</p>
<ul>
<li><p>将该日志转换为一个二部行为图。此图包含两种类型的顶点：事件和参数。在每个事件和参数之间绘制边，它们一起出现在日志中的同一行中。这样的图表示比初始的原始数据要紧凑得多。它对跟踪同一多处理程序的不同运行或分析过程的行为混淆所导致的任何行排列保持鲁棒性</p>
</li>
<li><p>之后，我们将自动从该图中提取特定的子图或行为模式。每个模式都包含与进程的特定活动相关的事件和相邻参数的子集，如网络通信、文件系统探索、系统寄存器的修改等</p>
</li>
<li>我们将每个“行为模式”压缩为一个稀疏的二进制向量。此向量的每个组件负责在模板中包含一个特定的事件或参数的令牌(与web、文件和其他类型的活动相关)。</li>
<li>训练后的深度神经网络将行为模式的稀疏二进制向量转换为称为<strong>模式嵌入</strong>的紧凑表示。然后将它们组合成一个单个向量，或进行<strong>日志嵌入</strong></li>
<li>最后，在日志嵌入的基础上，网络预测了日志的可疑性。</li>
</ul>
<p>所使用的神经网络的主要特征是所有的权值都是正的，所有的激活函数都是单调的。这些特性为我们提供了许多重要的优势：</p>
<ul>
<li><strong>在处理日志中的新行时，我们的模型的怀疑分数输出只随着时间的推移而增长。因此，恶意软件不能通过与它的主有效负载并行地执行额外的噪声或“干净的”活动来逃避检测</strong>。</li>
<li>由于模型的输出在时间上是稳定的，我们可能会免受由于在扫描干净日志时的预测波动而导致的最终错误警报。</li>
<li>在单调空间中处理日志样本，允许我们自动选择导致检测的事件，并更方便地管理假警报</li>
</ul>
<p>这样的方法使我们能够训练一个能够使用高级可解释的行为概念进行操作的深度学习模型。这种方法可以安全地应用于整个用户环境的多样性，并在其架构中集成了假告警修复能力。总之，所有这些都为我们提供了一种对行为检测最复杂的现代威胁的有力手段。</p>
<h3><span id="五-基础设施中的应用malware-hunter">五、基础设施中的应用（Malware Hunter）</span></h3><p>从有效处理卡巴斯基实验室的恶意软件流到维护大规模检测算法，<strong>机器学习在建立适当的实验室基础设施中发挥着同样重要的作用</strong>。</p>
<h4><span id="51-聚类传入的对象流">5.1 聚类传入的对象流</span></h4><p><strong>每天都有成千上万的样本进入卡巴斯基实验室，同时人工对新型样本进行注释的高昂成本，减少分析师需要查看的数据量成为一项至关重要的任务</strong>。使用有效的聚类算法，我们可以从难以忍受的独立的未知文件数量增加到合理数量的对象组。这些对象组的部分将根据其中已注释的对象自动处理。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181502650.png" alt="image-20220429232153712" style="zoom:50%;"></p>
<p>所有最近收到的传入文件都通过我们的实验室恶意软件检测技术进行分析，包括执行前和执行后。我们的目标是标记尽可能多的对象，但有些对象仍然未分类。我们想给它们贴上标签。为此，所有的对象，包括已标记的对象，都由<strong>多个特征提取器</strong>进行<strong>处理</strong>。然后，根据<strong>文件类型</strong>，它们通过几种聚类算法(例如k-means和dbscan)一起传递。这将产生类似的对象组。</p>
<p>在本文的这一点上，我们将面对四种不同类型的具有未知文件的结果集群：</p>
<ol>
<li>包含恶意软件和未知文件的集群；</li>
<li>包含干净和未知文件的集群；</li>
<li>包含恶意软件、干净和未知文件的集群；</li>
<li>仅包含未知文件的集群。</li>
</ol>
<p>对于类型1-3簇中的对象，我们使用额外的机器学习算法，如<strong>贝叶斯网络（belief propagation）</strong>，<strong>来验证未知样本与分类样本的相似性</strong>。在某些情况下，这甚至在第3类集群中也是有效的。这使我们能够自动标记未知文件，只为人类留下4型和部分3型的集群。这导致了每天所需的人类注释的急剧减少。</p>
<h4><span id="蒸馏工艺包装更新内容">蒸馏工艺：包装更新内容</span></h4><p>我们在实验室中检测恶意软件的方式不同于针对用户产品的最佳算法。一些最强大的分类模型需要大量的资源，如CPU/GPU的时间和内存，以及昂贵的特性提取器。</p>
<p>例如，由于大多数现代恶意软件编写者使用<strong>高级包装器和混淆器</strong>来隐藏有效负载功能，机器学习模型确实受益于使用具有高级行为日志的实验室内沙箱的执行日志。同时，在用户的机器上的预执行阶段收集这类日志的计算量可能会很高。它可能会导致显著的系统性能下降。</p>
<blockquote>
<p>对于加壳的恶意样本，需要沙箱分析，在用户终端</p>
</blockquote>
<p>在实验室中保存和运行那些<strong>“重型”</strong>模型更有效。一旦我们知道一个特定的文件是恶意软件，我们就会使用我们从模型中获得的知识来训练将在我们的产品中工作的<strong>轻量</strong>级分类器（to 用户）。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181502687.png" alt="image-20220429232807391" style="zoom:50%;"></p>
<p>在机器学习中，这个过程被称为<strong>蒸馏</strong>。我们用它来教我们的产品检测新的恶意软件：</p>
<ul>
<li>在我们的实验室中，我们首先从标记的文件中提取一些耗时的特征，并对它们训练一个“沉重的”在实验室内的模型。</li>
<li>我们取一个未知文件集群，并使用我们的“重”实验室模型来对它们进行标签。</li>
<li>然后，我们使用新标记的文件来扩充轻量级分类模型的训练集。</li>
<li>我们向用户的产品提供了这个轻量级的模型。</li>
</ul>
<p>蒸馏使我们能够有效地输出我们的知识的新的和未知的威胁给我们的用户。</p>
<h4><span id="总结">总结</span></h4><p>将常规任务传递给算法会给我们留下更多的时间来研究和创建。这使我们能够为客户提供更好的保护。通过我们的努力、失败和胜利，我们已经了解到什么对于让机器学习对恶意软件检测产生它的卓越影响是重要的。</p>
<h5><span id="亮点">亮点：</span></h5><ul>
<li><strong>有正确的数据</strong>：这是机器学习的燃料。这些数据必须具有代表性，与当前的恶意软件环境相关，并在需要时正确标记。我们成为了在提取和准备数据以及训练我们的算法方面的专家。我们用数十亿个文件样本进行了有效的收集，以增强机器学习的能力。</li>
<li><p><strong>了解理论机器学习以及如何将其应用于网络安全。</strong>我们了解机器学习是如何工作的，并跟踪该领域出现的最先进的方法。另一方面，我们也是网络安全方面的专家，我们认识到每一种创新的理论方法给网络安全实践带来的价值。</p>
</li>
<li><p><strong>了解用户的需求，并成为将机器学习实现到帮助用户满足其实际需求的产品中的关键</strong>。我们使机器学习有效和安全地工作。我们建立了网络安全市场所需要的创新解决方案</p>
</li>
<li><p><strong>建立一个足够的用户基础</strong>：这引入了“群众”检测质量的力量，并给我们需要的反馈，告诉我们是对是错。</p>
</li>
<li><strong>保持检测方法的多层协同作用</strong>。只要当今的先进威胁攻击载体如此多样化，网络安全解决方案就应该提供<strong>多层的保护</strong>。在我们的产品中，<strong>基于机器学习的检测与其他类型的检测协同工作，以一种多层的现代网络安全保护方法进行工作。</strong></li>
</ul>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（2）CrowdStrike《无文件攻击白皮书》解析</title>
    <url>/posts/17SDQ9V/</url>
    <content><![CDATA[<h2><span id="the-forrester-wavetm-endpoint-detection-and-response-providers-q2-2022"><strong>The Forrester Wave™: Endpoint Detection And Response Providers, Q2 2022</strong></span></h2><p>在我们对端点检测和响应提供商的 20 个标准评估中，我们确定了 15 个最重要的——Bitdefender、BlackBerry Cylance、Check Point Software Technologies、CrowdStrike、Cybereason、Elastic、FireEye、Fortinet、McAfee、Microsoft、Palo Alto Networks、SentinelOne 、Sophos、趋势科技和 VMware Carbon Black — 并对它们进行了研究、分析和评分。该报告显示了每个供应商如何衡量并帮助安全专业人员根据他们的需求选择合适的供应商。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181505869.png" alt="image-20220426183404208" style="zoom: 33%;"></p>
<h1><span id="crowdstrike无文件攻击白皮书解析">CrowdStrike《无文件攻击白皮书》解析</span></h1><blockquote>
<p>  原文链接：<a href="https://www.secrss.com/articles/26671">https://www.secrss.com/articles/26671</a></p>
</blockquote>
<p>CrowdStrike是端点保护平台（EPP）的最强者，是云交付的下一代端点保护的领导者。由于CrowdStrike邮件推送了“<strong>无文件攻击白皮书</strong>”《谁需要恶意软件？<strong>对手如何使用无文件攻击来规避你的安全措施</strong>》（Who Needs Malware? How Adversaries Use Fileless Attacks To Evade Your Security），笔者顺手对其进行了<strong>全文翻译</strong>。</p>
<p><strong>无文件攻击（Fileless attack）</strong>是<strong>不向磁盘写入可执行文件</strong>的攻击方法，难以被常规方法检测出来。根据CrowdStrike统计，“10个成功突破的攻击向量中有8个使用了无文件攻击技术。”即 <strong>80%的成功入侵都使用了无文件攻击</strong> <strong>。</strong> 根据二八原理，这显然是安全人员应该高度关注的技术类型。</p>
<p><strong>无文件攻击白皮书</strong>解释了<strong>无文件攻击的工作原理、传统解决方案失效的原因，以及CrowdStrike解决该难题的方法</strong>。CrowdStrike的解决方案是<strong>应用程序清单、漏洞利用阻断、攻击指标（IOA）、托管狩猎、无签名人工智能等技术的集成化方法</strong>。</p>
<p>除了全文翻译外，笔者主要在文末做了两块内容增补：一是关于CrowdStrike<strong>Threat Graph（威胁图</strong>）的技术思想<strong>，因为威胁图 是 CrowdStrike Falcon（猎鹰）端点保护平台的“ </strong>大脑” ；二是 关于CrowdStrike提出的<strong>攻击指标</strong>（IOA）<strong>与传统的失陷指标（IOC）</strong>的对比。<strong>笔者认为，这两类技术领域具有战略价值</strong>。如果说全文翻译只花费一小时的话，这些增补内容反而消耗了笔者两个小时。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181505079.png" alt="img" style="zoom:33%;"></p>
<p>随着安全措施在检测和阻止恶意软件和网络攻击方面越来越出色，对手和网络犯罪分子被迫不断开发新技术来逃避检测。其中一种高级技术涉及<strong><font color="red"> “无文件”（fileless）攻击，即不向磁盘写入可执行文件。</font></strong>这类攻击特别有效地躲避了传统防病毒（AV）解决方案，因为传统防病毒（AV）方法寻找被保存到磁盘上的文件并扫描这些文件以确定它们是否恶意。</p>
<p>虽然无文件攻击并不新鲜，但它们正变得越来越普遍。在2016年的调查中，CrowdStrike Services事件响应团队发现，<strong>10个成功入侵的攻击向量中有8个使用了无文件攻击技术</strong>。为了帮助您了解无文件攻击所带来的风险，本白皮书解释了无文件攻击的工作原理、当前解决方案对其无能为力的原因，以及CrowdStrike解决这一难题的行之有效的方法。</p>
<h3><span id="什么是无文件攻击">什么是无文件攻击？</span></h3><p>当攻击者通过消除将PE（可移植可执行文件）复制到磁盘驱动器的传统步骤来逃避检测时，就会发生无文件或无恶意软件的攻击。<strong>有多种技术可以采取这种方式危害系统</strong>。</p>
<ul>
<li><p><strong>漏洞利用和漏洞利用工具包</strong>，通常通过利用操作系统（OS）或已安装应用程序中存在的漏洞，直接在内存中执行攻击。</p>
</li>
<li><p><strong>使用盗用的凭证</strong>，是发起无文件攻击的另一种普遍方法。Verizon在其2017年的DBIR（数据泄露调查报告）中发现，81%的数据泄露涉及弱口令、默认口令或被盗口令，比上一年增加了18%。这使得攻击者能够像普通用户一样访问系统。</p>
</li>
<li><p>一旦实现初步突破，对手就可以<strong>依赖操作系统本身提供的工具</strong>，如Windows管理工具和Windows <strong>PowerShell</strong>，以执行进一步的操作，而不必将文件保存到磁盘。例如，它们可以通过在注册表、内核中隐藏代码，或者通过创建允许它们随意访问系统的用户帐户来<strong>建立持久化</strong>，而无需向磁盘写入任何内容。</p>
</li>
</ul>
<p>在安全行业，对上述这些技术的使用，通常被称为“<strong>living off the land</strong>”（离地生存？生存手段？）。</p>
<h4><span id="真实案例一个无文件入侵的解剖">真实案例：一个无文件入侵的解剖</span></h4><p>通过展示CrowdStrike Services事件响应团队发现的一个真实的案例，我们可以检查端到端的无恶意软件入侵是什么样子。</p>
<p>在本例中，首个目标是<strong>使用Microsoft ISS并运行SQL服务器数据库的web服务器</strong>。对于最初的入侵，攻击者使用了一个web shell，一个可以被上传到web服务器并在其上执行的简短脚本。脚本可以用web服务器支持的任何语言编写，比如Perl、Python、ASP或PHP。Web Shell在此类攻击中很流行，因为它们可以通过<strong>利用系统上存在的漏洞直接加载到内存中</strong>，而无需将任何内容写入磁盘。在这一特定的攻击中，对手使用SQL注入，将其web shell嵌入到服务器。</p>
<blockquote>
<p>  WEB Shell允许使用web浏览器来远程访问系统。它们可以用ASP或PHP或任何其他web脚本语言编写，代码可以非常小。如下所示：</p>
<p>  <img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181506159.png" alt="img" style="zoom:50%;"></p>
<p>  由于web服务器<strong>没有正确检查转义字符</strong>，攻击者能够简单地将web shell回传到服务器上。所用的web shell，名为“<strong>China Chopper</strong>”（中国菜刀/直升机），包含了JavaScript命令，值得注意的是它只使用了72个字符。在内存中执行web shell，使攻击者能够使用Chopper用户界面，对web服务器执行任意命令。通过对web服务器的完全远程访问，攻击者通过执行编码的PowerShell命令，来窃取凭据。</p>
</blockquote>
<p>第一步是从远程服务器下载脚本，将脚本直接加载到内存中，然后执行它。这个脚本反过来窃取了缓存在web服务器内存中的所有纯文本密码。在几秒钟内，攻击者获得了系统上所有帐户的多个用户名和密码。</p>
<p><strong>PowerShell是一个合法的Windows工具</strong>，它允许攻击者在受损系统上执行任何操作，而不必在磁盘上写入恶意软件。为了进一步做混淆，攻击者可以对其PowerShell脚本进行编码，如下所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181506444.png" alt="img" style="zoom:50%;"></p>
<p>下一步是让攻击者在服务器上实现<strong>持久化</strong>。为了在不需要任何恶意软件的情况下执行此操作，攻击者使用了一种称为“<strong>粘滞键</strong>”（Sticky Keys）的技术。通过修改Windows注册表中的一行，攻击者可以使用PowerShell或WMI命令，轻松完成此操作，从而将Windows屏幕键盘进程，设置为<strong>调试模式</strong>。</p>
<p><strong>粘滞键（Sticky Keys）</strong>是使攻击者<strong>无需登录凭据</strong>即可<strong>访问命令shell</strong>的注册表项。如下所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181506654.png" alt="img" style="zoom:50%;"></p>
<p>当设置为调试模式时，屏幕键盘允许具有远程访问权限的任何人，以系统权限打开命令行，而无需登录。一旦设置了该注册表项，攻击者可以通过打开到web服务器的远程桌面连接，随时返回。此外，访问系统而<strong>不在Windows事件历史记录中生成登录事件</strong>，会使攻击者的行为几乎无法追踪。</p>
<blockquote>
<p>  现在小结一下，本案例中，<strong>在不同的攻击阶段所使用的文件技术</strong>包括：</p>
<ul>
<li><p><strong>初始入侵</strong>：针对Web服务器的<strong>SQL注入</strong>攻击；</p>
</li>
<li><p><strong>命令与控制（C2）</strong>：“中国菜刀/直升机”的<strong>Web Shell</strong>；</p>
</li>
<li><p><strong>提升权限</strong>：使用<strong>PowerShell脚本</strong>转储<strong>凭据</strong>；</p>
</li>
<li><p><strong>建立持久化</strong>：修改<strong>注册表</strong>粘滞键技术。</p>
</li>
</ul>
</blockquote>
<h3><span id="作案工具真实世界的无文件恶意软件">作案工具：真实世界的无文件恶意软件</span></h3><p>无文件恶意软件经常使用的工具和技术包括：</p>
<ul>
<li><strong>漏洞利用工具包</strong></li>
<li><strong>利用合法工具，如WMI和PowerShell</strong></li>
<li><strong>使用被盗凭证</strong></li>
<li><strong>注册表驻留恶意软件</strong></li>
<li><strong>内存型（Memory-only）恶意软件</strong></li>
</ul>
<p><strong>1）漏洞利用工具包（Exploit kits）</strong></p>
<p>漏洞利用是一种允许攻击者利用操作系统或应用程序漏洞来访问系统的技术。<strong>漏洞利用是一种高效的无文件技术</strong>，因为它们可以直接注入内存中，而无需将任何内容写入磁盘。</p>
<p>通过允许攻击者自动化和大规模执行初始突破，漏洞利用工具包使得攻击者的生活更轻松、工作更高效。<strong>所需要做的只是，诱使受害者进入漏洞利用工具包服务器</strong>，办法通常是网络钓鱼或社会工程。</p>
<p>这些工具包通常提供对许多漏洞的攻击，以及一个管理控制台，一旦成功利用漏洞，攻击者就可以控制失陷的系统。有些漏洞利用工具包甚至提供了扫描受害者系统中的漏洞的功能，因此可以快速构建并启动成功的漏洞攻击。</p>
<p><strong>2）注册表驻留恶意软件</strong></p>
<p>注册表驻留恶意软件是安装在Windows注册表中的恶意软件，以便在逃避检测的同时，保持持久性。第一种是<strong>Poweliks</strong>，此后就出现了许多变体。一些变体，如Kovter，使用了类似的注册表隐藏技术，来保持不被发现。Poweliks<strong>调用C2（命令和控制）服务器</strong>，攻击者可以从该服务器向受损系统发送进一步的指令。所有这些操作，都可以在没有任何文件写入磁盘的情况下进行。</p>
<p><strong>3）内存型（Memory-only）恶意软件</strong></p>
<p>有些恶意软件只存在于内存中，以逃避检测。新版本的<strong>Duqu蠕虫</strong>就是这种情况，它只驻留在内存中，不会被发现。Duqu 2.0有两个版本：第一个是后门，它允许攻击者在组织中站稳脚跟。如果攻击者认为目标值得攻击，他可以使用Duqu 2.0的高级版本，该版本提供了诸如侦察、横向移动、数据渗出等附加功能。Duqu2.0以成功攻破电信行业的公司以及至少一家知名的安全软件提供商而闻名。</p>
<p><strong>4）无文件型勒索软件</strong></p>
<p>甚至勒索软件攻击者，现在也在使用无文件技术，来实现他们的目标。在这类勒索软件中，<strong>恶意代码要么嵌入文档中以使用本机脚本语言（如宏），要么使用漏洞直接写入内存</strong>。然后，勒索软件<strong>使用合法的管理工具如PowerShell，来加密人质文件</strong>，而所有这些都不需要写入磁盘。</p>
<h3><span id="为何传统技术无法抵御无文件攻击">为何传统技术无法抵御无文件攻击</span></h3><p>由于传统安全解决方案极难检测到无文件攻击，因此无文件攻击正在增加。让我们来看看，为什么当今市场上的一些端点保护技术，对这些无恶意软件入侵如此脆弱。</p>
<p><strong>1）传统防病毒（AV）</strong>旨在寻找已知恶意软件的特征码。由于无文件攻击没有恶意软件，所以AV没有可检测的特征码。</p>
<p><strong>2）基于机器学习（ML）</strong>的反恶意软件方法，在应对无文件攻击时，面临着与传统AV相同的挑战。<strong>ML动态分析</strong>未知文件，并将其区分为好的或坏的。但是我们已经注意到，在无文件攻击中，<strong>没有要分析的文件</strong>，因此ML无法提供帮助。</p>
<p><strong>3）白名单方法</strong>包括列出一台机器上所有良好的进程，以防止未知进程执行。无文件攻击的问题在于，它们利用易受攻击的合法白名单应用程序，并利用内置的操作系统可执行文件。阻止用户和操作系统共同依赖的应用程序，并不是一个好的选项。</p>
<p><strong>4）使用失陷指标（IOC）工具</strong>来防止无文件攻击也不是很有效。本质上，<strong>IOC类似于传统的AV签名，因为它们是攻击者留下的已知恶意制品</strong>。然而，由于它们利用合法的进程，并且在内存中操作，所以无文件攻击不会留下制品，因此IOC工具几乎找不到任何东西。</p>
<p><strong>5）另一种方法涉及沙箱</strong>，它可以采取多种形式，包括基于网络的爆破和微虚拟化。由于无文件攻击不使用PE文件，因此沙盒没有什么可爆破的。即便真有东西被发送到沙箱，因为无文件攻击通常会劫持合法进程，大多数沙箱也都会忽略它。</p>
<h3><span id="crowdstrike的解决方案">CrowdStrike的解决方案</span></h3><p>正如我们所看到的，如果您依赖基于签名的方法、沙盒、白名单甚至机器学习保护方法，那么想检测无文件技术是非常有挑战性的。</p>
<p>为了抵御秘密的、无文件的攻击，<strong>CrowdStrike独特地将多种方法结合到一个强大的集成式方法中，提供了无与伦比的端点保护</strong>。CrowdStrikeFalcon平台通过单个的轻量级代理提供了云原生的下一代端点保护，并提供一系列互补的预防和检测方法：</p>
<ul>
<li><p><strong>应用程序清单（Application inventory）</strong>：可以发现在您的环境中运行的任何应用程序，帮助您找到漏洞，以便您可以修补或更新它们，使之不会成为漏洞利用工具包的目标。</p>
</li>
<li><p><strong>漏洞利用阻断（Exploit blocking）</strong>：通过未修补漏洞的漏洞利用方法，来阻断无文件攻击的执行。</p>
</li>
<li><p><strong><font color="red"> 攻击指标（IOA，Indicators of Attack）：</font></strong>在攻击的早期阶段，识别并阻止恶意活动，以免其完全执行并造成损害。此能力还可以防止那些新的勒索软件类别，那些勒索软件不使用文件加密受害者系统。**</p>
</li>
<li><p><strong>托管狩猎（Managed hunting）</strong>：全天候地主动搜索由于无文件技术而产生的恶意活动。</p>
</li>
</ul>
<h4><span id="ioa攻击指标的力量">IOA（攻击指标）的力量</span></h4><blockquote>
<p>  <strong>IOA检测恶意软件或攻击完成其任务所必须执行的事件序列, IOA关注的是所执行的行为、它们之间的关系、它们的顺序、它们的依赖性，将它们视为揭示一系列事件背后真实意图和目的的指标。IOA不关注攻击者使用的特定工具和恶意软件。</strong></p>
</blockquote>
<p>IOA之所以引人注目，是因为它们提供了<strong>针对无文件攻击的独特的主动预防能力</strong>。<strong>IOA寻找攻击可能正在进行的迹象，而不是关心攻击的步骤是如何执行的</strong>。<strong>迹象可以包括代码执行、试图隐身、横向移动等等</strong>。<strong>如何启动或执行这些步骤对IOAs来说并不重要</strong>。</p>
<p><strong><font color="red"> IOA关注的是所执行的行为、它们之间的关系、它们的顺序、它们的依赖性，将它们视为揭示一系列事件背后真实意图和目的的指标。IOA不关注攻击者使用的特定工具和恶意软件。</font></strong></p>
<p>此外，在无文件攻击的情况下，恶意代码可以利用诸如PowerShell之类的合法脚本语言，而无需写入磁盘。正如我们所看到的，这对于基于签名的方法、白名单、沙箱甚至机器学习来说都是一个挑战。相比之下，<strong>IOA检测恶意软件或攻击完成其任务所必须执行的事件序列。</strong>这可以暴露最隐秘的无文件方法，因此它们可以被迅速处理。</p>
<p>最后，由于IOA会查看意图、上下文、活动序列，因此<strong>即使恶意活动是使用合法帐户实施的，也可以检测和阻止这些活动</strong>，攻击者使用窃取的凭据时通常会出现这种情况。</p>
<p>所有这些使得<strong>IOA成为防止无文件恶意软件攻击的突破口</strong>。IOA不再基于磁盘上可执行文件的存在与否，来开展无文件攻击这场徒劳的战斗，而是在<strong>造成任何损害之前监视、检测、阻止此类攻击的影响</strong>。</p>
<h4><span id="托管狩猎">托管狩猎</span></h4><p>托管狩猎（Managed Hunt）是针对文件攻击的另一种独特而有效的防御措施。<strong>Falcon OverWatch</strong>（猎鹰看守）是Falcon平台的<strong>威胁搜索组件</strong>，它提供了一个<strong>额外的保护层</strong>，来抵御无文件攻击。</p>
<p><strong>利用Falcon平台的强大功能，OverWatch（看守）团队全天候主动狩猎威胁，监控客户的环境，并狩猎那些标准安全技术无法检测到但可能表明正在发生攻击的狡猾活动。</strong>Falcon OverWatch确保即使是最复杂和最隐蔽的攻击，也能在发生时被发现。它通过狩猎和识别难以检测的、复杂的、尖端的攻击，生成有意义的警报和精确的指导性补救建议，从而提高您对抗无文件技术的效率。</p>
<h4><span id="结论">结论</span></h4><p>由于漏洞利用工具包的存在，导致了攻击组合的高效性和易创建性，很可能会提高无文件黑客技术的普及率。不幸的是，鉴于传统杀毒软件无法阻止无文件攻击，犯罪黑客越来越可能将注意力集中在这些隐形技术上。因此，安全专家需要在他们的安全策略中，考虑无文件恶意软件和无文件攻击的存在。</p>
<p>正如本文所解释的那样，传统的安全措施在面对无文件攻击时可能不够有效，需要新的保护方法。CrowdStrike Falcon（猎鹰平台）提供了一种全面的解决方案，不仅可以防御无文件攻击，而且还可以很好地防御已知和未知的恶意软件威胁。</p>
<h3><span id="威胁图和攻击指标">威胁图和攻击指标</span></h3><p>CrowdStrike将自身定位为<strong>云交付</strong>的<strong>下一代端点保护</strong>的领导者。CrowdStrike是第一家也是唯一一家统一了<strong>下一代防病毒、端点检测和响应（EDR）、IT卫生、漏洞评估、全天候托管狩猎服务</strong>的公司——全部通过<strong>一个轻量级代理</strong>提供，彻底革新了端点保护。CrowdStrike的<strong>端点保护平台</strong>（EPP）如下图所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181507229.png" alt="img" style="zoom:50%;"></p>
<h4><span id="威胁图threat-graph">威胁图（Threat Graph）</span></h4><p>CrowdStrike Falcon（猎鹰）平台由两种紧密集成的<strong>专有技术</strong>组成：一个是易于部署的智能<strong>轻量级代理</strong>；另一个是基于云的动态<strong>图数据库</strong>，即上面提到的<strong>威胁图</strong>（Threat Graph）。</p>
<p><strong>下表进一步梳理了威胁图的主要特性</strong>：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181507209.png" alt="img" style="zoom:50%;"></p>
<p><a href="https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/ioa-vs-ioc/">https://www.crowdstrike.com/cybersecurity-101/indicators-of-compromise/ioa-vs-ioc/</a></p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（3）Sophos《Learning from Context: Exploiting and Interpreting File Path Information for Better Malware Detection》</title>
    <url>/posts/3PHFHZZ/</url>
    <content><![CDATA[<h2><span id="learning-from-context-exploiting-and-interpreting-file-path-information-for-better-malware-detection">Learning from Context: Exploiting and Interpreting File Path Information for Better Malware Detection</span></h2><ul>
<li><a href="https://ai.sophos.com/presentations/learning-from-context-a-multi-view-deep-learning-architecture-for-malware-detection/">https://ai.sophos.com/presentations/learning-from-context-a-multi-view-deep-learning-architecture-for-malware-detection/</a></li>
</ul>
<blockquote>
<p>  用于静态可移植可执行（PE）恶意软件检测的机器学习（ML）通常使用每个文件的数字特征向量表示作为训练期间一个或多个目标标签的输入。然而，可以从查看文件的上下文中收集到许多正交信息。在本文中，我们<strong>建议使用静态上下文信息源（PE文件的路径）作为分类器的辅助输入</strong>。虽然文件路径本身不是恶意或良性的，但它们确实为恶意/良性判断提供了有价值的上下文。与动态上下文信息不同，文件路径的可用开销很小，并且可以无缝集成到多视图静态ML检测器中，在非常高的吞吐量下产生更高的检测率，同时基础结构的更改也很小。在这里，我们提出了一种多视图神经网络，它从PE文件内容以及相应的文件路径中提取特征向量作为输入并输出检测分数。为了确保真实的评估，我们使用了大约1000万个样本的数据集—来自实际安全供应商网络的用户端点的文件和文件路径。<strong>然后，我们通过LIME建模进行可解释性分析，以确保我们的分类器已学习到合理的表示，并查看文件路径的哪些部分对分类器得分的变化贡献最大</strong>。我们发现，我们的模型学习了文件路径的有用方面以进行分类，同时还学习了测试供应商产品的客户的工件，例如，通过下载恶意软件样本目录，每个样本都命名为其哈希。我们从我们的测试数据集中删减了这些工件，并证明在10−3假阳性率（FPR），10时为33.1%−4 FPR，基于类似拓扑的单输入PE文件内容模型。</p>
</blockquote>
<h3><span id="摘要">摘要</span></h3><p>用于恶意软件检测的机器学习（ML）分类器通常在进行恶意/良性判断时使用每个文件内容的数字表示。<strong>然而，也可以从文件所在的上下文中收集相关信息，这些信息通常被忽略。<font color="red"> 上下文信息的一个来源是文件在磁盘上的位置。</font></strong>例如，如果检测器可以清楚地利用有关其所在路径的信息，则伪装为已知良性文件（例如Windows系统DLL）的恶意文件更有可能显得可疑。了解文件路径信息还可以更容易地检测那些试图通过将自己放置在特定位置来逃避磁盘扫描的文件**。文件路径也可以使用，开销很小，并且可以无缝集成到多视图静态ML检测器中，在非常高的吞吐量和最小的基础结构更改下，可能产生更高的检测率。</p>
<p>在这项工作中，我们提出了一种 <strong>multi-view</strong> 深度神经网络结构，该结构将PE文件内容中的特征向量以及相应的文件路径作为输入并输出检测分数。我们对大约1000万个样本的商业规模数据集进行了评估，这些样本是来自实际安全供应商提供服务的用户端点的文件和文件路径。<strong>然后，我们通过LIME建模进行可解释性分析，以确保我们的分类器已学习到合理的表示，并检查文件路径如何在不同情况下改变分类器的分数</strong>。我们发现，与只对PE文件内容进行操作的模型相比，<strong>我们的模型学习了文件路径的有用方面，在0.001假阳性率（FPR）下，真阳性率提高了26.6%，在0.0001 FPR下，提高了64.6%</strong>。</p>
<p><strong>keyword</strong> : 静态PE检测、文件路径、深度学习、多视图学习、模型解释</p>
<h3><span id="一-说明">一、说明</span></h3><p>商用便携式可执行（PE）恶意软件检测器由静态和动态分析引擎组成。<strong>静态检测通常首先用于标记可疑样本，它可以快速有效地检测大部分恶意软件</strong>。它涉及分析磁盘上的原始PE映像，可以非常快速地执行，但易受代码混淆技术的影响，例如压缩和多态/变形转换<strong>[1]</strong>。相比之下，动态检测需要在模拟器中运行PE，并在运行时分析行为[2]。当动态分析工作时，它不太容易受到代码混淆的影响，但与静态方法相比，它需要更大的计算容量和执行时间。此外，有些文件很难在仿真环境中执行，但仍然可以进行静态分析。<strong>因此，静态检测方法通常是端点恶意软件预防（在执行恶意软件之前阻止恶意软件）管道中最关键的部分</strong>。最近，由于采用了机器学习，静态检测方法的性能有所提高[3]，其中高度表达的分类器（例如深层神经网络）适合于数百万个文件的标记数据集。训练这些分类器时，它们使用静态文件内容作为输入，但不使用辅助数据。<strong>然而，我们注意到，由于辅助数据（例如网络流量、系统调用等），动态分析工作得很好。</strong>在这项工作中，我们试图使用文件路径作为正交输入信息来增强静态ML检测器。<strong>文件路径是静态可用的，无需操作系统的任何附加工具</strong>。通过将文件路径作为辅助输入，我们希望能够将有关文件的信息与在特定位置看到此类文件的可能性的信息结合起来，<strong>并识别与已知恶意软件和良性文件相关的常见目录层次结构和命名模式。</strong></p>
<blockquote>
<p>  <strong>静态检测</strong>：通用模块，快速有效地标记可疑样本；易受代码混淆技术（压缩和多态/变形转换）的影响。</p>
<p>  [1] A. Moser, C. Kruegel, and E. Kirda, “Limits of static analysis for malware detection,” in Twenty-Third Annual Computer Security Applications Conference (ACSAC 2007). IEEE, 2007, pp. 421–430.</p>
<p>  <strong>动态检测</strong>：分析模块，需要更大的计算容量和执行时间；有些文件很难在仿真环境中执行。</p>
</blockquote>
<p>我们将分析重点放在三个模型上：</p>
<ul>
<li>仅基线文件内容（PE）模型，仅将PE功能作为输入并输出恶意软件置信度得分。</li>
<li>另一个基准文件路径仅内容（FP）模型，仅将文件的文件路径作为输入，并输出恶意软件置信度得分。</li>
<li>我们提出的多视图PE文件内容+上下文文件路径（PE+FP）模型，该模型同时考虑PE文件内容特征和文件路径，并输出恶意软件置信度得分。</li>
</ul>
<p>三个模型的示意图如图1所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181509919.png" alt="image-20220528162748125" style="zoom:50%;"></p>
<p>我们对从一家大型反恶意软件供应商的遥测数据中收集的时间分割数据集进行分析，发现我们在文件内容和上下文文件路径上训练的分类器在ROC曲线上，尤其是在低误报率（FPR）区域，产生了统计上显著更好的结果。</p>
<p><strong>本文的贡献如下：</strong></p>
<ul>
<li>我们从安全供应商的客户端点（而不是恶意软件/供应商标签聚合服务）获得一组真实、精心策划的文件和文件路径数据集。</li>
<li>我们证明，我们的多视图PE+FP恶意软件分类器在我们的数据集上的性能明显优于单独使用文件内容的模型。</li>
<li>我们将本地可解释模型不可知解释（LIME）<strong>[4]</strong>扩展到PE+FP模型，并使用它分析文件路径如何影响模型的恶意/良性决策。</li>
</ul>
<blockquote>
<p>  [4] M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should i trust you?: Explaining the predictions of any classifier,” in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM, 2016, pp. 1135–1144.</p>
</blockquote>
<p>本手稿的其余部分结构如下：第二节涵盖重要的背景概念和相关工作。第三节讨论数据集收集和模型制定。第四节将我们的新多视图方法与拓扑结构相似的纯内容基线模型进行了比较，并对我们的模型进行了可解释性分析。第五节结束。</p>
<h3><span id="二-背景和相关工作">二、背景和相关工作</span></h3><p>在本节中，我们将描述机器学习如何普遍应用于静态PE检测，以及我们的方法如何通过提供上下文信息作为辅助输入而在高层意义上有所不同。然后，我们介绍了其他机器学习领域的相关工作。</p>
<h4><span id="21-静态ml恶意软件检测">2.1 静态ML恶意软件检测</span></h4><p>机器学习已经应用于计算机安全领域多年了[5]，但在商业规模上使用ML的静态PE模型的破坏性性能突破是一个较新的现象。商业模型通常依赖深度神经网络[6]或增强的决策树集合[7]，并已扩展到其他静态文件类型，包括web内容[8]、[9]、office文档[10]和档案[10]。</p>
<p>大多数用于信息安全的静态ML（ML-Sec）分类器操作的是文件部分（例如，标题）上的学习嵌入[11]，整个文件上的学习嵌入[12]，或者最常见的是，操作的是设计用于总结每个文件内容的预设计数字特征向量[6]、[13]–[19]。预先构建的特征向量表示往往更具可伸缩性，可以快速从每个文件中提取内容，同时保留有用的信息。有很多方法可以构建特征向量，包括在滑动窗口上跟踪每字节统计信息【6】、【18】、字节直方图【7】、【18】、ngram直方图【13】、将字节视为图像中的像素值（文件内容的可视化）【13】、【18】、操作码和函数调用图统计信息【18】、符号统计信息【18】、哈希/数字元数据值【6】、【7】、【18】–例如。，入口点作为文件的一部分，或散列导入和导出，以及分隔标记的散列[10]、[19]。在实际应用中，从文件内容中提取的几种不同类型的特征向量通常连接在一起，以获得优异的性能。</p>
<h4><span id="22-learning-from-multiple-sources">2.2 Learning from Multiple Sources</span></h4><p>使用深度神经网络进行静态ML恶意软件检测的相关研究已经检验了从多个信息源学习的方法，但这些方法与我们的方法有根本不同：Huang等人【20】和Rudd等人【21】对多个辅助损失函数使用多目标学习【22】，【23】，他们发现这些函数在主要恶意软件检测任务中的性能有所提高。这两项工作都在培训期间使用元数据作为辅助目标标签，为模型提供额外的信息，并在部署时使用单个输入来做出分类决策。我们的方法利用了多种输入类型/模式——一种是以类似于[6]的PE特征向量的形式描述恶意样本的内容，另一种是将原始字符串提供给一个字符嵌入层（类似于[8]），该层提供了有关该样本出现位置的信息。这种技术是一种多视图学习方法[24]。顾名思义，多视图学习的大多数应用都是在计算机视觉中进行的，在计算机视觉中，多个视图实际上是由来自不同输入摄像头/传感器的视图或来自同一摄像头/传感器在不同时间的不同视图组成的。<strong>在ML-Sec空间中，我们只能找到两种专门将自己称为多视图的方法：即[25]，Narayanan等人将多内核学习依赖图应用于Android恶意软件分类，以及[26]，</strong>Bai等人将多视图集合用于PE恶意软件检测。虽然这些方法在某些方面与我们的方法相似，但据我们所知，我们是第一个在商业规模上使用外部上下文反馈到深层神经网络并结合文件内容特征来执行恶意软件检测多视图建模的方法。</p>
<blockquote>
<p>  [25] A. Narayanan, M. Chandramohan, L. Chen, and Y. Liu, “A multi-view context-aware approach to android malware detection and malicious code localization,” Empirical Software Engineering, pp. 1–53, 2018.</p>
<p>  [26] J. Bai and J. Wang, “Improving malware detection using multi-view ensemble learning,” Security and Communication Networks, vol. 9, no. 17, pp. 4227–4241, 2016.</p>
</blockquote>
<h3><span id="三-实施细节">三、实施细节</span></h3><p>在本节中，我们将介绍我们的方法的实现细节，包括从客户端点获取PE文件和文件路径的数据收集过程、我们的特征化策略以及我们的多视图深度神经网络和比较基线的体系结构。</p>
<h4><span id="31-数据集">3.1 数据集</span></h4><p>在我们的实验中，我们从一家著名反恶意软件供应商的遥测数据中收集了三个不同的数据集：一个训练集、一个验证集和一个测试集。培训集由9148143个样本组成，这些样本首次出现在2018年6月1日至11月15日之间，其中693272个样本被标记为恶意样本。验证集包括在2018年11月16日至12月1日期间观察到的2225094个不同样本，其中85041个被标记为恶意样本。最后，测试集在2019年1月1日至1月30日期间共有249783个样本，其中38767个被标记为恶意。这些文件的恶意/良性标签是使用类似于[6]、[8]的标准计算的，但结合其他专有信息可以生成更准确的标签。</p>
<h4><span id="32-特征工程">3.2 特征工程</span></h4><p><strong>为了使用文件路径作为神经网络模型的输入，我们首先将可变长度字符串转换为固定长度的数字向量</strong>。我们使用类似于<strong>[8]</strong>的矢量化方案来实现这一点，方法是在每个字符上创建一个键控的查找表，该表用一个表示每个字符的数值（介于0和字符集大小之间）来表示。实际上，我们将此表实现为Python字典。在遥测和早期实验数据的指导下，我们将文件路径缩减到最后100个字符。短于100个字符的文件路径的功能用零填充。对于字符集，我们考虑整个Unicode字符集，但将词汇限制为150个最常见的字符。见附录？？供进一步讨论。作为PE文件内容的特征，我们使用了由四种不同特征类型组成的浮点1024维特征向量，类似于[6]。总的来说，我们将每个样本表示为两个特征向量：<strong>1024维的PE内容特征向量和100维的上下文文件路径特征向量。</strong></p>
<blockquote>
<p>  [8] J. Saxe and K. Berlin, “expose: <strong>A character-level convolutional neural network with embeddings for detecting malicious urls, file paths and registry keys</strong>,” arXiv preprint arXiv:1702.08568, 2017.</p>
<p>  <img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181509845.png" alt="image-20220530152922273" style="zoom: 25%;"></p>
</blockquote>
<h4><span id="33-网络体系结构">3.3 网络体系结构</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181509415.png" alt="image-20220528204724740" style="zoom:50%;"></p>
<p>我们的多视图体系结构如图2所示。该模型有两个输入，<strong>1024个元素的PE内容特征向量xPE和100个元素的文件路径整数向量xFP</strong>，如第III-B节所述。每个不同的输入分别通过一系列具有各自参数θPE和θFP的层，用于PE特征和FP用于文件路径特征，并在训练期间联合优化。然后将这些层的输出连接（串联）并通过一系列最终隐藏层，即参数θO的联合输出路径。网络的最终输出由密集层和sigmoid激活组成。</p>
<ul>
<li><strong>PE 特征</strong>：PE输入臂θPE使xPE通过一系列块，每个块由四层组成：一个完全连接的层，一个使用[27]中所述技术实现的层规范化层，一个丢失概率为0.05的丢失层，以及一个校正线性单元（ReLU）激活。其中五个块依次连接，密集层大小分别为1024、768、512、512和512个节点。</li>
<li><strong>文件路径</strong>：<ul>
<li>文件路径输入arm θFP 将 xFP（<strong>长度为100的向量</strong>）传递到嵌入层 ？？？</li>
<li><strong>该嵌入层将文件路径的每个字符转换为32维向量，从而为整个文件路径生成100x32的嵌入张量</strong>。</li>
<li><strong>然后将该嵌入馈入4个单独的卷积块</strong>，其中包含一个具有128个滤波器的1D卷积层、一个层归一化层和一个1D求和层，以将输出平坦化为向量。4个卷积块包含卷积层，卷积层的大小分别为2、3、4和5，用于处理2、3、4和5克的输入文件路径。</li>
<li>然后将这些卷积块的平坦输出<strong>串联</strong>起来，作为大小为1024和512个神经元的两个密集块的输入（与PE输入臂中的形式相同）。</li>
</ul>
</li>
<li><strong>输出层</strong>：PE arm和文件路径arm的完全连接块的输出随后被连接并传递到由θO参数化的联合输出路径。该路径由层大小为512、256和128的密集连接块（与PE输入arm中的形式相同）组成。然后将这些块的128D输出馈送至致密层，该致密层将输出投射至1D，然后进行sigmoid激活，以提供模型的最终输出。</li>
</ul>
<p>仅PE模型只是PE+FP模型，但没有FP臂，输入xPE并拟合θPE和θO参数。类似地，FP模型是PE+FP模型，但没有3个授权的许可PE arm，采用输入xFP拟合θFP和θO参数。适当调整输出子网络的第一层，以匹配前一层的输出。我们使用二进制交叉熵损失函数拟合所有模型。给定标签为y的输入x的深度学习模型f（x；θ）的输出∈ {0，1}，模型参数θ损失为：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220528171648185.png" alt="image-20220528171648185" style="zoom:50%;"></p>
<p>通过优化器，我们求解ˆθ的最佳参数集，以最小化数据集上的组合损失：</p>
<p><img src="../../../../../../Library/Application Support/typora-user-images/image-20220528171710886.png" alt="image-20220528171710886" style="zoom:50%;"></p>
<p>其中M是数据集中的样本数，y（i）和x（i）分别是第i个训练样本的标签和特征向量。我们使用Keras框架构建和训练模型，使用Adam优化器和Keras的默认参数和1024个小批量。每个模型都经过15个阶段的训练，我们确定这些阶段足以使结果收敛。</p>
<h3><span id="4-实验分析">4、实验分析</span></h3><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181509685.png" alt="image-20220528172252766" style="zoom:50%;"></p>
<h3><span id="5-总结">5、总结</span></h3><p>我们已经证明，深度神经网络恶意软件检测器可以从合并来自文件路径的上下文信息中获益，即使这些信息本身不是恶意或良性的。<strong>将文件路径添加到我们的检测模型中不需要任何额外的端点检测，并且在整个相关FPR区域的总体ROC曲线中提供了统计上显著的改善</strong>。我们直接在客户端点分布上测量模型的性能这一事实表明，我们的多视图模型实际上可以部署到端点以检测恶意软件。我们在第IV-B节中进行的LIME分析表明，多视图模型能够提取暗示实际恶意/良性概念的上下文信息；不只是数据集的统计伪影，尽管正如我们所观察到的，它还可以学习这些伪影。除了端点部署之外，这项研究可以应用的另一个潜在领域是端点检测和响应（EDR）环境，在该环境中，我们的模型的输出可以用于根据事件的可疑程度对磁盘上的事件进行排序。有趣的是，石灰等技术在这方面也有应用。使用从LIME或类似方法得出的解释，可以创建分析工具，允许非恶意软件/取证专家的用户执行某种程度的威胁搜寻。如图4所示，重要性突出显示不仅对用户有用，而且是最近邻/相似性可视化方法的替代方法，该方法不会显示其他用户的潜在可识别信息（PII）。</p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（4）阿里云-郑翰-安全智能应用</title>
    <url>/posts/1CZSV4N/</url>
    <content><![CDATA[<h2><span id="安全智能应用的一些迷思"></span></h2><h4><span id="1-文章主旨">1、文章主旨</span></h4><p>本文是一个面向安全学术圈和工业界同行的介绍性和探讨性议题，议题的前半部分会介绍一些工业实践中被证明有效的落地实践，后半部分更多地是希望抛砖引玉，通过抽象和定义最新的问题，吸引更多学术研究员的关注和合作。</p>
<h4><span id="2-目前可以做到哪些">2、目前可以做到哪些</span></h4><p>第一部分，本次演讲从目前工业界中智能算法的一些落地实践情况切入说起，总结目前智能安全从概念到落地的应用情况，主要目的是希望阐述，有哪些问题是已经得到解决，或者部分解决的，包括:</p>
<ol>
<li><strong>在海量、富类型的样本集支持下，现有的深度学习和机器学习框架已经可以很好的实现有监督学习和预测的目标</strong>，复杂模型结构层面的调整对最终结果的提升非常有限，更多的瓶颈是在如何发现更多的打标数据上，即<strong>样本集概率空间覆盖度问题</strong>。</li>
<li><strong>文本内容检测</strong>是现在落地应用最多的场景之一(例如<strong>WAF</strong>、<strong>Webshell检测</strong>、<strong>二进制病毒检测</strong>、<strong>网页敏感内容检测</strong>、<strong>明码流量检测</strong>等)，<strong>传统的NLP和图形领域的特征工程和建模方法可以较好发挥作用</strong>。</li>
<li>针对<strong>简单场景问题</strong>(例如<strong>暴力破解攻击检测</strong>、<strong>异地登录检测</strong>、<strong>真实入侵证据发现</strong>)，<strong>简单统计</strong>和<strong>假设检验</strong>可以发挥较好作用。</li>
<li><strong>时序建模</strong>和<strong>时序异常检测算法</strong>在<strong>ddos、cc、定点API接口爆破检测</strong>上可以发挥较好效果，但受限于安全领域中存在较多的突然性、偶然性事件，时序周期性假设常常无法成立，这点极大限制了时序异常检测算法在安全领域内的应用。</li>
<li><strong>相似性匹配算法</strong>(例如<strong>simhash、ssdeep、kmeans</strong>)目前的主要落地场景主要是，扩展原有规则模型的泛化能力。纯粹无监督的相似性聚类由于缺乏可解释性，目前更多用于辅助专家决策。</li>
</ol>
<p>总结来说，当前工业界和学术界智能算法的应用可以综合概括为，”<strong>基于历史经验样本下的的拟合学习</strong>“，即”<strong>基于知识的对抗</strong>“，机器学习在其中充当的角色更多地是一种记忆学习，缺点是难以提供更多的泛化检测和0day发现能力。</p>
<h4><span id="3-还未解决的难题">3、还未解决的难题</span></h4><p>第二部分，笔者希望将我们在企业一线工作的经历进行总结和抽象，将目前智能安全中的一些未解决问题，用学术课题的方式明确地定义出来，将智能安全中的问题转化为学术研究课题，目标是争取更广大的国内科研高效和机构的研究力量，将更多的研究重点投入在实际的问题上，避免对历史老问题的重复研究和建设，包括:</p>
<ol>
<li><strong>安全风险定量评估函数建模</strong>: 以恶意样本检测为例，恶意样本检测0day发现能力(对未知的未知发现能力)本质上是一个搜索优化问题，如何对每一个样本的威胁性(值越大表示恶意性越大，0或负值表示是正常样本)进行定量的定义和分析，是问题的关键。<strong>定义了明确的量化损失函数，恶意样本的检测就会从有监督学习问题转化为搜索优化问题。</strong></li>
<li><strong>基于威胁性定量评估损失函数下的随机搜索问题</strong>: 在基于对各个场景建立了明确的损失函数(例如某个ttp的风险分值、某个http payload的恶意分值、某个文本文件的恶意分值)之后。接下来的工作就是结合安全问题的特点，开发针对性的优化搜索算法，例如<strong>蒙特卡洛搜索</strong>、<strong>随机梯度下降搜索</strong>。</li>
<li>非完整观测下的复杂事件动态推理过程: 入侵检测是安全攻防领域一个很重要的问题，这个问题本质上是一个<strong>复杂事件马尔科夫推理过程</strong>，各种日志采集点代表了可观测量，但实际情况是，我们永远不可能获得一个安全事件的完整观测视角(受限于日志采集的种类和完整性)。所以安全研究员要解决的问题是，<strong>如果在不完整观测的条件下，进行贝叶斯信念网络的建模，并基于该信念网络进行复杂事件推理</strong>。</li>
<li><strong>模型衰减对抗问题</strong>: 类似于自然界所有物理都在朝着熵增的方向演进，安全攻防中的所有模型都存在”性能衰退“的问题，在开发测试阶段完美适配了当前问题场景的模型在上线运行一段时间后，面临误报和漏报的风险会不断提高。</li>
<li>针对攻击入侵链路回溯的有向无环图推理问题: 入侵回溯场景中面对的主要问题有如下几不同事件节点之间的因果依赖推导: 因为攻击在逻辑上是存在逻辑先后关系的多条路径(攻击事件链路)的合并: 一台机器可能不只遭到一次和一个攻击者的攻击异构节点的融合: 一次成功的入侵回溯包括对已知告警节点的因果串联，以及融合其他可以提供更多线索证据的日志节点这两项工作子图融合: 从不同的日志视角可能获得多条攻击链路，入侵回溯师需要能够识别出其中的底层联系，将多条攻击链路合成到一个大的攻击视角中，为后续的决策提供更丰富的攻击者和攻击面信息。</li>
</ol>
<h4><span id="4-我们目前在尝试的项目">4、我们目前在尝试的项目</span></h4><p>第三部分，笔者会介绍一些目前我们公司团队在进行的课题研究方向，包括，</p>
<ol>
<li>通过LSTM自动生成webshell黑样本</li>
<li>基于<strong>GAN网络绕过</strong>现有深度学习AV检测模型</li>
<li>基于<strong>遗传优化算法</strong>的的自动化0day样本生成</li>
<li><strong>基于贝叶斯信念网络的入侵回溯推理</strong></li>
<li><strong>==通过攻击链路中已回溯出来的信息（进程、网络、文件）横向关联其他被这个团伙入侵的机器，然后继承他们的入侵原因==</strong></li>
</ol>
<h4><span id="5-历史外部演讲">5、历史外部演讲</span></h4><ul>
<li>《云环境自动化入侵溯源实战》, KCon 2019 <a href="http://link.zhihu.com/?target=https%3A//static.cdxy.me/201908-%E4%BA%91%E7%8E%AF%E5%A2%83%E8%87%AA%E5%8A%A8%E5%8C%96%E5%85%A5%E4%BE%B5%E6%BA%AF%E6%BA%90%E5%AE%9E%E6%88%98-KCon.pdf">[slides]</a></li>
<li>“Hunting zero-days for millions of websites on Alibaba Cloud”, XCon 2019 <a href="http://link.zhihu.com/?target=https%3A//static.cdxy.me/XCON-2019-EN.pdf">[slides]</a></li>
<li>“Webshell Detection via Attention-Based Opcode Sequence Classification”, Artificial Intelligence for Business Security Workshop (AIBS @ IJCAI-19). Macao, CN. 10-12 Aug 2019. <a href="http://link.zhihu.com/?target=https%3A//static.cdxy.me/AIBS_2019_paper_3.pdf">[paper]</a></li>
<li>“Enhance Security Awareness with Data Mining”, BlueHat Shanghai 2019</li>
<li><a href="http://link.zhihu.com/?target=https%3A//www.butian.net/datacon">[DataCon 2019]</a> 1st place solution of malicious DNS traffic &amp; DGA analysis. <a href="http://link.zhihu.com/?target=https%3A//www.cdxy.me/%3Fp%3D806">[writeup]</a></li>
<li>《企业安全数据分析思考与实践》, FreeBuf公开课 <a href="http://link.zhihu.com/?target=http%3A//static.cdxy.me/data-knowledge-action_cdxy.pdf">[slides]</a></li>
<li>《从数据视角探索安全威胁》, 先知白帽大会2018 <a href="http://link.zhihu.com/?target=https%3A//xzfile.aliyuncs.com/upload/zcon/2018/10_%E4%BB%8E%E6%95%B0%E6%8D%AE%E8%A7%86%E8%A7%92%E6%8E%A2%E7%B4%A2%E5%AE%89%E5%85%A8%E5%A8%81%E8%83%81_cdxy.pdf">[slides]</a></li>
</ul>
<h2><span id="企业安全数据分析实践与思考">企业安全数据分析实践与思考</span></h2><p><a href="https://live.freebuf.com/detail/c5e504cf96a4e1826a609553bf6054f9">https://live.freebuf.com/detail/c5e504cf96a4e1826a609553bf6054f9</a></p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（5）蚂蚁安全-柳星《FXY：Security-Scenes-Feature-Engineering-Toolkit》</title>
    <url>/posts/2A4A099/</url>
    <content><![CDATA[<h2><span id="fxysecurity-scenes-feature-engineering-toolkit">FXY：<em>Security-Scenes-Feature-Engineering-Toolkit</em></span></h2><blockquote>
<p>  <a href="https://github.com/404notf0und/FXY/blob/master/docs/%E9%9C%80%E6%B1%82%E5%92%8C%E8%AE%BE%E8%AE%A1.md">https://github.com/404notf0und/FXY/blob/master/docs/%E9%9C%80%E6%B1%82%E5%92%8C%E8%AE%BE%E8%AE%A1.md</a></p>
</blockquote>
<h3><span id="介绍">介绍</span></h3><p>FXY是一款特征工程框架，用于安全场景中数据预处理、数据预分析、数据特征化和向量化等任务。FXY这个名字一方面代表这款工具的目的是从原始安全数据中获取Feature X和Feature Y用于对接人工智能算法，另一方面寓意着人工智能的本质，函数Y=F(X)。FXY的特性是支持多种安全场景多种安全数据的预处理和特征化，内置多种NLP通用特征提取方法，内置脚本扩展支持二次开发。</p>
<h3><span id="需求">需求</span></h3><p>无论机器学习、深度学习还是强化学习应用在哪个领域，<strong>其处理流程主要有五个环节：问题-&gt;数据-&gt;特征化-&gt;算法-&gt;结果</strong>，数据的数字化，狭义的来说是数据的特征化，在整个流程中起到了承上启下的关键作用，承上，<strong>特征化的好坏直接反映了对问题本质的理解深入与否</strong>，启下，作为算法的输入，一定程度上决定了最终结果的天花板。这是FXY定位于安全场景下特征工程环节的一点原因。另一点原因是考虑到算法环节的不确定性因素和确定性因素，不确定性因素导致难以形成统一的范式，确定性因素导致问题已被解决。就算法的应用来说，机器学习算法、深度学习算法和超参数众多，在同一特征化方法下，难以客观比较不同算法的性能，并且找到泛化性强的SOTA算法。<strong>就算法本身来说，现有的框架tensorflow、keras等对算法的封装已经很完美了，重复造轮子意义不大。如果给算法环节盖上安全场景的帽子，问题依然如此，这是FXY不选择定位于安全场景下算法环节的原因。</strong></p>
<h3><span id="架构设计">架构设计</span></h3><p><img src="https://i.imgur.com/d2Rq9hc.png" alt></p>
<p>因为机器学习解决安全问题的流程固定为安全问题-&gt;数据-&gt;数字化-&gt;算法-&gt;结果，<strong>具体到FXY的架构设计，从下到上依次是安全场景层-&gt;数据的数据层-&gt;数据清洗层-&gt;特征层-&gt;算法层-&gt;API层</strong>，对应的FXY各模块层次结构依次为内置函数模块-&gt;数据预处理模块-&gt;特征工程模块-&gt;tensorflow/keras-&gt;控制器模块。</p>
<p>扩展可扩展的，因为安全场景较多且杂，完全不可能用一种或少数几种特征方法解决所有问题，想到的一种解决方式是针对安全问题做特征方法的插件化扩展，把每个安全问题对应每个CMS，每个feature engineering方法对应每个POC，那么就可以像写CMS POC一样专注于安全场景的底层数据feature engineering。</p>
<h3><span id="集成">集成</span></h3><p>笔者Github上AI-for-Security-Learning仓库专注于知识，而此FXY仓库专注于工具，现依赖前者仓库，笔者开始二刷，站在前人的肩膀上，不断集成优质方法到FXY框架中，此框架不做未知的创新。现已集成4种安全场景，4种特征工程方法，四种安全场景分别是<strong>LSTM识别恶意HTTP请求@cdxy，AI-Driven-WAF@exp-db，Phishing URL Classification@surajr，使用深度学习检测XSS@Webber，基于深度学习的恶意样本行为检测@ApplePig@360云影实验室</strong>，四种特征工程方法分别是<strong>钓鱼url的统计特征，恶意url和恶意软件api的词典索引特征，恶意url的TF-IDF特征，xss的word2vec词嵌入向量</strong>。</p>
<p>在二刷并集成的过程中，需要彻底读懂原作者的文章思路和代码，然后改写到FXY限定的框架中，学到了很多。同时输出一份二刷笔记，里面不但包括已集成代码到框架中的原文理解，还包括一些暂时无法集成的文章的理解，文档化记录了原作者用到的安全场景、解决的思路、数据的构成、数据预处理方法、特征的方法、使用的模型、有无监督分类，二刷笔记持续更新。</p>
<h3><span id="潜在问题">潜在问题</span></h3><p><strong>FXY框架专注于安全问题、数据和特征化三个环节</strong>:这其中数据环节存在数据源难获取的问题，有些文章中的数据属于公司级数据不会开源，较难获取，这导致只能使用开源数据集或自己采集数据集复现原作者的实验，集成并测试框架，虽说不会影响FXY框架的预处理、预分析和特征化等主要功能，但这会导致数据环节数据本身的价值变小，一定程度上减小了FXY框架的价值，因为可能大多数人遇到的问题不是没有方法，而是没有数据，数据本身的价值和数据分析的价值都很高，前者价值甚至大于后者。而CMS的POC框架可以靠各种搜索引擎和爬虫来获取数据源，输送给POC脚本，就不会存在此问题。</p>
<p>这促使我们是不是可以通过爬虫爬取更多异源开源数据，用开源弥补闭源，或是本地搭建环境采集数据，缓解数据源缺失的问题，从而使FXY框架的价值不只在于数据分析，更在于数据集本身，采集数据集是个脏活累活，在规划中。</p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（5）蚂蚁安全-柳星《我对安全与NLP的实践和思考》</title>
    <url>/posts/3DTA3RC/</url>
    <content><![CDATA[<h1><span id="柳星-我对安全与nlp的实践和思考">柳星-我对安全与NLP的实践和思考</span></h1><blockquote>
<p>  <a href="https://github.com/404notf0und">404notf0und</a>/<strong><a href="https://github.com/404notf0und/FXY">FXY</a></strong></p>
</blockquote>
<h3><span id="一-个人思考">一、个人思考</span></h3><p><strong><font color="red"> 通过对安全与NLP的实践和思考，有以下三点产出：</font></strong></p>
<ul>
<li><strong>首先，产出一种通用解决方案和轮子，一把梭实现对各种安全场景的安全检测。</strong>通用解决方案给出一类安全问题的解决思路，打造轮子来具体解决这一类问题，而不是使用单个技术点去解决单个问题。<strong>具体来说，将安全与NLP结合，在各种安全场景中，将其安全数据统一视作文本数据，从NLP视角，统一进行文本预处理、特征化、预训练和模型训练。</strong>例如，在Webshell检测中，Webshell文件内容，在恶意软件检测中，API序列，都可以视作长文本数据，使用NLP技术进行分词、向量化、预训练等操作，同理，在Web安全中，SQl、XSS等URL类安全数据，在DNS安全中，DGA域名、DNS隧道等域名安全数据，同样可以视作短文本数据。因此，只要安全场景中安全数据可以看作文本数据，<a href="https://github.com/404notf0und/FXY">FXY：<em>Security-Scenes-Feature-Engineering-Toolkit</em></a> 中，内置多种通用特征化方法和多种通用深度学习模型，以支持多种安全场景的特征化和模型训练，达到流水线式作业。</li>
<li><strong>其次，是对应用能力和底层能力的思考</strong>。之前写过一篇文章《<a href="https://4o4notfound.org/index.php/archives/188/">应用型安全算法工程师的自我修养</a>》，在我当时预期想法中，我理解的应用型，重点在于解决实际安全问题，不必苛求于对使用技术本身的理解深度，可以不具备研究型、轮子型的底层能力。映射到我自身，我做安全和算法，最初想法很好，安全和算法两者我都要做好，这里做好，仅仅指用好。之后，面试时暴露了问题，主管给出的建议是两者都要做好。这里做好，不单单指用好，还要知其所以然。举个例子，就是不仅要调包调参玩的6，还要掌握算法的底层原理，这就是底层能力。当时，懂，也不懂，似懂非懂，因为，说，永远是别人的，悟，才是自己的。在实现通用解决方案和轮子的过程中，遇到关于word2vec底层的非预期问题，才深刻体会到，底层能力对应用能力的重要性。过程中遇到的预期和非预期问题，下文会详述。<strong>现在我理解的应用型，重点还是在解决安全问题，以及对安全问题本身的理解，但应用型还需具备研究型、轮子型等上下游岗位的底层能力。</strong>安全算法是这样，其他细分安全领域也是一样，都需要底层能力，以发展技术深度。</li>
<li>最后，带来思考和认识的提升。<strong>从基于机器学习的XX检测，基于深度学习的XX检测，等各种单点检测，到基于NLP的通用安全检测，是一个由点到面的认知提升</strong>。从安全和算法都要做好，到安全和算法都要做好，其中蕴含着认知的提升。从之前写过一篇安全与NLP的文章《<a href="https://www.4o4notfound.org/index.php/archives/190/">当安全遇上NLP</a>》，到现在这篇文章。对一件事物的认识，在不同阶段应该是不一样的，甚至可能完全推翻自己之前的认识。我们能做的，是保持思考，重新认识过去的经历，提升对事物的认知和认知能力。这个提升认知的过程，类似boosting的残差逼近和强化学习的奖惩，是一个基于不知道不知道-&gt;知道不知道&gt;知道知道-&gt;不知道知道的螺旋式迭代上升过程。</li>
</ul>
<h3><span id="二-预期问题">二、预期问题</span></h3><h4><span id="21-分词粒度">2.1 分词粒度</span></h4><p><strong>首先是分词粒度，粒度这里主要考虑字符粒度和词粒度。在不同的安全场景中，安全数据不同，采用的分词粒度也可能不同：</strong></p>
<ul>
<li><strong><font color="red">恶意样本检测的动态API行为序列数据，需要进行单词粒度的划分。</font></strong></li>
<li><strong>域名安全检测中的域名数据，最好采用字符粒度划分。</strong></li>
<li><strong>URL安全检测中的URL数据，使用字符和单词粒度划分都可以。</strong></li>
<li><strong>XSS检测文中，是根据具体的XSS攻击模式，写成正则分词函数，对XSS数据进行划分，这是一种基于攻击模式的词粒度分词模式，但这种分词模式很难扩展到其他安全场景中。</strong></li>
</ul>
<p><strong>FXY特征化类wordindex和word2vec中参数char_level实现了该功能</strong>。在其他安全场景中，可以根据此思路，写自定义的<strong>基于攻击模式的分词</strong>，但适用范围有限。我这里提供了两种通用词粒度分词模式，第一种是忽略特殊符号的简洁版分词模式，第二种是考虑全量特殊符号的完整版分词模式，这两种分词模式可以适用于各种安全场景中。FXY特征化类word2vec中参数punctuation的值‘concise’，‘all’和‘define’实现了两种通用分词和自定义安全分词功能。下文的实验部分，会测试不同安全场景中，使用字符粒度和词粒度，使用不同词粒度分词模式训练模型的性能对比。</p>
<h4><span id="22-语料库">2.2 语料库</span></h4><p><strong>关于预训练前字典的建立（语料库）</strong>。特征化类word2vec的预训练需求直接引发了字典建立的相关问题。在word2vec预训练前，需要考虑预训练数据的产生。基于深度学习的XSS检测文中，是通过<strong>建立一个基于黑样本数据的指定大小的字典，不在字典内的数据全部泛化为一个特定词，将泛化后的数据作为预训练的数据</strong>。这里我们将此思路扩充，增加使用全量数据建立任意大小的字典。具体到word2vec类中，参数one_class的True or False决定了预训练的数据来源是单类黑样本还是全量黑白样本，参数vocabulary_size的值决定了字典大小，如果为None，就不截断，为全量字典数据。下文的实验部分会测试是<strong>单类黑样本预训练</strong>word2vec好，还是<strong>全量数据预训练</strong>更占优势，是<strong>字典截断</strong>好，还是用全量字典来预训练好。</p>
<h4><span id="23-序列">2.3 序列</span></h4><p><font color="red"> <strong>关于序列的问题，具体地说，是长文本数据特征化需求</strong>。</font><strong>webshell检测等安全场景，引发了序列截断和填充的问题。</strong>短文本数据的特征化，可以保留所有原始信息。而在某些安全场景中的长文本数据，特征化比较棘手，保留全部原始信息不太现实，需要对其进行截断，截断的方式主要有<strong>字典截断、序列软截断、序列硬截断</strong>。</p>
<ul>
<li><strong>序列软截断</strong>是指对不在某个范围内（参数num_words控制范围大小）的数据，直接去除或填充为某值，长文本选择直接去除，缩短整体序列的长度，尽可能保留后续更多的原始信息。如果长本文数据非常非常长，那么就算有字典截断和序列软截断，截断后的序列也可能非常长，超出了模型和算力的承受范围;</li>
<li><strong>序列硬截断</strong>（参数max_length控制）可以发挥实际作用，直接整整齐齐截断和填充序列，保留指定长度的序列数据。这里需要注意的是，为了兼容后文将说到的“预训练+微调”训练模式中的<strong>预训练矩阵</strong>，序列填充值默认为0。</li>
</ul>
<h4><span id="24-词向量">2.4 词向量</span></h4><p>词向量的问题，具体说，是词嵌入向量问题。词嵌入向量的产生有三种方式：</p>
<ul>
<li>词序列索引+有嵌入层的深度学习模型</li>
<li>word2vec预训练产生词嵌入向量+无嵌入层的深度学习模型</li>
<li><strong>word2vec预训练产生预训练矩阵+初始化参数为预训练矩阵的嵌入层的深度学习模型。</strong></li>
</ul>
<p>这里我把这三种方式简单叫做微调、预训练、预训练+微调，从特征工程角度，这三种方式是产生词嵌入向量的方法，从模型角度，也可以看作是模型训练的三种方法。第一种微调的方式实现起来比较简单，直接使用keras的文本处理类Tokenizer就可以分词，转换为词序列，得到词序列索引，输入到深度学习模型中即可。第二种预训练的方式，调个gensim库中word2vec类预训练，对于不在预训练字典中的数据，其词嵌入向量直接填充为0，第三种预训练+微调的方式，稍微复杂一点，简单来说就是前两种方式的组合，用第二种方式得到预训练矩阵，作为嵌入层的初始化权重矩阵参数，用第一种方式得到词序列索引，作为嵌入层的原始输入。下文的实验部分会测试并对比按这三种方式训练模型的性能，<strong>先说结论：预训练+微调&gt;预训练&gt;微调</strong>。</p>
<h3><span id="三-非预期问题">三、非预期问题</span></h3><h4><span id="31-已知的库和函数不能满足我们的需求">3.1 已知的库和函数不能满足我们的需求</span></h4><p>使用keras的文本处理类Tokenizer预处理文本数据，得到词序列索引，完全没有问题。<strong>但类Tokenizer毕竟是文本数据处理类，没有考虑到安全领域的需求。</strong></p>
<ul>
<li><strong><font color="red"> 类Tokenizer的单词分词默认会过滤所有的特殊符号，仅保留单词，而特殊符号在安全数据中是至关重要的，很多payload的构成都有着大量特殊符号，忽略特殊符号会流失部分原始信息。</font></strong></li>
<li><strong>首先阅读了keras的文本处理源码和序列处理源码</strong>，不仅搞懂了其结构和各函数的底层实现方式，还学到了一些trick和优质代码的特性。搞懂了其结构和各函数的底层实现方式，还学到了一些trick和优质代码的特性。下图为Tokenizer类的结构。借鉴并改写Tokenizer类，加入了多种分词模式，我们实现了wordindex类。</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181514240.png" alt="img"></p>
<h4><span id="32-对word2vec的理解不到位">3.2 对word2vec的理解不到位</span></h4><p>第二个非预期问题是，对word2vec的理解不到位，尤其是其底层原理和代码实现，导致会有一些疑惑，无法得到验证，这是潜在的问题。虽然可以直接调用gensim库中的word2vec类暂时解决问题，但我还是决定把word2vec深究深究，一方面可以答疑解惑，另一方面，就算不能调用别人的库，自己也可以造轮子自给自足。限于篇幅问题，不多讲word2vec的详细原理，原理是我们私下里花时间可以搞清楚的，不算是干货，对原理有兴趣的话，这里给大家推荐几篇优质文章，在github仓库<a href="https://github.com/404notf0und/Always-Learning">Always-Learning</a>中。</p>
<p><strong>word2vec本质上是一个神经网络模型，具体来说此神经网络模型是一个输入层-嵌入层-输出层的三层结构，我们用到的词嵌入向量只是神经网络模型的副产物，是模型嵌入层的权重矩阵。</strong>以word2vec实现方式之一的skip-gram方法为例，此方法本质是通过中心词预测周围词。如果有一段话，要对这段话训练一个word2vec模型，那么很明显需要输入数据，还要是打标的数据。以这段话中的某个单词为中心词为例，在一定滑动窗口内的其他单词都默认和此单词相关，此单词和周围其他单词，一对多产生多个组合，默认是相关的，因此label为1，即是输入数据的y为1，而这些单词组合的one-hot编码是输入数据的x。<strong>那么很明显label全为1，全为positive sample，需要负采样来中和。这里的负采样不是简单地从滑动窗口外采样，而是按照词频的概率，取概率最小的一批样本来做负样本（这个概念下面马上要用到），因为和中心词毫不相关，自然label为0。</strong></p>
<p><strong><font color="red"> tensorflow中的nce_loss函数实现了负采样。</font></strong></p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（6）滴滴安全《安全运营之自动编排SOAR的探索》</title>
    <url>/posts/26TF5T6/</url>
    <content><![CDATA[<h2><span id="滴滴安全运营之自动编排-soar-的探索">滴滴安全运营之自动编排 (SOAR) 的探索</span></h2><blockquote>
<p>  DiDi:<a href="https://www.secrss.com/articles/25896">https://www.secrss.com/articles/25896</a></p>
</blockquote>
<h4><span id="困难和挑战">困难和挑战</span></h4><ul>
<li><strong>海量异构的日志数据源</strong><ul>
<li><strong>覆盖广</strong>：通过各类sensor采集的数据，覆盖了办公网和客服网的终端，以及测试网生产网的服务器，还有公有云的虚拟机等等。</li>
<li><strong>来源多</strong>：HIDS，网络层的NTA，终端EDR，还有VPN的认证日志，DNS的解析记录，802.1x的认证，AD域控的日志，还有防火墙邮件服务器密罐的日志等等。</li>
<li><strong>量级大</strong>：每天用于安全审计的原始日志达到了10Tb及以上的量级。</li>
<li><strong>异构性</strong>：Hive、ElasticSearch、Kafka等等，而有些日志是设备通过Syslog和Webhook的方式打给我们的，这些日志都是异构的。</li>
</ul>
</li>
<li><strong>有效的告警会淹没在误报当中</strong><ul>
<li>黑名单的规则来定义异常，无法感知未知；白名单规则，通过定义正常来发现异常；</li>
<li><strong>统计规则阈值的选取也和误报息息相关</strong>；</li>
<li><strong>在甲方日常的安全运营中，团队可能会受 KPI的影响</strong></li>
</ul>
</li>
<li><strong>告警研判推理的挑战</strong><ul>
<li><strong><font color="red"> 如果我们缺少有效的关联分析能力，就很难从各个孤立的告警中还原出攻击者的一次战术动作。</font></strong></li>
<li><strong>告警研判过程中存在重复低效的二次取证的工作</strong></li>
<li><strong>不完备的资产实体库</strong></li>
<li><strong>研判过程中还缺乏知识的指引</strong></li>
</ul>
</li>
</ul>
<h4><span id="如何针对事件检测响应去建构知识体系"><strong>如何针对事件检测响应去建构知识体系？</strong></span></h4><ul>
<li><strong>知识模型</strong>：STRIDE的模型、kill Chain杀链模型、ATT&amp;CK模型</li>
<li><strong>知识交换</strong>：指交换威胁情报和lOC识别的知识，目前业内比较通用的有像<strong>==STIX2.0==，还有TAXll协议</strong>。</li>
<li><strong>知识运用</strong>：如何运用知识来指导我们做lOC规则的开发，如何做研判策略的设计，以及如何制定事件处置的SOP流程等等。</li>
<li><strong>知识迭代</strong>：知识模型自身的迭代，比如说近期ATT&amp;CK模型也推出了<strong>Sub-techniques</strong>，就是子技术的概念。另一方面就是日常运营的案件如何反馈到知识模型来做迭代。</li>
</ul>
<h4><span id="如何建立科学的度量和评价体系"><strong>如何建立科学的度量和评价体系？</strong></span></h4><p>我们在事件检测运营中有很多指标，比如<strong>围绕资产实体的，有HIDS部署的覆盖率，比如终端EDR的安装覆盖率，还有跨网络、跨安全域网络边界、南北向流量检测覆盖率</strong>等等。</p>
<p>还有围绕运营流程的，比如像比较核心的MTTD/MTTR指标。还有围绕能力矩阵的，比如ATT&amp;CK矩阵的覆盖率。<strong>指标的选取，它关系到能否把控安全态势的全局以及能否做好牵引能力的建设，因此选取有效的度量和评价体系，也是存在挑战的。</strong></p>
<blockquote>
<p>  <strong>MTTD = 故障与检测之间的总时间/事件数量</strong></p>
<p>  <strong>MTTA = 指从系统产生告警到人员开始注意并处理的平均时间。</strong></p>
</blockquote>
<h3><span id="安全编排自动化与响应soar它能为安全事件检测与响应流程带来哪些改善"><strong>安全编排自动化与响应SOAR，它能为安全事件检测与响应流程带来哪些改善?</strong></span></h3><blockquote>
<p>  <strong>SOAR分为安全编排自动化，安全事件响应平台，以及威胁情报平台三种技术工具的融合。</strong></p>
</blockquote>
<h4><span id="soar如何加速事件检测和响应"><strong>SOAR如何加速事件检测和响应</strong></span></h4><p>首先在IDR运营流程中，我们接收到一个异常的事件Event，我们如何通过SOAR的思想来处理这个事件，从而提升IDR流程的效率？</p>
<ul>
<li><strong>告警分诊</strong>:一个原始的告警里边可能只包含了少量的事件信息，我们需要在这个阶段使<strong>告警丰富化</strong>，也就是 Enrichment概念——<strong>将原始告警中的IP服务器，终端ID这样的字段，在我们内部的资产库当中查询出详细的信息，并且自动补充到告警信息中。</strong></li>
<li><strong>初步决策</strong>，比如有些字段命中了<strong>白名单库</strong>，或者威胁情报显示这是一个非恶意的良性的特征。将告警作为误报直接关闭，减少后面人工审计的运营负担。</li>
<li><strong>调查取证</strong>：通过SOAR的自动调用能力，可以调用后台的数据，收集更多的IOC信息，我们也可以调用沙箱这个能力对可疑文件进行动态的检测，得到检测结果，从而实现证据的自动收集。</li>
<li><strong>溯源关联分析</strong>：实现告警事件的上下文相关联事件的聚合。</li>
</ul>
<blockquote>
<p>  比如说同一个告警事件，它发生在不同的资产实体上，或者说同一个资产实体，它在一定的时间段内，发生了多类的告警事件.</p>
</blockquote>
<p><strong>经过前期的告警分诊、误报关闭、调查取证的几个阶段，原始的事件event就转化为了一个需要人工验证的incident案件</strong>。在这个环节安全工程师会根据前面SOAR自动补充和取证的信息做出研判，进入到对这个事件的响应的流程。<strong>响应阶段也可以利用SOAR自动地执行安全处置的动作，包括邮件IM通知员工，或创建处置或漏洞工单，或是向防火墙/终端EDR下发安全策略（比如封禁）等等</strong>。这样我们就通过SOAR完成了一次告警事件的检测与响应的流程。</p>
<h4><span id="剧本编排的概念"><strong><font color="red"> 剧本编排的概念</font></strong></span></h4><p><strong>以钓鱼邮件检测与响应的剧本为例。</strong></p>
<ul>
<li><strong>检测钓鱼邮件</strong>时，首先是提取<strong>邮件的信息</strong>，包括<strong>发件人</strong>、<strong>收件人</strong>、正文里可点击的<strong>url链接列表</strong>、<strong>附件</strong>等等。<ul>
<li>从AD里查询出<strong>员工的相关信息</strong>，可以自动去邮件服务器的访问日志里面去查看员工近期有没有<strong>异常的登录行为</strong>，比如说异地登录，或者是使用非常设备登录等等。</li>
<li><strong>URL链接</strong>，我们首先去威胁情报里查一下有没有命中情报样本。针对可疑的URL链接，我们可以结合像Whois信息，像域名的信息，对 URL进行评分。</li>
<li><strong>针对邮件的附件</strong>也可以做静态分析，看是否包含 office鱼叉。我们还可以利用Cuckoo这样的动态沙箱对邮件附件里的可执行文件做行为检测。我们还可以利用外部的比如Virus Total这样的样本分析平台，来看是否命中恶意样本。</li>
</ul>
</li>
<li>经过信息的自动化收集和分析的动作，我们进入到最后的<strong>人工审计环节</strong>。这个时候安全工程师会结合前面自动收集的信息去做研判。一旦安全工程师识别出这是一个有效的钓鱼邮件，也会通过<strong>剧本的方式去执行后续的这些自动化的动作，包括向员工发送告警工单，要求他修改域账号的密码</strong>。我们还可以将发件人的邮箱加入邮件安全网关的发件人黑名单列表里，防止他再给其他员工继续发邮件。我们也可以将恶意的或可疑的钓鱼邮件链接的域名加入到我们DNS封禁列表里，<strong>来防止进一步的扩散</strong>。</li>
</ul>
<h4><span id="结合滴滴的实践经验和探索介绍贴合甲方实际场景的soar建设思路"><font color="red"> 结合滴滴的实践经验和探索，介绍贴合甲方实际场景的SOAR建设思路</font></span></h4><p><strong>需要明确SOAR</strong>在事件检测响应体系中的定位，也就是它<strong>与SIEM/SOC/安全事件响应平台SIRP之间的关系</strong>，还有它<strong>与TIP威胁情报平台之间的关系</strong>。SOAR可以理解为是事件响应平台或者是SOC的扩展能力。 当然SOAR也可以作为一个独立的平台，与SOC和TIP实现打通。</p>
<blockquote>
<p>  根据Gartner的定义，SOAR是一系列技术的合集，它能够帮助企业和组织收集安全运营团队监控到的各种信息，也包括各种安全系统产生的告警，并对这些信息进行告警分诊和事件分析。</p>
</blockquote>
<p><strong>SOAR在甲方如何落地，主要考虑三方面：</strong></p>
<ul>
<li><strong>实现路径</strong>:<ul>
<li>可以采用商业化的产品，近期我们也看到很多国内外知名安全厂商陆续推出了SOAR这款产品。</li>
<li>我们也可以基于开源工具做二次开发，比如说剧本编排引擎，它特别类似于一个<strong>Workflow的工作流引擎</strong>，我们可以基于开源的像Activity或者是Airflow这样的工作流引擎去做二次的开发。</li>
<li>使用自研的方式。</li>
</ul>
</li>
<li><strong>技术选型</strong>：主要是考虑可视化剧本的编排引擎，还有剧本的执行引擎。</li>
<li><strong>系统设计</strong>：SOAR虽然是一个扩展的能力，但是从系统设计的角度来说，一旦我们引入SOAR，就会将它串联到我们整个的 IDR流程当中。所以SOAR自身的稳定性，还有一些其他的技术指标，比如像EPS每秒处理的事件数，SLA，包括一些其他的benchmark等等，这些也是我们关注的重点。刚才也提到SOAR会串联到IDR流程里，所以它有可能会引入或导致一个单点问题，所以我们也会考虑分布式的部署。还有降级，一旦SOAR不可用的时候，我们的SOC或者事件响应平台能否降级到没有SOAR的状态。</li>
</ul>
<h4><span id="如何评估soar的效果和收益"><strong><font color="red"> 如何评估SOAR的效果和收益？</font></strong></span></h4><ul>
<li><strong>对IDR核心运营指标MTTD和MTTR的提升</strong>，它能让我们技术运营投入更少的人力去做更多的事，提升人效。</li>
<li><strong><font color="red"> 他能否通过SOAR来识别攻击者完整的战术动作，也就是TTP。</font></strong></li>
<li>通过将剧本的引入，将流程案件知识固化下来，牵引我们能力侧的建设。</li>
</ul>
<h4><span id="结合滴滴的经验和探索介绍一下soar的系统设计思想"><strong>结合滴滴的经验和探索，介绍一下SOAR的系统设计思想？</strong></span></h4><p><strong>首先我们从各个sensor采集到的数据经过ETL存储在大数据的组件当中</strong>。我们的策略规则是作用在这些大数据的计算引擎上，像 Spark，Hadoop，还有Flink这样实时的引擎，也包括我们自研的异常检测的引擎，最终产生的异常告警事件会打到我们的event gateway通用事件网关上。这一阶段被我们称为异常检测阶段。</p>
<p>事件网关主要做两个事：一，做<strong>标准化</strong>，将这些异构的数据源产生的各种类型的告警里的字段格式和数据类型做标准化，以便后面我们在做SOAR编排的时候降低成本。二，<strong><font color="red"> 在这个环节我们会做 index，把原始的告警事件索引到数据库里，以便我们后面做关联分析，或者我们可以回溯的时候去实时地查询历史的告警事件数据。</font></strong></p>
<p>【<strong>告警分诊</strong>】经过事件网关以后，我们紧接着做两个事情，<strong>一个是做Enrichment丰富化</strong>，第二个是做<strong>威胁情报</strong>。我们在丰富化这个阶段会补齐像服务器地址、员工信息、终端信息和调研我们内部的核心的资产库，将告警信息做丰富化。 第二就是我们会初步匹配告警字段里边比如像域名，像文件哈希，去我们本地的威胁情报库里面做匹配。</p>
<p>【<strong>调查取证</strong>】接下来就进入到我们的核心检测阶段SOAR编排环节，在这个环节我们将各种检测能力抽象成为各种检测引擎，比如像攻击检测引擎、误报检测引擎、调查取证引擎和关联分析引擎等等。</p>
<ul>
<li><p>【<strong>黑名单</strong>】<strong>攻击检测</strong>引擎是做什么？主要是根据告警事件里的一些字段去我们本地的黑名单库列表里做匹配，一旦确认命中我们的黑名单，就可以不需要做后面一些列复杂的调查取证和关联分析工作，可以直接交给人工来做研判，甚至对它可以绕过人工来做自动化响应。</p>
</li>
<li><p>【<strong>白名单</strong>】<strong>误报检测</strong>是根据字段里边的一些特征，以及我们之前配置的白名单规则，命中了白名单，这个事件我们可以把它自动关闭掉，以减少后面调查取证的负担。</p>
</li>
<li><strong>调查取证</strong>我们是将一些通用的外部接口和能力封装成一些函数或者脚本，来做自动化的调用。而这些封装的能力之间，我们也是以一个子剧本的方式来进行编排，它可以根据剧本流程的配置来做自动化的执行和调用。</li>
<li><strong><font color="red"> 关联分析引擎也是基于我们配置好的一些关联分析的规则，来针对这一个告警事件的上下文，或者一段时间内它同资产的一些其他告警事件来做关联和聚合，上报给人工去做研判。</font></strong></li>
</ul>
<p><strong>这些不同的检测引擎之间，我们也是通过剧本的方式把它进行一个整体的编排</strong>。有些我们可以先经过攻击检测引擎，误报检测引擎，再做调查取证和关联分析；而有一些告警类型，我们通过剧本的编排，它就不需要去做攻击检测了，比如他通过误报检测就可以直接到调查取证检测。这些其实都是通过剧本来实现一个动态编排。</p>
<p>【<strong>人工验证</strong>】经过这个阶段的检测，原始事件就形成了一个具体的需要人工验证的案件，也就是<strong>incident</strong>。从原始的事件到案件，这个阶段我们称它为是检测阶段的SOAR编排。【<strong>自动处置</strong>】这阶段经过人工的验证，如果是一个有效的案件需要经过处置的话，它就会进入到后续的自动处置的流程里面。而这一阶段我们也是通过剧本的方式，将各种处置能力封装来自动编排上。这里边包括像通过邮件和IM消息的方式来通知用户，也包括我们调用工单系统，还有就是我们调用 EDR/IPS/防火墙的一些封禁策略等等，把它封装成自动的脚本，通过剧本的方式做编排，做自动的调用。</p>
<h4><span id="在做soar系统设计的时候是如何把知识体系来融合到系统设计里的呢"><strong>在做SOAR系统设计的时候，是如何把知识体系来融合到系统设计里的呢？</strong></span></h4><p>在上文提到的情报交互里有一个<strong>STIX2.0协议</strong>，STIX2.0有很多个构件，其中有几个构件其实是可以指导我们去做异常检测规则的开发，以及SOAR编排里的关联分析和处置动作的。</p>
<ul>
<li><p><strong>indicator</strong>，就可以指导我们去做异常检测阶段的IOC规则开发；</p>
</li>
<li><p><strong>Attack Pattern</strong>，描述的其实就是TTP，可以指导我们在SOAR检测阶段去做关联分析规则；</p>
</li>
<li><strong>course of action</strong>构件，它是指导我们在做响应处置阶段的SOP的流程。</li>
</ul>
<p>我们前面也提到了ATT&amp;CK模型，其实<strong>ATT&amp;CK模型和STIX2.0之间是有映射关系的</strong>，我们可以将我们的异常检测规则映射到ATT&amp;CK模型上，主要是做两个事，第一个就是我们根据现有的检测点，可以总体来看我们<strong>对ATT&amp;CK的覆盖率</strong>，这样它能牵引我们去做能力侧的建设，也就是<strong>检测策略建设</strong>。<strong>当我们发现缺少哪一部分的检测能力，我们就可以去部署新的sensor，开发新的IOC规则。</strong></p>
<p><strong>我们也可以结合ATT&amp;CK模型去和我们的真实的日常运营中的案件做结合，去查看我们ATT&amp;CK热力图，去从整体安全态势上看我们哪些场景是经常会被攻击的</strong>。我们也可以结合资产的重要性、等级和实际发生的案件，通过一个公式来计算出我们整体的风险值。</p>
<p>【<strong>异常检测评估指标</strong>】整个SOAR流程和指标体系也是紧密结合的，包括我们在异常检测阶段有能力矩阵的覆盖率这样的指标。还有我们在检测阶段的SOAR编排决定了我们的MTTD（平均检测时间）的指标，以及在响应阶段SOAR关联了我们的MTTR（平均响应时间）指标。</p>
<p>这样我们就围绕着SOAR的系统设计，将IDR事件检测与响应流程、SOAR的自动编排、知识体系和指标体系，都融合在了我们整个的SOAR的系统设计思想里。</p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（7）AVClass2-自动恶意软件标记工具</title>
    <url>/posts/169EKJ8/</url>
    <content><![CDATA[<h1><span id="如何利用多杀软结果提取恶意软件标签">如何利用多杀软结果提取恶意软件标签</span></h1><ul>
<li>secrss.com/articles/33242</li>
<li><strong>通过多杀软结果挖掘得到更多关于样本的上下文信息是一个经久不衰的研究点</strong></li>
</ul>
<p><strong>从杀软标签中自动提取标签是对大量样本进行分类和索引的有效方法</strong>。此前的 AVClass 和 Euphony 等工作已经能够从杀软标签中提取家族名称。而杀软标签包含有价值的信息不止是家族，还有类别（例如勒索软件、下载器、广告软件）和行为（例如垃圾邮件、DDoS、信息窃取）等。</p>
<p><strong>恶意软件属性枚举和表征（MAEC）等标准定义了一种用于共享恶意软件分析结果的语言</strong>。然而，由于使用严格的受控词汇表（即预定义标签），这些词汇表可能并不总是符合分析师的需求，需要频繁更新，并且必然是不完整的，因此它们的采用率很低，例如，MAEC 中就不包括<strong>恶意软件家族</strong>。</p>
<p>杀软引擎有一些通用标签，标明恶意软件的类别、家族、文件属性和动态行为。也有一个通用标签（malicious, application）和特定杀软引擎（deepscan, cloud）才有的，或者是恶意软件家族变种（aghr, bcx）标签。</p>
<h3><span id="工作设计">工作设计</span></h3><p><strong>AVClass2 的目标是分辨提供有用信息的 Token，识别不同杀软引擎 Token 之间的关系，最后转换成分类法的标签。</strong></p>
<p>AVClass2 是一个自动恶意软件标记工具，可为样本提取一组干净的标签。AVClass2 附带一个默认的开放分类法，可将杀软标签的名词分类到不同的类别，捕获标签之间关系的默认标记规则和扩展规则。AVClass2 有一个更新模块，使用标签共现来识别标签之间的关系，以在杀软厂商引入新标签时保持工具更新。</p>
<p>AVCalss2 基于 AVClass 进行了最少必要更改，继承了 AVClass 的主要特点：可扩展性好、杀软引擎独立性好、平台无关性好、不需要样本文件、开源。</p>
<p>基本架构如下所示：</p>
<p><img src="https://s.secrss.com/anquanneican/13c4e4d7d81462e10c618bee59fbf971.png" alt="img"></p>
<p><strong>主要是两大模块：Labeling 模块和 Update 模块。</strong></p>
<ul>
<li>Labeling 模块将<strong>多个杀软的标签结果作为输入</strong>，同时可以提供使用的杀软引擎列表，如果不提供默认使用所有杀软引擎的标签结果。给出一组<strong>标记规则</strong>、一个<strong>可选扩展规则</strong>以及可将标签<strong>分类合并</strong>的分类法。</li>
<li>Update 模块将<strong>共现统计、标记规则、扩展规则和分类法作为输入</strong>。识别标签之间的<strong>强关联</strong>，生成新的标记规则、扩展规则和分类法。</li>
</ul>
<h3><span id="标签">标签</span></h3><p><strong>Labeling 模块分为三部分：标记化（Tokenization）、标记（Tagging）、扩展（Expansion）。</strong></p>
<ul>
<li><strong>标记化（Tokenization）将每个杀软标签拆分为一个 Token 列表</strong>。标记化是与厂商无关的，VirusTotal 现在已经支持超过一百个引擎。每个厂商的格式也不完全一致，经常修改。如果尝试为引擎的标签定义格式或者自动推断格式，可能会得到数百个格式模板。不仅选择正确格式进行解析很困难，遇到未知格式的标签还可能出现错误。</li>
<li><strong>标记（Tagging）会用分类法中的一组 Tag 替换杀软标签中的 Token，即将杀软标签中的 Token 转换为分类法中概念明确的 Tag</strong>。大多数标记规则会映射到单个标记，例如 downldr、dloader 会被映射到 downloader 上；finloski 和 fynloski 会被映射到 darkkomet 上。也存在一对多的关系，比如 ircbot 会映射到 irc 和 bot。</li>
<li><strong>扩展（Expansion）用于处理未知的 Token，使用扩展规则定义一个标签隐含一组其他标签</strong>。例如有 95% 的标签在带有 virut 的同时也带有 virus，virut 就会是 virus 的扩展规则。扩展规则一共分为两类，一类是类内规则一类是类间规则，处理顺序是先类间规则再类内规则。类内规则由分类法中统一类别的父子关系隐式定义，例如 adware 是 grayware 的子类。类间规则由分类法中不同类别的隐式关系定义，例如 filemodify 行为归属于 virus 类。</li>
</ul>
<p><strong>整体流程如下所示：</strong></p>
<p><img src="https://s.secrss.com/anquanneican/018d45d7ed61a0cca153cfd53d145ade.png" alt="img"></p>
<h3><span id="分类法">分类法</span></h3><blockquote>
<p>  <strong>行为、类型、文件属性和家族</strong></p>
</blockquote>
<p>分类法定义了标记规则使用的标签之间的父子关系。AVClass2 的分类法被构造为树型结构，默认包含四个类型（<strong>行为 BEH、类型 CLASS、文件属性 FILE、家族 FAM</strong>）。</p>
<p><img src="https://s.secrss.com/anquanneican/070902d5885e1c46d17142d43813a9fe.png" alt="img"></p>
<p>标签是自上而下进行描述的，例如 CLASS:grayware:adware。</p>
<p><strong>自带的默认分类法如下所示：</strong></p>
<p><img src="https://s.secrss.com/anquanneican/45bbf65333720324166aeab69f49ece1.png" alt="img"></p>
<ul>
<li><strong>行为</strong>：例如 infosteal（信息窃取）、sendssms（发送短信）、spam（垃圾邮件）、mining（挖矿）等</li>
<li><strong>类别</strong>：例如 worm（蠕虫）、virus（病毒）、ransomware（勒索）、downloader（下载）。Trojan 问题很大，原来特指某类，后来变成了默认类型，故而认为 Trojan 为通用 Token。</li>
<li><strong>文件属性</strong>：例如<strong>文件类型</strong>（例如 pdf、flash、msword）、<strong>操作系统</strong>（android、linux、windows）、<strong>壳类型</strong>（pecompact、themida、vmprotect）、<strong>编程语言</strong>（autoit、delphi、java）</li>
<li><strong>家族</strong>：默认分类家族不包括父子关系</li>
</ul>
<h3><span id="update">Update</span></h3><p>为了新的家族、新的行为都能够通过 AVClass2 自动更新，需要根据共现关系识别数据集中的强关系，迭代更新到规则中。基于 VAMO 引用的杀软标签共现关系，在 AVClass 和 Euphony 中也用于合并家族。（Roberto Perdisci and U. ManChon. 2012. VAMO: Towards a Fully Automated Malware Clustering Validity Analysis. In Annual Computer Security Applications Conference.）。共现的判断需要确定阈值，以AVClass的经验选择 𝑛 = 20 和 𝑇 = 0.94。</p>
<p><img src="https://s.secrss.com/anquanneican/9d67ba609a2c45feb86dcd605dd5c06f.png" alt="img"></p>
<h3><span id="工作准备">工作准备</span></h3><p>使用 11 个数据集进行评估，数据集之间存在重复（例如 Drebin 是 MalGenome 的超集）但并未去重，为了便于单独映射结果。</p>
<p><img src="https://s.secrss.com/anquanneican/18030348ab876b5fed8137aa52b61655.png" alt="img"></p>
<h3><span id="工作评估">工作评估</span></h3><p>通过在 4200 万恶意样本中评估 AVClass2，并且与 AVClass 和 Euphony 进行了比较，测试其效果。</p>
<h3><span id="标记覆盖">标记覆盖</span></h3><p>标签覆盖率如下所示：</p>
<p><img src="https://s.secrss.com/anquanneican/0060066c74a362e126f439c6efc4b669.png" alt="img"></p>
<ul>
<li><p>选择至少四个杀软引擎标记为恶意的样本，最近的研究表明 2-14 个杀软引擎判定的筛选范围有利于平衡精度和召回率。</p>
<blockquote>
<p>  Shuofei Zhu, Jianjun Shi, Limin Yang, Boqin Qin, Ziyi Zhang, Linhai Song, and Gang Wang. 2020. <strong>Measuring and Modeling the Label Dynamics of Online Anti-Malware Engines</strong></p>
</blockquote>
</li>
<li><p>AVClass2 可以为 89% 以上的样本提取至少一个标签，无法提取的基本都是检测结果较少的文件</p>
</li>
<li><p>测试时可识别的 975 个标签已经超过了 VirusTotal 的 335 个标签，VirusTotal 的标签基本都对应于文件属性和样本行为。其中，与 VirusTotal 重合的共有 259 个标签</p>
</li>
</ul>
<p>每个类别 TOP10 的标签如下所示：</p>
<p><img src="https://s.secrss.com/anquanneican/9a1f118fb14f2944de07888d2beb9a08.png" alt="img"></p>
<ul>
<li>超过 10% 的样本对应了四个类别的标签。例如 CLASS:grayware:adware:multiplug 是通过浏览器插件进行广告推广的软件。</li>
<li>Trojan 如果不是通用Token被剔除的话，会被分配给 86% 的样本。</li>
<li>最多的家族是 vobfus，占到了总数的十分一。</li>
<li>除了恶意软件外，grayware 也是常见家族的大赢家（loadmoney、softpulse、installererex、domaiq、firseria）。</li>
</ul>
<h3><span id="知识更新">知识更新</span></h3><p>使用 Andropup 数据集举例说明 update 模块的用法。首次测试观察到 65% 的样本包含一个未知标签，执行 update 模块后会下降到 16%。</p>
<p><img src="https://s.secrss.com/anquanneican/78fad75a47f80238fc0ca5a40ea263b4.png" alt="img"></p>
<p>共现关系共计 30107 个，有归属于 11 类的 968 个强关联。96% 的强关联涉及未知 Token，从中自动识别出了 486 个新类别实体、216 个新标记规则、461 个扩展规则。处理完成后只剩下 3 个强关联不能自动更新，需要手动处理。</p>
<p>手动检查了更新的内容，1163 个更新中只有 11 个（0.9%）是需要调整的、3 个是需要手动检查的。</p>
<h4><span id="执行速度">执行速度</span></h4><p><img src="https://s.secrss.com/anquanneican/3cee7fc69bc91ff6f01b24c764ed46ef.png" alt="img"></p>
<ul>
<li>AVClass2 和 AVClass* 在四个数据集中获得了最好的 F1 成绩，而 AVClass 在 Malheur 上排名第一。</li>
<li>AVClass 最快，AVClass2 其次，Euphony 则比 AVClass 慢 7 到 34 倍。对特大的数据集 Euphony 会很慢或者因内存不足而崩溃。</li>
</ul>
<h2><span id="工作思考">工作思考</span></h2><p>AVClass2 对通过多杀软结果处理实现提取 VirusTotal 类的 Tag 标签很有帮助，实际上没有必要合并成一个完整的分类法的语法树结构。<strong>通过多杀软结果挖掘得到更多关于样本的上下文信息是一个经久不衰的研究点</strong>，本文作者也在 AVClass 的基础上再进一步做出了 AVClass2，<strong>==两个工作分别发表在 RAID 2016 与 ACSAC 2020 都是很不错的成绩。==</strong></p>
<p>像 AVClass++ 指出的那样，AVClass 在杀软引擎结果较少时效果较差，那些新提交到 VirusTotal 的样本会因此效果较差。另外就是杀软结果中也存在随机生成类的结果，这两点实际上都可能是未来在这条路上的研究进展，AVClass++ 的解决方法是否很优则见仁见智，但仍不失为一个极佳的参考。</p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（8）STIX协议 《网络威胁情报协议》</title>
    <url>/posts/3X70Q09/</url>
    <content><![CDATA[<h2><span id="网络威胁情报之-stix-21">网络威胁情报之 STIX 2.1</span></h2><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/365563090">https://zhuanlan.zhihu.com/p/365563090</a></p>
</blockquote>
<h3><span id="一-说明">一、说明</span></h3><p>STIX（Structured Threat Information Expression）是一种用于交换网络威胁情报（cyber threat intelligence，CTI）的语言和序列化格式。STIX的应用场景包括：<strong>协同威胁分析、自动化威胁情报交换、自动化威胁检测和响应</strong>等。</p>
<h5><span id="stix对网络威胁情报的描述方法如下">STIX对网络威胁情报的描述方法如下：</span></h5><p><img src="https://pic4.zhimg.com/80/v2-c9e59ff49083188d1bf41879c9e1756b_1440w.jpg" alt="img"></p>
<p><strong>STIX Domain Objects</strong>（SDO）：威胁情报主要的分类对象，包含了一些威胁的behaviors和construct，共有18种类型：<strong>Attack Pattern</strong>, Campaign, <strong>Course of Action</strong>, Grouping, Identity, Indicator, Infrastructure, Intrusion Set, Location, Malware, Malware Analysis, Note, Observed Data, Opinion, Report, Threat Actor, Tool, and Vulnerability.</p>
<p><img src="https://pic3.zhimg.com/80/v2-e5fbd4d7693f6dae450a18d6e5981c6a_1440w.jpg" alt="img"></p>
<p><img src="https://pic2.zhimg.com/80/v2-acb4fcb44a2a7d8dd065c75aa848cdc1_1440w.jpg" alt="img"></p>
<p><strong>STIX Cyber-observable Objects</strong>（SCO）：威胁情报中具体的可观察对象，用于刻画<strong>基于主机或基于网络的信息</strong>。</p>
<ul>
<li>SCO会被多种SDO所使用，以提供上下文支持，如<em>Observed Data</em> SDO，表示在特定时间观察到的raw data；在STIX2.0中，SCO在SDO中出现时只会以Observed Data的形式出现，在STIX2.1则不限于此。</li>
<li>SCO本身不包括who，when和why的信息，但是将SCO和SDO关联起来，可能会得到这些信息以及对威胁更高层面的理解。</li>
<li>SCO可以捕获的对象包括文件、进程、IP之间的流量等。</li>
</ul>
<p><strong>STIX Relationship Objects</strong>（SRO）：用于SDO之间、SCO之间、SDO和SCO之间的关系。SRO的大类包括以下两种：</p>
<p><img src="https://pic1.zhimg.com/80/v2-2e19804c43197207ae032bc991594ca0_1440w.jpg" alt="img"></p>
<p><strong>generic SRO（Relationship）</strong>：大多数关系所采用的类型，其relation_type字段包括：内置关系：如Indicator到Malware之间的关系，可以用<em>indicates</em> 表示，它描述了该Indicator可用于检测对应的恶意软件；自定义关系；</p>
<p><strong>Sighting SRO</strong>：用于捕获实体在SDO中出现的案例，如sighting an indicator。没有明确指明连接哪两个object。之所以将其作为独立的SRO，是因为其具有一些独有的属性，如<em>count</em>。</p>
<p>除了SRO，STIX还用ID references来表示嵌入关系（embedded relationship）。当使用嵌入关系时，表示该属性时该对象的内置属性，从而不需要使用SRO表示，如<em>create_by_ref。</em>因此，SRO可以视为两个节点直接的边，而embedded relationship则可以视为属性（只不过其表示了二元关系）</p>
<ul>
<li><strong>STIX Meta Objects</strong>：用于丰富或扩展STIX Core Objects</li>
<li><strong>STIX Bundle Object</strong>：用于打包STIX内容</li>
</ul>
<p><strong>STIX是一种基于图的模型，其中SDO和SCO定义了图的节点，而STIX relationships定义了边</strong>。</p>
<p><strong>STIX Patterning language</strong>：STIX模式语言可以实现网络或终端的威胁检测。该语言目前使用STIX Indicator对象，来匹配时序的observable data。</p>
<h3><span id="二-通用数据类型">二、<strong>通用数据类型</strong></span></h3><p><img src="https://pic3.zhimg.com/80/v2-2d7ff334fee36adda542c0d3281c5d62_1440w.jpg" alt="img" style="zoom:67%;"></p>
<h3><span id="三-stix-通用概念">三、 <strong>STIX 通用概念</strong></span></h3><ul>
<li>STIX common properties</li>
</ul>
<p><img src="https://pic3.zhimg.com/80/v2-4502a49dc00fc1bfae5913c20a191a3e_1440w.jpg" alt="img" style="zoom:67%;"></p>
<h3><span id="四-stix-domain-objects"><strong>四、 STIX Domain Objects</strong></span></h3><p>每个SDO对应 交换网络威胁情报CTI中的唯一概念。<strong>使用SDO，SCO和SRO作为基本模块</strong>，用户可以方便的创建和共享CTI。</p>
<p><strong>SDO</strong>：</p>
<ul>
<li>Property：通用属性、SDO转专用属性</li>
<li>Relationship：embedded relationships、common relationships</li>
</ul>
<p>一些相似的SDO可以被归为一个大类，如：</p>
<ul>
<li><strong>Attack Pattern, Malware, and Tool可以被归为TTP，因为它们描述了攻击行为和资源</strong></li>
<li>Campaign, Intrusion Set, and Threat Actor 可以被描述为“攻击者发动攻击的原因，以及如何组织（why and how）”</li>
</ul>
<h4><span id="41-attack-pattern">4.1 <strong>Attack Pattern</strong></span></h4><p>TTP类型之一，它描述了攻击者试图破坏目标的<strong>方式，</strong>对应于TTP中的<strong>战术</strong>。可用于帮助对<strong>攻击进行分类</strong>，将特定的<strong>攻击概括为其遵循的模式</strong>，并提供有关<strong>如何进行攻击的详细信息</strong>。</p>
<blockquote>
<p>  如spear fishing就是一种攻击模式，而更具体的描述，如被特定攻击者实施的spear fishing也是一种攻击模式。</p>
</blockquote>
<h4><span id="42-campaign"><strong>4.2 Campaign</strong></span></h4><p>表示某次具体的攻击活动。A Campaign is a grouping of adversarial behaviors that describes a set of malicious activities or attacks (sometimes called waves) that occur over a period of time against a specific set of targets. Campaigns usually have well defined objectives and may be part of an Intrusion Set.</p>
<blockquote>
<p>  战役是一组敌对行为，描述了针对特定目标集在一段时间内发生的一组恶意活动或攻击（有时称为WAVE）。活动通常有明确的目标，可能是入侵集的一部分。</p>
</blockquote>
<h4><span id="43-course-of-action-响应的行为">4.3 <strong>Course of Action (响应的行为)</strong></span></h4><p>用于预防攻击或对攻击做出响应的行为，它回包含技术，自动化响应（补丁、重新配置防火墙），或高级别的动作（如员工培训或者策略制定）。</p>
<h4><span id="44-grouping"><strong>4.4 Grouping</strong></span></h4><p>Grouping表示分析和调查<strong>过程中</strong>产生的数据（待确认的线索数据）；还可以用来声明<strong>其引用的STIX对象与正在进行的分析过程有关</strong>，如当一个安全分析人员正在跟其它人合作，分析一系列Campaigns和Indicators的时，<strong>Gouping会引用一系列其它SDO、SCO和SRO（Grouping就表示协作分析吧）。</strong></p>
<p>除了embedded relationship和common relationship之外，没有明确定义Grouping对象和其它STIX对象之间的关系。</p>
<h4><span id="45-identity"><strong>4.5 Identity</strong></span></h4><p><strong>Identity可以代表特定的个人、组织或团伙</strong>；也可以代表一类个人、组织、系统或团伙。Identity SDO可以捕获基本标识信息，联系信息以及Identity所属的部门。 Identity在STIX中用于表示攻击目标，信息源，对象创建者和威胁参与者身份。</p>
<h4><span id="46-incident"><strong>4.6 Incident</strong></span></h4><p>stub对象，待完善，没有专门定义的property和relationship。</p>
<h4><span id="47-indicator"><strong>4.7 Indicator</strong></span></h4><p>Indicator表示可用于检测可疑行为的模式。如用STIX Patterning Language来描述恶意域名集合（第九章）。</p>
<h4><span id="48-infrastructure"><strong>4.8 Infrastructure</strong></span></h4><p><strong>TTP的类型之一，用于描述系统、软件服务等其它的物理或虚拟资源</strong>；如攻击者使用的C2服务器，防御者使用的设备和服务器，以及作为被攻击目标的数据库服务器等；</p>
<p>基于此我们可以将受保护网络中的设备纳入知识图谱，采用类似于这样的关系：</p>
<p><img src="https://pic2.zhimg.com/80/v2-1d2970679ce3243e8ccefe0eda890351_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h4><span id="49-intrusion-set"><strong>4.9 Intrusion Set</strong></span></h4><p>Intrusion set是由<strong>某个组织</strong>所使用的恶意行为和资源的集合。一个Intrusion Set可能会捕获多个Campaigns，他们共同指向一个Threat Actor。新捕获的活动可以被归因于某个Intrusion Set，而Actors可以在Intrusion之间跳转，甚至从属于多个Intrusion Set。</p>
<p>如在 apt1.json 中，整个报告被打包在bundle中，而Intrusion Set用来指示APT组织：</p>
<p><img src="https://pic3.zhimg.com/80/v2-e40a11cbdf0b033063367a3d045d77ba_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>Intrusion Set和Campaigns对比：</strong></p>
<p><strong><font color="red"> 如果 Campaigns 是在一段时间内针对一组特定目标进行的一组攻击，以实现某些目标，那么入侵集就是整个攻击包，可以在多个活动中长期使用，以实现潜在的多个目的.</font></strong>由Intrusion Set找出Threat Actors，nation state或者nation state中的某个APT组织，是一个<strong>溯源</strong>的过程。</p>
<h4><span id="410-location"><strong>4.10 Location</strong></span></h4><p>表示具体地点，可以与Identity或Intrusion Set相关联，表示其位置；与Malware或Attack Pattern相关联，表示其目标。</p>
<h4><span id="411-malware"><strong>4.11 Malware</strong></span></h4><p>TTP类型之一，表示<strong>恶意软件或代码。</strong></p>
<h4><span id="412-malware-analysis"><strong>4.12 Malware Analysis</strong></span></h4><p>捕获了在恶意软件实例或恶意软件家族分析过程中，动态分析或静态分析的结果。</p>
<h4><span id="413-note"><strong>4.13 Note</strong></span></h4><p>其他对象中不存在的额外信息；例如，分析人员可以在一个Campaign对象中添加注释，以表明他在黑客论坛上看到了与该Campaign相关的帖子。同样，Note对象也没有定义与其他STIX Object之间的关系。</p>
<h4><span id="414-observed-data"><strong>4.14 Observed Data</strong></span></h4><p><strong>网络安全相关的可观察对象（raw information）集合，其引用对象为SCO，包含从analyst reports, sandboxes, and network and host-based detection tools等收集的信息。</strong></p>
<p><strong>必须包含objects或者object_refs属性，表示对SCO的引用</strong>：</p>
<p>Observed Data只有反向关系。此外，还会被Sighting SRO所指向：Sightings represent a relationship between some intelligence entity<strong> that was seen</strong> (e.g., an Indicator or Malware instance), <strong>where it was seen</strong>, and <strong>what evidence was actually seen.</strong> The <strong>evidence (or raw data) in that relationship is captured as Observed Data（Sighting中的证据就是Observed Data）。</strong></p>
<h4><span id="415-opinion"><strong>4.15 Opinion</strong></span></h4><p>Opinion是对STIX对象中信息正确性的评估。</p>
<h4><span id="416-report"><strong>4.16 Report</strong></span></h4><p>威胁情报报告。</p>
<h4><span id="417-threat-actor"><strong>4.17 Threat Actor</strong></span></h4><p>攻击的个人、团体或组织；其与Intrusion Set不同，Threat Actor会同时支持或附属于不同的Intrusion Set、团体或组织。</p>
<h4><span id="418-tool"><strong>4.18 Tool</strong></span></h4><p><strong>Tool是威胁参与者可以用来执行攻击的合法软件。与Malware不同，Tool一般是合法软件，如Namp、VNC。</strong></p>
<h4><span id="419-vulnerability"><strong>4.19 Vulnerability</strong></span></h4><p>漏洞。用于连接相关漏洞的外部描述（external_references），或还没有相关描述的0-day漏洞。</p>
<p><strong>Q&amp;A：</strong></p>
<ul>
<li><strong>Q1：embedded relationship和节点property有啥区别？</strong>property是节点属性，embedded relationship是带有二元关系的节点属性</li>
<li><strong>Q2：Observed Data和SCO有啥区别？</strong>Observed Data观察行为与观察对象的信息，而<strong>SCO是具体可观察实体的信息，二者是引用与被引用的关系</strong></li>
<li><strong>Q3：Intrusion Set、Identity和Threat Actor的区别？</strong>Intrusion Set是最高层的实体，其包括Identity和Threat Actor，如APT1（高层APT组织）为Intrusion Set，其包含一些个人（Ugly Gorilla）或团体（SuperHard）的Threat Actor，而Identity是用真实名称描述的个人或组织（如Ugly Gorilla指向Wang Dong）。由此看来，Threat Actor也可以用真是名称描述（Communist Party of China），但是明显指示了其表示威胁主体，而Identity本身不显示其角色信息。</li>
</ul>
<h3><span id="五-stix-relationship-objects"><strong>五、 STIX Relationship Objects</strong></span></h3><h4><span id="51-relationship"><strong>5.1 Relationship</strong></span></h4><p><strong>Type Name: relationship</strong></p>
<p>用于连接STIX中的SDO或SCO; STIX中的Relationship在每个SDO或SCO的定义中进行了描述, 用户还可以自定义关系。STIX中所有内置的Relationship详见文档Appendix B。注意, Relationship本身也是一个对象, 因此其也有自身的 Property 和 Relationships。</p>
<h4><span id="52-sighting"><strong>5.2 Sighting</strong></span></h4><p><strong>Type Name: sighting</strong></p>
<p><strong>原文定义</strong>:目击(sighting)表示认为在CTI中看到了某些东西（例如指示器、恶意软件、工具、威胁因素等）。目击用于跟踪目标是谁和什么，如何实施攻击，以及跟踪攻击行为的趋势。</p>
<blockquote>
<p>  A Sighting denotes the belief that something in CTI (e.g., an indicator, malware, tool, threat actor, etc.) was seen. Sightings are used to track who and what are being targeted, how attacks are carried out, and to track trends in attack behavior.</p>
</blockquote>
<p>Sighting 没有连接两个对象, 但却被定义为关系, 原因是:目击包括三部分的内容:• 发现的内容，如指示器、恶意软件、活动或其他SDO（sighting\u of\u ref）•发现者和/或发现地点，表示为身份（where\u sighted\u refs）•系统和网络上实际看到的内容，表示为观察数据（SECURED\u Data\u refs）</p>
<blockquote>
<p>  Sighting is captured as a relationship because you cannot have a sighting unless you have something that has been sighted. Sighting does not make sense without the relationship to what was sighted</p>
</blockquote>
<p>Sighting包括三部分的内容:</p>
<ul>
<li><strong>What</strong> was sighted, such as the <strong>Indicator, Malware, Campaign, or other SDO</strong> (<em>sighting_of_ref</em>)</li>
<li><strong>Who</strong> sighted it and/or where it was sighted, represented as an <strong>Identity</strong> (<em>where_sighted_refs</em>)</li>
<li><strong>What</strong> was actually seen on systems and networks, represented as <strong>Observed Data</strong> (<em>observed_data_refs</em>)</li>
</ul>
<p>Sighting和Observed Data的区别:</p>
<p><strong>目击与观察到的数据不同，因为目击是一种情报断言（“我看到了这个威胁参与者”）</strong>，而观察到的数据只是信息（“我看到了这个文件”）。当您通过包含来自目击的链接观测数据（Observed\u Data\u refs）来组合它们时，您可以说“我看到了这个文件，这让我觉得我看到了这个威胁参与者”。</p>
<blockquote>
<p>  Sighting is distinct from Observed Data in that Sighting is an <strong>intelligence assertion</strong> (“I saw this threat actor”) while Observed Data is simply information (“I saw this file”). When you combine them by including the linked Observed Data (observed_data_refs) from a Sighting, you can say “I saw this file, and that makes me think I saw this threat actor”.</p>
</blockquote>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（9）阿里云恶意软件检测平台</title>
    <url>/posts/2A5FQTF/</url>
    <content><![CDATA[<h2><span id="一-阿里云恶意文件检测平台">一、阿里云恶意文件检测平台</span></h2><p>Linux沙箱 ｜ 阿里云恶意文件检测平台开放Linux二进制文件检测：<a href="https://www.anquanke.com/post/id/276349#10006-weixin-1-52626-6b3bffd01fdde4900130bc5a2751b6d1">https://www.anquanke.com/post/id/276349#10006-weixin-1-52626-6b3bffd01fdde4900130bc5a2751b6d1</a></p>
<h3><span id="11-简介">1.1 简介</span></h3><p>在病毒检测方向，一直以来常见的两种手段就是<strong>静态特征检测</strong>与<strong>动态行为检测</strong>。两者各有优势与不足，<strong>静态特征检测方案实施成本更低，检出结果也更精准，但是其泛化能力不足</strong>，针对具有高级对抗能力的恶意文件显得力不从心，并且天然地处于被动的地位，人力运营成本会更高。</p>
<p>动态行为检测实施成本相对较高，需要有足够的资源，而且检出的结果在一定程度上不如静态检测精准，但是它最大的优势是可以从<strong>恶意行为、技术手段</strong>的角度识别恶意文件，具有极大的泛化能力。对于需要检测大量恶意文件的安全厂商来说，人工运营所有样本并提取静态特征是不现实的，而沙箱的作用也就显现出来了：在海量的文件中，识别出最值得关注的恶意文件。</p>
<h3><span id="12-沙箱优势">1.2 沙箱优势</span></h3><h4><span id="高性能的环境仿真">高性能的环境仿真</span></h4><p><strong>云沙箱依托于阿里云神龙架构，在具备高性能的仿真的同时，还支持资源池化和自动化运维的能力</strong>。利用自定义的虚拟化技术和定制的沙箱OS内核，对恶意样本使用的反虚拟化的技术具备天然的对抗能力，配合上专门打造的二进制检测探针，可以在安全、高效、仿真的隔离环境中对二进制进行深度的行为分析。</p>
<h4><span id="全面的动态行为分析">全面的动态行为分析</span></h4><p><strong>基于虚拟化构建的沙箱深度分析技术，对进程、文件、网络、敏感系统调用、rootkit、漏洞利用等进行全面监控</strong>，配合智能模型规则检测引擎，快速分析出样本潜在的恶意行为。</p>
<h4><span id="海量的数据积累">海量的数据积累</span></h4><p>阿里云沙箱服务于阿里云安全云上恶意文件检测，积累了海量样本数据，提炼出大量有独检优势的行为检测规则。</p>
<h3><span id="33-检测优势">3.3 检测优势</span></h3><h4><span id="算法模型覆盖未知威胁">算法模型——覆盖未知威胁</span></h4><p>基于阿里云平台海量样本数据和强劲计算能力，<strong>采用“机器智能(神经网络)”与“专家智能(行为标签、ATT&amp;CK)”结合的智能安全思想</strong>，挖掘海量样本数据中可疑内容信息和行为标签威胁值，构建智能的威胁检测模型发现新威胁。</p>
<p>将专家知识与海量数据结合智能化构建以<strong>ATT&amp;CK为核心的多模态特征表示</strong>，对样本行为从技术战术度量、敏感信息表征、意图逻辑推理等角度进行多维度刻画、分析，同时依赖机器智能的学习泛化能力、检测模型能覆盖更多的未知，拓展威胁发现边界。</p>
<p><img src="https://p2.ssl.qhimg.com/t019938cd9a1afcd563.png" alt="img"></p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（10）图机器学习在蚂蚁推荐业务中的应用</title>
    <url>/posts/25Z7M1R/</url>
    <content><![CDATA[<h2><span id="图机器学习在蚂蚁推荐业务中的应用">图机器学习在蚂蚁推荐业务中的应用</span></h2><blockquote>
<p>  <a href="https://mp.weixin.qq.com/s/BgNQdW3RvLw6rS1Wy-emzA">https://mp.weixin.qq.com/s/BgNQdW3RvLw6rS1Wy-emzA</a></p>
</blockquote>
<h3><span id="一-相关背景">一、相关背景</span></h3><p><strong>导读：</strong>本文将介绍图机器学习在蚂蚁推荐系统中的应用。<strong>在蚂蚁的实际业务中，有大量的额外信息，比如知识图谱、其他业务的用户行为等，这些信息通常对推荐业务很有帮助，我们利用图算法连接这些信息和推荐系统，来增强用户兴趣的表达。</strong></p>
<p>==全文主要围绕以下几方面内容展开：==</p>
<ul>
<li>背景</li>
<li>基于图谱的推荐</li>
<li>基于社交和文本的推荐</li>
<li>基于跨域的推荐</li>
</ul>
<p><img src="640.png" alt="图片" style="zoom:50%;"></p>
<p>支付宝除了最主要的支付功能外还有大量的推荐场景，包括腰封推荐、基金推荐和消费券推荐等等。支付宝域内的推荐相比于其他推荐最大的区别是用户的行为稀疏，活跃度较低，很多用户打开支付宝只是为了支付，不会关注其他东西。所以<strong><em>推荐网络中UI边的记录是非常少的，我们的关注点也是低活目标的推荐。</em></strong>比如为了提升DAU，可能只会给低活用户在腰封投放内容，正常用户是看不到的；基金推荐板块我们更关注的是那些没有理财或理财持仓金额较低的用户，引导他们买一些基金进行交易；消费券的推荐也是为了促进低活用户的线下消费。</p>
<p><strong>低活用户历史行为序列信息很少，一些直接根据UI历史行为序列来推荐的方法可能不太适用于我们的场景</strong>。因此我们引入了下面三个场景信息来增强支付宝域内的UI关系信息：</p>
<ul>
<li><strong>社交网络的UU关系</strong></li>
<li><strong>II图谱关系</strong></li>
<li><strong>其他场景的UI关系</strong></li>
</ul>
<p>通过社交网络的UU关系可以获取低活用户好友的点击偏好，根据同质性就可以推断出该用户的点击偏好，物品与物品之间的图谱关系可以发现、扩展用户对相似物品的喜好信息，最后跨域场景下的用户行为对当前场景的推荐任务也有很大帮助。</p>
<h3><span id="二-基于图谱的推荐">二、<strong>基于图谱的推荐</strong></span></h3><p><strong>很多推荐场景中用户的行为是稀疏的，尤其是在对新用户进行刻画时，可利用的行为信息很少，所以通常要引入很多辅助信息</strong>，比如attribute、contexts、images等等，我们这里引入的是knowledge graph—知识图谱。</p>
<p>知识图谱是一个大而全的历史专家知识，有助于我们的算法推荐，但是还存在两个问题：</p>
<ul>
<li><p><strong>一是图谱本身可能并不是为了这个业务而设计的，所以里面包含很多无用信息，训练过程也非常耗时。</strong>一个常用的解决办法是只保留图谱中能关联上我们商品的边，把其他边都删掉，但这又可能会造成一些信息损失，因为其他边也是有用的。</p>
</li>
<li><p><strong>二是图谱用做辅助信息时，没办法将用户的偏好聚合到图谱内部的边上</strong>。==如上图所示，用户1喜欢电影1和电影2的原因可能是因为它们有同一个主演，而用户2喜欢电影2和电影3的原因是它们的类型相同==。如果只用普通的图模型的UI、II关系来建模，只能得到用户和电影的相关性，而没办法将用户的这些潜在意图聚合到图谱中。</p>
</li>
</ul>
<p>所以我们后面主要解决图谱蒸馏和图谱精炼这两个问题。</p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（12）2023 BlackHat：利用基于流的离群值检测和 SliceLine 来阻止分布广泛的机器人攻击</title>
    <url>/posts/1WG0TJH/</url>
    <content><![CDATA[<h2><span id="leveraging-streaming-based-outlier-detection-and-sliceline-to-stop-heavily-distributed-bot-attacks">Leveraging Streaming-Based Outlier Detection and SliceLine to Stop Heavily Distributed Bot Attacks</span></h2><blockquote>
<ul>
<li>利用基于流的离群值检测和 SliceLine 来阻止分布广泛的机器人攻击:</li>
<li>DataDome:<a href="https://datadome.co/">https://datadome.co/</a></li>
</ul>
</blockquote>
<p>在此演示文稿中，我们将讨论如何利用基于流的异常值检测和 SliceLine 来快速安全地生成大量可用于阻止恶意流量的规则/签名。</p>
<p>虽然 ML 的使用变得越来越广泛，但规则仍然相关。事实上，公司在能够快速评估大量规则的高效规则引擎方面投入了大量资金。此外，规则通常更便于创建、操作和解释，这使得它们除了 ML 方法之外仍然有价值。</p>
<p>我们证明，虽然 SliceLine 最初设计用于识别 ML 模型表现不佳的数据子集，但它的使用可以适应以无监督方式生成大量与攻击相关的规则，即不使用标记数据。此外，我们利用机器人检测问题来说明如何使用 SliceLine 即时生成大量恶意签名。</p>
<p>我们还将展示我们优化的 SliceLine Python 开源实现，并展示如何将其用于特定但困难的机器人检测子集：分布式凭证填充攻击，攻击者利用数千个受感染的 IP 地址进行攻击和绕过传统的安全机制，例如速率限制策略。</p>
<p>通过一个真实世界的例子，我们将首先解释如何使用基于流的检测来检测此类攻击，以及我们如何使用数据建模在服务器端信号（HTTP 标头、TLS 指纹、IP 地址等）上应用 <strong>SliceLine</strong> 来识别并生成与分布式攻击相关的阻止签名。这种方法使我们能够在去年阻止 59 位客户超过 2.85 亿次恶意登录尝试。</p>
<p>最后，我们将解释这种方法如何推广到除机器人检测之外的其他安全用例，以及如何在不同的规则引擎中使用它。</p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（14）阿里妈妈-流量反作弊算法实践</title>
    <url>/posts/3RSRTFF/</url>
    <content><![CDATA[<h2><span id="阿里妈妈-流量反作弊算法实践">阿里妈妈-流量反作弊算法实践</span></h2><blockquote>
<p>  不光要思考需要哪些技术能推动产品成熟，还要思考一个产品成熟的样本；</p>
</blockquote>
<h3><span id="一-广告风控流程"><strong>一、广告风控流程</strong></span></h3><p>下图是广告主投放内容与风控团队、下游业务团队的简易交互流程。广告素材通过<strong>内容风控</strong>审核后，即可以在线上进行<strong>展示</strong>。在展示期间，广告主可能会主动作弊、也可能受到其他广告主攻击。风控团队需要对无效流量进行过滤，保护广告主的利益，维护健康的广告投放环境。本文重点介绍在线展示期间，<strong>流量、淘客交易</strong>场景下的业务风险与算法体系。详细的解决方案在未来的文章中逐一介绍。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019363.png" alt="图片"></p>
<blockquote>
<p>  <strong><font color="red"> 离线无监督样本库：标签指导（误报率）对准实时进行修正；</font></strong></p>
</blockquote>
<h3><span id="二-无效流量"><strong>二、无效流量</strong></span></h3><p><strong>流量反作弊系统</strong>的核心能力就是<strong>清洗、过滤无效流量</strong>。但是<strong>无效流量</strong>并不等价于<strong>作弊流量</strong>。我们将这部分流量的定义分为2个层面：</p>
<p>1）<strong>低质量</strong>：重复点击计费策略、频率控制策略、剧烈波动策略等；</p>
<p>2）<strong>作弊</strong>：转化效果概率为0的流量；</p>
<p>作弊流量转化期望概率一定为0，比如<strong>爬虫</strong>产生的点击流量（<strong>重复traceid</strong>）。但后续实际<strong>频率</strong>为0的流量不一定是作弊。比如新商品累计1万点击后仍没有转化，只能说频率为0。不能直接断定为作弊流量。</p>
<p>常见的无效流量包括：</p>
<p>1）消耗竞争对手；</p>
<p>2）提升自身排名；</p>
<p>3）自然宝贝刷单误伤广告主；</p>
<p>4）非恶意无效流量。如下图所示，一名诚信投放的广告主，可能受到多种维度的影响。</p>
<blockquote>
<p>  <strong><font color="red"> 网约车业务的冒转率可能受到哪些影响？</font></strong></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019652.png" alt="图片"></p>
<h4><span id="21-消耗竞争对手">2.1 消耗竞争对手</span></h4><p>广告主在设置投放策略的时候，通常有预算限制。一些广告主，通过构造虚假流量，攻击其他广告主，消耗预算致使广告下架。如原定计划可以投放7日的广告内容，在第2天突然被完全消耗。这种情况下，很容易引起受害广告主的投诉，影响恶劣。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019707.png" alt="图片"></p>
<h4><span id="22-提升自身排名">2.2 提升自身排名</span></h4><p>广告排名由出价和质量评分决定。<strong>一些广告主会雇佣黑产刷单，提高广告的转化率</strong>。通过低成本获得靠前的广告排名。这些作弊利益驱动属性也很强，比较容易被平台和相关广告主感知到。对平台的影响也较为恶劣。</p>
<h4><span id="23-自然宝贝刷单"><strong>2.3 自然宝贝刷单</strong></span></h4><p><strong>一些广告主通过雇佣黑产提高店铺的成交数、好评数、加购收藏数等。刷手为了更好地隐藏自己，往往会装作“货比三家”，查看多个宝贝信息</strong>。该过程偶尔会误伤了广告展示宝贝。这种作弊对广告生态的影响比较弱。感知程度会偏低一些。此外，人工刷手往往伪装的更好，在流量甄别上难度比较大。</p>
<h4><span id="24-非恶意无效流量">2.4 非恶意无效流量</span></h4><p>除上述带有恶意的虚假流量。还有非恶意、非薅羊毛的<strong>无效流量</strong>需要被过滤。<strong>比如一些浏览器在打开淘宝首页时，会预加载所有的宝贝链接后续跳转网页。显然这些是无效流量</strong>。又比如，爬虫或浏览器劫持而产生的流量，不应该计入广告主的费用中。</p>
<h4><span id="25-淘客交易作弊">2.5 淘客交易作弊</span></h4><p>淘宝联盟是阿里妈妈平台给淘宝客推广者搭建的推广平台，在淘宝联盟后台可以完成取链、推广和提现等一系列操作。而淘客交易作弊，不满足作弊流量转化概率为0的假设。根据计费方式不同，常见的2种作弊形式为：1）流量劫持；2）黑灰产淘客拉新。</p>
<h5><span id="251-流量劫持"><strong>2.5.1 流量劫持</strong></span></h5><p>CPS计费下的主要作弊手法是流量劫持。常见的流量劫持有2种。第一种，是<strong>篡改记录用户流量来源</strong>，将其他淘宝客的拉新流量据为己有。广告主会明显感知到自然流量变少，拉新流量增加。第二种，是<strong>修改用户跳转链接</strong>，使得用户跳转到自己的宝贝页面。会导致用户在不知情的情况下购买了另一家店铺的商品。此时商家会在销量层面有一定感知。</p>
<h5><span id="252-黑灰产淘客拉新">2.5.2 黑灰产淘客拉新</span></h5><p>CPA计费下的主要问题是<strong>虚假地址</strong>。常见的CPA通常发生在产品拉新中，如用户注册、用户下单…等。在一些淘宝客拉新场景中，需要拉新用户完成注册、下单等一系列流程。此时一些淘宝客通过批量注册，下单廉价商品来赚取拉新差价。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019823.png" alt="图片" style="zoom: 67%;"></p>
<h4><span id="26-下游任务影响">2.6 下游任务影响</span></h4><p>对于阿里妈妈来说，虚假流量不单影响着其他广告主的权益，同时影响着阿里生态的下游业务。<strong>搜索、推荐、广告等业务的收益，强依赖于其基于用户行为数据的在线学习</strong>。如：个性化推荐、点击率预估、流量分发、广告定价等。而当<strong>这些任务中混入虚假流量时，会对其真实线上的精度造成极大影响</strong>。</p>
<blockquote>
<p>  <strong><font color="red"> 虚假的无效流量对下游任务的影响；</font></strong></p>
</blockquote>
<h3><span id="三-算法实践"><strong>三、算法实践</strong></span></h3><p>相比于其他正向业务，流量反作弊对于精度的要求尤其高。<strong><font color="red"> 多过滤导致平台收益减少、少过滤引起广告主投诉，破坏投放生态。</font></strong>而且业务场景对实时返款的诉求越来越强烈，同时作弊对抗升级，从集中式、大规模转向分布式、稀疏化攻击，识别难度增大。亟需基于高维异常检查的新系统能力。为此，我们建立了集<strong>异常主动感知、人工洞察分析、自动处置过滤、客观评价</strong>高效循环一体的风控系统。</p>
<blockquote>
<p>  <strong><font color="red"> 1、被动反馈case驱动、领域经验处置 —&gt; 主动感知、洞察分析、标准化的处置体系、成熟的指标大盘</font></strong></p>
<p>  <strong><font color="red"> 2、领域经验驱动算法迭代 —&gt; 数据驱动算法迭代</font></strong></p>
</blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019092.png" alt="图片" style="zoom: 67%;"></p>
<h4><span id="31-感知">3.1 感知</span></h4><p><strong><font color="red"> 在历史的风控体系中，往往是Case驱动的。即遇到问题通过滞后的算法或策略迭代来覆盖风险</font></strong>。为了提前发现问题，尽可能减少投诉，净化投放环境，我们引入了感知。通过感知捕捉与常见分布不同的数据，输出异常列表。</p>
<p>我们将可感知异常流量分为:</p>
<ul>
<li><strong>受害者可感知</strong>：业务本身可感知，冒转率，高峰期流量、营销流量不属于；</li>
<li><strong>平台可感知</strong>：接口安全（接入层、风控）、业务安全；</li>
<li><strong>实战攻防可感知</strong>：</li>
<li><strong>假想攻防可感知</strong>：</li>
<li><strong>算法挖掘可感知</strong>：</li>
</ul>
<p><strong><font color="red"> 感知是重召回的，但并不是单纯为了更多地召回现有风险，它设计的核心是去感知所有的“异常”</font></strong>。以2020年初为例，由于骑行政策的调整，售卖头盔商家的访问量显著偏高，连带着必然影响到<strong>点击率、转化率</strong>等一系列指标。这些异常是<strong>商铺可感知</strong>的，需要被我们捕捉到。但并不属于作弊流量。所以不会被流量反作弊系统所过滤。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019109.png" alt="图片" style="zoom:67%;"></p>
<p>那感知究竟如何来做呢？以“点击流量反作弊”来说，<strong>作弊一定会导致点击量增加</strong>。如果可以预估出一个商品每天的点击数量。则超出该值的点击一定为作弊。因此流量反作弊感知的核心之一，就是<strong>如何在大盘召回率未知的情况下，精准预估正常流量值</strong>。这部分内容在后续文章中进行介绍。</p>
<blockquote>
<p>  接口画像：预估出一个接口每天的点击量，<strong><font color="red"> 因此流量反作弊感知的核心之一，就是如何在大盘召回率未知的情况下，精准预估正常流量值</font></strong>。</p>
</blockquote>
<h4><span id="32-洞察">3.2 洞察</span></h4><p><strong>为了确认感知到的异常流量哪些属于作弊，分析人员需要进行洞察分析</strong>。<strong><font color="red"> “洞察 ”的目的是从“感知”到的异常中将风险抽离出来，进而发现新的风险模式。</font></strong>我们将洞察分为：1）受害者洞察；2）攻击者洞察；3）套利漏洞洞察；4）流量实例洞察。</p>
<p><strong><font color="red"> 传统洞察需要人工挑选可疑特征（如停留时长、注册时长、ua、token_str），并与大盘好样本(正常业务曲线)进行比较。</font></strong>如下图。这就对领域经验有强依赖。而领域专家毕竟是少数。<strong>并且随着作弊越发高级，单一维度或少量维度下逐渐难以发现作弊。</strong>为此，我们引入了<strong>高维数据下的可视化洞察分析技术</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019492.png" alt="图片" style="zoom: 50%;"></p>
<p><strong><font color="red"> 在洞察环节，首先需要对样本进行高度抽象表示。如何在高维数据中选择合适的子空间投影，是非常具有挑战性的课题。</font></strong>后续文章会展开介绍。确定合适的子空间后，除了和大盘比较，我们还引入了<strong>时间维度的分布同比</strong>，如下图所示。对于分布稳定的某个广告，3月6日降维图中突然出现明显不同的一簇（红圈内），很可能是新的异常模式。（图中“样本库”指最终被识别为作弊的流量，在3.3节进行介绍）</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019579.png" alt="图片"></p>
<p><strong>洞察的难点在于，如何减轻未召回的作弊对正常分布的污染。</strong>比如上图中蓝色线条内部分可能也存在作弊，这时通过同比就无法发现异常。如何跳出既有认知去召回未知异常模式，以及非常棘手的冷启动问题，这些都是后续文章的重点内容。</p>
<h4><span id="33-处置">3.3 处置</span></h4><p><strong><font color="red"> 处置，指对风险进行处置。对于不同的风险实体、风险类型，会使用不同的处置方法。</font></strong></p>
<h5><span id="331-流量反作弊的处置">3.3.1 流量反作弊的处置</span></h5><blockquote>
<p>  <strong><font color="red"> 所有的策略和模型都可以用标准的统计学习方法来描述；</font></strong></p>
</blockquote>
<p><strong><font color="red"> 传统的算法迭代模式，是根据洞察分析的结果，指导规则、统计模型为主的无监督过滤系统。</font></strong>对领域经验比较依赖，而且效率低下、难以形成沉淀。因此，对于流量反作弊的处置，我们部署了<strong>实时流式、小时批处理双重防线</strong>。其逻辑如下图所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019002.png" alt="图片" style="zoom: 67%;"></p>
<blockquote>
<p>  <strong><font color="red"> 实时有监督过滤系统，依赖数据维度的多样性（场景要区分端、app、小程序和web的数据维度不同）+ 低代价特征 + 高效的模型结构；</font></strong></p>
</blockquote>
<p>在线实时过滤系统，综合了<strong>无监督、半监督的特征工程，以及监督的集成（Ensemble）异常检测器</strong>。相比于单条策略的独立决策，集成的容错性更高（召回能力下降，适用于精度高的场景）。例如，<strong>PC端反作弊策略依赖于网页采集的前端行为、鼠标点击行为等，当数据采集出错时，过渡依赖某一策略将导致大面积误差。</strong></p>
<p>同时，<strong><font color="red"> 我们会尽可能使用更触及作弊本质、更具有鲁棒性的特征</font></strong>。和正向业务不同，我们不会在特征设计层面，过分聚焦于正样本的区分度。比如绝大部分爬虫流量都是PC端带来的，“是否是PC”就是一个极强的特征。但一旦这种作弊没有继续攻击，模型的效果就大打折扣。因此更多会使用各个维度上计算与Normal分布的偏差、到Normal簇的距离…等。</p>
<p>实时过滤系统基本可以解决90%的问题。<strong>为了更好地拟合高级作弊，我们又引入了小时级别过滤系统，使用开销更大的特征与更复杂的模型。而且广告结算支持事后返款，可以使用小时级结果对实时流模型进行修正，用于结算与展示</strong>。当然，处置能力最终收敛于实时流过滤系统，会是我们更长期的追求。</p>
<blockquote>
<p>  小时级的离线策略的意义：能否对准实时修正，</p>
</blockquote>
<p>此外，在线实时过滤系统可以让新同学快速上手迭代其中的组件，将不同同学的产出解耦，更客观的评价业务贡献。</p>
<h5><span id="332-淘客交易反作弊的处置">3.3.2 淘客交易反作弊的处置</span></h5><p>对于过滤系统判定作弊的淘客，我们首先<strong>冻结其佣金</strong>。搜集证据后下达处罚结果。并通过“预估佣金”、“异常特征”来对待处罚淘客进行分级处置。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019490.png" alt="图片" style="zoom:50%;"></p>
<p>此外，传统的处罚机制为月结，从媒体开始作弊到下达处罚有一定延迟。一方面不利于及时管控风险，另一方面会导致非主观恶意作弊淘客的强烈反弹，为提升管控的时效，减少淘客的损失，同时提升用户体验，我们在原有的月结机制基础上，<strong>增加周/天的处罚机制</strong>。</p>
<h4><span id="34-评价">3.4 评价</span></h4><blockquote>
<p>  拦截率和攻击量：相当于对已知攻击的覆盖能力；</p>
<p>  准确率和召回率：相当于对全量攻击的防御能力；</p>
</blockquote>
<p>对于整个流量反作弊系统，我们有4部分需要评价：</p>
<ul>
<li>在线有监督精度；</li>
<li>在线有监督召回；</li>
<li>离线无监督精度；</li>
<li>离线无监督召回；</li>
</ul>
<p>因为没有Ground Truth，为了客观评价在线有监督过滤系统的<strong>精度</strong>与<strong>召回</strong>，我们建立了<strong>离线无监督样本库</strong>。使用离线无监督样本库的最终结果，作为在线有监督系统的Groud truth，就可以评估其分类效果。但也引入了后面2部分无监督评价问题。</p>
<h5><span id="341-有监督过滤系统的评价">3.4.1 有监督过滤系统的评价</span></h5><p>在线与离线2者的关系如下图所示。基于纯无监督的挖掘体系，我们的底线是消灭3.2.1中提到的5种可感知异常流量中的作弊流量，终局则是消灭不可感知的作弊流量。<strong><font color="red"> 通过天级别的事后信息引入，以无监督的方式对线上实时系统过滤结果进行修正，并将标签用于后续在线监督系统学习。</font></strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019764.png" alt="图片" style="zoom:67%;"></p>
<p><strong><font color="red"> 基于现有标签的AUC、KS、MAX-F1…等指标，会过分高估风控模型效果。</font></strong>例如，实时模型的AUC很容易高于0.99。然而这其中绝大多数的样本都来自于简单的爬虫、或傻瓜式疯狂点击，如下图离散分布的红点。在更高级的作弊上AUC可能不足0.8，如下图红圈中的样本。为了更客观地评价模型，我们引入了“<strong>样本库分级</strong>”，将“简单作弊”与“高级作弊”区分开。并通过结构化采样构造封闭评测集，指导模型迭代。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182019507.jpeg" alt="图片" style="zoom: 50%;"></p>
<h5><span id="342-无监督精准评价">3.4.2 无监督精准评价</span></h5><p>无监督系统的精准与召回评价一直是业内的难题。传统的评价方法是通过数据抽样，由专家进行标注进行评估。效率低下且非常主观。为此我们借助淘系生态数据，为无监督系统引入了自动化评价体系。基于无效流量转化概率为0的假设，通过统计推断，得到模型在指定置信度下的精度指数下限。基于区间估计的精度推断方法，在后续专题文章中进行介绍。</p>
<h5><span id="343-无监督召回评价">3.4.3 无监督召回评价</span></h5><p>真实环境下的召回评价，是难以定量的。除了大盘抽样巡检外，由于引入了完备的感知、洞察体系。将所有的异常流量，均归纳至一个风险池。无论何时有需要对流量进行处置（临时止血或迭代模型），我们都可以迅速定位到问题根源。于是将安全感最大化。</p>
<h3><span id="四-总-结">四、总 结</span></h3><p>高维数据下的<strong>异常检测、大规模图学习、机器学习可解释性、数据可视化方法</strong>等，都是我们的重点研究方向。在我们看来，风控可能是当前机器学习领域，对算法鲁棒性和解释性要求最高、精度要求最极致、系统规模和时效性挑战最大、最能用钱衡量的工业级业务。这就需要我们具备卓越的业务数据洞察能力、工程架构能力，让研究成果转换成坚实的工业级解决方案。</p>
<p>本文重点介绍了我们在流量反作弊场景下所遇到的问题，以及相应的解决方案。希望通过这篇文章，可以让读者理解我们在流量反作弊领域所遇到的问题，以及解决问题的思路。</p>
<h3><span id="当前思路总结">当前思路总结</span></h3><ul>
<li><p><strong><font color="red"> 离线无监督样本库：标签指导（误报率）对准实时进行修正；</font></strong></p>
</li>
<li><p><strong><font color="red"> 网约车业务的冒转率可能受到哪些影响？</font></strong></p>
</li>
<li><p><strong><font color="red"> 虚假的无效流量对下游任务的影响；</font></strong></p>
</li>
<li><p><strong><font color="red"> 1、被动反馈case驱动、领域经验处置 —&gt; 主动感知、洞察分析、标准化的处置体系、成熟的指标大盘</font></strong></p>
<p><strong><font color="red"> 2、领域经验驱动算法迭代 —&gt; 数据驱动算法迭代</font></strong></p>
</li>
<li><p>接口画像：预估出一个接口每天的点击量，<strong><font color="red"> 因此流量反作弊感知的核心之一，就是如何在大盘召回率未知的情况下，精准预估正常流量值</font></strong>。</p>
</li>
<li><p><strong><font color="red"> 所有的策略和模型都可以用标准的统计学习方法来描述；</font></strong></p>
</li>
<li><p><strong><font color="red"> 实时有监督过滤系统，依赖数据维度的多样性（场景要区分端、app、小程序和web的数据维度不同）+ 低代价特征 + 高效的模型结构；</font></strong></p>
</li>
</ul>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（15）阿里妈妈-“广告主套利”风控技术分享</title>
    <url>/posts/3W8D2M2/</url>
    <content><![CDATA[<h2><span id="阿里妈妈-广告主套利风控技术分享">阿里妈妈-“广告主套利”风控技术分享</span></h2><h3><span id="一-北京介绍">一、北京介绍</span></h3><h4><span id="11-套利的影响">1.1 套利的影响</span></h4><p>由于平台的有效总曝光有限，当套利广告主占据了高质量位置后，真实点击率和成交率低于模型预期，平台产生的总点击、总成交就会相应减少，从而导致该资源位的收入降低。这里我们统一使用<strong>千次展现收益</strong>（Revenue Per Mile，以下简称：RPM）来代表地价。即<strong>套利会在一定程度上使对应位置的RPM降低</strong>。</p>
<p>由于现阶段ctr、cvr预估模型有在线更新机制，从长周期来看是具备自愈能力的。但模型的更新有一段时间延迟，在每个模型更新的空窗期内，广告主不会恰好都补单补量至模型的预期水平。最终就导致了模型不断被欺骗又修复的过程。如下图所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182020935.png" alt="图片"></p>
<p>因此，在与排序模型的博弈中，广告主周期性地实现着套利。随着online模型更新的时效性提升，套利空间在不断被压缩。也导致广告主更加倾向于高频、低程度地进行操作，识别难度进一步增大。</p>
<h4><span id="12-困难与挑战">1.2 困难与挑战</span></h4><p>由于<strong>作弊手法千变万化</strong>、<strong>真实标签难以界定</strong>、<strong>作弊Ground-Truth未知</strong>，风控场景很难通过<strong>监督训练</strong>等手段获得<strong>通用解</strong>。具体到广告主套利上，我们还面临着一些其他的问题。</p>
<h5><span id="121-众包人工流量的识别">1.2.1 众包人工流量的识别</span></h5><p><strong>相比于以往的无效流量甄别，众包人工流量往往更加贴近平常用户的行为</strong>。难度远超以往的爬虫、机械性攻击。高并发的广告场景，对识别的精度和召回，要求都非常高。而且即使是刷手，也会产出正常的流量。</p>
<p>如何精细化区分刷手的每次行为以及是否是受雇佣的，是极具挑战的一个课题。</p>
<h5><span id="122-精度难评价">1.2.2 精度难评价</span></h5><p><strong>由于众包人工流量会有一定比例的成交，不符合0转化假设。</strong>高效评价流量识别模型的精度和召回，是很困难的。此外，套利广告主检测本身也需要找到合适的假设，没有客观高效的评价方式，难以指导模型迭代。</p>
<h5><span id="123-区分主动与被动">1.2.3 区分主动与被动</span></h5><p>存在任务流量的广告主，不仅是主动套利的，还有一部分是被其他广告主雇佣的刷手误伤、或者受到人工攻击消耗的。如何无监督、高精度、高召回、鲁棒地挖掘广告主的主动性，也是我们需要重点关注的。</p>
<h3><span id="二-方案概览">二、方案概览</span></h3><p>在正式开始介绍方案之前，我们针对1.3节的问题，分别介绍一下思路。为了解决1.3中提到的3个问题，我们针对广告主套利开发了一套同样集<strong>感知</strong>、<strong>洞察</strong>、<strong>处置、评价</strong>于一体的检测框架，其架构图如下图所示。框架理念可以参考这篇文章：<a href="http://mp.weixin.qq.com/s?__biz=Mzg3MDYxODE2Ng==&amp;mid=2247484728&amp;idx=1&amp;sn=3305a981f34fe30e2464c627f3842ec8&amp;chksm=ce8a4061f9fdc977cb18acf91d212352a26d9f060dbae6ead52514102e95b9e31cd32523e564&amp;scene=21#wechat_redirect">《阿里妈妈流量反作弊算法实践》</a>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182020589.png" alt="图片"></p>
<ol>
<li><strong>众包流量识别</strong>，分别由统计基线、行为序列、图关系3个模型一起召回，并使用黑话模型的产出评价标准，指导模型迭代；</li>
<li><strong>感知</strong>部分，通过对<strong>RPM的鲁棒预估</strong>，计算广告主实际产生的RPM与平台预期的diff，从而召回RPM偏低的广告主；</li>
<li>通过<strong>洞察</strong>分析平台对列表中的实例进行分析，获取新模式认知的同时进行标注作为验证样本；</li>
<li>将认知抽象为策略或模型（当前为双模型因果推断），产出了套利广告主名单用于区分“主动”与“被动”，最后在下游中进行分类<strong>处置</strong>；</li>
</ol>
<h3><span id="三-众包流量识别">三、众包流量识别</span></h3><p>在介绍感知、洞察、处置体系之前，我们首先对挖掘套利广告主的基础能力进行介绍——众包人工流量识别。该流量不满足0成交转化，模型的迭代和监控保障，也显然不能依赖低效的人工抽检。首先需要寻找一种可以批量校验、又和处置严格正交的评价方法。整体方案如下图所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182020065.png" alt="图片"></p>
<h4><span id="31-黑话模型">3.1 黑话模型</span></h4><p>考虑到直通车场景下，刷手需要高频地进行搜索，从而定位到自己的任务目标，不可能所有的内容都手敲。风控工程团队基于淘宝的搜索记录，对历史文本信息进行了系统地整合，使得黑话凝聚在标准化的文本库中。</p>
<h5><span id="311-特性">3.1.1 特性</span></h5><p>黑搜索的文本信息采集，受<strong>设备型号、手滑粘贴、误点搜索的影响</strong>，导致产出上并不稳定，所以没有直接用于召回。但同设备类型、应用、天维度同比是有意义的，可以<strong>作为精度和召回的评价指标</strong>。因此，我们构造了和众包流量构成强相关、但召回有限的黑话模型。</p>
<p><strong>典型的黑话如下所示：</strong></p>
<ul>
<li>“3️⃣看图👆拍苐一个,π下多少仮溃”</li>
</ul>
<p>根据我们的分析显示，黑话具有以下特点：1）文字语序混乱；2）拼音、中、英文混输；3）表情、形近字替代；4）快速迭代、分析低效。这涉及到多种语义的还原，并且新的变种不断产生。基于文本出发的黑话模型开发非常困难。</p>
<h5><span id="312-用户维度-关键词与树">3.1.2 用户维度、关键词与树</span></h5><p>综合考虑各个方案的性价比后，我们尝试不完全依赖文本，而是从用户出发，<strong>2段式、半监督地召回黑话。</strong></p>
<ol>
<li>首先构造用户维度的处罚特征，无监督的召回候选人群集；</li>
<li>再以变换后的关键词为目标，天维度更新Cart回归树模型，选择额外80%召回的动态阈值，得到离线样本库；</li>
</ol>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182020453.png" alt="图片"></p>
<h5><span id="313-bert提效">3.1.3 Bert提效</span></h5><p>我们基于黑话样本库与大盘可信白样本，训练了基于Bert的2分类模型，用于对全量文本进行打分，从而减轻黑话样本库覆盖度不足的影响。使用召回人群多日黑话得分均值，来作为众包流量识别的精度指数，指导模型快速迭代，并持续以黑话精度作为模型的监控指标。</p>
<h4><span id="32-统计基线">3.2 统计基线</span></h4><p>众包人工流量虽然更难以识别，但是我们始终相信黑灰产用户是不可能完美隐藏的。<strong>我们从用户行为角度设计了一系列的异常检测方案。</strong>首先调研了黑灰产平台。发现用户需要通过一系列行为来完成任务的交付（考虑到攻防属性，暂不透出具体行为）。据此我们构造了本质、鲁棒性的统计特征，结合统计异常检测模型，产出了套利识别的基线。在<a href="http://mp.weixin.qq.com/s?__biz=Mzg3MDYxODE2Ng==&amp;mid=2247484728&amp;idx=1&amp;sn=3305a981f34fe30e2464c627f3842ec8&amp;chksm=ce8a4061f9fdc977cb18acf91d212352a26d9f060dbae6ead52514102e95b9e31cd32523e564&amp;scene=21#wechat_redirect">《阿里妈妈流量反作弊算法实践》</a>中介绍过，本质特征更有助于挖掘异常；如用“偏离对应人群的分布的程度”，代替“绝对数量”。</p>
<p>由于统计特征的粒度偏粗，导致我们并不能区分刷手当天的哪部分流量属于众包，哪部分属于正常流量。所以还需要更加精细化的识别能力建设。</p>
<h4><span id="33-行为序列">3.3 行为序列</span></h4><p>近期我们一直在尝试，使用更为<strong>细粒度的原始行为序列</strong>、<strong>良好的序列设计</strong>、以及<strong>合理的辅助任务</strong>，训练大规模无监督预训练模型。<strong>Zero-shot</strong>在下游进行应用，实现精细化地捕捉众包流量特点。当前行为序列研究的整体框架如下图所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182020951.png" alt="图片"></p>
<h5><span id="331-序列信息">3.3.1 序列信息</span></h5><p><strong><font color="red"> 首先是通用的行为选择。为了更好地适用于大多数场景，我们只选择了搜索和点击两个行为，并辅以行为的属性与时间信息，共同组成行为序列。</font></strong></p>
<p>为了方便刻画行为模式，我们使用下划线拼接行为和属性，构造类似于NLP中单词概念的行为元素，形式为：()<em>()</em>()_()。例如：<strong>clk_true_3_2 表示为 一次点击 &amp; 是广告点击 &amp; 商品属性为编号3 &amp; 距离上一次行为时间间隔属于分箱编号2。</strong></p>
<p>由于刷手需要接单大量的任务，产品类型、任务目的可能各不相同。通过合理的session划分，将不同的基础行为进行聚合，可以使信息更加凝聚。</p>
<p>结合搜索广告的特点，我们限制一个session必须以搜索开始，时间间隔在一定范围内都同属于一个session。</p>
<h5><span id="332-辅助任务设计">3.3.2 辅助任务设计</span></h5><p>为了使模型学习到session排列的模式，我们使用<strong>Transformer结构 + Cut-paste任务</strong>，对序列整体进行表示。</p>
<p><strong><font color="red"> Cut-paste算法是用于图像异常检测的一种辅助任务。通过对原始图片进行随机地局部剪裁，再随机粘贴至新的位置，原位置以黑色阴影覆盖(向量值均为0)，从而获得大量的负样本，并以大量的数据增强产生正样本。</font></strong>然后通过原图与新图的二分类任务，促使模型学习到剪裁边缘的连续性。算法内容详见[1]。</p>
<p>类似地，根据刷手行为特点，对序列进行Cut-paste衍生，得到大量的样本用于二分类任务，从而得到完整序列表示。最终的预训练网络结构如下所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182020120.png" alt="图片" style="zoom:67%;"></p>
<h5><span id="333-异常检测">3.3.3 异常检测</span></h5><p>根据序列表示的结果，我们得到了N维的Embedding向量：</p>
<ol>
<li>将序列Embedding与COPOD模型结合，产出Embedding每个维度偏离分布的异常分(1)；</li>
<li>固定基础行为的Embedding层权重，二次训练Seq2Seq模型，将重构误差产出异常分(2)；</li>
</ol>
<p>将2个方案计算的异常分结合，得到下游任务的打分。</p>
<h4><span id="34-图关联挖掘">3.4 图关联挖掘</span></h4><p><strong>截止目前，我们通过统计基线 + 行为序列对众包人工流量进行了识别</strong>。在阿里妈妈广告场景下，这些流量会在受害者/套利者维度上呈现出聚集。且因为模型的评估依赖于人工抽检，为了保证精度符合预期，显然单体识别模型的精度与召回都会受限。</p>
<p><strong>因此我们增加了基于图关联的召回。以统计基线为基础节点，用行为序列结果剪枝，最终通过半监督的GAT网络对行为稀疏、但访问宝贝和基础节点重合度高的人工众包流量进行扩召回。</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182020148.png" alt="图片" style="zoom: 50%;"></p>
<p>下图是2021年至今为止，最新挖掘的每日人工众包的设备数量趋势（具体数字已隐藏）。除大型节日外外，流量基本趋于稳定。相比于原始统计基线，组合模型可以精细定位到刷手与宝贝的对应关系，目前已额外新增30%的广告点击召回。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182020955.png" alt="图片" style="zoom: 67%;"></p>
<h3><span id="四-套利感知">四、套利感知</span></h3><p>为了建立内部自驱循环的异常检测框架，感知系统是必备的。在<a href="http://mp.weixin.qq.com/s?__biz=Mzg3MDYxODE2Ng==&amp;mid=2247484728&amp;idx=1&amp;sn=3305a981f34fe30e2464c627f3842ec8&amp;chksm=ce8a4061f9fdc977cb18acf91d212352a26d9f060dbae6ead52514102e95b9e31cd32523e564&amp;scene=21#wechat_redirect">《阿里妈妈流量反作弊算法实践》</a>文章中我们介绍过：<strong>感知重召回，其理念是召回一切认知之外的异常</strong>。从作弊的结果（果性）出发，才能召回受害者可感知的全部流量。</p>
<p>在广告主套利场景下，受害者是平台。但广告主的手法各不相同，从结果上看，未必会成功。虽然都需要处置，但在感知角度，我们更关注那些成功套利的案例。因此，我们从RPM低于平台预期的广告主的流量出发，<strong>召回套利成功流量的超集</strong>。</p>
<p>除了作为感知手段以外，估准RPM才能计算出该问题究竟为平台带来了多大影响。而且先找到一批RPM低于预期的广告主，才能用于最终，套利广告主名单的精度、召回评价。</p>
<h4><span id="41-评价方案">4.1 评价方案</span></h4>]]></content>
      <categories>
        <category>工业落地</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>工业落地（13）微信UFA-Unveiling Fake Accounts at the Time of Registration: An Unsupervised Approach</title>
    <url>/posts/1BJ949D/</url>
    <content><![CDATA[<h1><span id="ufa-在注册环节识别虚假账户的无监督检测算法">UFA-在注册环节识别虚假账户的无监督检测算法</span></h1><blockquote>
<p>  <strong>2021 KDD 论文</strong>：<strong>Unveiling Fake Accounts at the Time of Registration: An Unsupervised Approach</strong></p>
<ul>
<li><p><a href="https://dl.acm.org/doi/10.1145/3447548.3467094">https://dl.acm.org/doi/10.1145/3447548.3467094</a></p>
<font color="blue"> 小伍哥聊风控：https://mp.weixin.qq.com/s/PE4OSByfngPEJOW8UWVkMw</font>

</li>
</ul>
</blockquote>
<h3><span id="引言">引言</span></h3><p>今天分享一篇腾讯的论文，是一篇非常偏工程化的文章，其中也有很多技术的创新，但是工程上的一些实践感觉还是非常有价值的，<font color="red"> 其实在风控业务中，业务的抽象和提取方法，可能远大于算法本身，特别是图学习领域，一个垃圾的图，算法学的越准，错的越离谱。</font></p>
<span id="more"></span>
<h3><span id="一-概述">一、概述</span></h3><p>在线社交网络（OSN）充斥着虚假账户。现有的虚假账户检测方法要么需要手动标记的训练集，这既耗时又昂贵，要么依赖于OSN账户的丰富信息，例如内容和行为，这会导致检测虚假账户的显著延迟。在这项工作中，我们提出了UFA（揭开虚假账户的面纱）来在虚假账户以无监督的方式注册后立即检测它们。</p>
<p>首先，通过对真实世界注册数据集上的注册模式的测量研究，我们观察到虚假账户倾向于聚集在异常注册模式上，例如IP和电话号码。然后，我们设计了一种无监督学习算法来学习所有注册账户的权重及其特征，以揭示异常注册模式。</p>
<p><strong>接下来，我们构建了一个注册图来捕捉注册帐户之间的相关性，并通过分析注册图结构，利用社区检测方法来检测虚假帐户</strong>。我们使用真实世界的微信数据集评估UFA。我们的结果表明，UFA在召回率为80%的情况下达到了94%的精度，<strong>而监督变体需要600K手动标签才能获得可比较的性能</strong>。此外，微信已经部署UFA来检测虚假账户一年多了。UFA检测到500𝐾 通过微信安全团队的手动验证，每天伪造账户的准确率平均为93%。</p>
<p><strong>本文主要有四大贡献：</strong></p>
<ol>
<li>本文基于微信数据提出了大规模的<strong>度量学习</strong>的方法，并使用注册数据发现了虚假账号在异常注册模式的聚集趋势</li>
<li>本文基于微信注册数据，提出了非监督UFA模型用以识别虚假账户</li>
<li>系统性的评估了UFA和其他监督和非监督模型的效果</li>
<li>UFA模型在微信已经部署超过一年，在线上取得了很好的效果</li>
</ol>
<h3><span id="二-数据探索">二、<strong>数据探索</strong></span></h3><h4><span id="21-ip地址分析">2.1 <strong>IP地址分析</strong></span></h4><p>在论文使用的数据集中，总共有895,879个不同的IP地址。我们将使用同一IP地址注册的所有帐户组合在一起。因此，组的大小表示从一致的IP地址注册的帐户数量。图8a 显示了注册给定数量账户计数的IP地址数。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181521354.png" alt="图片" style="zoom: 50%;"></p>
<p>当给定的帐户数从0-5增加到5-10时，我们观察到IP地址数会有一个数量级的下降。这表明大多数IP地址注册了少量帐户（少于5个帐户），而少数IP地址注册了大量帐户。此外，假冒帐户更有可能使用相同的IP地址注册。</p>
<p>图8b显示了从IP地址注册的帐户中假冒帐户的比例，该IP地址注册了给定数量的帐户。我们观察到，当一个IP地址注册了少量帐户（例如10个）时，很难仅仅根据这些帐户共享IP地址的事实来判断这些帐户是否为假帐户。但是，当大量（例如100）帐户从同一IP地址注册时，这些帐户更有可能是伪造帐户。（量变引起质变）</p>
<h4><span id="22-手机号码分析">2.2 <strong>手机号码分析</strong></span></h4><p>用户在注册微信账号时必须提供电话号码。由于我们只能访问电话号码的前 7 位数字（运营商加区号），因此我们将使用电话号码的前缀来研究假帐户和良性帐户之间的区别。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522297.png" alt="图片" style="zoom: 50%;"></p>
<p>图 9a 显示了注册给定数量账户的电话号码前缀的数量，而图 9b 显示了从注册给定数量账户的电话号码前缀注册的账户中虚假账户的比例。与 IP 地址一样，我们观察到大多数电话前缀只具有少量注册帐户，而少数电话前缀具有大量注册帐户。更具体地说，如果一个电话前缀有超过 30 个注册帐户，这些帐户中的大部分可能是假帐户。</p>
<h4><span id="23-设备的device-id这里用的imei码">2.3 <strong>设备的device id（这里用的imei码</strong>）</span></h4><p>图10a显示了注册给定数量帐户的设备（由IMEI标识）的数量，图10b显示了注册给定数量帐户的设备中注册的帐户中伪造帐户的比例。我们观察到相似的异常模式，也就是说，伪帐户可能是从相同的设备注册的。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522769.png" alt="图片" style="zoom:50%;"></p>
<h4><span id="24-注册时间">2.4 <strong>注册时间</strong></span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522140.png" alt="图片" style="zoom:50%;"></p>
<p>图11显示了在一天中注册的帐户的注册时间的分布。我们观察到大多数良性用户从早上八点开始注册帐户，到晚上24点左右停止注册，午夜时很少有用户活跃。然而，假账户的数量似乎一整天都没有变化。大多数在午夜注册的账户可能是假账户。</p>
<h4><span id="25-不一致的地理位置"><strong>2.5 不一致的地理位置</strong></span></h4><p>IP地址和电话号码都可以映射到一个大概的地理位置。因此，我们可以分析每个注册的用户的两个地理位置是否相同（ip和电话号码可以查询到用户的归属地）。我们观察到65%的假帐户具有不同的基于IP的位置和基于电话号码的位置。一个可能的原因是攻击者利用云服务和从本地获取的电话号码注册假帐户。此外，用户还可以在注册帐户时在配置文件中指定其位置（例如国家）。我们发现96%的伪造账户指定的国家与基于IP的定位不一致。一个可能的原因是这些假账户的目的是攻击来自特定地点的良性账户。</p>
<h4><span id="26-微信和设备的版本"><strong>2.6 微信和设备的版本</strong></span></h4><p>我们分析了微信版本和操作系统版本，发现大多数从过时的微信或/和操作系统版本注册的账户都是假账户。例如，从某个过时的Android版本注册的账户只有2K个，其中96.5%是假的。同样的现象也发生在iOS中。例如，我们发现从iOS 8（过期的DOS版本）注册的99%的帐户都是假帐户。一个可能的原因是，攻击者使用脚本自动注册假帐户，其中微信和操作系统版本尚未更新。</p>
<h4><span id="27-道德和隐私"><strong>2.7 道德和隐私</strong></span></h4><p>微信在隐私政策中规定，用户注册帐户时将收集用户数据。此外，在提交微信服务器之前，所有注册属性都已匿名，以保护用户隐私。例如，电话号码的客户代码（即最后四位数字）将被删除。IP地址逐段散列，WiFi MAC和设备ID全部散列。特别是，我们有一个与WeChat合作的实习计划，因此我们可以访问存储在微信服务器上的上述数据。</p>
<h3><span id="三-相关研究">三、相关研究</span></h3><p>paper中的related部分其实就类似于我们平常做项目之前的一些解决方案的调研，传统的检测方法：这些方法可以分为基于特征的方法和基于结构的方法。</p>
<h4><span id="31-基于特征的方法">3.1 <strong>基于特征的方法</strong></span></h4><p>通常将虚假账户检测的建模转化为为一个二分类问题，并利用机器学习技术进行解决。具体来说，<strong>首先从每个账户的内容（如推特中的URL）、行为（如点击流和喜欢的行为记录）、注册信息（如IP地址和代理）中提取特征</strong>。然后，基于这些提取的特征和标记的训练集（由标记的假账户和标记的良性账户组成）训练监督分类器（如logistic回归），并使用训练的分类器检测假账户。</p>
<h4><span id="32-基于结构的方法">3.2 <strong>基于结构的方法</strong></span></h4><p>利用社交图的结构来检测假账户，并且通常基于这样一个观察结果，即如果一个账户与其他假账户相关联，该账户可能也是虚假的账户。这些基于扩展图的机器学习技术的方法，例如<strong>随机行走</strong>和<strong>信念传播</strong>，分析用户之间社交图的结构。</p>
<p><strong><font color="red"> 上述传统检测方法的主要局限性在于检测假账户时存在严重延迟，因为这些方法需要获取假账户生成的足够信息进行分析，例如，丰富的内容、行为，和/或社交图。</font></strong>因此，当检测到伪造帐户时，它们可能已经进行了各种恶意活动。</p>
<h3><span id="四-模型的原型设计">四、模型的原型设计</span></h3><p>UFA旨在以无监督的方式在注册时检测出虚假账户。图1概述了UFA。它由四个关键部分组成，即<strong>特征提取、无监督的权重学习、注册图的构建和假账户检测</strong>。在特征提取方面，受注册模式测量研究的启发，我们提取了揭示虚假账户异常注册模式的特征。<strong><font color="red"> 在无监督的权重学习中，我们首先构建了一个注册-特征大图来捕捉注册账户和特征之间的关系。大图中的每个节点都代表一个注册账户或一个特征，注册节点和特征节点之间的每条边都表示该注册账户具有该特征。</font></strong></p>
<h4><span id="41-整体框架">4.1 整体框架</span></h4><p>接下来，我们设计了一种统计方法来初始化大图中每个节点的权重。节点的权重量化了该节点的异常情况。我们的统计方法不需要标记的虚假账户或/和标记的良性账户，因此它是无监督的。这里，初始权重没有考虑特征之间和注册账户之间的关系。为了解决这个问题，我们采用了一种最先进的基于结构的方法，称为线性化信念传播[26]，来传播注册-特征大图中节点的初始权重，并迭代更新每个节点的权重。每个注册账户的最终权重表示该账户被伪造的概率。</p>
<p>UFA主要包括了四个步骤：特征提取、无监督权重学习、注册图构造、虚假账户检测</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191151673.png" alt="图片"></p>
<p>在特征提取中，受之前的数据挖掘部分得到的分析结果的启发，我们提取了可以反应假账户异常注册模式的特征。在无监督权重学习中，我们<strong>首先构造一个注册特征-用户 的二部图来捕捉注册账户和特征之间的关系</strong>。二部图中的每个节点表示注册帐户或特征，注册节点和特征节点之间的每个边表示注册帐户具有该特征。我们初始化Bigraph中每个节点权重使用了基于统计的方法。节点的权重量化了节点的异常。我们的统计方法不需要标记假账户或/和标记良性账户，因此不是有监督的方法。在这里，初始权值不考虑特征和注册账户之间的关系。为了解决这个问题，我们采用了一种最先进的基于结构的方法，称为线性化置信传播（《Structure-based sybil detection in social networks via local rule-based propagation》），在注册特征bigraph中传播节点的初始权重，并迭代更新每个节点的权重。每个注册帐户的最终权重表示该帐户被伪造的可能性。</p>
<p>无监督权重学习无法学习注册帐户之间的相关性，因为注册特征-用户的bigraph中注册帐户之间没有边。因此，我们构建了一个图来直接捕获注册帐户之间的相关性。具体来说，我们将注册特征的bigraph映射到一个注册图中，其中每个节点都是注册帐户，如果两个注册帐户之间的相似度高于阈值，则在它们之间添加一条边（还是相似度构图的思路啊。。。）。两个注册帐户之间的相似性定义为两个帐户共享的特征权重之和。在构建的注册图中，假账户可能紧密连接，而真账户可能稀疏连接。最后，由于假账户在注册图中紧密连接，我们使用<strong>社区检测算法</strong>进行社群检测。如果社区中的节点的规模大于阈值，则我们将社区中的所有帐户视为假帐户。</p>
<h4><span id="42-特征提取">4.2 <strong>特征提取</strong></span></h4><p>注册阶段的用户特征，当用户注册一个微信账号时，微信会收集该账号的多个注册属性。表1列出了具有代表性的注册属性。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522816.png" alt="图片" style="zoom:50%;"></p>
<p>例如，电话号码是用户用来注册帐户的号码。每个电话号码只能用于注册一个帐户和作为帐户ID。WiFi MAC是手机用于注册帐户的WiFi路由器的MAC地址，DeviceID是用于注册帐户的手机的IMEI/Adsource。根据数据挖掘部分的分析结果，与良性账户相比，虚假账户倾向于具有异常值注册模式。特别是，假账户可能使用相同的IP，使用相同地区的相同电话号码，从相同的设备注册，在午夜活跃，使用罕见且过时的微信和操作系统版本。一个可能的原因是攻击者的资源有限（例如IP、电话号码和设备），只能使用脚本自动注册假帐户。</p>
<p>那么我们怎么做特征提取呢？我们提取了可以用来反映虚假帐户异常注册模式的特征。<strong>具体来说，我们将每个注册表的属性和属性值解析为一个字符串，格式为“%%FeaturePrefix%%.%%value%%”</strong>。表2中列出了所有特性前缀。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522987.png" alt="图片" style="zoom:50%;"></p>
<p>每个特征前缀只与一个属性相关，并表示该属性的预处理函数；并且每个值都是预处理后生成的结果。为了简单起见，我们只展示如何提取特征前缀为“TimeAbnormal”的特征，因为其他特征可以从其特征前缀和描述中轻松理解：</p>
<p>TimeAbnormal：根据我们的数据挖掘的结果，我们观察到在深夜注册的账户很可能是假账户，而良性账户主要在白天注册。因此，我们定义了一个特征Prefix TimeAbnormal，它使用时间戳特征。时间异常用于指示帐户的注册时间是否异常。也就是说，如果账户在白天注册，则时间异常值为假，如果账户在午夜晚些时候注册，则时间异常值为真，在我们的工作中，我们将午夜定义为凌晨2点到5点之间。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522169.png" alt="图片" style="zoom:50%;"></p>
<p>这里的意思是根据注册时间构建一个节点，这个节点的id为 timeanomaly true，如上图。</p>
<p><strong><font color="red"> 整体来看，特征提取的思路很简单，其实本质上就是通过将用户的属性转化为节点，从而将单个用户节点分裂为以用户节点为中心的用户-属性的k部图，不过文中用的描述是2部图，其实就是把所有的特征都定义为类型为“特征”的节点，问题不大。</font></strong></p>
<p>在构图之前，对于特征论文中进行统一的string话处理，格式为 “%%FeaturePrefix%%_%%Value%%”，假设我们现在手头有两个用户uid001和uid002，对于特征的处理和图的构建就像下图这样：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">data = <span class="punctuation">&#123;</span></span><br><span class="line">    &#x27;uid001&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        &#x27;PHONE&#x27;<span class="punctuation">:</span> <span class="number">123456</span><span class="punctuation">,</span></span><br><span class="line">        &#x27;MAC&#x27; <span class="punctuation">:</span> <span class="string">&quot;abcd&quot;</span><span class="punctuation">,</span></span><br><span class="line">        &#x27;IP&#x27; <span class="punctuation">:</span> &#x27;<span class="number">11.22</span><span class="number">.33</span><span class="number">.44</span>&#x27;</span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    &#x27;uid002&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        &#x27;PHONE&#x27; <span class="punctuation">:</span> <span class="number">123456</span><span class="punctuation">,</span></span><br><span class="line">        &#x27;MAC&#x27; <span class="punctuation">:</span> &#x27;dcba&#x27;<span class="punctuation">,</span></span><br><span class="line">        &#x27;IP&#x27;<span class="punctuation">:</span> &#x27;<span class="number">44.33</span><span class="number">.22</span><span class="number">.11</span>&#x27;</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>构图数据预处理过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feature_to_node</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">    </span><br><span class="line">    result = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    <span class="keyword">for</span> uid, features <span class="keyword">in</span> data.items():</span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">            node = <span class="string">f&#x27;<span class="subst">&#123;feature&#125;</span>_<span class="subst">&#123;features[feature]&#125;</span>&#x27;</span></span><br><span class="line">            result[uid].add(node)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">feature_to_node(data)</span><br><span class="line">defaultdict(<span class="built_in">set</span>,</span><br><span class="line">            &#123;<span class="string">&#x27;uid001&#x27;</span>: &#123;<span class="string">&#x27;IP_11.22.33.44&#x27;</span>, <span class="string">&#x27;MAC_abcd&#x27;</span>, <span class="string">&#x27;PHONE_123456&#x27;</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;uid002&#x27;</span>: &#123;<span class="string">&#x27;IP_44.33.22.11&#x27;</span>, <span class="string">&#x27;MAC_dcba&#x27;</span>, <span class="string">&#x27;PHONE_123456&#x27;</span>&#125;&#125;)</span><br></pre></td></tr></table></figure>
<h4><span id="43-无监督的权重学习">4.3 <strong>无监督的权重学习</strong></span></h4><p><strong>在无监督权重学习中，我们的目标是学习每个提取特征和每个注册帐户的权重。</strong>权重量化异常，范围在0到1之间。较高的权重表示异常的可能性较大。具体来说，我们首先构造一个图来捕获特征和注册帐户之间的关系，其中每个节点表示一个特征或一个注册帐户，两个节点之间的每条边表示注册帐户具有该特征。<strong><font color="red"> 我们设计了一种统计方法，以无监督的方式初始化构造图中每个节点的权重，并采用基于结构的方法在构造的图中传播节点的初始权重，并迭代更新每个节点的权重。每个注册节点的最终权重可被视为该注册帐户的伪造概率。</font></strong></p>
<p>我们构造用户-特征的二部图来捕获注册帐户和提取特征之间的关系：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522902.png" alt="图片" style="zoom:50%;"></p>
<p>如上图所示。<strong>具体来说，我们用 （R，F，E）来表示这个图</strong>，其中R表示一组注册节点，就是注册用户的意思，F 表示一组特征节点，就是每个F节点都表示某个特征（当然这里的特征都是离散的，连续特征没法直接作为节点需要离散化之后才能作为节点），E，就表示edge了，（因为注册账户节点和特征节点之间都是 注册账户节点“拥有”特征节点 这么一个抽象的关系，所以edge的权重默认均为1.）注册节点和特征节点之间的边表示注册节点，即注册用户具有这个特征。</p>
<p>例如，在上图中， 节点连接了 如“IP32<em>10.xxx.xxx.10”、“PN</em>+861585321xxxx”、“OSV_Android 7.0”这三个特征节点，这意味着 注册时，使用IP地址“10.xxx.xxx.10”、电话号码“+861585321xxxx”和手机操作系统版本“Android 7.0”。</p>
<p>特征节点的权重的初始化策略，（<strong>这篇paper的思路是先通过无监督的方法给特征节点的权重进行初始化，然后通过信念传播的方式给注册账户节点进行权重的传播赋值</strong>）。我们设计了一种统计方法，为每个特征节点和每个注册帐户节点分配初始权重，这个过程无需手动打标虚假账户。我们的统计方法依赖于三个概念：<strong><font color="red"> 特征频率、特征比率和特征模式</font></strong>。</p>
<ul>
<li><strong>特征频率</strong>：特征频率表示某个特征有多少个注册账户有，比如有100个注册账户在注册的时候都使用了“IP32_10.xxx.xxx.10”，则“IP32_10.xxx.xxx.10”这个特征节点的特征频率为100；</li>
<li><strong>特征比率</strong>：其实就是同类特征的占比啦。比如说我们的手机号这个特征下有10000个手机号，其中“PN<em>+861585321xxxx”有5000个，则“PN</em>+861585321xxxx”这个特征节点的特征比率为0.5.</li>
<li><strong>特征模式：</strong>特征模式不是相对于某个特征节点的，而是对于某个特征下所有的特征节点的，还是举个例子，假设OSV前缀的，即手机系统版本这个特征下有Android 7.0，Android 8.0，Android 9.0，占比分别为0.25，0.25，0.5，则最终我们得到 OSV 这个特征的特征模式为 Android 9.0,其实就是某个特征下某个最大取值对应的特征节点的名字。</li>
</ul>
<p>现在我们可以通过使用上述的特征频率，特征比率和特质模式来定义每个特征的权重。<strong>我们通过定义特征的权重来量化特征的异常。</strong></p>
<p>为了描述清晰，这里还是强调一下，特征频率和特征比率都是针对于特征节点的，而特征模式是针对于某一类特征节点的群体的，以性别为例（因为性别特征就两个取值，比较好理解），性别有男女，则按照上述的思路，我们会有一个 “sex<em>男”和“sex</em>女”的这两种特征节点，“sex<em>男”和“sex</em>女”的特征频率假设分别60和40，则其特征比率分别为0.6和0.4，性别这个特征的特征模式为0.6.具体地说，是这么做的，首先，我们将所有的特质划分为两个组，一个pre-A，一个pre-B。我们以数据挖掘分析部分的一个case为例：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522966.png" alt="图片"></p>
<p>deviceid的共享情况，这个图解释一下，左边的直方图，横轴表示device id 共享的账户的数量，按照直方图的划分可以知道，每一个device id根据其共享的账户数量打上了标签，分别为“ 0～5”，“5～10”.。。。“&gt;=30”，然后我们统计每一种标签下的device的数量，就是纵轴的取值了，可以看到，deviceid这种东西反映出来的趋势是 deviceid 一般情况下不太可能被多个账户共享，大部分的device 所共享的账户很少，极少部分的deivce id 存在大量账户共享的情况，所以我们把device id 这种特征分到 pre-B 组中去；像ip，phone 这些的反应出来的情况都是一样的，所以都放到pre-B中去了，显然，我们可以知道，放到pre-B中的特征都是不太容易产生高账户共享问题的字段。</p>
<p>原文提到了pre-A中的特征有一个是osv，即手机的版本号，比如android 7.0 ，ios xxx 这类的。pre-A中的特征就是比较容易产生大量账户共享的，比如手机版本，app的版本，时间上的异常等等（<strong>我以前对于这种容易产生大量账户共享的特征都是删除不拿来建图的，因为容易产生很稠密的图，坏处就是损失了一部分信息，这里给到的思路还可以。</strong>）而对于pre-B中的特征，例如phone number 手机号这种东西，一般不太容易被大量的账户共享，那么对于手机号这种特征，其特征节点，例如“861585321xxx” 这个号段，如果被大量账户共享，则我们认为这种情况很异常，因为手机号本上就趋向于被很少的账户共享或不被共享。文中的pre-A和pre-B的选择为：Pre_A={ , ℎ , , }andPre_B={ 32, , , }</p>
<p>我们这么来定义特征节点的权重，可以看到，上述的公式应该是很清晰了，对于pre-A中的特征，特征节点的节点权重为1-ratio（x），即这个特征节点共享的账户越多，越正常，节点权重越小；对于pre-B中的特征，特征节点的权重为ratio（x），即这个特征节点共享的账户越多，越不正常，节点权重越大。</p>
<p><strong><font color="red"> 然而，有一个严重的问题，上述定义的特征节点的权重，不能用于直接比较不同特征下的不同特征节点的节点权重</font></strong>，例如经过这样的计算，android7.0 这样的特征节点和“861585321xxx” 这样的特征节点没有可比性，比如手机系统版本和手机号这两个特征的特征分布，前者的特征比分布是比较集中的（大部分用户都是集中使用比较新的几个版本），而手机号的特征比分布则比较均匀（用户均匀的分配到不同的手机号段上），这个时候算出来的ratio肯定差别很大的。为了解决这个问题，对上图的公式进行优化和重新定义，使用了一种称为类别特征的“特征匹配”的技术（《Guansong Pang, Longbing Cao, and Ling Chen. 2016. Outlier Detection inComplex Categorical Data by Modeling the Feature Value Couplings.. InIJCAI.》），具体来说，是这么做的。</p>
<p>特征节点的节点权重wx的公式转化如下：</p>
<script type="math/tex; mode=display">
w_x=\frac{1}{2}(\operatorname{dev}(x)+\operatorname{base}(x))</script><p>where</p>
<script type="math/tex; mode=display">
\operatorname{dev}(x)= \begin{cases}1-\frac{\operatorname{ratio}(x)}{\text { ratio }(\text { mode }(\text { pre }(x)))} & \text { if } \operatorname{pre}(x) \in \text { Pre-A } \\ 1-\frac{1-\operatorname{Aatio}(x)}{\text { ratio }(\text { mode }(\text { pre }(x)))} & \text { if } \operatorname{pre}(x) \in \text { Pre-B}\end{cases}</script><p>and</p>
<script type="math/tex; mode=display">
\operatorname{base}(x)= \begin{cases}1-\operatorname{ratio}(\operatorname{mode}(\operatorname{pre}(x))) & \text { if } \operatorname{pre}(x) \in \text { Pre-A } \\ \operatorname{ratio}(\operatorname{mode}(\operatorname{pre}(x))) & \text { if } \operatorname{pre}(x) \in \text { Pre-B }\end{cases}</script><p>本质上就是做了一个标准化的操作, 使得不同特征下的特征节点的权重统一到一个量纲下（<strong>对于离散特征的这种权重的处理在图中具有很好的推广作用, 有时间再看看文中提到的这几篇论文, 很不错</strong>)<br>$\operatorname{dev}(\mathrm{x})$ 考虑了特征节点的异常，base $(x)$ 考虑了特征的异常。因为 $\operatorname{dev}(x)$ 的取值必然是0~1之间的， base (x)也必然是0~1之间的, 因此, 最终我们的特征节点权重w $(x)$ 也必然是0~1之间的。然后, 注册账户节点的初始化权重就是和这个注册账户节点连接的所有的特征节点的权重的平均值。</p>
<script type="math/tex; mode=display">
w_r=\frac{\sum_{x \in \Gamma(r) w_x}}{|\Gamma(r)|}</script><p><strong>初始化了权重之后, 使用线性的信念传播来进行特征节点和注册账户节点的权重的更新迭代,</strong></p>
<script type="math/tex; mode=display">
\begin{split} p_u^{(t)}=w_u+2 \sum_{v \in \Gamma(u)}\left(p_v^{(t-1)}-0.5\right) \cdot\left(h_{v u}-0.5\right) \end{split}</script><p>文中提到了，作者做了10次的迭代来更新所有节点的权重。(<strong>信念传播还不太懂, 暂时可以先理解类似于用户自定义初始权重的个性化pagerank</strong>)</p>
<h4><span id="44-注册信息构图">4.4 注册信息构图</span></h4><p><strong>我们构建了一个加权图来直接捕捉注册账户之间的关联性。构建的图是由注册-特征图而来的</strong>。具体来说, 对于注册特征图中的每一对注册节点 $u, v \in \mathbb{R}$, 我们在 $u$ 和 $v$ 之间创建一条边 $(u, v)$ ，只要 $u$ 和 $v$ 的相似度大于阈值。特别是, 我们把 $u$ 和 $v$ 之间的相似性 $\operatorname{sim}(u, v)$ 定义为 $u$ 和 $v$ 的共享特征的最终权重之和。这背后的直觉是，如果两个注册节点共享许多特征, 而且这些特征的最终权重很大（异常概率大），那么这两个注册 账户也有很大的相似性, 倾向于是假账户。如下图:</p>
<script type="math/tex; mode=display">
\operatorname{sim}(u, v)=\sum_{s \in \Gamma(u) \bigcap \Gamma(v)} p_s</script><p>此外，我们设置了边的权重$(𝑢,𝑣)$ 在新的图中成为相似性$sim(𝑢,𝑣)$. 我们将构建的加权图称为注册图，因为图中的所有节点都是注册帐户。虚假账户更有可能以较大的边缘权重相互连接，良性账户更有可能通过较小的边缘权重稀疏连接。最终我们将注册账户节点-特征节点的异构图转化为了<strong>注册账户节点-注册账户节点的基于相似度的同构图。</strong></p>
<h4><span id="45-社区发现">4.5 <strong>社区发现</strong></span></h4><p>在注册图中，我们注意到虚假账户密集连接，而良性账户稀疏连接。<strong>为了检测欺诈账户，我们需要识别注册图中的密集子图</strong>。自然的选择是利用社区发现算法。我们首先采用社区检测方法，例如<strong>Louvain方法[2]</strong>，将节点分为不同的社区（即密集子图）。其次，我们检测发现出来的社区中的所有注册帐户的数量，如果其大小超过了我们设定的阈值，则认为这个社群中所有的注册账户都是有问题的虚假账户。</p>
<ul>
<li>我们首先采用社区检测方法将节点聚集到不同的社区（即密集子图）。</li>
<li>其次，我们预测社区中所有规模大于阈值的注册账户都是假账户。</li>
</ul>
<h3><span id="五-模型评估">五、模型评估</span></h3><h4><span id="51-实验设置">5.1 实验设置</span></h4><ul>
<li><strong>数据集</strong></li>
</ul>
<p>我们从微信中获得了七个数据集。每个数据集都包括一天内的注册信息。表3显示了这些数据集的细节。这些标签是由微信安全团队提供的，他们手工验证了这些标签（大厂就是牛逼），并发现这些标签的准确率大于95%。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522980.png" alt="图片" style="zoom:50%;"></p>
<ul>
<li><strong>评价指标</strong></li>
</ul>
<p>我们使用三个指标，即<strong>准确率、召回率和 F-score 来评估性能</strong>。对于一个检测方法来说，精确率是其预测的假账户在测试集中为真实假账户的比例，召回率是测试集中真实假账户被该方法预测为假账户的比例，而F分数是精确率和召回率的调和平均值。比较的方法。我们将UFA与几种无监督的变体进行比较，包括UFA-naive、UFA-noLBP、UFA-noRG和超级视觉方法，包括Ianus[35]、XGBoost[6]和深度神经网络（DNN）。</p>
<h4><span id="52-实验结果">5.2 实验结果</span></h4><p>表4显示了ufa在七个数据集上的预测结果。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522407.png" alt="图片" style="zoom:50%;"></p>
<p>UFA可以在所有数据集中检测到约80%的假帐户，其精确度约为90%。一个关键原因是，通过无监督的权重学习和注册图构造，注册图中的假账户连接紧密，而良性账户连接稀疏。</p>
<p>例如，在第五天由构建出来的注册图中（从这里可以看出，腾讯构图的方式是构建天级别的图），平均而言，一个假帐户与22.32个其他假帐户存在连接，而与1个良性账户存在连接，而对于一个良性帐户而言，仅与2.04个其他良性账户相互连接。</p>
<p>然后，Louvain的方法可以很容易地检测到这些密集连接的假帐户。我们发现大约85%的假账户聚集在一起，剩下的15%的假账户是分散的，也就是说，他们与其他人没有共同的特征（这种就没办法了，毕竟ufa是完全从关联上出发去检测的）。因此，根据ufa的80%左右召回表现，证明了其在检测假账户方面的有效性。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522512.png" alt="图片" style="zoom:50%;"></p>
<ul>
<li><h5><span id="相似阈值的影响"><strong>相似阈值的影响</strong></span></h5></li>
</ul>
<p>回顾一下，<strong>UFA预先定义了一对注册账户之间的相似度阈值，以确定两个注册账户之间是否添加了一条边。一个自然的问题是这个相似度阈值对UFA的检测性能有什么影响</strong>。图3a显示了UFA与1.0和1.5之间的相似度阈值的结果。我们观察到，当阈值增加时，精确度增加，召回率下降，而F-Score首先增加，然后下降。原因是较大的阈值使一对注册账户在我们的注册图中更难连接。当使用较高的阈值时，注册图中的连接节点更有可能是假账户，所以我们可以有更高的精度。同时，更少的假账户可以相互连接，因此召回率下降。我们还注意到，当相似度阈值在1.2左右时，F-Score达到最大值。因此，我们在实验中选择默认的相似度阈值为1.2。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522878.png" alt="图片" style="zoom:50%;"></p>
<p>我们观察到，当阈值增加时，精度增加，而F分数先增加后减少。原因是阈值越大，在注册图中连接一对注册帐户就越困难。当使用更高的阈值时，注册图中连接的节点更有可能是假帐户，因此我们可以获得更高的精确度。同时，可以相互连接的假帐户更少，从而减少了recall。我们还注意到，当相似性阈值在1左右时，F分数达到最大值。因此，我们在实验中选择了一个默认的相似性阈值为1.2。</p>
<ul>
<li><h5><span id="社区规模的影响">社区规模的影响</span></h5></li>
</ul>
<p>UFA使用Louvain方法检测社区并预测社区中的注册帐户，这些帐户的大小大于预定义的阈值，即假帐户。我们研究了不同社区规模对UFA的检测性能的影响，结果如图3b所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522494.jpeg" alt="图片" style="zoom:50%;"></p>
<p>我们观察到，随着社区规模的阈值变大，精确度和F分数先增加后降低。当阈值大于15时，所有三个指标都是稳定的。所以，我们在实验中选择了社区规模的默认阈值为15。</p>
<h3><span id="六-在微信场景的实际部署">六、<strong>在微信场景的实际部署</strong></span></h3><p>UFA已经被微信部署了一年多，用于检测虚假账户。它是在Spark上用Python实现的，并被部署在微信的内部云计算平台YARD上。UFA以流程模式工作，其部署图如图6所示。具体来说，UFA系统有两个阶段：<strong>系统初始化和系统更新。</strong></p>
<h4><span id="61-系统初始化">6.1 <strong>系统初始化</strong></span></h4><p>微信最初部署时，UFA收集一定数量的注册账号，提取特征，构建注册特征bigraph，学习特征权重和注册帐户，并构建注册图。微信在部署后第一周使用所有注册账号完成系统初始化。初始化后，我们拥有特征节点和注册帐户的权重。系统初始化中的所有步骤都在流式处理服务器上执行。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304181522379.png" alt="图片"></p>
<h4><span id="62-系统更新">6.2 <strong>系统更新</strong></span></h4><p>系统初始化后，当WeChatserver收到新的注册帐户时，UFA先提取帐户特征，然后执行以下步骤。</p>
<ul>
<li>在注册特征的Bigraph中创建新节点/边。UFA先为不在当前注册特征的bigraph中的特征创建新注册节点和新特征节点。然后，UFA在注册节点及其特征节点之间创建新边。此步骤在流数据服务器上运行。</li>
<li>在注册图中创建新节点/边。UFA在此步骤中检索与新注册帐户接近的现有注册帐户。具体而言，UFA寻找所有现有注册帐户与新注册帐户之间共享的注册节点。然后，UFA使用特征节点的当前权重计算新注册帐户和找到的注册帐户之间的相似性。接下来，在注册图中，如果两个帐户之间的相似性大于预定义阈值，UFA将为新注册帐户创建一个新节点，并在新注册帐户和找到的每个注册帐户之间创建一条边。此步骤在流式处理服务器上运行。</li>
<li>定期更新特征/注册节点的权重并检测假帐户。<strong>为了减轻计算负担，UFA更新特征/注册节点的权重，并每小时检测一次假帐户</strong>。权重更新和伪造帐户检测都定期在检测服务器上执行。<strong>具体来说，流式处理服务器上的UFA会每小时更新注册-特征节点bigraph和注册-注册节点的同构图，并将这些图保存到数据库中。</strong>然后，从数据库加载图，并运行无监督权重学习算法和社区检测算法来检测假帐户。接下来，UFA将更新的特征/注册节点的权重和检测到的假帐户从检测服务器传输到流处理服务器。检测到的假帐户存储在数据库中。流服务器使用更新的特征/注册节点权重进行下一次定期系统更新。</li>
</ul>
<h4><span id="63-ufa在真实场景中的表现">6.3 <strong>UFA在真实场景中的表现</strong></span></h4><p>微信所有检测到的假帐户，其中一些可能是良性帐户。如果良性帐号被封禁，微信安全团队会收到注册该帐号的用户的投诉。微信使用投诉数量来评估UFA的表现。</p>
<p>具体来说，微信安全团队有6名安全分析师来处理用户的投诉。总体而言，申请解锁账户在被封禁的账户中不到6%。此外，微信安全团队随机抽取了200个被UFA检测到的虚假账户，对UFA的性能进行评估，UFA的精度达到93%，与投诉测得的结果一致。</p>
<p>综上所述，UFA 平均每天检测 50 万个假账户，自 UFA部署以来总共检测到 1.8 亿个假账户（在 4 亿个注册账户中）。</p>
<p>Ianus 在现实世界中的部署限制，在 UFA 之前，微信已经部署了 Ianus [35] 大约六个月。<strong>Ianus 是一种监督式虚假账户检测方法，它也利用了账户的注册信息。然而，Ianus在部署一段时间后就暴露了它的弱点</strong>。</p>
<ul>
<li>首先，手动标记非常耗时。微信安全团队有6名安全分析师，他们的职责是对注册账号进行标记，每个分析师每天可以标记大约1000个注册。人工标注，比如600K个虚假账户，大概需要100天左右，很费时间。很难获得准确的标注，毕竟人也会犯错。许多虚假帐户仅在检查其注册信息，类似于良性帐户。</li>
<li>而在一定数量的噪音标签下，Ianus的性能将下降。</li>
<li>除此之外，需要经常对Ianus进行retraining，以保持较高的检测性能。</li>
</ul>
<p>图7显示了Ianus在连续14天的预测中的表现，不进行retraining。为了简单起见，我们只展示了分数，我们在精度和调用方面也有类似的观察结果。我们看到，Ianua的F-Score从第七天开始大幅下降。68%对55%。92%，在14天内。<strong>一个可能的原因是攻击者不断改变他们的注册模式以逃避Ianus的检测。</strong>因此，基于旧注册模式训练的Ianus不足以检测具有新注册模式的假帐户。为了保持较高的检测性能，需要经常使用新注册模式上的准确标签对Ianus进行重新训练。然而，正如我们上面提到的，它是耗时的，并且很难准确地标记注册帐户，所以靠谱的retraining需要的标签也不多。相比之下，UFA的分数相对稳定，表明UFA在实际部署中要比Ianus更好。</p>
<h3><span id="七-总结">七、 总结</span></h3><p>在本文中，我们开发了一种无监督的方法UFA来检测虚假账户，在他们被注册后立即检测。我们首先提取了揭示假账户异常注册模式的特征，其灵感来自于对微信中真实世界注册数据集的测量研究。然后，我们设计了一种无监督的权重学习算法来学习提取的特征和注册账户的权重。此外，我们构建了一个注册图来直接捕捉注册账户之间的关联性，如虚假账户是密集连接的，而良性账户是稀疏连接的。最后，我们采用了一种社区检测算法，通过分析注册图结构来检测虚假账户。我们使用微信的真实世界数据集对UFA进行了评估。我们的结果显示，UFA取得了94%的精度和80%的召回率。UFA已经被微信部署在检测虚假账户上一年多了，并且达到了93%的精度。</p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>业务安全</category>
      </categories>
      <tags>
        <tag>无监督</tag>
        <tag>账号安全</tag>
        <tag>图社区发现</tag>
      </tags>
  </entry>
  <entry>
    <title>风控反欺诈（2）GraphSAGE算法在网络黑产挖掘中的思考</title>
    <url>/posts/3ADMGV5/</url>
    <content><![CDATA[<h2><span id="graphsage图算法在网络黑产挖掘中的思考">GraphSAGE图算法在网络黑产挖掘中的思考</span></h2><blockquote>
  <font color="blue"> Harry 高级研究员-DataFunTalk https://mp.weixin.qq.com/s/sZ7VQz26c5mrWAsnMKx8Hw</font>

  <font color="blue"> **graphSAGE-pytorch**：https://github.com/twjiang/graphSAGE-pytorch/tree/master/src</font>

<p>  <strong>导读：</strong>虚拟网络中存在部分黑产用户，这部分用户通过违法犯罪等不正当的方式去谋取利益。作为恶意内容生产的源头，管控相关黑产用户可以保障各业务健康平稳运行。<strong>当前工业界与学术界的许多组织通常采用树形模型、社区划分等方式挖掘黑产用户，但树形模型、社区划分的方式存在一定短板，为了更好地挖掘黑产用户，我们通过图表征学习与聚类相结合的方式进行挖掘</strong>。本文将为大家介绍图算法在网络黑产挖掘中的思考与应用，主要介绍：</p>
<ul>
<li>图算法设计的背景及目标</li>
<li>图算法GraphSAGE落地及优化</li>
<li>孤立点&amp;异质性</li>
<li>总结思考</li>
</ul>
</blockquote>
<h3><span id="一-图算法设计的背景"><strong>一、 图算法设计的背景</strong></span></h3><p>在虚拟网络中存在部分的黑产用户，这部分用户通过违法犯罪等不正当的方式去谋取利益，比如招嫖、色情宣传、赌博宣传的行为，更有甚者，如毒品、枪支贩卖等严重的犯罪行为。当前工业界与学术界的许多组织推出了基于图像文字等内容方面的API以及解决方案。而本次主题则是介绍基于账号层面上的解决方法，为什么需要在账号层面对网络黑产的账号进行挖掘呢？</p>
<p><strong>原因主要有三：</strong></p>
<ul>
<li><strong>恶意账号是网络黑产的源头，在账号层面对网络黑产的账号进行挖掘可以对黑产的源头进行精准地打击</strong>；</li>
<li>账号行为对抗门槛高，用户的行为习惯以及关系网络是很难在短期内作出改变的，而针对单一的黑产内容可以通过多种方式避免被现有的算法所感知，虽然黑产用户可能不懂算法，但其可以通过“接地气”的方式来干扰算法模型，譬如在图片上进行简单的涂抹，在敏感处打上马赛克，在图片处加上黑框，通过简单的对抗手段会对基于黑产内容的算法产生较大的影响；</li>
<li>可以防范于未然，通过账号层面的关联提前圈定可疑账号，在其进行违法犯罪行为之前对账号进行相应的处理以及管控。</li>
</ul>
<p><strong>具体通过什么方式挖掘黑产账号？</strong></p>
<p>首先，简单介绍下在推荐场景中应用。比如广告推荐，通常上，广告商会给予平台方用户的用户标签，用户存在用户标签之后，平台方则会将相关类别的用户找出，然后将广告推送给对应的用户；另一种方式是广告方提供种子包给平台方，平台方会找到相似的用户，然后将广告推送给相关的用户，常见的应用场景有Facebook look like、Google similar audiences。</p>
<h4><span id="11-应用场景">1.1 应用场景</span></h4><p><strong>在黑产场景中与推荐场景中的应用类似，主要分为两个任务场景：</strong></p>
<ul>
<li>找出目标恶意类别用户。比如需要找出散播招嫖信息的用户，则给定该类用户招嫖的标签，类似于一个用户定性的问题；</li>
<li>黑产种子用户扩散，即利用历史的黑产用户进行用户扩散以及用户召回，可以通过染色扩散以及相似用户检索等方式完成。</li>
</ul>
<p><strong><font color="red"> 针对恶意用户定性的传统方法，通常采用树形模型，比如说XGboost、GBDT等</font></strong>。这类算法短板显而易见，其<strong>缺乏对用户之间的关联进行考虑</strong>；另外一种<strong>用户召回方式为用户社区划分（相似用户召回），其中比较常用的社区划分算法有FastUnfolding、Copra等</strong>。这类算法的缺陷也相当明显，其由于原本社区规模小，所以最终召回的人数也少。且会存在多个种子用户在同一个社区的情况，<strong>难以召回大量可疑用户</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533000.jpeg" alt="图片" style="zoom:50%;"></p>
<p><strong><font color="red"> 通过图表征学习与聚类相结合的方式进行召回</font></strong>。<strong>通过图表征学习将图结构的节点属性以及结构特征映射到一个节点低维空间，由此产生一个节点特征，然后再去进行下游的任务，如用户定性即节点分类等</strong>。其中，图表征学习的关键点在于在进行低维的映射当中需要保留原始图的结构和节点属性信息。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533575.jpeg" alt="图片" style="zoom:50%;"></p>
<h4><span id="12-图算法设计">1.2 图算法设计</span></h4><ul>
<li>算法的覆盖率和精准度；</li>
<li>用户分群规模合理，保证分群的可用性；</li>
<li><strong>支持增量特征，下游任务易用性</strong>。</li>
</ul>
<p>由于业务场景更多为动态网络，当新增节点时，如果模型支持增量特征，则不需要重复训练模型，可以极大的减少开发的流程，节省机器学习的资源，缩短任务完成的时间。</p>
<h3><span id="二-图算法graphsage落地及优化">二、图算法GraphSAGE落地及优化</span></h3><h4><span id="21-graphsage核心思想">2.1 GraphSAGE核心思想</span></h4><p><strong><font color="red"> GraphSAGE核心思想主要为两点：邻居抽样；特征聚合。</font></strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533811.jpeg" alt="图片" style="zoom:50%;"></p>
<p><strong>GraphSAGE的聚合过程实际是节点自身的属性特征和其抽样的邻居节点特征分别做一次线性变换，然后将两者concat在一起，再进行一次线性变换得到目标节点的embedding特征</strong>。最后利用得到的目标节点的embedding特征进行下游的任务，训练的方式的可以采用无监督的方式，如<strong>NCE Loss</strong>。</p>
<h4><span id="22-graphsage的优点">2.2 <strong>GraphSAGE的优点</strong></span></h4><p><strong>GraphSAGE通过邻居抽样的方式解决了GCN内存爆炸的问题</strong>，同时可以将<strong>==直推式学习转化为归纳式学习==</strong>，<strong>==避免了节点的embedding特征每一次都需要重新训练的情况，支持了增量特征==</strong>。为什么通过邻居随机抽样就可以使得直推式的模型变为支持增量特征的归纳式模型呢？</p>
<p>在原始的GraphSAGE模型（直推式模型）当中，节点标签皆仅对应一种局部结构、一种embedding特征。在GraphSAGE引入邻居随机抽样之后，节点标签则变为对应多种局部结构、多种embedding特征，这样可以防止模型在训练过程过拟合，增强模型的泛化能力，则可以支持增量特征。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533971.jpeg" alt="图片" style="zoom:50%;"></p>
<h4><span id="23-graphsage的缺点">2.3 <strong>GraphSAGE的缺点</strong></span></h4><ul>
<li><strong>原GraphSAGE无法处理加权图，仅能够邻居节点等权聚合</strong>；</li>
<li>抽样引入随机过程，推理过程中同一节点embedding特征不稳定；</li>
<li>抽样数目限制会导致部分局部信息丢失；</li>
<li>GCN网络层太多容易引起训练中过度平滑问题。</li>
</ul>
<h4><span id="24-graphsage的优化">2.4 <strong>GraphSAGE的优化</strong></span></h4><p>为解决上述GraphSAGE存在的缺点，对GraphSAGE进行优化。</p>
<h5><span id="聚合优化">聚合优化:</span></h5><p>解决等权聚合的问题。相对于直接将邻居节点进行聚合，将边权重进行归一化之后，点的邻居节点的特征进行点燃，最后再进行特征融合。这样做的好处主要有两点：边权重越大的邻居，对目标节点影响越大；节点边权重归一化在预处理阶段完成，几再与目标节乎不影响算法速度。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533699.jpeg" alt="图片" style="zoom:50%;"></p>
<h5><span id="剪枝优化">剪枝优化:</span></h5><p>解决embedding特征不稳定的问题。<strong>在训练的过程希望通过引入随机过程防止模型出现过拟合的现象</strong>，<strong>但是在模型的推理过程式是想要去掉这样一个随机过程</strong>。直接对原始网络进行剪枝操作，仅保留每个节点权重最大的K条边，在模型进行推理的时候，会将目标节点所有的K个邻居节点的特征都聚合到目标节点上，聚合方式同样为加权的方式。<strong>这样做的好处主要有两个点：在网络结构不变的情况下，保证同节点embedding特征相同；在保证算法精度的前提下，大幅度降低图的稠密程度，降低内存开销。</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533517.jpeg" alt="图片" style="zoom:50%;"></p>
<h5><span id="采样优化">==采样优化:==</span></h5><p><strong>解决局部信息丢失以及训练过平滑的问题</strong>。主要通过<strong>DGL的抽样方式代替原有的抽样方式</strong>，具体的做法为：提前将每一个节点的属性特征与它所有的邻居节点的属性特征的均值进行concat，这样可以使得每一个节点初始状态下已经包含了周围一些邻居节点的一些信息，通过这种方式，在采样相同节点的前提下，可以获得更多的局部信息。<strong>一般情况下，GCN模型采用两层网络模型，当增加至第三层的时候则将存在内存爆炸的问题</strong>；当增加至第四层时，则将出现过平滑的问题，将导致特征分布去重，这样则导致节点没有区分性。而采用DGL采样，通过采样两层GCN模型而实际上采样了三层，而且不会出现过平滑问题。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533122.jpeg" alt="图片" style="zoom:50%;"></p>
<h4><span id="25-效果评估"><strong>2.5 效果评估</strong></span></h4><p>效果评估的指标主要有两个：<strong>聚类（社区）准确率；召回恶意率</strong>。相对于原有的fastunfolding以及<strong>node2vec</strong>从聚类准确率、召回恶意率、平均社区规模、运行时间作一个横向对比：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533948.jpeg" alt="图片" style="zoom:50%;"></p>
<h4><span id="三-孤立点amp异质性">三、<strong>孤立点&amp;异质性</strong></span></h4><h4><span id="31-黑产挖掘场景中的孤立点的解决思路"><strong>3.1 黑产挖掘场景中的孤立点的解决思路</strong></span></h4><p><strong>黑产用户在被处理后，通常会快速地申请新的账号或使用备用账号，因为在对黑产的挖掘过程中就不可避免地会出现孤立点</strong>，类似在推荐算法中的冷启动问题。以node2vec算法为例，算法通常会通过游走去构造训练的节点段，那么如果孤立节点没有连边的话，节点是无法出现在训练集当中。<strong><font color="red"> 为了解决该问题，引入一个解决推荐系统冷启动的算法——EGES</font></strong>，将每一个节点的属性特征映射到一个embedding特征，然后将每一个属性的embedding特征置于注意力层进行处理，比如将N个随机特征通过注意力加权，可以获得最终的一个节点层面的embedding特征，新增的节点将不再依赖于关系网络以及用户的一些交互行为，新增的节点可以通过自身的属性特征就直接获得我们的embedding特征，不需要考虑用户关系从而解决孤立点的问题。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533883.jpeg" alt="图片" style="zoom:50%;"></p>
<p>在具体落地过程中，提出了GraphSAGE-EGES算法，实际上是综合了两种算法的优势，GraphSAGE的节点本身的初始特征将其替换成了EGES增强之后的属性特征，通过此类方式，最终的算法框架如下图所示：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533345.jpeg" alt="图片" style="zoom:50%;"></p>
<h4><span id="32-黑产网络中异质性的解决思路"><strong>3.2 黑产网络中异质性的解决思路</strong></span></h4><p>在正常的网络结构当中，一个用户的一阶邻居基本上都是同一类的用户，比如说在学术引用当中，一篇数据挖掘的论文，引用其的论文也多是与数据挖掘相关的。这一类的网络称之为同质性网络。<strong>但在黑产的关系网络当中，图的异质性就非常高了，黑产用户不仅仅与黑产用户相关，其也可以与正常用户建立关系，这种特殊的网络结构就会存在一些弊端</strong>，以下图异质性网络为例，圈住的正常节点的一阶邻居节点一半为恶意账号，算法进行预测、聚类时，该节点很多概率会被判定为恶意账号。圈住的恶意节点的一阶邻居3个皆为正常账号，算法进行预测、聚类时，该节点则大概率被判定为正常节点，导致算法的精度下降。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533084.jpeg" alt="图片" style="zoom:50%;"></p>
<p>为了解决上述问题，需要去考虑网络的结构是否合理。为了构建合理的网络结构，需要将恶意账号与正常账号之间存在的联系剔除掉，并将恶意账号之间的联系进行一定的增强。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533269.jpeg" alt="图片" style="zoom:50%;"></p>
<p>当网络结构合理时，算法进行预测、聚类时会更加准确，因此引入图结构学习的概念，尝试用LDS算法解决这类问题。</p>
<p>LDS算法的思想：在训练GCN模型的参数的同时对网络的结构进行调整，在最初的时候给予一个网络结构（邻接矩阵），先固定GCN的模型，然后训练邻接矩阵，通过几轮迭代之后再固定邻接矩阵，再训练GCN模型，通过几轮迭代之后，可以得出一个合理的网络结构。</p>
<p><strong>总的来说，这个算法实际上就是一个极大似然估计以及伯努利分布的问题。在LDS算法学习邻接矩阵的时候实际就是学习两个点的邻边是否应该存在，实际上为一个0-1分布。</strong>最终通过网络结构以及节点的标签去预估在当前数据标签的情况下，更应该得到什么样的一个网络结构，以上即为该算法的核心思想。</p>
<p>实际上，在许多业务场景当中会存在许多不合理的图结构，甚者在某些业务场景中不存在关系信息，这样的话，在最初达不到完整网络的情况时，通常会使用KNN的方式对网络进行初始化，然后再去学习一个更加合理的网络结构，最终达到一个更好节点预测、聚类的目的。</p>
<p><strong><font color="red"> 实际上，在许多业务场景当中会存在许多不合理的图结构，甚者在某些业务场景中不存在关系信息，这样的话，在最初达不到完整网络的情况时，通常会使用KNN的方式对网络进行初始化，然后再去学习一个更加合理的网络结构，最终达到一个更好节点预测、聚类的目的。</font></strong></p>
<h3><span id="四-总结思考">四、<strong>总结思考</strong></span></h3><p>下面分享几点在算法落地以及算法选择中的一些工作总结与思考：</p>
<ul>
<li><strong><font color="red"> 针对图算法这块，特征工程和图的构建方式是非常重要的</font></strong>。如果图的结构不合理的话，即使算法模型再强大、特征工程处理得再好，算法训练出的结果也不是最终理想的效果；</li>
<li><strong><font color="red"> 多数业务场景的区分度是不一样的，不存在一个普适的算法可以解决所有业务场景存在的问题</font></strong>，如上述的FastUnfolding、node2vec在某些特定的业务场景下效果可以比GraphSAGE的效果更好，所以在面临具体问题的时候，需要结合场景作算法选择以及优化；</li>
<li><strong>在工业界落地的算法通常比较直接、明了，这样的算法往往效果更好</strong>。</li>
</ul>
<h3><span id="五-graphsage应用">五、GraphSAGE应用</span></h3><p>本例中的训练，评测和可视化的完整代码在下面的git仓库中</p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/shenweichen/GraphNeuralNetwork">shenweichen/GraphNeuralNetworkgithub.com/shenweichen/GraphNeuralNetwor</a></p>
<p><strong>这里我们使用引文网络数据集Cora进行测试，Cora数据集包含2708个顶点, 5429条边,每个顶点包含1433个特征，共有7个类别。</strong></p>
<p>按照论文的设置，从<strong>每个类别中选取20个共140个顶点作为训练</strong>，<strong>500个顶点作为验证集合</strong>，<strong>1000个顶点作为测试集</strong>。 <strong>采样时第1层采样10个邻居，第2层采样25个邻居。</strong></p>
<ul>
<li>节点分类任务结果</li>
</ul>
<p>通过多次运行准确率在0.80-0.82之间。</p>
<ul>
<li>节点向量可视化</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191533940.jpg" alt="img" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>风控反欺诈（3）Fraudar：二部图反欺诈</title>
    <url>/posts/VSDR5Z/</url>
    <content><![CDATA[<h2><span id="fraudar二部图反欺诈">Fraudar：二部图反欺诈</span></h2><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/172423556">Fraudar：二部图反欺诈</a></p>
<p>  <a href="https://zhuanlan.zhihu.com/p/369534469">图异常簇检测：<em>FRAUDAR</em></a></p>
<p>  <a href="https://zhuanlan.zhihu.com/p/528366101">京东图计算团队：一文读懂电商广告作弊与反作弊</a></p>
<p>  <strong>图异常检测系列文章：</strong></p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/348278101">图异常点检测：OddBall</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/348713075">图异常簇检测：LOCKINFER</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/369534469">图异常簇检测：FRAUDAR</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/380421374">图异常检测：GDN和Meta-GDN</a></li>
</ol>
</blockquote>
<p>可以说电商的发展，滋生并带火了一个由出资店铺、刷单中介、各级代理、刷手、空包物流组成的刷单产业。</p>
<h3><span id="一-黑产与反欺诈">一、黑产与反欺诈</span></h3><h4><span id="关系网络与二部图"><strong>关系网络与二部图</strong></span></h4><p><strong>不同于对个体自身特征的分析，网络提供了对多个对象的关系之间另一种看问题的视角</strong>。把对象看做结点，把交互看成边，对象间的发生的各种关联自然会构成一张关系网络。<strong>从图论的角度出发，根据结点属性的不同可以把网络分为同构图和异构图</strong>。同构图是由同一种结点组成的关系网络，如家庭亲属关系、社交好友关系、论文间的引用关系等。历史上对于同构图的网络表示有很多研究，早在十九世纪就形成了几何拓扑学这一数学分支。在现代的同构关系研究中也逐步提出了基于网页链接的谷歌PageRank网页评级、基于结点关系紧密度的Louvian社区发现等重要算法。<strong>不过异构图在生活中的表现更为广泛，异构图是由不同属性的结点组成的关系网络，如由买方卖方以及中介组成的交易网络、由大V和其关注者组成的关注网络、由手机号\地址\证件号\IP地址组成的复杂关系网络等。</strong>二部图（也叫二分图）是异构网络的一种，它由两类结点组成，并且同类结点之间通常没有关联。前述的刷单欺诈，即是以<strong>出资店铺</strong>和<strong>刷手</strong>这两类结点组成的<strong>交易关系二部图</strong>。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191534576.jpg" alt="img" style="zoom: 67%;"></p>
<h4><span id="二部图下的欺诈"><strong>二部图下的欺诈</strong></span></h4><p>两类结点组成的欺诈场景还可以举出很多例子，如电商场景下<strong>用户对商户的薅羊毛、刷好评</strong>，如<strong>社交场景下水军账号的虚假关注、转发</strong>，又如<strong>消金场景下用户与商户勾结对平台的消费贷套现欺诈</strong>。<strong><font color="red"> 这些行为都会使两类结点之间出现异常的连接分布，从整体网络看来其呈现出了一张致密的双边连接子图，且该子图内的结点与图外结点联系相对较少</font></strong>。我们把这种大量的、同步的非正常关联行为模式称之为Lockstep，即在本不应出现聚集行为的二部图自然关系网络中，出现了双边聚集性行为。</p>
<p>只要能把欺诈行为总结成一种模式，自然可以将其分离出来。但是欺诈者往往会对自己做出某种伪装以使自己看起来有向好的一面，意图绕过风控系统。如在刷单欺诈场景下，<strong>为了尽量贴近真实用户的购买习惯，刷单平台会对刷手提出一系列要求</strong>，比如要求货比三家、要求最低浏览时长、要求滚动浏览高度及停留时间、要求对于正常热销商品做一定购买等，这些行为都会导致风控经验指标和模型特征的部分失效。在二部图交易网络中，对于正常热销商品的购买体现为刷手为自己增加了一些优异的边连接，使自己看起来更像一个正常的消费者结点。<strong><font color="red"> 我们需要一种能从这种复杂关系网络中对抗伪装、抽丝剥茧的提取出异常致密子图的算法。</font></strong>接下来对症下药引入Fraudar。<strong>Fraudar算法来源于2016年的KDD会议，并获得了当年的最佳论文奖。</strong></p>
<h3><span id="二-fraudar算法介绍-无监督异常簇检测">二、<strong>Fraudar算法介绍</strong>- 无监督异常簇检测</span></h3><blockquote>
<p>  <a href="https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/pdf/10.1145/2939672.2939747">FRAUDAR: Bounding Graph Fraud in the Face of Camouflagedl.acm.org/doi/pdf/10.1145/2939672.2939747</a></p>
</blockquote>
<p><strong>简单来说，Fraudar定义了一个可以表达结点平均可疑度的全局度量G(·)</strong>，在逐步贪心移除可疑度最小结点的迭代过程中，使G(·)达到最大的留存结点组成了可疑度最高的致密子图。接下来我们稍微细化一下算法过程，以刷单交易场景为例（定义二部图结点集合S=[A,B]，其中A、B分别代表消费者和店铺的结点集合），看Fraudar是如何从交易网络剥离出刷手和其出资店铺的。</p>
<p>网络水军、刷量刷单等行为在互联网中屡见不鲜，如何检测网络中的该类行为，即<strong>异常簇，</strong>是值得研究的问题。<strong>本文介绍一种图异常簇检测方案-FRAUDAR，该方法在具有14.7亿条边的Twitter社交网络中成功检测出了一系列刷量账户。</strong></p>
<h4><span id="21-论文贡献"><strong>2.1 论文贡献</strong></span></h4><ol>
<li>提出了一组满足公理的指标，且兼具多种优点。</li>
<li>提出了一个可证明的界限，即某个欺诈账户可以在图中有多少欺诈行为而不被抓获，即使是在伪装的情况下。接着通过新的优化改进了该界限，使其能够更好地区分真实数据中的欺诈行为和正常行为。</li>
<li>该算法在超大规模的Twitter网络中被证明了是有效的且能够在<strong>接近线性时间复杂度</strong>内完成任务。</li>
</ol>
<h4><span id="22-欺诈行为的特点">2.2 欺诈行为的特点</span></h4><p>网络中存在不少水军刷量刷单的现象，该类欺诈行为在社交网络中尤其显著，其中”刷量者”可称为<strong>follower</strong>，”刷单目标”可以称为<strong>followee。</strong>下图展示了Twitter中的“僵尸粉”购买服务案例：左图红色和蓝色颜色的柱子分别代表了正常用户和欺诈用户，用户所属的两个柱子中左柱子表示followee的数量，右柱子表示follower的数量；右图展示了某个刷量账号，其个人简介中表示“只要你follow他那么他就会follow你”。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191534862.jpg" alt="img" style="zoom:50%;"></p>
<p>欺诈行为在网络的<a href="https://www.zhihu.com/search?q=邻接矩阵&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;369534469&quot;}">邻接矩阵</a>视角下具有显著特征，某些“聪明”的刷量分子为了逃避检测往往会采取伪装（camouflage）来让自己看起来更像普通用户。下图从左到右分别展示了邻接矩阵中的随机伪装者、偏置伪装者和被劫持账户参与刷量行为。</p>
<h4><span id="23-fraudar算法">2.3 FRAUDAR算法</span></h4><p>FRAUDAR算法基于二部图表示，即网络中存在两种类型的节点Users和Objects。文中假设可能存在一个或多个User受到相关某个实体的控制，进而与Objects的某个子集交互而产生连边。算法所需的符号表示如下：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191535249.jpg" alt="img" style="zoom:50%;"></p>
<p>算法的目标是找到一个 $S$, 并使得 $S$ 构成子图的嫌疑指标 $g(S)$ 最大。</p>
<p>子图内节点嫌疑度，即密集度指标 (density metrics) 可描述为： $g(S)=\frac{f(S)}{|S|}$</p>
<p>子图总嫌疑度 (total suspiciousness) 可描述为: </p>
<script type="math/tex; mode=display">
\begin{aligned} f(S) & =f_\nu(S)+f_{\varepsilon}(S) \\ & =\sum_{i \in S} a_i+\sum_{i, j \in S \wedge(i, j) \in \varepsilon} c_{i j}\end{aligned}</script><p>其中, $a<em>i$ 和 $c</em>{i j}$ 都是大于 0 的常数, $a<em>i$ 即某个节点 $i$ 的嫌疑度, 而 $c</em>{i j}=\frac{1}{\log \left(d_j+c\right)}$ 可即边 $(i, j)$ 的嫌疑度, $d_j$ 表示Object $j$ 的度, $c$ 是一个较小的常量。</p>
<p><strong>由此，FRAUDAR算法可描述为如下过程</strong>：</p>
<ol>
<li>将优先级最高的节点移出二部图</li>
<li>更新与移出节点相关的节点可疑度</li>
<li>反复执行步骤1和步骤2，直至所有节点都被移出</li>
<li>最后比较各轮迭代中节点的可疑度，找到最大可疑度对应的子图</li>
</ol>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191535525.jpg" alt="img" style="zoom:50%;"></p>
<p>算法中有如下两点值得注意：</p>
<p><strong>1. 如何构造<a href="https://www.zhihu.com/search?q=优先树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;369534469&quot;}">优先树</a></strong></p>
<p>优先树本质上是一个小顶堆，其叶子节点为二部图中的Users或Objects，排序规则依赖于节点的嫌疑度。由此，根结点将记录全局嫌疑度最小的节点，将根节点从二部图中移出使得新子图中的节点具有最大的平均嫌疑度。</p>
<p><strong>2. 如何更新可疑度</strong></p>
<p>删除网络中的节点导致<a href="https://www.zhihu.com/search?q=二部图&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;369534469&quot;}">二部图</a>结构被改变，因此每轮迭代后都需要在保持边可疑度不变的前提下，更新与被删除节点相关的节点可疑度，即相关节点减去被删节点的嫌疑度。</p>
]]></content>
      <categories>
        <category>工业落地</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意加密流量（1）DataCon2020恶意加密流量检测</title>
    <url>/posts/3G00B93/</url>
    <content><![CDATA[<h2><span id="datacon2020-恶意加密流量检测">DataCon2020-恶意加密流量检测</span></h2><blockquote>
<ul>
<li>思科AISec’16：《Identifying Encrypted Malware Traffic with Contextual Flow Data》：<a href="https://www.cnblogs.com/bonelee/p/9604530.html">https://www.cnblogs.com/bonelee/p/9604530.html</a></li>
</ul>
</blockquote>
<h2><span id="一-流量处理的工具">一、流量处理的工具</span></h2><h3><span id="zeek"><strong>zeek</strong></span></h3><p><a href="https://darkdefender.medium.com/https-medium-com-melanijan93-analysing-pcaps-with-bro-zeek-33340e710012">使用 Bro/Zeek 分析 PCAP</a></p>
<p><a href="https://www.freebuf.com/sectool/235587.html">流量分析的瑞士军刀：Zeek</a></p>
<h3><span id="joy">joy</span></h3><p><a href="https://github.com/ahlashkari/CICFlowMeter">CICFlowMeter</a>：</p>
<h2><span id="二-a-novel-malware-encrypted-traffic-detectionframework-based-on-ensemble-learning">二、A Novel Malware Encrypted Traffic DetectionFramework Based On Ensemble Learning</span></h2><blockquote>
<p>  zeek:<a href="https://docs.zeek.org/en/master/logs/index.html">https://docs.zeek.org/en/master/logs/index.html</a><br>  zeek-flowmeter:<a href="https://github.com/zeek-flowmeter/zeek-flowmeter">https://github.com/zeek-flowmeter/zeek-flowmeter</a><br>  zeek-tls-log-alternative:<a href="https://github.com/0xxon/zeek-tls-log-alternative">https://github.com/0xxon/zeek-tls-log-alternative</a></p>
</blockquote>
<h4><span id="概述">概述：</span></h4><p>在我们的工作中，有一些定义。首先，<strong>将5元组定义为源IP、源端口、目标IP、目标端口和协议</strong>。<strong>其次，前向流是从客户端到服务器具有相同5元组的数据包集</strong>。反向词流与正向流相似，但与五元组相反。最后，流动是向前流动和向后流动的结合。由于我们从流量中提取的特征是异构的，我们不能简单地将它们合并并以常见的方式将它们装配到单个分类器中。我们将不同的特征匹配到相应的分类器中，并进行多数投票以获得最终结果。同时，我们考虑了flowlevel和host级别的行为。对不同级别的特性进行聚合和分析，以提高框架的TPR和较低的FPR。图1：。概述了我们的框架。我们将介绍每个特性以及相应的分类器、实现和性能。</p>
<h3><span id="21-数据包级">2.1 数据包级</span></h3><h4><span id="1长度分布">（1）长度分布</span></h4><p>根据Cisco的研究【17】，<strong>恶意软件和普通软件在正向流和反向流中的数据包长度分布不同</strong>。<strong><font color="red"> 例如，当我们使用谷歌搜索时，客户端向服务器发送少量数据包，然后服务器返回大量数据包。然而，恶意软件的作用恰恰相反：恶意软件通常让客户端将数据传输到服务器，然后服务器定期返回调度命令。</font></strong>无论是否加密，数据包长度始终可见，因此它适合作为一种功能。我们将数据包长度和方向编码为一维独立特征。我们推测，感染恶意软件的客户端和服务器之间的一些控制消息的长度总是相似且频繁的，这具有很好的区分程度。<strong>我们考虑每个可能的数据包长度和方向元组。由于Internet上的最大传输单元（MTU）是1500字节，并且数据包的方向有两个发送或接收方向，因此我们的长度分布特征是3000维。</strong>为了提取这些特征，我们计算具有不同长度的所有数据包的数量，并进行规范化以确保概率分布。我们使用随机森林（RF）算法来处理这些特征。因为它能更好地处理高维特征，并且具有可解释性。</p>
<h4><span id="2长度序列">（2）长度序列</span></h4><p>第二部分，在不使用序列信息的情况下，我们只使用了数据包长度的统计特征，这可能会在时间上丢失一些信息，因此我们提取了数据包长度序列。<strong>我们在每个客户端的双向上取前1000个数据包长度</strong>，并将其放入TextCNN算法中以提取局部序列关系。因为该算法运行速度快，精度高。文本数据的卷积神经网络TextCNN【22】是一种用于句子分类任务的有用的深度学习算法。<strong>在这种情况下，我们将每个数据包的长度视为一个单词，长度序列相当于一个句子</strong>。</p>
<h4><span id="3服务器ip">（3）服务器IP</span></h4><p>在我们的数据集中，服务器IP地址是一个重要的标识符。<strong>我们假设，在同一地区，如果客户端感染了相同的恶意软件，则可能会导致其访问相同的服务器IP地址</strong>。因此，我们还考虑了对服务器IP地址的访问。<strong>值1或0表示是否访问了特定的服务器IP地址（一个热编码）</strong>。我们使用朴素贝叶斯（NB）算法来处理这些特征。由于朴素贝叶斯算法是一种非参数算法，其本质是寻找特征和标签之间的关系。因此，它可以被视为一个黑名单。</p>
<h4><span id="4词频分类器">（4）词频分类器</span></h4><p><strong>X509证书在Internet上广泛使用</strong>。它们用于验证实体之间的信任。证书颁发机构通常将X509证书链接在一起。如图2所示。[23]，<strong>X509证书提供URL、组织、签名等信息</strong>。我们从培训集中每个客户端的TLS流中提取X509证书链，并获取证书中<strong>主题</strong>和<strong>颁发者</strong>中包含的单词。我们将所有单词组合在一起，并将客户的流量视为由这些单词组成的句子。与B部分类似，我们计算每个单词的数量并将其用作特征。0我们使用朴素贝叶斯（NB）算法来处理这些特征。如果测试集样本证书中的所有单词从未出现在训练集中，我们将直接推断它是恶意的。因为训练集包含最流行的域名。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182036362.png" alt="image-20220621131035005" style="zoom: 67%;"></p>
<h4><span id="5tcp状态马尔可夫">==（5）TCP状态马尔可夫==</span></h4><p>我们发现<strong>恶意流量和正常流量之间TCP连接状态的分布是不同的</strong>。表一说明了可能的TCP连接状态[24]:</p>
<blockquote>
<p>  [24]:</p>
</blockquote>
<p>我们按照流出现的时间对其进行排序，然后使用马尔可夫随机场转移矩阵（MRFTM）对该特征进行编码。MRFTM在建模连接状态序列时很有用。MRFTM[i，j]中的每个条目统计第i个和第j个状态之间的转换次数。最后，我们对MRFTM的行进行规范化，以确保适当的马尔可夫链。然后我们将其重塑为一维向量，也就是说，我们使用MRFTM的条目作为特征。我们使用随机森林（RF）算法来处理这些特征。</p>
<h3><span id="22-会话流级">2.2 会话流级</span></h3><h4><span id="6会话流量统计">（6）会话流量统计</span></h4><p>在加密流量中，上述5种分类器在主机级使用不同的特征提取方法和分类方法。此外，为了进一步提高准确率，防止恶意软件由于缺乏领域知识而欺骗分类器，我们还提取了TLS握手中的明文信息。在这个分类器中，我们首先考虑流级特征。我们仅选择TLS流，并分析每个流。一旦推断流是恶意的，就会推断相应的客户端被感染。我们对TCP和TLS协议进行了深入分析，<strong>提取了1000多个维度的流级特征</strong>，包括以下部分：</p>
<ul>
<li><strong>TCP连接状态特性</strong>：如F部分所述，我们对每个流的TCP连接状态进行一次热编码。</li>
<li><strong>统计特征</strong>：我们还提取常规统计特征，表II显示了相关特征名称和描述。</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037399.png" alt="image-20220621141516374" style="zoom: 50%;"></p>
<ul>
<li><strong>长度马尔可夫特征</strong>：数据包长度序列的操作类似于F部分中的TCP连接状态序列。长度值被离散为大小相等的容器。长度数据马尔可夫链有10个箱子，每个箱子150字节。假设一个1500字节的MTU，任何观察到的大小大于1350字节的数据包都被放入同一个bin中。</li>
<li><strong>TLS握手功能</strong>：我们发现客户端和服务器的TLS协议版本在恶意和良性TLS流之间有不同的分布，因此我们对客户端和服务器的TLS版本进行了一次热编码。此外，由于恶意软件可能使用旧的密码套件，我们在客户端和服务器上都对密码套件和扩展进行n-hot编码，即将所有密码套件和扩展扩展扩展为一维0向量，如果当前流使用某个密码套件或扩展，则相应的位置集值为1。</li>
<li><p><strong>TLS证书特性</strong>：我们发现，在恶意流中，很大一部分叶证书是自签名的，或者自签名证书出现在证书链中。恶意软件喜欢利用的自签名证书的成本很低。因此，我们分析从服务器发送的证书：<strong>证书链是否包含自签名证书、叶证书是否过期、证书版本、证书有效期、公钥长度、是否发生警报</strong>。同时，考虑到之前的词频分类器，我们发现一些词无法区分恶意和良性，因此我们还将流词频添加到流特征中。</p>
</li>
<li><p>合并和适配：我们将上述功能合并到5元组（源IP、目标IP、源端口、目标端口和协议）中，并将其适配到随机林（RF）算法中。一旦流被推断为恶意，相应的客户端就会被推断为受到恶意影响。</p>
</li>
</ul>
<p>在本节中，我们使用来自真实恶意软件的加密流量来评估我们的框架。首先，我们描述了数据集的格式。其次，我们分析了每个分类器的有效性以及集成后的有效性。然后，我们展示了我们的集成学习方法的优势。</p>
<h4><span id="数据表示">数据表示：</span></h4><p>如表三所示，列车数据包括3000台正常主机产生的正常流量和3000台恶意软件感染主机产生的恶意流量。我们的任务是检测测试数据中的客户端IP地址是否感染了恶意软件。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037660.png" alt="image-20220621132459446" style="zoom:50%;"></p>
<p><strong>表V显示了每个分类器的得分</strong>。我们将每个分类器与投票结果进行比较，发现集成后的最终效果优于任何单个分类器的得分。我们的方法得分为86.6，TPR为95.0%，FPR为8.4%。流量统计分类器得分最高，其次是长度序列分类器和长度分布分类器。我们在每个分类器中使用默认参数。由于流级特征包含最丰富的信息，因此<strong>流统计分类器的性能最好</strong>，正如我们所期望的那样。我们可以推断，<strong>数据包长度</strong>是一个重要特征。事实上，在流量分类（非恶意流量检测）领域，有很多优秀的作品使用数据包长度进行分类，并取得了很好的效果[11][25]。目前，已有的一些基于机器学习的工作[8][9]与我们的流统计分类器或流合并分类器相似，也就是说，它们只使用流级特征或主机级特征。经过实验，我们的最终效果已经超过了每一个分类器。这表明我们的框架是有效和健壮的。此外，框架可以很容易地调整每个分类器的概率阈值，以提高TPR或降低FPR，以适应不同的场景。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037087.png" alt="image-20220621132751726" style="zoom:50%;"></p>
<h2><span id="三-知乎-s1mple">三、知乎-s1mple</span></h2><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/262533938"><em>DataCon</em>2020<em>加密</em>恶意<em>流量</em>检测初赛Writeup及总结反思</a></p>
</blockquote>
<ul>
<li><p><strong>特征选择（Feature Selection）</strong>，选取对于区分正常/恶意流量有明显作用的 Features。</p>
<ul>
<li><strong>TLS客户端指纹信息</strong><ul>
<li><strong>客户端支持的加密套件数组</strong>（Cipher suites），<strong>服务器端选择的加密套件</strong>。</li>
<li><strong>支持的扩展</strong>（TLS extensions），若分别用向量表示客户端提供的密码套件列表和 TLS 扩展列表，可以从服务器发送的确认包中的信息确定两组向量的值。</li>
<li><strong>客户端公钥长度</strong>（Client public key length），从密钥交换的数据包中，得到密钥的长度。</li>
<li><strong>Client version</strong>，the preferred TLS version for the client</li>
<li><strong>是否非CA自签名</strong>，统计数据表示，恶意流量约70%出现非CA认证服务器且自签名的情况，非恶意流量约占0.1%。此项判断的依据是：未出现 <code>CA: True</code> 字段（默认非 CA 机构）且 <code>signedCertificate</code> 中的 <code>issuer</code> 字段等于 <code>subject</code> 字段。</li>
</ul>
</li>
</ul>
<blockquote>
<p>  在进行TLS握手时，会进行如下几个步骤：</p>
<ol>
<li><strong>Client Hello</strong>，客户端提供支持的加密套件数组（cipher suites）；</li>
<li><strong>Server Hello</strong>，由服务器端选择一个加密套件，传回服务器端公钥，并进行认证和签名授权（<strong>Certificate</strong> + Signature）；</li>
<li>客户端传回客户端公钥（<strong>Client Key Exchange</strong>），客户端确立连接；</li>
<li><p>服务器端确立连接，开始 HTTP 通信。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037145.jpg" alt="img" style="zoom:50%;"></p>
</li>
</ol>
</blockquote>
<ul>
<li><strong>数据包元数据</strong><ul>
<li><strong>数据包的大小</strong>，数据包的长度受 UDP、TCP 或者 ICMP 协议中数据包的有效载荷大小影响，如果数据包不属于以上协议，则被设置为 IP 数据包的大小。</li>
<li><strong>到达时间序列</strong></li>
<li><strong>字节分布</strong></li>
</ul>
</li>
<li><strong><font color="red"> HTTP头部信息</font></strong><ul>
<li><strong>Content-Type</strong>，正常流量 HTTP 头部信息汇总值多为 <code>image/*</code>，而恶意流量为 <code>text/*、text/html、charset=UTF-8</code> 或者 <code>text/html;charset=UTF-8</code>。</li>
<li><strong>User-Agent</strong></li>
<li><strong>Accept-Language</strong></li>
<li><strong>Server</strong></li>
<li><strong>HTTP响应码</strong></li>
</ul>
</li>
<li><strong><font color="red"> DNS响应信息</font></strong><ul>
<li><strong>==域名的长度==</strong>：正常流量的域名长度分布为均值为6或7的高斯分布（正态分布）；而恶意流量的域名（FQDN全称域名）长度多为6（10）。</li>
<li><strong>==数字字符及非字母数字(non-alphanumeric character)的字符占比==</strong>：正常流量的DNS响应中全称域名的数字字符的占比和非字母数字字符的占比要大。</li>
<li><strong>DNS解析出的IP数量</strong>：大多数恶意流量和正常流量只返回一个IP地址；其它情况，大部分正常流量返回2-8个IP地址，恶意流量返回4或者11个IP地址。</li>
<li><strong>TTL值</strong>：正常流量的TTL值一般为60、300、20、30；而恶意流量多为300，大约22%的DNS响应汇总TTL为100，而这在正常流量中很罕见。</li>
<li><strong>域名是否收录在Alexa网站</strong>：恶意流量域名信息很少收录在Alexa top-1,000,000中，而正常流量域名多收录在其中。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>特征提取（Feature Extraction）</strong>，从 pcap 文件中提取上述 Features，并转换为模型训练所需要的格式。我们选择的特征提取工具为 <strong>Zeek</strong>。特征提取我们采用的工具是 <strong>Zeek</strong>，它的前身是 Bro，一款网络安全监视（Network Security Monitoring）工具，它定义了自己的 DSL 语言，<strong>支持直接处理 pcap 文件生成各类日志文件</strong>，包括 dns、http、smtp 等：Zeek 网上有一些现成的脚本，我们采用的是 <strong>Zeek FlowMeter</strong>，它基于 OSI 七层协议的网络层和传输层，可以分析并生成一些 Packets 到达时间序列、Packet 字节大小和元数据等新特征。<strong>在使用时，我们需要在 <code>local.zeek</code> 配置文件中加入 <code>@load flowmeter</code>，这样 Zeek 在执行时会加载 <code>flowmeter.zeek</code> 并生成对应的 <code>flowmeter.log</code>，下面列出了 FlowMeter 提取出的一些特征，包括上下行包总数、包负载均值方差等。</strong>其他详细的特征请见 <a href="https://link.zhihu.com/?target=https%3A//github.com/zeek-flowmeter/zeek-flowmeter">zeek-flowmeter GitHub官方文档</a>。除 <code>flowmeter.log</code> 之外我们还需要关注 <code>conn.log</code>、<code>ssl.log</code> 和 <code>X509.log</code>。这几个日志共同字段 <code>uid</code> 是 Zeek 根据一次连接的源/目的 IP、源/目的端口四元组生成的唯一 ID。为了方便后续的处理，我们将这几个日志文件统一读入，使用 <code>uid</code> 字段连接后转成 csv 格式输出到文件。<strong>最终我们提取的特征如下：</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037409.png" alt="image-20220621141516374" style="zoom: 50%;"></p>
</li>
<li><p><strong>模型训练（Model Training）</strong>，选择合适的机器学习模型对三类 pcap 文件进行训练和预测。我们选择的 Python 机器学习库为 <strong>scikit-learn</strong>。</p>
<ul>
<li><strong>Anomaly Detector</strong>：在 black 数据集中同时存在恶意流量和正常流量，没有明确的标注，无法直接用于训练分类器。而 white 数据集中都是正常流量，可以先用 white 数据集来训练一个 Anomaly Detector 分类器。然后用这个分类器在 black 数据集中推理得到哪些是恶意流量。我们的模型选取的隔离森林 <code>IsolationForest</code>。</li>
<li><strong>Misuse Detector</strong>：<strong>假设这些由异常检测器识别的可疑流量是恶意流量，我们就有了恶意流量的标注</strong>。接下来我们用这些恶意流量 labels，结合 white 数据集中的正常流量 labels，来训练一个 Misuse Detector。我们选取了 XGBoost 基于树的模型，目标选取为多分类问题（分类数为2）。</li>
</ul>
</li>
</ul>
<h2><span id="四-加密恶意流量检测方向清华大学hawkeye战队"><strong><font color="red">四 、加密恶意流量检测方向（清华大学HawkEye战队）</font></strong></span></h2><blockquote>
<p>  <strong>清华大学HawkEye战队</strong>：<a href="https://datacon.qianxin.com/blog/archives/122">https://datacon.qianxin.com/blog/archives/122</a></p>
</blockquote>
<h3><span id="41-概述">4.1 概述</span></h3><p>我们所采用的检测方法的总体结构是让多个分别利用不同的<strong>异构特征训练</strong>而成的分类器进行多数投票 (Majority Voting) 的方式来获取最终的判定结果。</p>
<p><strong>由于我们所采用的多种特征是异构数据， 且具有不同的组织特点，我们并没有直接采用将这些特征统一编码并输入到集成学习分类器中的常规方式，而是针对各个特征的特点分别构建对应的分类器</strong>，并利用他们的分类结果进行投票，最终取得多数票的分类结果被定为最终的分类结果。</p>
<p>参与投票的多个模型中部分使用了多维特征综合分析，<strong>另一部分使用经过分析后黑白样本区分较大的、置信度较高的单维特征对多维特征中的潜在的过拟合和判断错误进行消解</strong>。同时，我们考虑到了<strong>数据包级、流级、主机级多维度的行为建模</strong>，将不同层次的数据进行聚合分析，提升对于黑白样本建模的准确度。图<a href="applewebdata://7EAAA994-C40A-4F1C-A798-46DA93A65C37#_bookmark0">1</a>展 示了我们方案的整体流程。下面我们将分别介绍我们所采用的特征及对应的分类器、投票机制、 实现和性能以及展望和总结。</p>
<h3><span id="42-特征选取与子分类器">4.2 <strong>特征选取与子分类器</strong></span></h3><p>下面我们将详细介绍六个参与投票的子模型中的每一个模型所采用的特征及对应的分类器， 以及设计的动机和意义。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037655.png" alt="image-20220621153938899" style="zoom: 50%;"></p>
<h4><span id="1包长分布特征3000维">（1）<strong>包长分布特征</strong>（3000维）</span></h4><p>不同的软件在通信数据的体量上具有不同的特点，我们认为功能或实现相似的软件会具有 相同的数据包体量分布特点，正如视频软件的下行流量通常远大于上行流量，而恶意键盘记录 器的上行流量总是远大于下行流量一样。</p>
<p>将每一个可能的报文长度和方向的二元组视为包长分布特征中的一个独立的维度。由 于以太网最小帧长为64 字节，而<strong>互联网的最大传输单元通常是 1500 字节，报文的方向只有收发两个方向</strong>，我们的包长分布特征是一个维数约 3000 的概率分布。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037030.png" alt="image-20220623173625843" style="zoom:50%;"></p>
<p><strong>编码方式与分类器</strong>：</p>
<p><strong>对该特征的提取，我们采用 以频率估计概率的方式，统计每个流量样本中的各个长度和方向的报文的数量，并除以报文总 数得到其概率分布</strong>。</p>
<p>由于包长分布的特征本质上是一个离散概率分布，最直接的分类器应该是基于度量的分类 算法，通过度量分布之间的相似性大小来判断其类别所属。<strong>我们采用了 Hellinger 距离的度量方式与kNN分类算法，Hellinger 距离是在概率分布空间对欧氏距离的模拟</strong>。由于kNN算法并不需要训练过程，而在分类过程中需要计算待分类的样例与已有样本集中的每个样例的相似性度量结果，因此其运行效率十分低下，于是我们也考虑了采用随机森林的分类器来利用这一特征的方式。</p>
<h4><span id="2证书主体与签发者黑白名单特征">（2）<strong>证书主体与签发者黑白名单特征</strong></span></h4><p>我们认为决定一个软件是否是<strong>恶意软件不仅仅取决于其通信内容，也取决于其通信的对象</strong>， 而通信内容由于加密无法作为可辨识的特征，因此我们着重考虑了客户端流量样本中的通信对象。</p>
<p><strong><font color="red"> 在 TLS 建立连接的过程中，服务端发来的证书中的叶子证书的 Subject 字段表明了客户端的直接通信对象，而 Issuer 字段则表明了该证书的直属签发机构。</font></strong></p>
<p><strong>Subject 字段里的 common name 通常是一个域名</strong>，我们认为这将是一个很重要的标识信息，因为恶意软件与恶意域名关联的可 能性较大。图3中展示了恶意和正常证书中主体和签发者的情况，可以看出黑白样本有明显区别 并都存在着访问频次较高的项。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037876.png" alt="image-20220623191438824" style="zoom:50%;"></p>
<p><strong>编码方式与分类器</strong>：</p>
<p><strong>我们对训练集中的每个流量样本统计了其中的叶子证书所涉及到的不同 Subject 和 Issuer 的 数量，并记录每个流量样本与他们通信的频数。</strong></p>
<p><strong>X509证书在Internet上广泛使用</strong>。它们用于验证实体之间的信任。证书颁发机构通常将X509证书链接在一起。如图2所示。[23]，<strong>X509证书提供URL、组织、签名等信息</strong>。我们从培训集中每个客户端的TLS流中提取X509证书链，并获取证书中<strong>主题</strong>和<strong>颁发者</strong>中包含的单词。我们将所有单词组合在一起，并将客户的流量视为由这些单词组成的句子。与B部分类似，我们计算每个单词的数量并将其用作特征。0我们使用朴素贝叶斯（NB）算法来处理这些特征。如果测试集样本证书中的所有单词从未出现在训练集中，我们将直接推断它是恶意的。因为训练集包含最流行的域名。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037096.png" alt="image-20220621131035005" style="zoom: 67%;"></p>
<h4><span id="3通信-ip-地址黑白名单特征">（3）<strong>通信</strong> <strong>IP</strong> <strong>地址黑白名单特征</strong></span></h4><p>除了证书中的 Subject 和 Issuer 信息外，服务端 IP 地址是另一个表示软件通信对象的标识 符。有少数流量样本中出现了缺少证书的情况，且我们认为同一地区遭受同类恶意软件感染很 可能会造成他们对相同的 IP 地址的访问，因此我们将流量样本中的远程 IP 地址的访问情况也 作为一个判断是否包含恶意流量的依据。</p>
<h4><span id="4流级荷载无关特征提取"><strong><font color="red"> （4）流级荷载无关特征提取</font></strong></span></h4><blockquote>
<p>  对于黑样本的流标记问题存在一定问题，由于被感染的主机也可能会进行正常通信，而且 恶意软件也可能出现正常行为</p>
</blockquote>
<p>我们首先考虑流级特征，由于 TCP 连接可以由流的四元组确定，所以恶意通信一般是指流 级别的，分析每条流的黑或白是十分合理的。我们综合考虑了已有相关工作 [<a href="applewebdata://CE580100-416D-47E9-B704-9182656D72ED#_bookmark4">1</a>–<a href="applewebdata://CE580100-416D-47E9-B704-9182656D72ED#_bookmark5">3</a>]，以及对 TCP、 TLS/SSL 协议进行深入分析，共提取了超过 1000 维与荷载无关的流级特征，包含如下几部分:</p>
<ul>
<li><strong>元数据（Metadata）</strong>：即单条流的基本统计数据，包含持续时间（duration）、总的流入（inbound）/流出（outbound）的字节数、数据包个数；</li>
<li><strong>窗口序列统计特征</strong>：我们对出/入流量分别维护了包<strong>时间间隔</strong>和<strong>包长度的窗口</strong>，每个窗口提取统计特征，包含<strong>平均值、标准差、最大值、最小值</strong>；除此而外，我们发现仅仅关注一整个窗口的特征粒度不够细化，为了更加精细化的对流量行为进行捕获和建模，我们关注窗口中的每个数据包，并使用马尔科夫转移矩阵的方式捕获相邻包之间的关系。以构建包长窗口的马尔科夫矩阵为例，我们首先将包长均匀分成 15 个桶（bin），然后建立 15×15 的矩阵，每个元素表示从行转移到列所代表的的桶，最后对矩阵进行行的归一化处理，将其转化为概率形式；</li>
<li><strong>TLS/SSL</strong> <strong>握手包特征</strong>：首先我们发现本赛题中客户端和服务器端都出现了多种不同的协议类型，而黑和白训练集中客户端和服务器不同版本的分布是不同的，因此协议版本可以 作为一个特征，我们将客户端和服务器端使用的 TLS 版本进行独热（one-hot）编码；同时 我们关注到 Hello 包中 GMT Unix Time 字段在黑/白训练集中的分布也有所不同，因此我 们将客户端和服务器端的 GMT Unix Time 的是否存在、是否使用随机时间编码为 0/1 的特征；此外，<strong>由于恶意软件和恶意服务器使用的加密套件和列表</strong>很有可能与正常的加密流量 有所区别，<strong><font color="red"> 我们将客户端与服务器端的加密套件（列表）和扩展列表进行独热编码，即将 所有可能的加密套件和扩展列出展成一维 0 向量，当前流使用的套件和扩展对应的位置赋 值为 1；</font></strong></li>
<li><strong>TLS/SSL证书特征</strong>：服务器端证书对于区分正常和恶意流量有着重要的作用，如恶意通信的证书多采用自签名的方式。因此我们对服务器端证书进行了特征挖掘，主要选用如下的 0/1 特征：<strong><font color="red"> 是否自签名、是否过期、版本号、证书有效期、公钥长度；</font></strong>同时，我们考虑到之 前模型发现有些<strong>证书主体域名</strong>单一使用无法区分黑/白的情况，因此也嵌入到多维特征中， 使用独热编码对训练集常用证书主体域名进行独热编码。</li>
</ul>
<h4><span id="5主机级荷载无关特征聚合-流级统计信息">（5）<strong>主机级荷载无关特征聚合</strong> (流级统计信息)</span></h4><p><strong>单独的看每条流可能漏掉了流之间的关联行为即主机级别的行为</strong>，比如恶意软件在发出正常的访问谷歌流量后可能就要开始进行恶意传输。再比如，有少量正常流也会出现自 签名，如果我们单独看流，可能就会误判，但是如果我们基于主机提取特征发现同一 IP 下有多条流都是自签名，则我们就会有很大的信心认为这是恶意的。因此，我们将上一小节中流级别 的特征进行聚合，并以流为基本单位提取主机级别特征。</p>
<p><strong>主机级特征聚合部分主要考虑了如下的特征:</strong></p>
<ul>
<li><strong>总包个数</strong>，<strong>每条流的平均包个数</strong>，<strong>时间间隔、包长的均值</strong>，以及上一个小节中证书部分的相关特 征，即<strong>自签名流数量，过期流数量，有效期过长（比如 100年）的流数量及其均值。</strong></li>
<li>TLS 半连接 和无连接</li>
</ul>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意加密流量（2）Datacon2021恶意加密流量检测</title>
    <url>/posts/28SSFDQ/</url>
    <content><![CDATA[<h2><span id="datacon2021-网络流量分析-writeup">Datacon2021 网络流量分析 Writeup</span></h2><h4><span id="一-恶意流量分析">一、恶意流量分析</span></h4><h4><span id="11-比赛题目">1.1 比赛题目</span></h4><p>本题文件中提供了一段时间的失陷主机的通讯流量，参赛选手需审计该流量包，进行追踪溯源，找到真正的恶意服务器，最终获取key，此外本题还设有可以获得额外分数的彩蛋题目。</p>
<p>以主机端的网络流量为基础，通过流量审计和主动探测的方式，对攻击者进行追踪溯源和行为分析。</p>
<h4><span id="二-恶意攻击指令识别">二、恶意攻击指令识别</span></h4><h4><span id="12-比赛题目">1.2 比赛题目</span></h4><p>本题中提供了一段时间的失陷主机的通讯流量和单个指令的流量样本的提供给参算选手，参赛选手需通过恶意软件通讯特征筛选出恶意流量，并通过提供的单一的下发指令流量作为样本，分析出通讯流量（http、dns、https）中下发的指令序列。</p>
<blockquote>
<p>  域前置技术：</p>
</blockquote>
<p>各命令对应的数据流也有不同的特征。比如，screen 命令会有来自服务端和客户端双向的大流量，hash命令有来自服务端的大流量和客户端的小流量，sleep 命令仅有get数据包，没有post数据包等，如图所示。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037518.png" alt="image-20220618161916376"></p>
<h3><span id="三-加密代理流量分析">三、加密代理流量分析</span></h3><h4><span id="31-比赛题目-阶段一-流量审计">3.1 比赛题目-阶段一 【流量审计】</span></h4><p>本题中，<strong>管理员获取了内网中由数个不明用户构建的加密代理节点</strong>，这些软件使用的通信协议不完全一致，管理员只分别提取了一定数量的样本。参赛选手需要通过不同的加密代理特征，按照样本对目标流量进行分类。</p>
<p>概述：本题共有11个加密代理。对应11个类别，其中11个类别共包含带标签样本51个。无标签样本1000个，需要们通过分析51个带有标签样本，找到11个类别所对应的具体的特征，从而对1000个测试样本进行分类。</p>
<p>由于此题类别数目较少，只有圱圱个类别，且每个类别对应的带标签样本数目也很少，因此我们采取规则匹配的方法来实现分类。我们的实验框架如图所示。</p>
<p>首先通过分析不同的类别特征制定每个类别对应的规则，从1000个测试样本的pcap包里根据规则从提取相应的数据，继而基于这些数据进行匹配分类。最终对于极少数没有匹配结果的样本，采用人工审查的方式进行分类。值得注意的是，由于同一个样本可能满足多个类别的特点，所以不同类别的规则匹配顺序会对分类结果有一定影响。</p>
<h4><span id="311-加密代理特点分析">3.1.1 加密代理特点分析</span></h4><p><strong>加密流量的特征</strong>一般包含<strong>协议特征</strong>、<strong>数据元统计特征</strong>。尤其应该注意<strong>TLS协议的流量特点</strong>。</p>
<p>对于恶意加密流量和良性加密流量的区别，cisco的研究表明【6】显示，对于TLS协议特征，在ClientHello数据包中，恶意软件提供于普通客户端完全<strong>不同的一组密码套件</strong>，此外，这些密码套件通常很脆弱或已经过时。相比之下，几乎所有良性应用程序都提供相同的密码套件。除此之外，<strong>恶意软件通常提供很少的扩展</strong>，而正常用户最多有9个拓展。此外<strong>客户端的公钥长度也存在很大的差异</strong>。在ServerHello和Certificate中，<strong>恶意软件查询的服务器会选择不常见的密码套件</strong>，因为优先提供的<strong>密码套件的大小受到限制</strong>。此外，证书的有效期和SAN条目数量也存在区别，而且<strong>恶意服务器发送自签名证书的比例也比普通服务器高一个数量级</strong>。</p>
<p><strong><font color="red"> 对于数据元统计特征，恶意流量于良性流量的特征差异主要表现在数据信息、数据包的大小、到达时间序列和字节分布等。</font></strong></p>
<h4><span id="312-特征分析与规则制定">3.1.2 特征分析与规则制定</span></h4><p>基于上述加密代理的特点, 我们分析 51 个带标签的样本, 并着重关注了 TLS 特征。我们对协议信息、TLS 应用数据、TLS 握手信息、数据信息、数据 包大小等特征进行了分析, 找到了有效区分的方法。</p>
<p>其中有三个类别有明显的区别, 只关注协议信息即可区分开来</p>
<ul>
<li>类别 0 对应的 pcap 包所有的通信流量只包含 UDP 协议, 而在 11 个类别中, 只有类别 0 具有此特点。</li>
<li>类别 6 中, 发现只有类别 6 的流量中同时出现了 TCP 和 UDP 协议, 此外, 只有类别 6 中出现了 OCSP 协议。</li>
<li>类别 8 中, 我们发现了 WireGuard 协议, WireGuard 是一种 VPN 协议, 它的大量出现使得类别 8 的特 征变得明显。通过以上方法, 我们可以通过协议信息将类别 0、类别 6、类别 8 区分出来。</li>
</ul>
<p>剩下的 8 类需要我们做进一步区分, 我们发现类别 4、类别 5、类别 7、类别9 的通信流量中只包含 TCP 协议, 类别 1、类别 2、类别 3、类别 10 的通信流量 由 TCP+TLS 协议组成。</p>
<p>到了此时, 我们的进度开始变得缓慢, 因为剩下的 8 类 没有很明显的区分特征。但是在我们的不觖努力下, 我们逐渐有了突破, 我们 发现：</p>
<ul>
<li>在类别 1 中, 存在 tls.app_data 字段的前 12 位为 00:00:00:00:00:00 的流量, 如图3.2所示; </li>
<li>在类别 7 中, 存在 data.data 字段的前 6 位为 00:00:00 的 流量; </li>
<li>在类别 5 中, 存在 data.data 字段的第 5 位到第 12 位为 $48: 00: 02: 8 \mathrm{a}$ 的 流量, 如图3.3所示; </li>
<li>在类别 3 中, 存在 tls.handshake.extensions_length 字段值 为 156 的流量; 在类别 10 中, 存在 tls.handshake.ciphersuite 值为 $0 \times 1301$ 并且 tls.handshake.type 值为 2 的流量; </li>
<li>在类别 2 中, 存在 tls.handshake.ciphersuite 值为 0x1303 并且 tls.handshake.type 值为 2 的流量; </li>
<li>在类别 4 中, 存在 frame.len 值为 1514 和 71 的流量, 值得注意的是, 当出现 frame.len 为 1514 时, 通常下一 条流量的 frame.len 为 71 ; </li>
<li>在类别 9 中, 存在 data.len 的值为 1424 的流量。<br>综上所述, 我们可以通过以上方法将 11 个类别区分出来, 文字描述略显繁 杂, 表3.1展示了我们制定的匹配规则。</li>
</ul>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037861.png" alt="image-20220618165211660" style="zoom:50%;"></p>
<h3><span id="32-比赛题目-阶段二"><strong><font color="red"> 3.2 比赛题目-阶段二</font></strong></span></h3><blockquote>
<p>  同一代理协议、少量样本标签</p>
</blockquote>
<p><strong>身份不明的用户或恶意软件可能使用未授权的加密代理进行通信</strong>，访问恶意网站或正常网站。确认这些行为的详细信息有利于对可疑用户行为和恶意软件进行分析，但截获的加密流量无法进行破解。</p>
<p>本题中，<strong>管理员使用机器自动产生了恶意软件与多个恶意网站（或正常网站）通信产生的，经过加密代理的流量</strong>。根据已知的信息，管理员提取了一部分可以确定类别的样本信息。参赛选手需要利用管理员生成的样本，标记每个流量包可能访问站点的标签。</p>
<h4><span id="321-概述">3.2.1 概述</span></h4><p><strong>题目主要考察加密流量识别能力，需要选手根据提供的已知类别样本，识别测试数据可能可能访问何种恶意站点。</strong>经过对题目和数据的初步分析，我们发现所<strong><font color="red"> 提供加密流量数据均由同一加密代理软件产生</font></strong>，且数据包中并未观测到特殊应用层协议可供分析。因此，我们确定了从<strong>统计特征</strong>和<strong>行为特征</strong>两个角度角度对加密流量样本进行审计分析，深入挖掘标识访问不同恶意通信流量的有效特征，从而实现对恶意流量的自动化分类的基本思路。</p>
<p>图3.6展示了我们方案的整体流程。具体来讲，我们分别提取训练样本的<strong>多维统计特征</strong>及<strong>标识行为特征的包长序列特征</strong>，分别构建分类器，基于投票机制结合多维分类结果来获得最终判定结果，从而避免异构数据混淆和权重问题带来的误分类情况。接下来我们将分别介绍我们所采用的特征及对应的分类器、实现和性能以及展望和总结。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182037437.png" alt="image-20220618170112661" style="zoom:50%;"></p>
<h4><span id="322-结题思路">3.2.2 结题思路</span></h4><p><strong>数据分析</strong>：拿到数据后我们首先对提供的训练样本和测试样本进行初步分整理分析;<code>train_data</code>文件夹中， 共包含1401个流量样本，除第85类只有2个样本，29、52、94只有6个样本，平均每个家族都有13-15个样本。下面我们将详细介绍<strong>六个参与投票的子模型</strong>中的每一个模型所采用的特征及对应的分类器，以及设计的动机和意义。</p>
<h5><span id="1数据包特征">（1）数据包特征</span></h5><p><strong>由于网络中不同的服务商提供的服务不同，导致数据流中的==数据分组大小==存在一定的差异</strong>，如流媒体的数据分组较小以提高播放流畅度而文件下载通常是以最大的负载进行传输[7]。由于数据分组大小与网络服务有关且不受加密技术的影响，因此可以根据数据分组的分布对加密流量进行识别。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182038655.png" alt="image-20220618170711029" style="zoom:50%;"></p>
<p>我们根据方向的不同，首先针对每个pacp文件分别提取<strong>前向包有效负载长度序列</strong>和<strong>后向有效负载长度序列</strong>。然后每个序列分别提取方差、均值、四分位值、众数、平均数、最大值等特征从而描述分布特点。</p>
<blockquote>
<p>  由于包长分布的特征本质上是一个离散概率分布，特征值之间的差值可以很好的描述不同分布之间的差异，因此我们选择<strong>基于欧拉距离度量的KNN分类算法</strong>。KNN通过测量不同特征值之间的距离来讲待检测样本分类到训练样本之中，因此并不适用于特征维度过高的数据，我们所提取的圱圲维特征可以较好地应用于此。</p>
</blockquote>
<h5><span id="2-流级特征">(2) 流级特征</span></h5><p><strong>根据五元组（源IP、目的IP、源端口，目的端口，协议）划分得到的会话数据能够最大程度的保留客户端和服务端之间的通信特征信息，因此成为了流量分析领域常用的分析对象[8].</strong>通信双方的<strong><font color="red"> 会话级别的统计特征</font></strong>能够很好的反映不同类型应用的属性。</p>
<ul>
<li><strong>恶意通信往往具有持续时间短、传输速率快、出入站比例大等特点</strong>；</li>
<li><strong>正常流量则往往具有持续时间长、包长分布均匀、数据包到达间隔稳定、出入站比例较小等特点。</strong></li>
</ul>
<p>本题中提供的加密流量只是对数据包的载荷进行加密，对流的特征属性的影响较小，因此可以根据流量的属性如间隔时间，报文大小，流持续时间等提取相应的流量特征，根据流量特征准确的识别出访问不同网站的加密流量的类别。如图所示，为题目提供训练样本中不同类别的加密流量在流级统计特征上的分布差异。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182038992.png" alt="image-20220618171300097" style="zoom:67%;"></p>
<h5><span id="提取方式"><strong><font color="red"> 提取方式：</font></strong></span></h5><p><strong>我们利用SplitCap工具按照（源IP、目的IP、源端口, 目的端口, 协议）五元组将提供的训练和测试数据分别拆分成单独的会话,</strong> 并过滤掉末完成三次握手的会话。我们认为末完成三次握手的会话可能是由于抓包或是题目设定原因出现了截断, 而这种不完整的会话若何混合在其他完整的会话中一同提取统计特征, 将会使特征变得混淆而不可用。因此我们从全部切分得到的46,667个会 话文件中过滤后得到可用于特征提取和模型训练的 <strong>22,211 条完整会话。</strong></p>
<blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/482187008"><em>SplitCap</em>划分pcap文件</a>:将pcap拆分为较小pcap的文件。</p>
</blockquote>
<p>在确定并获得训练数据后, 我们直接实现了从原始报文数据中的流量特征提取, 没有依赖tshark等第三方工具。具体来讲, <strong><font color="red"> 我们基于 python和dpkt库，直接将pcap文件作为二进制流读取并依据协议解析每个字段, 从而计算80+维的会 话统计特征。</font></strong>具体使用特征如表3.4所示:</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182038246.png" alt="image-20220618171529682" style="zoom:50%;"></p>
<blockquote>
<p>  dpkt:</p>
<p>  scapy:</p>
<p>  joy：</p>
</blockquote>
<p>特征提取后, 我们选择XGBoost作为流级特征的分类算法。XGBoost (eXtreme Gradient Boosting）是基于Boosting框架的一个算法工具包 (包括工程实现), 在 并行计算效率、缺失值处理、预测性能上都非常强大。同时基于树的方法可以 直接对特征重要性进行评分, 这对于后续挑选重要特征、降低特征维度、删除冗余特征十分方便，同时还可以对max_depth参数进行限制防止特征过于细化和线性相关带来的过拟合风险。</p>
<h5><span id="3-网站行为特征">(3) 网站行为特征</span></h5><p>由于本题数据说明提到所有样本均由同一加密代理生成，<strong>差异在于访问了不同的目标网站</strong>。而恶意网站由于其目的和功能的不同，<strong>在通信过程中往往存在特定的通信模式和规律，通过分析比较有效负载长度序列的异同，可有效定位不同类别的访问流量</strong>【9】</p>
<p>与流级特征中提取的包长统计特征不同，该部分重点关注的是<strong>序列特征</strong>，通过数据包长度的时序性交错变化来反映不同网站的应用和业务特征。在加密代理不变且网站业务功能稳定的情况下，利用基于包长序列的网站应用行为特征进行加密网站流量分类具备相当的稳定性和精确度。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182038582.png" alt="image-20220618171923243" style="zoom: 67%;"></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182038497.png" alt="image-20220618172009931" style="zoom:50%;"></p>
<p>为证明该特征的有效性, <strong>我们先将序列长度窗口设定为 11</strong> , 在 1401 个训练样本中获得了1396个样本所包含的独特包长序列, 并基于匹配的强规则确定4734个样本的标签。此次提交获得了55.18分, 即提交内容获得了94%左右的 正确率，从根本上证明该特征的有效性。</p>
<p>在获得以上的验证结果后，我们开始思考如何将以上的匹配规则“软化” 成机器学习可以处理的形式。我们希望能够兼顾包长序列在莫以类别的出现频 率和 “特异程度”, 结合以往课程和信息检索相关知识, </p>
<p><strong><font color="red"> 我们想到了 TF-IDF(term frequency-inverse document frequency) 技术。即将包长序列视为 $i$ tem, 拼接后形成 对应该类别的 “文档”, 随后基于tf-idf思想, 计算每个item的出现频率tf和特异性程度idf, 两者结合后用来代表该类别的特征向量。</font></strong>在本题提供的数据中, <strong>我们共得到了1306维的特征</strong>, 也就是说1396的训练样本和8109的测试样本, 共出 现不同的包长序列1306个。</p>
<p>在获得特征向量后, 我们依然选择了在前面就已经取得良好表现的梯度提升树算法一XGBoost作为该部分的子分类器。主要是考虑到该部分特征向量维 度较高, 且容易出现过拟合的问题。因此, 我们在设定树模型的超参数时, 集中 调整了如max_depth、colsample_bytree、subsample以及eta、lambda、alpha等 控制数据采样和正则化程度的超参数。</p>
<h4><span id="323-结果验证与评估">3.2.3 结果验证与评估</span></h4><p><strong>我们基于 python的sklearn和dpkt库实现了相应的机器学习模型和特征提取</strong>。 由于所提供数据包规模不大, dpkt并为遇到内存爆炸无法打开的情况, sklearn中 所包含的KNN模型和XGBoost模型则均采用之前多次试验得到的较好的超参数设置。而且<strong>在流级特征分类</strong>时, 我们基于XGBoost树模型特有的特征重要度评分功能, 进行了一定程度的特征选择, 最终得到了纬度较低且精确度较高的模型作为最终使用的判定模型。</p>
<p>给予我们的多级别特征提取和分类器投票结果, 提交多次后发现并末达到理想的分数。因此后期我们又基于KNN算法和tf-idf特征进行进一步分类, 将多 个模型判定结果均相同的 2270 个测试样本提取出来补充相对较少的训练样本集 重新训练各部分子分类器, 最终达到了87分的结果。</p>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意加密流量（3）西湖论剑AI大数据安全分析赛</title>
    <url>/posts/RGS2PC/</url>
    <content><![CDATA[<h2><span id="西湖论剑ai大数据安全分析赛加密恶意流量检测">西湖论剑AI大数据安全分析赛加密恶意流量检测</span></h2><blockquote>
<p>  数据集：csv</p>
<p>  加密恶意流量检测初赛第一名，决赛第二名方案：<a href="https://github.com/undefinedXD/WestLakeSwordComp">https://github.com/undefinedXD/WestLakeSwordComp</a></p>
</blockquote>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>恶意软件检测（1）科大讯飞A</title>
    <url>/posts/3KZY6WZ/</url>
    <content><![CDATA[<h2><span id="科大讯飞2021-ai开发者大赛恶意软件分类">科大讯飞2021 A.I.开发者大赛——恶意软件分类</span></h2><blockquote>
<p>  比赛链接：<a href="https://link.zhihu.com/?target=https%3A//challenge.xfyun.cn/topic/info%3Ftype%3Dmalware-classification">https://link.zhihu.com/?target=https%3A//challenge.xfyun.cn/topic/info%3Ftype%3Dmalware-classification</a></p>
<p>  <strong>操作码逆词频共现矩阵 +  Vision Transformer</strong> - 知乎 <a href="https://zhuanlan.zhihu.com/p/396207089">https://zhuanlan.zhihu.com/p/396207089</a></p>
</blockquote>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>软件程序分析（1）【draft】BDCI2022</title>
    <url>/posts/1A2QT8E/</url>
    <content><![CDATA[<h2><span id="linux跨平台二进制函数识别">Linux跨平台二进制函数识别</span></h2><p><a href="https://www.datafountain.cn/competitions/593/datasets">https://www.datafountain.cn/competitions/593/datasets</a></p>
<h2><span id="web攻击检测与分类识别">Web攻击检测与分类识别</span></h2><p><a href="https://www.datafountain.cn/competitions/596/ranking?isRedance=0&amp;sch=2006&amp;page=1">https://www.datafountain.cn/competitions/596/ranking?isRedance=0&amp;sch=2006&amp;page=1</a></p>
<h2><span id="大数据平台安全事件检测与分类识别">大数据平台安全事件检测与分类识别</span></h2><p><a href="https://www.datafountain.cn/competitions/595/datasets">https://www.datafountain.cn/competitions/595/datasets</a></p>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title>文本分类（1）搜狐文本情感分类</title>
    <url>/posts/7C1A6K/</url>
    <content><![CDATA[<h2><span id="搜狐情感分析-推荐排序算法大赛-baseline">搜狐情感分析 × 推荐排序算法大赛 baseline</span></h2><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTU5NTU4MQ==&amp;mid=2247490226&amp;idx=1&amp;sn=b080925450eb0fa2f8215a33e297214e&amp;chksm=9bc5f2e0acb27bf60296480678b8a8d20ddc0ca1e7e086fd7894fd70372cdd75d0260a915aa6&amp;mpshare=1&amp;scene=23&amp;srcid=0506hKi6p6tQeYmhSvBsgKQT&amp;sharer_sharetime=1651827176665&amp;sharer_shareid=984256fd6ff6c7f7ab7ae447f9006552%23rd">搜狐情感分析 × 推荐排序算法大赛 baseline</a></li>
<li><strong>比赛官网</strong>：<a href="https://www.biendata.xyz/competition/sohu_2022/">https://www.biendata.xyz/competition/sohu_2022/</a></li>
</ul>
<h3><span id="赛题背景"><strong>赛题背景</strong></span></h3><p>在工业界，推荐算法和自然语言处理是结合非常紧密的两个技术环节。本次大赛我们推出创新赛制——NLP 和推荐算法双赛道：探究文本情感对推荐转化的影响。情感分析是NLP领域的经典任务，本次赛事在经典任务上再度加码，研究文本对指定对象的情感极性及色彩强度，难度升级，挑战加倍。同时拥有将算法成果研究落地实际场景的绝佳机会，接触在校园难以体验到的工业实践，体验与用户博弈的真实推荐场景。</p>
<h3><span id="比赛任务"><strong>比赛任务</strong></span></h3><p><strong>比赛分为两部分：</strong></p>
<ul>
<li><strong>第一部分：==面向实体对象的文本描述情感极性及色彩强度分析==。情感极性和强度分为五种情况：极正向、正向、中立、负向、极负向。选手需要针对给定的每一个实体对象，从文本描述的角度，分析出对该实体的情感极性和强度。</strong></li>
<li><strong>第二部分：利用给出的用户文章点击序列数据及用户相关特征，结合第一部分做出的情感分析模型，对给定的文章做出是否会形成点击转化的预测判别。用户点击序列中涉及的文章，及待预测的文章，我们都会给出其详细内容。</strong></li>
</ul>
<h3><span id="一-任务1面向实体对象的文本情感分类">一、 <strong>任务1：面向实体对象的文本情感分类</strong></span></h3><h4><span id="21-数据加载">2.1 <strong>数据加载</strong></span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_file = <span class="string">&#x27;data/Sohu2022_data/nlp_data/train.txt&#x27;</span></span><br><span class="line">test_file = <span class="string">&#x27;data/Sohu2022_data/nlp_data/test.txt&#x27;</span></span><br><span class="line">sub_file = <span class="string">&#x27;data/submission/section1.txt&#x27;</span></span><br><span class="line"></span><br><span class="line">train = pd.read_json(train_file, lines=<span class="literal">True</span>)</span><br><span class="line">test = pd.read_json(test_file, lines=<span class="literal">True</span>)</span><br><span class="line">sub= pd.read_table(sub_file)</span><br></pre></td></tr></table></figure>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182040555.png" alt="图片"></p>
<h4><span id="22-文本长度统计">2.2 <strong>文本长度统计</strong></span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train[<span class="string">&#x27;text_len&#x27;</span>].quantile([<span class="number">0.5</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">0.96</span>])</span><br></pre></td></tr></table></figure>
<p><strong>大部分文本长度在562以内</strong>，在迭代过程中发现，输入到模型的文本越完整效果越好，所以可以尝试<strong>文档级的模型</strong>，比如<strong>ernie-doc</strong>或者<strong>xlnet</strong>等。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182040982.png" alt="图片" style="zoom:50%;"></p>
<h4><span id="23-实体情感标签统计">2.3 <strong>实体情感标签统计</strong></span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.countplot(sentiment_df.sentiment)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;sentiment value count&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182040704.png" alt="图片" style="zoom:50%;"></p>
<p>可以看出中性情感占到了绝大部分，极端情感最少。因为数据量比较大，大家可以使用一些<strong>采样策略</strong>：</p>
<ul>
<li><strong>中立情感负采样 ，但是有过拟合风险</strong></li>
<li><strong>保证情感比例采样：加快模型迭代速度</strong></li>
<li><strong>对同一个样本的重复情感可以负采样，ent1和ent2：1  text|ent1+ent2</strong></li>
</ul>
<h4><span id="24-数据预处理">2.4 <strong>数据预处理</strong></span></h4><p><strong>重复标签</strong>：同一样本的标签有多个，然后按照多个实体情感对样本进行复制，得到每个文本以及标签，处理代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lst_col = <span class="string">&#x27;sentiment&#x27;</span></span><br><span class="line">train = pd.DataFrame(&#123;</span><br><span class="line">    col: np.repeat(train[col].values, train[lst_col].<span class="built_in">str</span>.<span class="built_in">len</span>())</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> train.columns.difference([lst_col])</span><br><span class="line">&#125;).assign(**&#123;lst_col: np.concatenate(train[lst_col].values)&#125;)[train.columns.tolist()]</span><br></pre></td></tr></table></figure>
<h4><span id="模型定义"><strong>模型定义</strong></span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LastHiddenModel</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name, n_classes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        config = AutoConfig.from_pretrained(model_name)</span><br><span class="line">        self.model = AutoModel.from_pretrained(model_name, config=config)</span><br><span class="line">        self.linear = nn.Linear(config.hidden_size, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids</span>):</span><br><span class="line">        outputs = self.model(input_ids, attention_mask, token_type_ids)<span class="comment"># last_hidden_state和pooler out</span></span><br><span class="line">        last_hidden_state = outputs[<span class="number">0</span>] <span class="comment"># 所有字符最后一层hidden state # 32 400 768 ，但是PAD PAD</span></span><br><span class="line">        input_mask_expanded = attention_mask.unsqueeze(-<span class="number">1</span>).expand(last_hidden_state.size()).<span class="built_in">float</span>()</span><br><span class="line">        sum_embeddings = torch.<span class="built_in">sum</span>(last_hidden_state * input_mask_expanded, <span class="number">1</span>)</span><br><span class="line">        sum_mask = input_mask_expanded.<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">        sum_mask = torch.clamp(sum_mask, <span class="built_in">min</span>=<span class="number">1e-9</span>)</span><br><span class="line">        mean_embeddings = sum_embeddings / sum_mask</span><br><span class="line">        logits = self.linear(mean_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<h4><span id="扩展思路"><strong>扩展思路：</strong></span></h4><ul>
<li><strong>长文本处理：模型输入/模型预测:TTA</strong></li>
<li><strong>doc级文本模型：longformer</strong></li>
</ul>
<blockquote>
<p>  （<strong>xlnet</strong>） <a href="https://huggingface.co/hfl/chinese-xlnet-base">https://huggingface.co/hfl/chinese-xlnet-base</a></p>
<p>  (<strong>longformer_zh</strong>) <a href="https://huggingface.co/ValkyriaLenneth/longformer_zh">https://huggingface.co/ValkyriaLenneth/longformer_zh</a></p>
<p>  (longformer-chinese-base-4096) <a href="https://huggingface.co/schen/longformer-chinese-base-4096">https://huggingface.co/schen/longformer-chinese-base-4096</a></p>
</blockquote>
<ul>
<li><strong>轻量级模型：LSTM、GRU/Transformer等网络 600 word 300</strong></li>
<li><strong>选择使用不同预训练模型进行微调，chinese-roberta-wwm/nezha/xlnet/ernie/ernie-gram,其中ernie或者ernie-gram效果可能会好些</strong></li>
<li><strong>预训练模型输出的利用：CLS/PoolerOut/LastHiddenState/+(Bi)LSTM/LastFourConcat/etc…</strong></li>
<li><strong>训练优化：对抗训练(FGM/PGD/AWP)/EMA/MultiDropout/Rdrop</strong></li>
<li><strong>文本分类上分微调技巧实战</strong><ul>
<li>改进1 Last 4 Layers Concatenating</li>
<li>改进2 模型层间差分学习率: 对不同的网络层数使用不同的学习率，这样可以防止过拟合，有利于加速学习。</li>
</ul>
</li>
<li>==<strong>BERT长文本处理：《CogLTX: Applying BERT to Long Texts》</strong>==<ul>
<li><a href="https://github.com/Sleepychord/CogLTX">https://github.com/Sleepychord/CogLTX</a></li>
<li><strong>分类实例</strong>：<a href="https://github.com/Sleepychord/CogLTX/blob/main/run_20news.py">https://github.com/Sleepychord/CogLTX/blob/main/run_20news.py</a></li>
<li>COGLTX采用的策略是将每个子句从原句中移除判断其是否是必不可少的(t是一个阈值)：</li>
</ul>
</li>
</ul>
<p>​                <img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182040195.png" alt="图片" style="zoom: 67%;"></p>
<blockquote>
<p>  <a href="https://mp.weixin.qq.com/s?__biz=MzkzOTI4ODc2Ng==&amp;mid=2247484174&amp;idx=1&amp;sn=cd2d5d51d9874bbc03d50b0bca9f17f3&amp;scene=21#wechat_redirect"><strong>CogLTX : bert处理长文本代码解析</strong></a></p>
<ul>
<li><p><strong>XLNET分类模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyXLNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">35</span>, alpha=<span class="number">0.5</span></span>):</span><br><span class="line"></span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        <span class="built_in">super</span>(MyXLNet, self).__init__()</span><br><span class="line">        self.net = XLNetModel.from_pretrained(xlnet_cfg.xlnet_path).cuda()</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.net.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;layer.11&#x27;</span> <span class="keyword">in</span> name <span class="keyword">or</span> <span class="string">&#x27;layer.10&#x27;</span> <span class="keyword">in</span> name <span class="keyword">or</span> <span class="string">&#x27;layer.9&#x27;</span> <span class="keyword">in</span> name <span class="keyword">or</span> <span class="string">&#x27;layer.8&#x27;</span> <span class="keyword">in</span> name <span class="keyword">or</span> <span class="string">&#x27;pooler.dense&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">                param.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                param.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.MLP = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">768</span>, num_classes, bias=<span class="literal">True</span>),</span><br><span class="line">        ).cuda()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">        x = x.long()</span><br><span class="line">        x = self.net(x, output_all_encoded_layers=<span class="literal">False</span>).last_hidden_state</span><br><span class="line">        x = F.dropout(x, self.alpha, training=self.training)</span><br><span class="line">        x = torch.<span class="built_in">max</span>(x, dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        x = self.MLP(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(x)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<ul>
<li><strong>长文本理解模型 ERNIE-Doc</strong><ul>
<li>ERNIE-DOC，是一个基于Recurrence Transformers(Dai et al., 2019) 的文档级语言预训练模型。 本模型用了两种技术：<strong>回溯式feed机制和增强的循环机制</strong>，<strong>使模型具有更长的有效上下文长度，以获取整个文档的相关信息。</strong></li>
<li><a href="https://github.com/PaddlePaddle/ERNIE">https://github.com/PaddlePaddle/ERNIE</a></li>
</ul>
</li>
</ul>
<h3><span id="二-任务2文章点击预测"><strong>二、任务2：文章点击预测</strong></span></h3><p>第二部分：利用给出的<strong>用户文章点击序列数据</strong>及<strong>用户相关特征</strong>，结合第一部分做出的情感分析模型，对给定的文章做出是否会形成点击转化的预测判别。用户点击序列中涉及的文章，及待预测的文章，我们都会给出其详细内容。</p>
<h4><span id="21-数据加载">2.1 <strong>数据加载</strong></span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;data/Sohu2022_data/rec_data/train-dataset.csv&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;data/Sohu2022_data/rec_data/test-dataset.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train_data.shape&quot;</span>,train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;test_data.shape&quot;</span>,test.shape)</span><br></pre></td></tr></table></figure>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182040116.png" alt="图片"></p>
<p>训练集中每条样本包含pvId，用户id，点击序列（序列中的每次点击都包含文章id和浏览时间），用户特征（包含但不限于操作系统、浏览器、设备、运营商、省份、城市等），待预测文章id和当前时间戳，以及用户的行为(1为有点击，0为未点击)。</p>
<blockquote>
<p>  <strong>smapleId:样本的唯一id</strong></p>
<p>  <strong>label：点击标签</strong></p>
<p>  <strong>pvId：将每次曝光给用户的展示结果列表称为一个Group(每个Group都有唯一的pvId)</strong></p>
<p>  <strong>suv:用户id</strong></p>
<p>  <strong>itemId：文章id</strong></p>
<p>  <strong>userSeq:点击序列</strong></p>
<p>  <strong>logTs：当前时间戳</strong></p>
<p>  <strong>operator：操作系统</strong></p>
<p>  <strong>browserType：浏览器</strong></p>
<p>  <strong>deviceType:设备</strong></p>
<p>  <strong>osType：运营商</strong></p>
<p>  <strong>province：省份</strong></p>
<p>  <strong>city：城市</strong></p>
</blockquote>
<h4><span id="22-数据分析">2.2 数据分析</span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">statics</span>(<span class="params">data</span>):</span><br><span class="line">    stats = []</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> data.columns:</span><br><span class="line">        stats.append((col, data[col].nunique(), data[col].isnull().<span class="built_in">sum</span>() * <span class="number">100</span> / data.shape[<span class="number">0</span>],data[col].value_counts(normalize=<span class="literal">True</span>, dropna=<span class="literal">False</span>).values[<span class="number">0</span>] * <span class="number">100</span>, data[col].dtype))</span><br><span class="line">    stats_df = pd.DataFrame(stats, columns=[<span class="string">&#x27;Feature&#x27;</span>, <span class="string">&#x27;Unique_values&#x27;</span>, <span class="string">&#x27;Percentage_of_missing_values&#x27;</span>,<span class="string">&#x27;Percentage_of_values_in_the_biggest category&#x27;</span>, <span class="string">&#x27;type&#x27;</span>])</span><br><span class="line">    stats_df.sort_values(<span class="string">&#x27;Percentage_of_missing_values&#x27;</span>, ascending=<span class="literal">False</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> stats_df  </span><br><span class="line">stats_df=statics(train)</span><br><span class="line">stats_df</span><br></pre></td></tr></table></figure>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182040147.png" alt="图片"></p>
<h5><span id="标签分布如下">标签分布如下:</span></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.countplot(train.label)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;train label count&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182041629.png" alt="图片" style="zoom:67%;"></p>
<h4><span id="23-初步特征工程"><strong>2.3 初步特征工程</strong></span></h4><ul>
<li><strong>情感特征</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">amount_feas = [<span class="string">&#x27;prob_0&#x27;</span>, <span class="string">&#x27;prob_1&#x27;</span>, <span class="string">&#x27;prob_2&#x27;</span>, <span class="string">&#x27;prob_3&#x27;</span>,<span class="string">&#x27;prob_4&#x27;</span> ]</span><br><span class="line">category_fea = [<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> tqdm(amount_feas, desc=<span class="string">&quot;amount_feas 基本聚合特征&quot;</span>):</span><br><span class="line">    <span class="keyword">for</span> cate <span class="keyword">in</span> category_fea:</span><br><span class="line">        <span class="keyword">if</span> f != cate:</span><br><span class="line">            rec_item_sentiment[<span class="string">&#x27;&#123;&#125;_&#123;&#125;_medi&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;senti&#x27;</span>, f)] = rec_item_sentiment.groupby(cate)[f].transform(<span class="string">&#x27;median&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            rec_item_sentiment[<span class="string">&#x27;&#123;&#125;_&#123;&#125;_mean&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;senti&#x27;</span>, f)] = rec_item_sentiment.groupby(cate)[f].transform(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            rec_item_sentiment[<span class="string">&#x27;&#123;&#125;_&#123;&#125;_max&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;senti&#x27;</span>, f)] = rec_item_sentiment.groupby(cate)[f].transform(<span class="string">&#x27;max&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            rec_item_sentiment[<span class="string">&#x27;&#123;&#125;_&#123;&#125;_min&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;senti&#x27;</span>, f)] = rec_item_sentiment.groupby(cate)[f].transform(<span class="string">&#x27;min&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            rec_item_sentiment[<span class="string">&#x27;&#123;&#125;_&#123;&#125;_std&#x27;</span>.<span class="built_in">format</span>(<span class="string">&#x27;senti&#x27;</span>, f)] = rec_item_sentiment.groupby(cate)[f].transform(<span class="string">&#x27;std&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h5><span id="类别特征count特征">类别特征count特征：</span></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># count特征</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm(sparse_features):</span><br><span class="line">    data[col + <span class="string">&#x27;_count&#x27;</span>] = data.groupby(col)[<span class="string">&#x27;sampleId&#x27;</span>].transform(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">    dense_features.append(col + <span class="string">&#x27;_count&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h5><span id="用户特征">用户特征：</span></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># count特征</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm([<span class="string">&#x27;pvId&#x27;</span>,<span class="string">&#x27;itemId&#x27;</span> ]):</span><br><span class="line">    data[<span class="string">f&#x27;group_suv_<span class="subst">&#123;col&#125;</span>_nunique&#x27;</span>] = data[[<span class="string">&#x27;suv&#x27;</span>, col]].groupby(<span class="string">&#x27;suv&#x27;</span>)[col].transform(<span class="string">&#x27;nunique&#x27;</span>)</span><br><span class="line">    dense_features.append(<span class="string">f&#x27;group_suv_<span class="subst">&#123;col&#125;</span>_nunique&#x27;</span>)  </span><br></pre></td></tr></table></figure>
<h5><span id="物料特征">物料特征：</span></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pvId nunique特征</span></span><br><span class="line">select_cols = [<span class="string">&#x27;suv&#x27;</span>, <span class="string">&#x27;itemId&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm(select_cols):</span><br><span class="line"></span><br><span class="line">    data[<span class="string">f&#x27;group_pvId_<span class="subst">&#123;col&#125;</span>_nunique&#x27;</span>] = data[[<span class="string">&#x27;pvId&#x27;</span>, col]].groupby(<span class="string">&#x27;pvId&#x27;</span>)[col].transform(<span class="string">&#x27;nunique&#x27;</span>)</span><br><span class="line">    dense_features.append(<span class="string">f&#x27;group_pvId_<span class="subst">&#123;col&#125;</span>_nunique&#x27;</span>)      </span><br><span class="line"><span class="comment"># itemId nunique特征</span></span><br><span class="line">select_cols = [<span class="string">&#x27;pvId&#x27;</span>, <span class="string">&#x27;suv&#x27;</span>, <span class="string">&#x27;operator&#x27;</span>, <span class="string">&#x27;browserType&#x27;</span>, <span class="string">&#x27;deviceType&#x27;</span>, <span class="string">&#x27;osType&#x27;</span>, <span class="string">&#x27;province&#x27;</span>, <span class="string">&#x27;city&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm(select_cols):</span><br><span class="line">    data[<span class="string">f&#x27;group_itemId_<span class="subst">&#123;col&#125;</span>_nunique&#x27;</span>] = \</span><br><span class="line">        data[[<span class="string">&#x27;itemId&#x27;</span>, col]].groupby(<span class="string">&#x27;itemId&#x27;</span>)[col].transform(<span class="string">&#x27;nunique&#x27;</span>)</span><br><span class="line">    dense_features.append(<span class="string">f&#x27;group_itemId_<span class="subst">&#123;col&#125;</span>_nunique&#x27;</span>) </span><br></pre></td></tr></table></figure>
<h4><span id="nn模型-deepfm"><strong>NN模型-DeepFM</strong></span></h4><p>基于deepctr实现DeepFM训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_model_input = &#123;name: train[name] <span class="keyword">for</span> name <span class="keyword">in</span> feature_names&#125;</span><br><span class="line">valid_model_input = &#123;name: valid[name] <span class="keyword">for</span> name <span class="keyword">in</span> feature_names&#125;</span><br><span class="line">test_model_input = &#123;name: test[name] <span class="keyword">for</span> name <span class="keyword">in</span> feature_names&#125;</span><br><span class="line">model = DeepFM(linear_feature_columns, dnn_feature_columns, task=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(<span class="string">&quot;adam&quot;</span>, <span class="string">&quot;binary_crossentropy&quot;</span>, metrics=[<span class="string">&#x27;binary_crossentropy&#x27;</span>, <span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(train_model_input, train[target].values,</span><br><span class="line">                    batch_size=<span class="number">1024</span>, epochs=<span class="number">3</span>, verbose=<span class="number">1</span>, </span><br><span class="line">                    validation_data=(valid_model_input, valid[target].values))</span><br><span class="line"></span><br><span class="line">pred_ans = model.predict(valid_model_input, batch_size=<span class="number">1024</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;valid AUC&quot;</span>, <span class="built_in">round</span>(roc_auc_score(valid[target].values, pred_ans), <span class="number">4</span>))</span><br><span class="line">pred_ans = model.predict(test_model_input, batch_size=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="树模型-catboost"><strong>树模型-Catboost</strong></span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_model = CatBoostClassifier(iterations=<span class="number">15000</span>, depth=<span class="number">5</span>, learning_rate=<span class="number">0.05</span>, loss_function=<span class="string">&#x27;Logloss&#x27;</span>, logging_level=<span class="string">&#x27;Verbose&#x27;</span>, eval_metric=<span class="string">&#x27;AUC&#x27;</span>, task_type=<span class="string">&quot;GPU&quot;</span>, devices=<span class="string">&#x27;0:1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_model.fit(train_dataset, eval_set=eval_dataset, early_stopping_rounds=<span class="number">30</span>, verbose=<span class="number">40</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="特征工程思路扩展"><strong>特征工程思路扩展</strong></span></h4><ul>
<li><strong>高阶特征：类别特征组合、高阶聚合特征，比例特征</strong></li>
<li><strong>点击序列统计特征：当前用户|全局： item 众数当做类别特征；统计量 count或者nunique</strong></li>
<li><strong>序列 Embedding特征：word2vec，tfidf(词袋)+SVD、graph embedding(deepwalk)</strong></li>
<li><strong>点击转化率特征：itemid、pvId,类别组合 ..(提分) Kfold</strong></li>
</ul>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>风控算法（1）字节-色情导流用户识别</title>
    <url>/posts/1G5MF0A/</url>
    <content><![CDATA[<h1><span id="字节跳动安全ai挑战赛-大佬等等我">字节跳动安全AI挑战赛-大佬等等我</span></h1><ul>
<li><p><strong>比赛链接</strong>：<a href="https://security.bytedance.com/fe/ai-challenge#/challenge">https://security.bytedance.com/fe/ai-challenge#/challenge</a></p>
<ul>
<li><p>基于文本和多模态数据的风险识别</p>
<ul>
<li>电商黄牛地址识别</li>
<li><strong>色情导流用户识别</strong></li>
</ul>
</li>
<li><p>小样本半监督风险识别</p>
<ul>
<li>人机识别</li>
<li>少样本作弊样本检测任务</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><span id="色情导流用户识别">色情导流用户识别</span></h2><blockquote>
<p>  色情导流赛道 2ndSolution <a href="https://github.com/rooki3ray/2021BytedanceSecurityAICompetition_track1">https://github.com/rooki3ray/2021BytedanceSecurityAICompetition_track1</a></p>
</blockquote>
<h3><span id="一-赛题描述">一、赛题描述</span></h3><p>随着互联网的快速发展，网络黑产特别是色情导流也日益增多，给用户带来了极大的伤害。色情导流用户发布色情/低俗内容吸引用户，并且通过二维码、联系方式、短网址等完成导流。本赛题旨在通过提供用户相关数据，运用机器学习等方法对色情导流用户进行识别，提高模型检测的效果。</p>
<ul>
<li>输入：<strong>用户的特征，包括基础信息、投稿信息、行为信息</strong>。</li>
<li>输出：用户的标签（1表示色情导流用户，0表示正常用户）</li>
<li><strong>评价指标</strong>采用fβ（取β=0.3） <script type="math/tex">f_{\beta} = (1 + \beta^2)\frac{p*r}{\beta^2*p+r}</script></li>
</ul>
<h4><span id="基础信息">基础信息</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182038415.png" alt="image-20220629211352351" style="zoom:50%;"></p>
<h4><span id="投稿信息">投稿信息</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182038486.png" alt="image-20220629211559660" style="zoom:50%;"></p>
<h4><span id="行为信息">行为信息</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182038131.png" alt="image-20220629211703172" style="zoom:50%;"></p>
<h3><span id="二-数据构成">二、数据构成</span></h3><ul>
<li>用户基础信息<ul>
<li>性别、粉丝数、个签、关注人数……</li>
</ul>
</li>
<li>用户投稿信息<ul>
<li>视频标题、poi、省份、投稿时间</li>
</ul>
</li>
<li>用户行为信息<ul>
<li>播放次数、点赞数、分享数……</li>
</ul>
</li>
</ul>
<h3><span id="三-方案说明">三、方案说明</span></h3><ul>
<li><strong>特征工程</strong><ul>
<li><strong>log1p 数据平滑</strong></li>
<li>类别特征（<strong>LabelEncoder</strong>）</li>
<li>时间特征（<strong>min-max 归一化</strong>）</li>
<li>文本特征（长度、WordVec）</li>
<li>交叉特征</li>
</ul>
</li>
<li><strong>模型训练</strong><ul>
<li>10折lgb交叉验证，均值作为预测结果</li>
<li>伪标签</li>
</ul>
</li>
<li>最终分数线上第二（0.9906）。</li>
</ul>
<h3><span id="四-代码结构">四、代码结构</span></h3><blockquote>
<p>  from config import Config</p>
<p>  import argparse</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── 1_word2vec.py</span><br><span class="line">├── 2_merge_data.py</span><br><span class="line">├── 3_5_train_kfold.py</span><br><span class="line">├── 4_pseudo_label.py</span><br><span class="line">├── config.py</span><br><span class="line">├── data</span><br><span class="line">│   ├── pseudo.csv</span><br><span class="line">│   ├── raw</span><br><span class="line">│   │   ├── 测试数据</span><br><span class="line">│   │   └── 训练数据</span><br><span class="line">│   ├── sentence</span><br><span class="line">│   │   └── signature</span><br><span class="line">│   ├── test.csv</span><br><span class="line">│   ├── train.csv</span><br><span class="line">│   └── ...</span><br><span class="line">├── evaluate_kfold.py</span><br><span class="line">├── __pycache__</span><br><span class="line">├── readme.md</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── run.sh</span><br><span class="line">├── saved</span><br><span class="line">│   ├── <span class="number">1112_1315_0.985_0</span><span class="number">.9934</span></span><br><span class="line">│   │   └── ...</span><br><span class="line">│   ├── <span class="number">1112_1320_0.985</span>_pseudo_0<span class="number">.9934</span></span><br><span class="line">│   │   └── ...</span><br><span class="line">│   ├── 1112_1321_pseudo_0<span class="number">.985_0</span><span class="number">.9942</span></span><br><span class="line">│   │   ├── <span class="number">1112_1321_0.985</span>_results_kfold_0<span class="number">.9942</span>.csv</span><br><span class="line">│   │   ├── log.log</span><br><span class="line">│   │   └── ...</span><br><span class="line">└── utils.py</span><br></pre></td></tr></table></figure>
<h4><span id="41-runsh">4.1 run.sh</span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -x</span><br><span class="line">python 1_word2vec.py</span><br><span class="line">python 2_merge_data.py</span><br><span class="line">python 3_5_train_kfold.py</span><br><span class="line">python 4_pseudo_label.py</span><br><span class="line">python 3_5_train_kfold.py --pseudo</span><br></pre></td></tr></table></figure>
<h4><span id="42-1_word2vecpy">4.2 1_word2vec.py</span></h4><h4><span id="43-5_train_kfoldpy">4.3 <strong>5_train_kfold.py</strong></span></h4>]]></content>
      <categories>
        <category>算法比赛</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>风控算法（2）字节-小样本半监督风险识别</title>
    <url>/posts/1CE2BYX/</url>
    <content><![CDATA[<h2><span id="字节跳动安全ai挑战赛-大佬等等我">字节跳动安全AI挑战赛-大佬等等我</span></h2><ul>
<li><p><strong>比赛链接</strong>：<a href="https://security.bytedance.com/fe/ai-challenge#/challenge">https://security.bytedance.com/fe/ai-challenge#/challenge</a></p>
<ul>
<li><p>基于文本和多模态数据的风险识别</p>
<ul>
<li>电商黄牛地址识别</li>
<li><strong>色情导流用户识别</strong></li>
</ul>
</li>
<li><p>小样本半监督风险识别</p>
<ul>
<li>人机识别</li>
<li>少样本作弊样本检测任务</li>
</ul>
</li>
</ul>
</li>
<li><strong>人工智能竞赛复盘：2021安全AI挑战赛</strong> :<a href="https://www.bilibili.com/video/BV1PL4y1n7SN?spm_id_from=333.337.search-card.all.click&amp;vd_source=29387dc08d18f642078183a6816e93e8">https://www.bilibili.com/video/BV1PL4y1n7SN?spm_id_from=333.337.search-card.all.click&amp;vd_source=29387dc08d18f642078183a6816e93e8</a></li>
<li><strong>炼丹术士Zoro</strong>:<a href="https://www.zhihu.com/people/AIMuseum/posts">https://www.zhihu.com/people/AIMuseum/posts</a></li>
<li><strong>字节跳动安全AI挑战赛直播笔记</strong>  - 知乎 <a href="https://zhuanlan.zhihu.com/p/435018506">https://zhuanlan.zhihu.com/p/435018506</a></li>
</ul>
<h2><span id="小样本半监督风险识别">小样本半监督风险识别</span></h2><h3><span id="一-赛题描述">一、赛题描述</span></h3><p>在真实的社交网络中，存在的作弊用户会影响社交网络平台。在真实场景中，会受到多方面的约束，我们仅能获取到少部分的作弊样本和一部分正常用户样本，<strong>现需利用已有的少量带标签的样本，去挖掘大量未知样本中的剩余作弊样本</strong>。给定一段时间内的样本，其中包含少量作弊样本，部分正常样本以及标签未知的样本。参赛者应该利用这段时间内已有的数据，提出自己的解决方案，以预测标签未知的样本是否为作弊样本。数据处理方法和算法不限，但是参赛者需要综合考虑算法的效果和复杂度，从而构建合理的解决方案。</p>
<h4><span id="11-赛题数据与评价指标">1.1 赛题数据与评价指标</span></h4><p><strong>赛题数据</strong>：本次比赛给出的数据是T～T+N 时刻内点赞、关注事件下按比例抽样数据以及其对应账号的基础特征数据。</p>
<p><strong>评价指标</strong>：本赛题使用F1-score来评估模型的准召程度</p>
<h3><span id="二-解决方案一">二、 解决方案一</span></h3><p>首先明确本赛题实质上仍然是一个二分类的问题，我们也可以完全从此角度出来先构建出一个基础分类模型，然后再<strong>利用大量无标签的数据进行半监督学习来提升模型性能</strong>。</p>
<blockquote>
<p>  <strong>风险识别：第二名源代码</strong> <a href="https://github.com/Ljwccc/ByteDanceSecurityAI">https://github.com/Ljwccc/ByteDanceSecurityAI</a></p>
<ul>
<li>用户侧特征：<ul>
<li>账户本身的基础特征</li>
<li>账户本身的特征计数统计</li>
<li>粉丝量、关注量、发帖量、被点赞量、最后登陆时间-注册时间 乘除交叉</li>
<li>从请求数据中提取出来的device_type, app_version, app_channel类别特征，直接作为静态画像使用</li>
<li>类别特征下的数值统计特征 min/sum/max/std</li>
</ul>
</li>
<li>请求侧特征：<ul>
<li>用户请求的时间序列特征, 时间差序列特征 min/sum/max/std</li>
<li>w2v特征， 每个用户的请求ip序列建模</li>
</ul>
</li>
</ul>
</blockquote>
<h3><span id="21-特征工程">2.1 特征工程</span></h3><p>从序列特征中提取用户的设备信息、channel信息和app_version信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先 group</span></span><br><span class="line">user_request_cat_group = user_request.groupby([<span class="string">&#x27;request_user&#x27;</span>],as_index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">user_request_device_type = user_request_cat_group[<span class="string">&#x27;request_device_type&#x27;</span>].agg(&#123;<span class="string">&#x27;device_type_list&#x27;</span>:<span class="built_in">list</span>&#125;)</span><br><span class="line">user_request_channel = user_request_cat_group[<span class="string">&#x27;request_app_channel&#x27;</span>].agg(&#123;<span class="string">&#x27;channel_list&#x27;</span>:<span class="built_in">list</span>&#125;)</span><br><span class="line">user_request_app_version = user_request_cat_group[<span class="string">&#x27;request_app_version&#x27;</span>].agg(&#123;<span class="string">&#x27;app_version_list&#x27;</span>:<span class="built_in">list</span>&#125;)</span><br><span class="line"></span><br><span class="line">user_request_device_type[<span class="string">&#x27;device_type&#x27;</span>] = user_request_device_type[<span class="string">&#x27;device_type_list&#x27;</span>].apply(<span class="keyword">lambda</span> x:x[<span class="number">0</span>])</span><br><span class="line">user_request_channel[<span class="string">&#x27;channel&#x27;</span>] = user_request_channel[<span class="string">&#x27;channel_list&#x27;</span>].apply(<span class="keyword">lambda</span> x:x[<span class="number">0</span>])</span><br><span class="line">user_request_app_version[<span class="string">&#x27;app_version&#x27;</span>] = user_request_app_version[<span class="string">&#x27;app_version_list&#x27;</span>].apply(<span class="keyword">lambda</span> x:x[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">user_feat_from_action = pd.concat([user_request_device_type[[<span class="string">&#x27;request_user&#x27;</span>,<span class="string">&#x27;device_type&#x27;</span>]],user_request_channel[[<span class="string">&#x27;channel&#x27;</span>]], user_request_app_version[[<span class="string">&#x27;app_version&#x27;</span>]]],axis=<span class="number">1</span>).rename(columns=&#123;<span class="string">&#x27;request_user&#x27;</span>:<span class="string">&#x27;user&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>
<h4><span id="211-用户基础特征">2.1.1 用户基础特征</span></h4><p>类别特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 类别特征</span></span><br><span class="line">cat_cols = [<span class="string">&#x27;user_name&#x27;</span>,<span class="string">&#x27;user_profile&#x27;</span>,<span class="string">&#x27;user_register_type&#x27;</span>,<span class="string">&#x27;user_register_app&#x27;</span>,<span class="string">&#x27;user_least_login_app&#x27;</span>,<span class="string">&#x27;user_freq_ip&#x27;</span>,<span class="string">&#x27;user_freq_ip_3&#x27;</span>,<span class="string">&#x27;device_type&#x27;</span>,<span class="string">&#x27;channel&#x27;</span>,<span class="string">&#x27;app_version&#x27;</span>,<span class="string">&#x27;user_freq_ip_2&#x27;</span>,<span class="string">&#x27;user_freq_ip_1&#x27;</span>,]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手工类别特征</span></span><br><span class="line">user_info[<span class="string">&#x27;user_freq_ip_3&#x27;</span>] = user_info[<span class="string">&#x27;user_freq_ip&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="string">&#x27;.&#x27;</span>.join(<span class="built_in">str</span>(x).split(<span class="string">&#x27;.&#x27;</span>)[:<span class="number">3</span>])) <span class="comment"># 常用ip取前3位</span></span><br><span class="line">user_info[<span class="string">&#x27;user_freq_ip_2&#x27;</span>] = user_info[<span class="string">&#x27;user_freq_ip&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="string">&#x27;.&#x27;</span>.join(<span class="built_in">str</span>(x).split(<span class="string">&#x27;.&#x27;</span>)[:<span class="number">2</span>])) <span class="comment"># 常用ip取前2位</span></span><br><span class="line">user_info[<span class="string">&#x27;user_freq_ip_1&#x27;</span>] = user_info[<span class="string">&#x27;user_freq_ip&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="string">&#x27;.&#x27;</span>.join(<span class="built_in">str</span>(x).split(<span class="string">&#x27;.&#x27;</span>)[:<span class="number">1</span>])) <span class="comment"># 常用ip取前1位</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并从request中提取的基础特征</span></span><br><span class="line">user_info = user_info.merge(user_feat_from_action,on=<span class="string">&#x27;user&#x27;</span>,how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"><span class="keyword">del</span> user_feat_from_action</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类别特征的频次</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cat_cols:</span><br><span class="line">    user_info = freq_enc(user_info,col)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 对所有类别特征做label_encoder</span></span><br><span class="line">user_info = label_enc(user_info,cat_cols)</span><br></pre></td></tr></table></figure>
<p>点赞量，关注量等交叉特征，直接梭哈所有乘除法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_cols = [<span class="string">&#x27;user_fans_num&#x27;</span>,<span class="string">&#x27;user_follow_num&#x27;</span>,<span class="string">&#x27;user_post_num&#x27;</span>,<span class="string">&#x27;user_post_like_num&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col1 <span class="keyword">in</span> num_cols:</span><br><span class="line">    <span class="keyword">for</span> col2 <span class="keyword">in</span> [col <span class="keyword">for</span> col <span class="keyword">in</span> num_cols <span class="keyword">if</span> col!=col1]:</span><br><span class="line">        user_info[<span class="string">f&#x27;<span class="subst">&#123;col1&#125;</span>_<span class="subst">&#123;col2&#125;</span>_mul&#x27;</span>] = user_info[col1]*user_info[col2]</span><br><span class="line">        user_info[<span class="string">f&#x27;<span class="subst">&#123;col1&#125;</span>_<span class="subst">&#123;col2&#125;</span>_div&#x27;</span>] = user_info[col1]/(user_info[col2]+<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure>
<p>类别特征下粉丝量、关注量、发帖量、被点赞量、请求数量的统计值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_cols = [<span class="string">&#x27;user_fans_num&#x27;</span>,<span class="string">&#x27;user_follow_num&#x27;</span>,<span class="string">&#x27;user_post_num&#x27;</span>,<span class="string">&#x27;user_post_like_num&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> cat_col <span class="keyword">in</span> cat_cols:</span><br><span class="line">    cat_group = user_info.groupby(cat_col)[num_cols]</span><br><span class="line">    <span class="comment"># 平均值</span></span><br><span class="line">    cat_col_stat = cat_group.transform(np.mean)</span><br><span class="line">    cat_col_stat.rename(columns=&#123;name:<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span>_<span class="subst">&#123;cat_col&#125;</span>_mean&#x27;</span> <span class="keyword">for</span> name <span class="keyword">in</span> cat_col_stat.columns&#125;,inplace=<span class="literal">True</span>)</span><br><span class="line">    user_info = pd.concat([user_info,cat_col_stat],axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 和</span></span><br><span class="line">    cat_col_stat = cat_group.transform(np.<span class="built_in">sum</span>)</span><br><span class="line">    cat_col_stat.rename(columns=&#123;name:<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span>_<span class="subst">&#123;cat_col&#125;</span>_sum&#x27;</span> <span class="keyword">for</span> name <span class="keyword">in</span> cat_col_stat.columns&#125;,inplace=<span class="literal">True</span>)</span><br><span class="line">    user_info = pd.concat([user_info,cat_col_stat],axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 方差</span></span><br><span class="line">    cat_col_stat = cat_group.transform(np.std)</span><br><span class="line">    cat_col_stat.rename(columns=&#123;name:<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span>_<span class="subst">&#123;cat_col&#125;</span>_std&#x27;</span> <span class="keyword">for</span> name <span class="keyword">in</span> cat_col_stat.columns&#125;,inplace=<span class="literal">True</span>)</span><br><span class="line">    user_info = pd.concat([user_info,cat_col_stat],axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">del</span> cat_col_stat</span><br></pre></td></tr></table></figure>
<h4><span id="212-序列特征">2.1.2 序列特征</span></h4><p><strong>用户请求序列特征</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_request_list = user_request.groupby([<span class="string">&#x27;request_user&#x27;</span>],as_index=<span class="literal">False</span>)[<span class="string">&#x27;request_target&#x27;</span>].agg(&#123;<span class="string">&#x27;request_list&#x27;</span>:<span class="built_in">list</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先按照时间进行排序</span></span><br><span class="line">user_request = user_request.sort_values(by=<span class="string">&#x27;request_time&#x27;</span>,)</span><br><span class="line"><span class="comment"># 请求的数量</span></span><br><span class="line">user_action_feat = user_request.groupby([<span class="string">&#x27;request_user&#x27;</span>],as_index=<span class="literal">False</span>)[<span class="string">&#x27;request_user&#x27;</span>].agg(&#123;<span class="string">&#x27;request_num&#x27;</span>:<span class="string">&#x27;count&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户请求的时间统计量，但是80%的用户只有一次请求行为</span></span><br><span class="line">user_action_feat_temp = user_request.groupby([<span class="string">&#x27;request_user&#x27;</span>],as_index=<span class="literal">False</span>)[<span class="string">&#x27;request_time&#x27;</span>].agg(&#123;<span class="string">&#x27;time_list&#x27;</span>:<span class="built_in">list</span>&#125;)</span><br><span class="line">user_action_feat = user_action_feat.merge(user_action_feat_temp,on=<span class="string">&#x27;request_user&#x27;</span>,how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">user_action_feat[<span class="string">&#x27;time_min&#x27;</span>] = user_action_feat[<span class="string">&#x27;time_list&#x27;</span>].apply(<span class="built_in">min</span>)</span><br><span class="line">user_action_feat[<span class="string">&#x27;time_max&#x27;</span>] = user_action_feat[<span class="string">&#x27;time_list&#x27;</span>].apply(<span class="built_in">max</span>)</span><br><span class="line">user_action_feat[<span class="string">&#x27;time_var&#x27;</span>] = user_action_feat[<span class="string">&#x27;time_list&#x27;</span>].apply(np.var)</span><br><span class="line">user_action_feat[<span class="string">&#x27;time_max-min&#x27;</span>] = user_action_feat[<span class="string">&#x27;time_list&#x27;</span>].apply(<span class="keyword">lambda</span> x:np.<span class="built_in">max</span>(x)-np.<span class="built_in">min</span>(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 时间间隔的平均值，最大值，最小值，方差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">diff_value</span>(<span class="params">time</span>):</span><br><span class="line">    time_shift = <span class="built_in">list</span>(time[<span class="number">1</span>:])</span><br><span class="line">    time_shift.append(time[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    diff_time = time_shift-time</span><br><span class="line">    <span class="keyword">return</span> diff_time</span><br><span class="line">user_action_feat[<span class="string">&#x27;diff_time&#x27;</span>] = user_action_feat[<span class="string">&#x27;time_list&#x27;</span>].apply(<span class="keyword">lambda</span> x: diff_value(np.array(x)))</span><br><span class="line">user_action_feat[<span class="string">&#x27;diff_time_max&#x27;</span>] = user_action_feat[<span class="string">&#x27;diff_time&#x27;</span>].apply(<span class="built_in">max</span>)</span><br><span class="line">user_action_feat[<span class="string">&#x27;diff_time_var&#x27;</span>] = user_action_feat[<span class="string">&#x27;diff_time&#x27;</span>].apply(np.var)</span><br><span class="line">user_action_feat[<span class="string">&#x27;diff_time_mean&#x27;</span>] = user_action_feat[<span class="string">&#x27;diff_time&#x27;</span>].apply(np.mean)</span><br><span class="line">user_action_feat[<span class="string">&#x27;diff_time_min&#x27;</span>] = user_action_feat[<span class="string">&#x27;diff_time&#x27;</span>].apply(<span class="built_in">min</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>用户请求序列做一个embedding</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentences = user_request_list[<span class="string">&#x27;request_list&#x27;</span>].values.tolist()</span><br><span class="line">emb_size = <span class="number">64</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">    sentences[i] = [<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> sentences[i]]  <span class="comment"># 数字转化为字符串用于训练w2v</span></span><br><span class="line"></span><br><span class="line">model = Word2Vec(sentences, size=emb_size, window=<span class="number">5</span>, min_count=<span class="number">5</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, seed=<span class="number">1</span>, <span class="built_in">iter</span>=<span class="number">5</span>, workers=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">emb_matrix = []</span><br><span class="line"><span class="keyword">for</span> seq <span class="keyword">in</span> sentences:</span><br><span class="line">    vec = []</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> seq:</span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> model.wv.vocab:</span><br><span class="line">            vec.append(model.wv[w])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(vec) &gt; <span class="number">0</span>:</span><br><span class="line">        emb_matrix.append(np.mean(vec, axis=<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        emb_matrix.append([<span class="number">0</span>] * emb_size)</span><br><span class="line">emb_matrix = np.array(emb_matrix)</span><br><span class="line"></span><br><span class="line">emb_size = <span class="number">64</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(emb_size):</span><br><span class="line">    user_request_list[<span class="string">&#x27;action_emb_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i)] = emb_matrix[:, i]</span><br><span class="line"></span><br><span class="line">user_request_list = user_request_list.drop([<span class="string">&#x27;request_list&#x27;</span>],axis=<span class="number">1</span>)</span><br><span class="line">user_action_feat = user_action_feat.merge(user_request_list,how=<span class="string">&#x27;left&#x27;</span>,on=<span class="string">&#x27;request_user&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>合并基础特征和序列特征</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_info = user_info.merge(user_action_feat,how=<span class="string">&#x27;left&#x27;</span>,on=<span class="string">&#x27;user&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3><span id="22-模型训练">2.2 模型训练</span></h3><h4><span id="221-交叉验证">2.2.1 交叉验证</span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">folds = KFold(n_splits=<span class="number">10</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">546789</span>)</span><br><span class="line">oof_preds, test_preds, importances = train_model_cat(train, test, y, folds, cat_cols)</span><br><span class="line"></span><br><span class="line">test_preds[<span class="string">&#x27;label&#x27;</span>] = test_preds[<span class="string">&#x27;label&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="number">0</span> <span class="keyword">if</span> x&lt;<span class="number">0.4</span> <span class="keyword">else</span> <span class="number">1</span>)</span><br><span class="line">test_preds = test_preds.drop_duplicates(subset=[<span class="string">&#x27;user&#x27;</span>])   <span class="comment"># 去除相同的user</span></span><br><span class="line"><span class="comment"># 生成结果</span></span><br><span class="line">test_preds[[<span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]].to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>, header=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="222-模型训练">2.2.2 模型训练</span></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">useless_cols = [<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;user_status&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_cat</span>(<span class="params">data_, test_, y_, folds_, cat_cols, semi_data_=<span class="literal">None</span></span>):</span><br><span class="line">    oof_preds = np.zeros(data_.shape[<span class="number">0</span>])  <span class="comment"># 验证集预测结果</span></span><br><span class="line">    sub_preds = np.zeros(test_.shape[<span class="number">0</span>])  <span class="comment"># 测试集预测结果</span></span><br><span class="line">    feature_importance_df = pd.DataFrame()</span><br><span class="line">    feats = [f <span class="keyword">for</span> f <span class="keyword">in</span> data_.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> useless_cols]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 半监督每批训练数据</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> semi_data_ <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        semi_num = semi_data_.shape[<span class="number">0</span>]/<span class="number">5</span></span><br><span class="line">        semi_y = semi_data_[<span class="string">&#x27;user_status&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> n_fold, (trn_idx, val_idx) <span class="keyword">in</span> <span class="built_in">enumerate</span>(folds_.split(data_)):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> semi_data_ <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            semi_data_batch = semi_data_[feats].iloc[<span class="built_in">int</span>(n_fold*semi_num):<span class="built_in">int</span>((n_fold+<span class="number">1</span>)*semi_num)]</span><br><span class="line">            semi_y_batch = semi_y.iloc[<span class="built_in">int</span>(n_fold*semi_num):<span class="built_in">int</span>((n_fold+<span class="number">1</span>)*semi_num)]</span><br><span class="line">        </span><br><span class="line">            trn_x, trn_y = pd.concat([data_[feats].iloc[trn_idx],semi_data_batch]), pd.concat([y_.iloc[trn_idx],semi_y_batch])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]   <span class="comment"># 训练集数据</span></span><br><span class="line">            </span><br><span class="line">        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]   <span class="comment"># 验证集数据</span></span><br><span class="line">       </span><br><span class="line">        clf = CatBoostClassifier(</span><br><span class="line">            iterations=<span class="number">6000</span>,</span><br><span class="line">            learning_rate=<span class="number">0.08</span>,  <span class="comment"># 0.08</span></span><br><span class="line">            <span class="comment"># num_leaves=2**5,</span></span><br><span class="line">            eval_metric=<span class="string">&#x27;AUC&#x27;</span>,</span><br><span class="line">            task_type=<span class="string">&quot;CPU&quot;</span>,</span><br><span class="line">            loss_function=<span class="string">&#x27;Logloss&#x27;</span>,</span><br><span class="line">            colsample_bylevel = <span class="number">0.8</span>,</span><br><span class="line">            </span><br><span class="line">            subsample=<span class="number">0.9</span>,   <span class="comment"># 0.9</span></span><br><span class="line">            max_depth=<span class="number">7</span>,</span><br><span class="line">            reg_lambda = <span class="number">0.3</span>,</span><br><span class="line">            verbose=-<span class="number">1</span>,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        clf.fit(trn_x, trn_y, </span><br><span class="line">                eval_set= [(trn_x, trn_y), (val_x, val_y)], </span><br><span class="line">                verbose_eval=<span class="number">300</span>, early_stopping_rounds=<span class="number">100</span>,  <span class="comment"># 这个参数有点小，可以再大一点</span></span><br><span class="line">                cat_features = cat_cols</span><br><span class="line">               )</span><br><span class="line">        oof_preds[val_idx] = clf.predict_proba(val_x)[:, <span class="number">1</span>]   <span class="comment"># 验证集结果</span></span><br><span class="line">        </span><br><span class="line">        sub_preds += clf.predict_proba(test_[feats])[:, <span class="number">1</span>] / folds_.n_splits  <span class="comment"># 测试集结果</span></span><br><span class="line">        </span><br><span class="line">        fold_importance_df = pd.DataFrame()</span><br><span class="line">        fold_importance_df[<span class="string">&quot;feature&quot;</span>] = feats</span><br><span class="line">        fold_importance_df[<span class="string">&quot;importance&quot;</span>] = clf.feature_importances_</span><br><span class="line">        fold_importance_df[<span class="string">&quot;fold&quot;</span>] = n_fold + <span class="number">1</span></span><br><span class="line">        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Fold %2d AUC : %.6f&#x27;</span> % (n_fold + <span class="number">1</span>, roc_auc_score(val_y, oof_preds[val_idx])))</span><br><span class="line">        <span class="keyword">del</span> clf, trn_x, trn_y, val_x, val_y</span><br><span class="line">        gc.collect()</span><br><span class="line">    </span><br><span class="line">    oof_preds = [<span class="number">1</span> <span class="keyword">if</span> i &gt;= <span class="number">0.4</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> oof_preds]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Full F1 score %.6f&#x27;</span> % f1_score(y_, oof_preds))</span><br><span class="line">    test_[<span class="string">&#x27;label&#x27;</span>] = sub_preds</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> oof_preds, test_[[<span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]], feature_importance_df</span><br></pre></td></tr></table></figure>
<h2><span id="三-解决方案二">三、<strong>解决方案二</strong></span></h2><ul>
<li><strong>基于账户本身基础特征</strong>，可以做这些类别特征的计数统计、对于粉丝量等数值特征可以做除法的交叉、登录时间和注册时间特征可以做减法交叉，基于请求行为，我们可以对机型、ip、app_version、app_channel做频数统计</li>
<li><strong>基于用户的请求行为序列</strong>，我们可以构建w2v特征(把用户请求行为序列看成句子，行为看作词，训练Word2Vec模型得到每个行为的表征</li>
<li><strong>基于用户的请求时间</strong>，我们可以计算请求时间的均值方差、请求时间间隔的统计特征等等。</li>
</ul>
<h2><span id="四-a-novel-framework-for-social-bots-detection-in-online-social-networks-based-on-graph-embedding-and-community-detection">四、A Novel Framework for Social Bots Detection in Online Social Networks Based on Graph Embedding and Community Detection</span></h2><h3><span id="摘要">摘要</span></h3><p>随着在线社交网络的广泛普及，近年来用户数量也呈指数级增长。与此同时，社交机器人，即由程序控制的账户，也在上升。OSN的服务提供商经常使用它们来保持社交网络的活跃。与此同时，一些社交机器人也出于恶意目的注册。有必要检测这些恶意社交机器人，以呈现真实的舆论环境。我们提出了BotFinder，一个在OSN中检测恶意社交机器人的框架。具体来说，<strong>它将机器学习和图方法相结合</strong>，以便有效地提取社交机器人的潜在特征。关于特征工程，我们生成二阶特征，并使用编码方法对具有高基数的变量进行编码。这些特征充分利用了标记和未标记的样本。对于图，我们首先通过嵌入方法生成节点向量，然后进一步计算人类和机器人向量之间的相似性；然后，我们使用无监督的方法扩散标签，从而再次提高性能。为了验证该方法的性能，我们在由800多万条用户记录组成的人工竞赛提供的数据集上进行了广泛的实验。结果表明，我们的方法达到了0.8850的F1分数，这比最先进的方法要好得多</p>
<h3><span id="说明">说明</span></h3><p>与报纸等传统媒体相比。社交机器人，即由程序控制的帐户，可用于保持社交网络的活跃。虽然OSN中存在有益的社交机器人，但一些恶意社交机器人的出现会产生有害影响。例如，一些人可以出于各种目的注册大量帐户，例如增加粉丝数量或恶意喜欢。这些恶意行为已成为威胁社交网络平台健康发展的重要信息安全问题[1-2]。因此，有必要检测那些恶意的社交机器人，也称为社交机器人检测。特别是，当前的大多数研究涉及Twitter和其他国外平台，而很少有研究调查中国的OSN。因此，众多学者致力于研究社交机器人的检测问题。当前与社交机器人检测相关的工作主要分为两类，即机器学习方法和基于图的方法。然而，这一主题仍然存在一些挑战：</p>
<p>1） 一般来说，大多数方法依赖于单个算法来识别社交机器人，由于数据集的多样性，这可能不是理想的选择。</p>
<p>2） 实际上，大多数数据都是未标记的，这表明标签的数量通常很小。因此，<strong>有效利用未标记数据是一个巨大的挑战</strong>。</p>
<p>为了应对上述挑战，我们在此共同考虑用户的配置文件、行为以及它们之间的关系。此外，我们将特征工程和图方法相结合，提出了一种检测社交机器人的集成机制BotFinder。首先，在数据集上进行特征工程以提取全局信息。然后，通过嵌入方法生成节点向量。然后，我们计算人类和机器人的向量之间的相似性。最后，为了进一步提高性能，我们采用了无监督方法（这里考虑了社区检测算法）。使用所提出的算法，我们可以轻松地检测这些机器帐户。</p>
<p><strong>本文的贡献总结如下：</strong></p>
<p>1） 首先，在节点数较多、边数较少的情况下，在绘制过程中可能会忽略一些单个节点。然而，机器学习方法无法学习拓扑结构。因此，我们结合机器学习方法和图方法来克服这些问题。</p>
<p>2） 其次，在特征工程中，我们试图获得二阶特征，并采用编码方法对具有高基数的变量进行编码，或者换句话说，包含大量不同值的变量进行编码。对于图，我们通过嵌入方法生成节点向量。然后，我们利用无监督方法扩散标签以提高性能。这些方法充分利用了标记和未标记的样本。</p>
<p>本文的其余部分组织如下。在第二节中，我们回顾了一些相关的工作。在第3节中，我们介绍了拟议的框架BotFinder。然后，在第4节中，我们详细描述了所研究的数据集，并在充分分析的基础上进行了实验。最后，我们在第5节总结了我们的研究。</p>
<h3><span id="二-related-works"><strong>二、Related works</strong></span></h3><h4><span id="21-机器学习方法">2.1 机器学习方法</span></h4><p>在机器学习方法中，监督学习方法得到了广泛的研究。早期的反作弊算法仅利用用户配置文件或用户行为来构建模型。Breno等人[3]提出了一种使用人工神经网络进行数据预处理和挖掘的方法。Chang等人[4]提出了一种特征选择方法，然后使用决策树来检测机器人。Ganji等人[5]将K-最近邻（KNN）应用于信用卡欺诈检测。Ferrara等人[6-7]利用机器学习和认知行为建模技术分析了2017年法国总统选举和2017年加泰罗尼亚独立公投中的社交机器人。<strong>Denis等人[8]提出了一种用于检测Twitter上机器人的集成学习方法。</strong><br>随着深度学习方法（LSTM、CNN等）的发展，研究人员也尝试开发新的方法来检测社交机器人，以进一步提高检测精度。通过将用户内容视为时间文本数据，Cai等人[9]提出了BeDM方法用于机器人检测。Kudugunta等人[10]提取了用户元数据和推文文本，这些数据被视为LSTM深度网络的输入。在实践中，大多数真实世界的数据都是未标记的，而无监督学习方法被广泛研究，这通常依赖于社交机器人的共同特征。Cresci等人[11-12]提出了一种基于DNA启发技术的改进方法，以模拟在线用户行为。陈等人[13]提出了一种无监督的方法来实时检测推特垃圾邮件活动。姜等人[14]提出了CATCHSYNC，仅使用没有标签的拓扑来检测可疑节点。Su等人[15]提出了物联网RU。Mazza等人[16]将转发的时间序列转换为特征向量，然后进行聚类。</p>
<h4><span id="22-图算法">2.2 图算法</span></h4><p>机器学习方法只考虑节点的特征。然而，节点之间的关系也包含有价值和有用的信息。随着深度学习和图算法的发展，需要考虑图的拓扑信息以进一步改进。<strong>社交机器人具有图形聚合的特点</strong>。而社区检测用于发现网络中的社区结构，也可以看作是一种广义聚类算法。因此，社区检测算法可能适用于检测社交机器人。许多研究者对这一课题进行了不懈的研究。Guillaume等人[17]提出了一种基于模块化优化的启发式方法。李等人[18]提出了基于深度稀疏自动编码器的WCD算法。对于特征丰富的样本，很难充分挖掘特征中存在的信息。然后，提出了新的方法，首先将节点的拓扑信息转换为特征向量，然后使用机器学习算法进行训练和推理。例如，Lerer等人[19]提出的Pytorch BigGraph，<strong>Yu等人[20]提出的NetWalk</strong>，<strong>Grover等人[21]提出的Node2Vec</strong>，P<strong>ham等人[22]提出的Bot2Vec</strong>。此外，Kipf等人[23]提出了图卷积网络（GCN），对节点和网络拓扑的特征进行建模，<strong>Aljohani等人[24]将GCN应用于检测Twitter上的机器人</strong>。李等人[25]提出了用于网络免疫的BPD-DMP算法。聂等人[26]考虑了社交网络和发布内容；然后，他们提出了DCIM算法。高等人[27]对动态行为进行了表征，并提出了一种基于网络的模型。朱等人[28]研究了流行病在多层网络上的传播过程。Su等人[29]提出了检测车载网络中恶意节点的IDE。<br>大多数方法依赖于单个算法来识别社交机器人。在准确性和其他相关评估指标方面，以前的识别方法仍然有很大的局限性。</p>
<h3><span id="三-botfinder"><strong>三、BotFinder</strong></span></h3><p>在本节中，我们主要介绍BotFinder，它主要包括三个步骤：1）我们在表格数据上表示特征工程技术；2） 我们推导节点嵌入，然后测量人类和机器人之间的相似性；3） 我们应用社区检测算法来进一步提高性能</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182039206.png" alt="image-20220719205923660"></p>
<p>图1详细说明了这些步骤。第一步，利用特征工程技术生成特征矩阵。第二步，我们使用图嵌入方法生成相似矩阵，然后合并这两个矩阵。然后，我们采用LightGBM[30]来训练合并矩阵并推断临时结果。第三步，我们应用社区检测方法生成部分结果，并使用这些结果校正LightGBM的结果。</p>
<h4><span id="31-feature-engineering">3.1 <strong>Feature Engineering</strong></span></h4><p>在这里，我们试图获得<strong>二阶特征、时间间隔特征、计数编码和k倍目标编码</strong>。然后，我们应用LightGBM来训练获得的特征并推断临时结果。</p>
<p><strong><font color="red"> 二阶特征：为了表示表中分类变量的组合，我们假设二阶特征表示为 COUNT、NUNIQUE、RATIO</font></strong></p>
<ul>
<li><p><strong>COUNT 反映了活动程度。具体来说，我们选择一对变量（即 V1和 V2），并预计记录这对变量在数据集中出现的次数。我们将其缩写为 COUNT(v1, v2)。</strong>例如，用户向使用设备类型（ V1）iPhone12,1和应用程序版本（ V2）126.7.0组合的人thump-up，这种组合在数据集中出现了k次。然后，使用iPhone12、1和126.7.0的用户将获得k的 COUNT值。【记录元组出现的次数】【并行化】</p>
</li>
<li><p><strong>UNIQUE表明了给定范围内的多样性。我们使用一个变量（ V1）作为主键，并在另一个变量（V2）中记录唯一类别的数量。我们将其缩写为 UNIQUE(V1)[V2] 。</strong>例如，对于使用device type（ V1）iPhone12,1的用户，数据集中有k个不同的应用程序版本。然后，使用iPhone12,1的用户将获得 UNIQUE 值k。</p>
</li>
<li><strong>RATIO描述计数比例。它计算为 COUNT(v1, v2) / COUNT(v1)</strong>。例如，device type（V1）iPhone12,1 和 app version（ V2）126.7.0的组合在数据集中出现k次，device type（V1）iPhone12在数据集中出现V次。然后，所有使用iPhone12、1和126.7.0的用户将获得 k/v 的 RATIO 值。</li>
</ul>
<p><strong>时间间隔特性：请求时间间隔因用户而异。这里，我们主要考虑时间间隔的最大值、最小值、中值和和。</strong></p>
<p><strong>计数编码：计数编码是通过将类别替换为在数据集上计算的类别计数来进行的</strong>。然而，某些变量的计数可能相同，这可能导致两个类别可能编码为相同值的冲突。这将导致模型性能下降。因此，我们在此介绍一种目标编码技术。</p>
<p><strong>K-折叠目标编码（或似然编码、影响编码、平均编码）</strong>：目标编码是通过目标（标签）对分类变量进行计数。在这里，我们用目标的相应概率替换分类变量的每一类。为了减少目标泄漏，我们采用k倍目标编码。具体实现如下：</p>
<ul>
<li>将训练数据分成10折。</li>
<li>将折#2-10<strong>目标的平均值</strong>作为折#1的编码值，并类似地计算#2-10的编码值。</li>
<li>使用训练数据的目标来确定测试数据的编码值。</li>
</ul>
<h4><span id="32-similarity-calculation"><strong>3.2 Similarity Calculation</strong></span></h4><p><strong>在这里，我们采用Node2vec[21]来获得用户的节点嵌入（向量），然后计算用户和标记用户之间嵌入的余弦相似性</strong>。相似度值表示两个用户具有相同标签的概率；例如，如果user1和user2之间的余弦相似性相对较大，则它们很可能具有高概率的相同标签。</p>
<p>然后，对于训练集和测试集中的每个节点向量C，我们计算机器人和人类之间的最大和平均余弦相似度，该相似度表示为一个向量。【Smax1、Smean1，Smax0，Smean0】</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182039522.png" alt="image-20220719212846481" style="zoom:50%;"></p>
<h3><span id="33-community-detection">3.3 <strong>community detection</strong></span></h3><p><strong>对于社区检测，我们采用典型的Louvain方法</strong>[17]，将构建的图划分为社区。之后，我们将用以下规则标记社区：</p>
<blockquote>
<p>  [1]   Guillaume L. Fast unfolding of communities in large networks[J]. Journal Statistical Mechanics: Theory and Experiment, 2008, 10: P1008.</p>
</blockquote>
<p>1） 如果具有标签的用户属于同一社区，则社区中的所有用户都应该具有相同的标签。</p>
<p>2） 如果社区中的用户没有任何标签，或者用户具有不同的标签，我们将不会进行预测。</p>
<p><strong>然而，预测可能不会覆盖所有用户。因此，该规则的性能是有限的。但该规则的结果比LightGBM更准确。通过将上述两个步骤结合起来，可以进一步提高性能。</strong></p>
<h3><span id="四-实验">四、实验</span></h3><p>为了评估该机制的性能，我们从一个大型社交网络平台的数据中心收集了一个数据集(<a href="https://security.bytedance.com/fe/ai-challenge#/sec-project?id=2&amp;active=1）。它包含超过800万条记录，包括**用户配置文件**和**用户请求**（关注或喜欢某人）。数据集的基本信息如表1和表2所示：表1显示了用户的个人信息（配置文件），而表2说明了用户的行为（请求），包括当时用于启动请求的设备和应用程序版本。">https://security.bytedance.com/fe/ai-challenge#/sec-project?id=2&amp;active=1）。它包含超过800万条记录，包括**用户配置文件**和**用户请求**（关注或喜欢某人）。数据集的基本信息如表1和表2所示：表1显示了用户的个人信息（配置文件），而表2说明了用户的行为（请求），包括当时用于启动请求的设备和应用程序版本。</a></p>
<p><strong><font color="red"> 任务描述如下：给定用户配置文件及其请求。只有一小部分用户被标记。因此，我们必须建立一个合理、解释性和有效的模型来检测来自用户的恶意机器人。</font></strong></p>
<h3><span id="五-结论">五、结论</span></h3><p>本文提出了一种社交机器人检测方法BotFinder。为了验证所开发方法的性能，我们收集了一个包含800多万条用户记录的数据集。同时，应用机器学习和图方法从此类数据集中提取社交机器人的潜在特征。<strong>特别是，对于特征工程，我们生成二阶特征，并使用编码方法对高基数变量进行编码。</strong>在图方面，我们为账户生成节点向量，然后利用无监督方法（这里我们利用社区检测）扩散标签，以进一步提高性能。通过在收集的数据集上进行的实验，所提出的集成机制的有效性得到了相对较大的F1分数0.8850的保证。与现有方法相比，该方法的性能优越。</p>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>风控算法（3）阿里云-风险商品图算法识别</title>
    <url>/posts/1RE0J1S/</url>
    <content><![CDATA[<h2><span id="阿里安全-icdm-2022大规模电商图上的风险商品检测赛">【阿里安全 × ICDM 2022】大规模电商图上的风险商品检测赛</span></h2><blockquote>
<ul>
<li><p><strong>比赛链接</strong>：Https://tianchi.aliyun.com/competition/entrance/531976/introduction</p>
</li>
<li><p>ICDM 2022: Risk Commodities Detection on Large-Scale E-Commence Graphs：<a href="https://github.com/EdisonLeeeee/ICDM2022_competition_3rd_place_solution">https://github.com/EdisonLeeeee/ICDM2022_competition_3rd_place_solution</a></p>
</li>
<li><p>ICDM2022大规模电商图上的风险商品检测比赛（第六名）：<a href="https://github.com/JaySaligia/SGHQS">https://github.com/JaySaligia/SGHQS</a></p>
</li>
<li><strong>ICDM 2022 : 大规模电商图上的风险商品检测 — top1方案分享+代码</strong>：<a href="https://github.com/fmc123653/Competition/tree/main/ICDMCup2022-top1">https://github.com/fmc123653/Competition/tree/main/ICDMCup2022-top1</a></li>
<li><strong>【论文笔记】KGAT：融合知识图谱的 CKG 表示 + 图注意力机制的推荐系统</strong>：<a href="https://zhuanlan.zhihu.com/p/364002920">https://zhuanlan.zhihu.com/p/364002920</a></li>
</ul>
</blockquote>
<h3><span id="一-黑灰产vs风控系统">一、<strong>黑灰产VS风控系统</strong></span></h3><p>近年来，图计算尤其是图神经网络等技术获得了快速的发展以及广泛的应用。</p>
<p>在电商平台上的风险商品检测场景中，<strong>黑灰产和风控系统</strong>之间存在着<strong>激烈的对抗</strong>，黑灰产为了躲避平台管控，会蓄意掩饰风险信息，通过引入场景中存在的图数据，可以缓解因黑灰产对抗带来的检测效果下降。</p>
<p>在实际应用中，图算法的效果往往和图结构的质量紧密相关，由于风险商品检测场景中对抗的存在，<strong>恶意用户会通过伪造设备、伪造地址等方式，伪造较为“干净”的关联关系</strong>。<strong><font color="red"> 如何能够在这种存在着大量噪声的图结构数据中充分挖掘风险信息，是一个十分有挑战性的问题，另外该场景中还存在着黑白样本严重不均衡、图结构规模巨大且异构等多种挑战</font></strong>。</p>
<h3><span id="二-赛题背景">二、赛题背景</span></h3><p>在电商平台上，商品是最主要的内容之一。风险商品检测旨在识别平台上存在的假货商品、违禁商品等，对维护平台内容信息健康、保护消费者权益起着至关重要的作用。</p>
<p>风险商品检测和其他风控领域一样，<strong>面临风险的对抗和变异</strong>，如对商品风险内容的刻意隐藏等，而使用平台广泛存在的各类图关系数据可以提供更多证据，提升黑灰产的攻击成本。</p>
<p>本次比赛提供了<strong>阿里巴巴平台来源于真实场景的风险商品检测数据</strong>，需要参赛者利用大规模的异构图结构以及比例不均衡的黑白样本，利用图算法，检测出风险商品。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182039050.png" alt="图片"></p>
<h3><span id="三-结题思路">三、结题思路</span></h3><h4><span id="31-deepfm模型">3.1 DeepFM模型</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182039128.jpg" alt="img"></p>
<p><strong><font color="red"> 低阶和高阶特征选择</font></strong></p>
<h4><span id="32-序列模型">3.2 序列模型</span></h4><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182040137.jpg" alt="img"></p>
<p><img src="https://pic3.zhimg.com/v2-1e23798979f03dd7369306c91df35042_b.jpg" alt="img"></p>
<h4><span id="33-图模型">3.3 图模型</span></h4><p><img src="https://pic4.zhimg.com/v2-526f68f85c8b9d6116039bf2b98f3403_b.jpg" alt="img"></p>
<p><img src="https://pic4.zhimg.com/v2-9eadc122d63374a77d5090b3f62a8ff3_b.jpg" alt="img"></p>
<p><img src="https://pic4.zhimg.com/v2-4602cba212d520cb5babfde5865856a7_b.jpg" alt="img"></p>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>风控算法（4）西湖论剑-告警误报检测</title>
    <url>/posts/3PV2CVF/</url>
    <content><![CDATA[<h2><span id="西湖论剑-告警误报检测">西湖论剑-告警误报检测</span></h2><blockquote>
<p>  2020西湖论剑Ai大数据安全分析竞赛思路分享 - 杨航锋的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/295179638">https://zhuanlan.zhihu.com/p/295179638</a></p>
</blockquote>
]]></content>
      <categories>
        <category>算法比赛</category>
        <category>业务安全</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（9）决策树</title>
    <url>/posts/3AGJJRV/</url>
    <content><![CDATA[<h2><span id="机器学习决策树上id3-c45-cart">【机器学习】决策树（上）——ID3、C4.5、CART</span></h2><p><strong>决策树</strong>是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。本文将分三篇介绍决策树，第一篇介绍基本树（包括 <strong>ID3、C4.5、CART</strong>），第二篇介绍 <strong>Random Forest、Adaboost、GBDT</strong>，第三篇介绍 <strong>Xgboost</strong> 和 <strong>LightGBM</strong>。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>ID3（==分类==）</th>
<th>C4.5（==分类==）</th>
<th>CART（==分类和回归==）</th>
</tr>
</thead>
<tbody>
<tr>
<td>思想</td>
<td>奥卡姆剃刀：越是小型的决策树越优于大的决策树;ID3 算法的核心思想就是以<strong>信息增益</strong>来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间。</td>
<td>C4.5 算法最大的特点是<strong>克服了 ID3 对特征数目的偏重</strong>这一缺点，引入<strong>信息增益率</strong>来作为分类标准。</td>
<td>CART 算法的二分法可以<strong>简化决策树的规模</strong>，提高生成决策树的效率。CART 包含的基本过程有<strong>分裂</strong>，<strong>剪枝</strong>和<strong>树选择</strong>。</td>
</tr>
<tr>
<td><strong>划分标准</strong></td>
<td><strong>信息增益</strong>  =  类别熵 - 特征类别熵                                <strong>类别熵</strong>：$H(D)=-\sum_{k=1}^{K} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>} \log _{2} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}$                 <strong>特征类别熵</strong>：$H(D \mid A)=\sum_{i=1}^{n} \frac{\left</td>
<td>D_{i}\right</td>
<td>}{</td>
<td>D</td>
<td>} H\left(D_{i}\right)$</td>
<td>先从候选划分特征中找到信息增益高于平均值的特征，再从中选择<strong>增益率</strong>最高的。</td>
<td><strong>Gini 系数</strong>作为变量的<strong>不纯度量</strong>，<strong>减少了大量的对数运算</strong>；$G i n i(D)=\sum_{k=1}^{K} \frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}\left(1-\frac{\left</td>
<td>C_{k}\right</td>
<td>}{</td>
<td>D</td>
<td>}\right)$</td>
</tr>
<tr>
<td>剪枝策略</td>
<td><strong>无</strong></td>
<td><strong>悲观剪枝策略</strong></td>
<td>基于<strong>代价复杂度剪枝</strong></td>
</tr>
<tr>
<td>数据差异</td>
<td><strong>离散</strong>数据且<strong>缺失值</strong>敏感</td>
<td><strong>离散</strong>、<strong>连续特征离散化</strong>；【排序+离散化】</td>
<td><strong>连续型、离散型</strong></td>
</tr>
<tr>
<td><strong>连续值处理</strong></td>
<td>无</td>
<td><strong>排序</strong>并取相邻两样本值的<strong>平均数</strong>。</td>
<td><strong>排序</strong>并取相邻两样本值的<strong>平均数</strong>。<strong>CART 分类树</strong>【<strong>基尼系数</strong>】。<strong>回归树</strong>【<strong>和方差度量</strong>】。</td>
</tr>
<tr>
<td>缺失值处理</td>
<td><strong>无</strong></td>
<td>1、有缺失值特征，用没有缺失的样本子集所占比重来折算；2、将样本同时划分到所有子节点</td>
<td><strong>代理测试</strong>来估计缺失值</td>
</tr>
<tr>
<td>类别不平衡</td>
<td><strong>无</strong></td>
<td><strong>无</strong></td>
<td><strong>先验机制</strong>：其作用相当于对数据自动重加权，对类别进行均衡。</td>
</tr>
<tr>
<td><strong>==缺点==</strong></td>
<td>1、ID3 没有剪枝策略，容易过拟合；2、信息增益准则对可取值<strong>数目较多的特征有所偏好</strong>，类似“编号”的特征其信息增益接近于 1； 3、只能用于处理离散分布的特征； 没有考虑缺失值。</td>
<td>1、<strong>多叉树</strong>。2、<strong>只能用于分类</strong>。3、熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>。4、驻留于内存的数据集。</td>
<td>熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>。</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>划分标准的差异：</strong>ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li>
<li><strong>使用场景的差异：</strong>ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li>
<li><strong>样本数据的差异：</strong>ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li>
<li><strong>样本特征的差异：</strong>ID3 和 C4.5 层级之间只使用一次特征，==CART 可多次重复使用特征==；</li>
<li><strong>剪枝策略的差异：</strong>ID3 没有剪枝策略，C4.5 是通过<strong>悲观剪枝策略</strong>来修正树的准确性，而 CART 是通过<strong>代价复杂度</strong>剪枝。</li>
</ul>
<h2><span id="1-id3删特征">1. ID3【删特征】</span></h2><p>ID3 算法是建立在奥卡姆剃刀[<strong>“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情”</strong>]（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。</p>
<h3><span id="11-思想">1.1 思想</span></h3><p>从信息论的知识中我们知道：信息熵越大，从而样本纯度越低，。ID3 算法的核心思想就是以<strong>信息增益</strong>来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。 其大致步骤为：</p>
<ol>
<li>初始化特征集合和数据集合；</li>
<li>计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；</li>
<li>更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；</li>
<li>重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。</li>
</ol>
<h3><span id="12-划分标准">1.2 划分标准</span></h3><p>ID3 使用的分类标准是信息增益，它表示得知特征 A 的信息而使得样本集合不确定性减少的程度。</p>
<p>数据集的<strong>信息熵</strong>：</p>
<p>$H(D)=-\sum<em>{k=1}^{K} \frac{\left|C</em>{k}\right|}{|D|} \log <em>{2} \frac{\left|C</em>{k}\right|}{|D|}$</p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=C_k" alt="[公式]"> 表示集合 D 中属于第 k 类样本的样本子集。针对某个特征 A，对于数据集 D 的条件熵 $H(D \mid A)$为：</p>
<p>$\begin{aligned} H(D \mid A) &amp;=\sum<em>{i=1}^{n} \frac{\left|D</em>{i}\right|}{|D|} H\left(D<em>{i}\right) \ &amp;=-\sum</em>{i=1}^{n} \frac{\left|D<em>{i}\right|}{|D|}\left(\sum</em>{k=1}^{K} \frac{\left|D<em>{i k}\right|}{\left|D</em>{i}\right|} \log <em>{2} \frac{\left|D</em>{i k}\right|}{\left|D_{i}\right|}\right) \end{aligned}$</p>
<p><strong>信息增益</strong> = 信息熵 - 条件熵。信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</p>
<p><img src="https://www.zhihu.com/equation?tex=Gain%28D%2CA%29%3DH%28D%29-H%28D%7CA%29++%5C%5C" alt="[公式]"></p>
<p>信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</p>
<h3><span id="13-缺点没有剪枝-特征偏好-缺失值">1.3 缺点【没有剪枝、特征偏好、缺失值】</span></h3><ul>
<li>ID3 没有剪枝策略，容易过拟合；</li>
<li>信息增益准则==对可取值数目较多的特征==有所偏好，类似“编号”的特征其信息增益接近于 1；</li>
<li>只能用于处理离散分布的特征；</li>
<li>没有考虑缺失值。</li>
</ul>
<h2><span id="2-c45">2. C4.5</span></h2><p>C4.5 算法最大的特点是克服了 ID3 对==特征数目的偏重==这一缺点，引入信息增益率来作为分类标准。</p>
<p>C4.5 相对于 ID3 的缺点对应有以下改进方式： </p>
<ul>
<li>引入<strong>悲观剪枝策略进行后剪枝</strong>； </li>
<li>引入<strong>信息增益率</strong>作为划分标准； </li>
<li><strong>将连续特征离散化</strong>，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； </li>
<li>对于<strong>缺失值的处理</strong>可以分为两个子问题：</li>
<li>问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）<ul>
<li>C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；</li>
</ul>
</li>
<li>问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里） <ul>
<li>C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</li>
</ul>
</li>
</ul>
<h3><span id="22-划分标准">2.2 划分标准</span></h3><p>利用信息增益率可以克服信息增益的缺点，其公式为</p>
<p>$\begin{aligned} \operatorname{Gain}<em>{\text {ratio }}(D, A) &amp;=\frac{\operatorname{Gain}(D, A)}{H</em>{A}(D)}     \ H<em>{A}(D)=-\sum</em>{i=1}^{n}   \frac{\left|D<em>{i}\right|}{|D|} \log </em>{2} \frac{\left|D_{i}\right|}{|D|} \end{aligned}$</p>
<p>信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个<strong>启发式方法</strong>：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。</p>
<h3><span id="23-剪枝策略">2.3 剪枝策略</span></h3><p>为什么要剪枝：<strong>过拟合的树在泛化能力的表现非常差。</strong></p>
<p><strong>预剪枝和悲观剪枝</strong></p>
<h4><span id="231-预剪枝"><strong>2.3.1 预剪枝</strong></span></h4><p>在节点划分前来确定是否继续增长，及早停止增长的主要方法有：</p>
<ul>
<li>节点内数据样本低于<strong>某一阈值</strong>；</li>
<li>所有节点特征都已分裂；</li>
<li>节点划分前准确率比划分后准确率高。</li>
</ul>
<p>预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。</p>
<h4><span id="232-后剪枝悲观剪枝方法-httpgitlinuxnet2019-06-04-c45"><strong>2.3.2 后剪枝</strong>【悲观剪枝方法】  </span></h4><p>在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。</p>
<p>C4.5 采用的<strong>悲观剪枝方法</strong>，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。<strong>C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率</strong>。</p>
<p>后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。</p>
<h3><span id="24-缺点">2.4 缺点</span></h3><ul>
<li><strong>剪枝策略</strong>可以再优化；</li>
<li>C4.5 用的是<strong>多叉树</strong>，用二叉树效率更高；</li>
<li>C4.5 只能用于<strong>分类</strong>；</li>
<li>C4.5 使用的熵模型拥有大量耗时的<strong>对数运算</strong>，连续值还有<strong>排序运算</strong>；</li>
<li>C4.5 在构造树的过程中，<strong>对数值属性值需要按照其大小进行排序</strong>，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li>
</ul>
<h2><span id="3-cart">3. CART</span></h2><p>ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。</p>
<h3><span id="31-思想">3.1 思想</span></h3><p>CART 包含的基本过程有分裂，剪枝和树选择。 </p>
<ul>
<li><strong>分裂：</strong>分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去； </li>
<li><strong>剪枝：</strong>采用<strong>代价复杂度剪枝</strong>，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树； </li>
<li><strong>树选择：</strong>用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。</li>
</ul>
<p>CART 在 C4.5 的基础上进行了很多提升。 </p>
<ul>
<li>C4.5 为多叉树，运算速度慢，CART 为<strong>二叉树</strong>，运算速度快； </li>
<li>C4.5 只能分类，CART 既可以分类也可以<strong>回归</strong>； </li>
<li>CART 使用 ==<strong>Gini 系数作为变量的不纯度量</strong>，减少了<strong>大量的对数运算</strong>；== </li>
<li>CART 采用<strong>代理测试来估计缺失值</strong>，而 C4.5 以不同概率划分到不同节点中； </li>
<li>CART 采用<strong>“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法</strong>。</li>
</ul>
<h3><span id="32-划分标准">3.2 划分标准</span></h3><p><strong>熵模型拥有大量耗时的对数运算</strong>，基尼指数在简化模型的同时还保留了熵模型的优点。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。</p>
<p>$\begin{aligned} \operatorname{Gini}(D) &amp;=\sum<em>{k=1}^{K} \frac{\left|C</em>{k}\right|}{|D|}\left(1-\frac{\left|C<em>{k}\right|}{|D|}\right) =1- \sum</em>{k=1}^{K}\left(\frac{\left|C<em>{k}\right|}{|D|}\right)^{2}  &amp;\operatorname{Gini}(D \mid A) =\sum</em>{i=1}^{n} \frac{\left|D<em>{i}\right|}{|D|} \operatorname{Gini}\left(D</em>{i}\right) \end{aligned}$</p>
<p>==<strong>基尼指数</strong>反映了从<strong>数据集中随机抽取两个样本，其类别标记不一致的概率</strong>==。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等，<strong>基尼指数可以理解为熵模型的一阶泰勒展开。</strong></p>
<blockquote>
<p>  <strong><em>基尼指数是信息熵中﹣logP在P=1处一阶泰勒展开后的结果！所以两者都可以用来度量数据集的纯度</em></strong></p>
</blockquote>
<h3><span id="33-缺失值处理">3.3 缺失值处理</span></h3><p>上文说到，模型对于缺失值的处理会分为两个子问题：</p>
<ul>
<li><strong>如何在特征值缺失的情况下进行划分特征的选择？</strong></li>
</ul>
<p>对于问题 1，<strong>CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响</strong>（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。</p>
<ul>
<li><strong>选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？</strong></li>
</ul>
<p>对于问题 2，CART 算法的机制是为树的每个节点都找到<strong>代理分裂器</strong>，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是<strong>代替缺失值特征作为划分特征的特征</strong>），<strong>当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理</strong>，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。</p>
<h3><span id="34-剪枝策略">3.4 剪枝策略</span></h3><p><strong>基于代价复杂度的剪枝</strong>:<a href="https://www.bilibili.com/read/cv11066239">https://www.bilibili.com/read/cv11066239</a></p>
<p>采用一种<strong>“基于代价复杂度的剪枝</strong>”方法进行<strong>后剪枝</strong>，这种方法会生成一系列树，每<strong>个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点</strong>。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。<strong>这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树</strong>。</p>
<blockquote>
<p>  从完整子树 $T0$ 开始， 通过在 $Ti$ 子树序列中裁剪真实误差最小【考虑叶子节点的个数】的子树，得到 $Ti+1$。 </p>
<p>  <img src="image-20220321203204744.png" alt="image-20220321203204744" style="zoom: 25%;">【剪枝之后的误差 - 剪枝前的误差 / 叶子节点数 - 1】</p>
<p>  每次误差增加率最小的节点，得到一系列的子树，从中选择效果最好的【独立剪枝数据集】和【K折交叉验证】</p>
</blockquote>
<p><img src="image-20220320215056933.png" alt="image-20220320215056933" style="zoom:50%;"></p>
<p>我们来看具体看一下代价复杂度剪枝算法：</p>
<p>首先我们将最大树称为 <img src="https://www.zhihu.com/equation?tex=T_0" alt="[公式]"> ，我们希望减少树的大小来防止过拟合，但又担心去掉节点后预测误差会增大，所以我们定义了一个损失函数来达到这两个变量之间的平衡。损失函数定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=C_%5Calpha%28T%29%3DC%28T%29%2B%5Calpha%7CT%7C++%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"> 为任意子树， <img src="https://www.zhihu.com/equation?tex=C%28T%29" alt="[公式]"> 为预测误差， <img src="https://www.zhihu.com/equation?tex=%7CT%7C" alt="[公式]"> 为子树 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"> 的叶子节点个数， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 是参数， <img src="https://www.zhihu.com/equation?tex=C%28T%29" alt="[公式]"> 衡量训练数据的拟合程度， <img src="https://www.zhihu.com/equation?tex=%7CT%7C" alt="[公式]"> 衡量树的复杂度， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> <strong>权衡拟合程度与树的复杂度</strong>。</p>
<h3><span id="35-类别不平衡">3.5 类别不平衡</span></h3><p>CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其子冻消除不需要建模人员采取其他操作。</p>
<p>CART 使用了一种先验机制，其作用相当于对类别进行加权。这种先验机制嵌入于 CART 算法判断分裂优劣的运算里，在 CART 默认的分类模式中，总是要计算每个节点关于根节点的类别频率的比值，这就相当于对数据自动重加权，对类别进行均衡。</p>
<p>对于一个二分类问题，节点 node 被分成类别 1 当且仅当：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BN_1%28node%29%7D%7BN_1%28root%29%7D+%3E+%5Cfrac%7BN_0%28node%29%7D%7BN_0%28root%29%7D++%5C%5C" alt="[公式]"></p>
<p>比如二分类，根节点属于 1 类和 0 类的分别有 20 和 80 个。在子节点上有 30 个样本，其中属于 1 类和 0 类的分别是 10 和 20 个。如果 10/20&gt;20/80，该节点就属于 1 类。</p>
<p>通过这种计算方式就无需管理数据真实的类别分布。假设有 K 个目标类别，就可以确保根节点中每个类别的概率都是 1/K。这种默认的模式被称为“先验相等”。</p>
<p>先验设置和加权不同之处在于先验不影响每个节点中的各类别样本的数量或者份额。先验影响的是每个节点的类别赋值和树生长过程中分裂的选择。</p>
<h3><span id="36-连续值处理">3.6 连续值处理</span></h3><h4><span id="361-分类树">3.6.1 分类树</span></h4><ul>
<li><p><strong><font color="red">如果特征值是连续值：CART的处理思想与C4.5是相同的，即将连续特征值离散化。唯一不同的地方是度量的标准不一样，</font></strong> <strong>CART采用基尼指数，而C4.5采用信息增益比</strong>。</p>
</li>
<li><p>如果当前节点为连续属性，<strong>CART树中该属性（剩余的属性值）后面还可以参与子节点的产生选择过程</strong>。</p>
</li>
</ul>
<h3><span id="37-回归树">3.7 回归树</span></h3><p><strong>CART（Classification and Regression Tree，分类回归树），从名字就可以看出其不仅可以用于分类，也可以应用于回归</strong>。其回归树的建立算法上与分类树部分相似，这里简单介绍下不同之处。</p>
<h5><span id="连续值处理rss残差平方和"><strong>连续值处理</strong>：==RSS<strong>残差平方和</strong>==</span></h5><p>对于连续值的处理，<strong>CART 分类树采用基尼系数的大小来度量特征的各个划分点</strong>。<strong>在回归模型中，我们使用常见的和方差度量方式</strong>，对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> ，求出使 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 各自<strong>集合的均方差最小</strong>，同时 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=c_1" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_1" alt="[公式]"> 数据集的样本输出均值， <img src="https://www.zhihu.com/equation?tex=c_2" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=D_2" alt="[公式]"> 数据集的样本输出均值。</p>
<h5><span id="预测方式"><strong>预测方式</strong></span></h5><p>对于决策树建立后做预测的方式，上面讲到了 CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。</p>
<h3><span id="37-cart分类树建模时预测变量中存在连续和离散时会自动分别进行处理吗">3.7 CART分类树建模时，预测变量中存在连续和离散时，会自动分别进行处理吗？</span></h3><blockquote>
<p>  在使用sklearn的决策树CART建模时，预测变量中存在连续和离散时，会自动分别进行处理吗？ - 月来客栈的回答 - 知乎 <a href="https://www.zhihu.com/question/472579561/answer/2002434993">https://www.zhihu.com/question/472579561/answer/2002434993</a></p>
</blockquote>
<p><strong>对于这种连续型的特征变量，Sklearn中的具体做法（包括ID3、CART、随机森林等）是先对连续型特征变量进行排序处理</strong>，<strong><font color="red"> 然后取所有连续两个值的均值来离散化整个连续型特征变量。</font></strong></p>
<p>假设现在某数据集其中一个特征维度为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.5%2C+0.2%2C+0.8%2C+0.9%2C+1.2%2C+2.1%2C+3.2%2C+4.5%5D+%5C%5C" alt="[公式]"></p>
<p>则首先需要对其进行排序处理，排序后的结果为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.2%2C+0.5%2C+0.8%2C+0.9%2C+1.2%2C+2.1%2C+3.2%2C+4.5%5D+%5C%5C" alt="[公式]"></p>
<p>接着再计算所有连续两个值之间的平均值：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5B0.35%2C+0.65%2C+0.85%2C+1.05%2C+1.65%2C+2.65%2C+3.85%5D+%5C%5C" alt="[公式]"></p>
<p>这样，便得到了该特征离散化后的结果。最后在构造<a href="https://www.zhihu.com/search?q=决策树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;2002434993&quot;}">决策树</a>时，只需要使用式最后离散化后的特征进行划分指标的计算即可。同时，值得一说的地方是<strong>目前Sklearn在实际处理时，会把所有的特征均看作连续型变量进行处理</strong>。</p>
<p>下图所示为iris数据集根据sklearn中CART算法所建模的决策树的可视化结果：</p>
<p><img src="https://picx.zhimg.com/v2-9081bc3cd5f2ec069212b79d5c5ff7d3_b.jpg" alt="img" style="zoom:50%;"></p>
<p>从图中可以看到，<code>petal width</code>这个特征在前两次分类时的分割点分别为0.8和1.75。下面先来看看原始特征<code>petal width</code>的取值情况：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">1.</span>  <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">1.4</span> <span class="number">2.5</span> <span class="number">1.3</span> <span class="number">2.1</span> <span class="number">1.5</span> <span class="number">0.2</span> <span class="number">2.</span>  <span class="number">1.</span>  <span class="number">0.2</span> <span class="number">0.3</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">0.2</span> <span class="number">0.5</span> <span class="number">1.3</span> <span class="number">0.2</span> <span class="number">1.2</span> <span class="number">2.2</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">2.</span>  <span class="number">0.2</span> <span class="number">1.8</span> <span class="number">1.9</span> <span class="number">1.</span>  <span class="number">1.5</span> <span class="number">2.3</span> <span class="number">1.3</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">1.9</span> <span class="number">0.2</span> <span class="number">0.2</span> <span class="number">1.1</span> <span class="number">1.7</span> <span class="number">0.2</span> <span class="number">2.4</span> <span class="number">0.2</span> <span class="number">0.6</span> <span class="number">1.8</span> <span class="number">1.1</span> <span class="number">2.3</span> <span class="number">1.6</span> <span class="number">1.4</span> <span class="number">2.3</span> <span class="number">1.3</span> <span class="number">0.2</span> <span class="number">0.1</span> <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">0.3</span> <span class="number">0.2</span> <span class="number">1.5</span> <span class="number">2.4</span> <span class="number">0.3</span> <span class="number">2.1</span> <span class="number">2.5</span> <span class="number">0.2</span> <span class="number">1.4</span> <span class="number">1.5</span> <span class="number">1.8</span> <span class="number">1.4</span> <span class="number">2.3</span> <span class="number">0.2</span> <span class="number">2.1</span> <span class="number">1.5</span> <span class="number">2.</span>  <span class="number">1.</span>  <span class="number">1.4</span> <span class="number">1.4</span> <span class="number">0.3</span> <span class="number">1.3</span> <span class="number">1.2</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">1.8</span> <span class="number">2.1</span> <span class="number">0.4</span> <span class="number">1.</span>  <span class="number">2.5</span> <span class="number">1.6</span> <span class="number">0.1</span> <span class="number">2.4</span> <span class="number">0.2</span> <span class="number">1.5</span> <span class="number">1.9</span> <span class="number">1.8</span> <span class="number">1.3</span> <span class="number">1.8</span> <span class="number">1.3</span> <span class="number">1.3</span> <span class="number">2.</span>  <span class="number">1.8</span> <span class="number">0.2</span> <span class="number">1.3</span> <span class="number">1.7</span> <span class="number">0.2</span> <span class="number">1.2</span> <span class="number">2.1</span>]</span><br></pre></td></tr></table></figure>
<p>可以发现上面并没有0.8和1.75这两个取值。接着按上面的方法先排序，再取相邻两个值的平均作为离散化的特征，其结果为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">0.1</span>, <span class="number">0.15000000000000002</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, </span><br><span class="line"><span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.25</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.35</span>, <span class="number">0.4</span>, <span class="number">0.4</span>,</span><br><span class="line"> <span class="number">0.45</span>, <span class="number">0.55</span>, <span class="number">0.8</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.05</span>, <span class="number">1.1</span>, <span class="number">1.15</span>, <span class="number">1.2</span>, <span class="number">1.2</span>, <span class="number">1.25</span>, <span class="number">1.3</span>,</span><br><span class="line"> <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.3</span>, <span class="number">1.35</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.4</span>, <span class="number">1.45</span>, <span class="number">1.5</span>, </span><br><span class="line"><span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.55</span>, <span class="number">1.6</span>, <span class="number">1.65</span>, <span class="number">1.7</span>, <span class="number">1.75</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, </span><br><span class="line"><span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.8</span>, <span class="number">1.85</span>, <span class="number">1.9</span>, <span class="number">1.9</span>, <span class="number">1.95</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.05</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, <span class="number">2.1</span>, </span><br><span class="line"><span class="number">2.1500000000000004</span>, <span class="number">2.25</span>, <span class="number">2.3</span>, <span class="number">2.3</span>, <span class="number">2.3</span>, <span class="number">2.3499999999999996</span>, <span class="number">2.4</span>, <span class="number">2.4</span>, <span class="number">2.45</span>, <span class="number">2.5</span>, <span class="number">2.5</span>]</span><br></pre></td></tr></table></figure>
<h2><span id="4-总结">4. 总结</span></h2><p>最后通过总结的方式对比下 ID3、C4.5 和 CART 三者之间的差异。</p>
<p>除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：</p>
<ul>
<li><strong>划分标准的差异：</strong>ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li>
<li><strong>使用场景的差异：</strong>ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li>
<li><strong>样本数据的差异：</strong>ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li>
<li><strong>样本特征的差异：</strong>ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征（连续型）；</li>
<li><strong>剪枝策略的差异：</strong>ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。</li>
</ul>
<h2><span id="一-决策树">一、决策树</span></h2><h3><span id="11-介绍决策树id2-c45-cart-3种决策树及其区别和适应场景">1.1 介绍决策树ID2、C4.5、CART, 3种决策树及其区别和适应场景？</span></h3><h3><span id="12-决策树处理连续值的方法">1.2 决策树处理连续值的方法?</span></h3><p><strong>ID3 只能离散型</strong>。<strong>C4.5 将连续特征离散化</strong>，假设 n 个样本的连续特征 A 有 m 个取值，<strong>C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点</strong>，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； </p>
<p><strong>CART分类树：离散化+基尼指数，</strong></p>
<p><strong>CART回归树：均方差之和度量方式</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cmin%5Climits_%7Ba%2Cs%7D%5CBigg%5B%5Cmin%5Climits_%7Bc_1%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_1%7D%28y_i+-+c_1%29%5E2+%2B+%5Cmin%5Climits_%7Bc_2%7D%5Csum%5Climits_%7Bx_i+%5Cin+D_2%7D%28y_i+-+c_2%29%5E2%5CBigg%5D+%5C%5C" alt="[公式]"></p>
<h3><span id="13-决策树处理缺失值的方式">1.3 决策树处理缺失值的方式？</span></h3><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/84519568">ID3、c4.5、cart、rf到底是如何处理缺失值的？</a></p>
</blockquote>
<h4><span id="131-在特征值缺失的情况下进行划分特征的选择">1.3.1 <strong>在特征值缺失的情况下进行划分特征的选择？</strong></span></h4><p><strong>ID3</strong> 没有缺失值处理；</p>
<p><strong>C4.5</strong>：对于具有缺失值特征，用没有缺失的<strong>样本子集所占比重来折算</strong>；</p>
<p><strong>CART</strong>：<strong>初期</strong>：分裂特征评估时只能使用在该特征上没有缺失值的那部分数据<strong>；后续</strong>：CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响。</p>
<h4><span id="132-选定该划分特征对于缺失该特征值的样本如何处理">1.3.2 <strong>选定该划分特征，对于缺失该特征值的样本如何处理？</strong></span></h4><p><strong>ID3</strong> 没有缺失值处理；</p>
<p><strong>C4.5</strong>：<strong>将样本同时划分到所有子节点</strong>，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</p>
<p>==<strong>CART</strong>：==sklearn中的cart的实现是没有对缺失值做任何处理的，也就是说sklearn的cart无法处理存在缺失值的特征。</p>
<h3><span id="14-决策树如何剪枝">1.4 决策树如何剪枝？</span></h3><ul>
<li><strong>预剪枝</strong>：在树的生成过程中，提前停止生长。简单，适合解决大规模问题。<ul>
<li>深度</li>
<li>节点样本数</li>
<li>对测试集准确率的提升过小</li>
</ul>
</li>
<li><strong>后剪枝</strong>：生成一颗完全生长的二叉树，从低向上剪枝，将子树删除用叶子节点代替。【类别：多数投票】常见的剪枝方法：错误率降低剪枝（REP）、<strong>悲观剪枝（PEP）、代价复杂度剪枝（CCP）</strong>、最小误差剪枝（MEP）等。</li>
</ul>
<p><strong>代价复杂度剪枝（CCP）【CART树】</strong></p>
<p>从完整子树 $T0$ 开始， 通过在 $Ti$ 子树序列中裁剪真实误差最小【考虑叶子节点的个数】的子树，得到 $Ti+1$。 </p>
<p><img src="image-20220321203204744.png" alt="image-20220321203204744" style="zoom: 25%;">【剪枝之后的误差 - 剪枝前的误差 / 叶子节点数 - 1】</p>
<p>每次误差增加率最小的节点，得到一系列的子树，从中选择效果最好的【独立剪枝数据集】和【K折交叉验证】</p>
<h3><span id="15-决策树特征选择特征重要性判断">1.5 决策树特征选择？特征重要性判断？</span></h3><p><strong>XGBoost</strong>：</p>
<ul>
<li><p>该特征在所有树中被用作分割样本的特征的总次数。</p>
</li>
<li><p>该特征在其出现过的所有树中产生的平均增益。</p>
</li>
<li><p>该特征在其出现过的所有树中的平均覆盖范围。</p>
<blockquote>
<p>  注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。</p>
</blockquote>
</li>
</ul>
<h3><span id="16-svm-lr-决策树的对比">1.6 SVM、LR、决策树的对比？</span></h3><blockquote>
<p>  逻辑回归，决策树，支持向量机 选择方案: <a href="https://cloud.tencent.com/developer/article/1435642">https://cloud.tencent.com/developer/article/1435642</a></p>
<p>  广义线性模型？</p>
<p>  sigmod、softmax</p>
<p>  为什么逻辑回归的连续值也需要离散化？</p>
<p>  为什么逻辑回归要用交叉熵？</p>
<p>  交叉熵和KL散度（相对熵）和GAN的损失函数的区别？</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>线性回归</th>
<th>LR 逻辑回归</th>
<th>SVM</th>
<th>朴素贝叶斯</th>
<th>决策树</th>
</tr>
</thead>
<tbody>
<tr>
<td>场景</td>
<td>【回归问题】</td>
<td>逻辑回归 = 线性回归 + Sigmoid 函数（非线形）【分类问题】==【参数模型】==【统计方法】</td>
<td>【分类问题】【几何方法】【非参数模型】</td>
<td>【生成式模型】</td>
<td>【分类问题】【回归问题】【非参数模型】</td>
</tr>
<tr>
<td><strong>思想</strong></td>
<td></td>
<td><strong>思路：</strong>先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。<strong>细节</strong>：通过<strong>非线性映射减小了离分类平面较远的点的权重</strong>，相对提升了与分类最相关的数据点的权重；</td>
<td><strong>思想</strong>：SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</td>
<td></td>
<td><strong>思想</strong>：用启发算法来度量特征选择，选择特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间。</td>
</tr>
<tr>
<td><strong>关键样本</strong></td>
<td></td>
<td><strong>所有样本</strong>（通过非线性映射，大大减小了离分类平面较远的点的权重）</td>
<td><strong>支持向量</strong>（超平面到距离最近的不同标记样本集合）</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>目标函数</strong></td>
<td></td>
<td>$y=\frac{1}{1+e^{-\left(w^{T} x+b\right)}}$ 【极大似然函数】</td>
<td><img src="image-20220322131856478.png" alt="image-20220322131856478" style="zoom:150%;"> <img src="https://pic2.zhimg.com/80/v2-0e87b3bf410cd798efd05a2837b83589_1440w.png" alt="img"></td>
<td></td>
<td>信息增益、信息增益率、Gini指数</td>
</tr>
<tr>
<td><strong>损失函数</strong></td>
<td></td>
<td><img src="https://pic3.zhimg.com/80/v2-ee1ddd22da5171fa44e079582cefe20a_1440w.png" alt="img" style="zoom:150%;"></td>
<td><strong><a href="https://www.zhihu.com/question/47746939">HingeLoss</a></strong>【合页损失函数】：<img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5EN%5B1-y_i%28w%C2%B7x_i+%2B+b%29%5D_%2B+%2B+%5Clambda%7C%7Cw%7C%7C%5E2+%5C%5C+%5Bz%5D_%2B+%3D+%5Cbegin%7Bequation%7D+%5Cleft%5C%7B++++++++++++++%5Cbegin%7Barray%7D%7Blr%7D+++++++++++z%2C+z%3E0+%26++%5C%5C++++++++++++++0.z%5Cleq0+%26+++++++++++++++%5Cend%7Barray%7D++%5Cright.+%5Cend%7Bequation%7D+%5C%5C+" alt="[公式]" style="zoom:150%;"></td>
<td></td>
<td>信息增益、信息增益率、Gini指数【方差和】</td>
</tr>
<tr>
<td>决策面</td>
<td></td>
<td>线性可分</td>
<td>【核函数映射】从而使得样本数据线性可分</td>
<td></td>
<td>矩形【非线性】</td>
</tr>
<tr>
<td>连续值处理</td>
<td></td>
<td>==离散化？==</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td></td>
<td>类别的概率</td>
<td>类别</td>
<td></td>
<td>类别、回归</td>
</tr>
<tr>
<td>过拟合</td>
<td></td>
<td><strong>正则化</strong> L1: <img src="image-20220316144906846.png" alt="image-20220316144906846" style="zoom:50%;"> L2: <img src="image-20220316145437180.png" alt="image-20220316145437180" style="zoom:50%;"></td>
<td>\</td>
<td></td>
<td>预剪枝和后剪枝</td>
</tr>
<tr>
<td>优势</td>
<td></td>
<td><strong>本质其实是为了模型参数服从某一分布</strong>；1、对观测样本的概率值输出 2、实现简单高效3、<strong>多重共线性的问题可以通过L2正则化来应对</strong>。 4、大量的工业界解决方案5、支持online learning</td>
<td>1、可以处理<strong>高维特征</strong> 2、使用<strong>核函数</strong>轻松应对非线的性特征空间 3、分类面不依赖于所有数据4、关重要的<strong>关键样本</strong></td>
<td></td>
<td>1、直观的决策过程 2、能够处理非线性特征 3、考虑了<strong>特征相关性</strong></td>
</tr>
<tr>
<td>劣势</td>
<td></td>
<td>1、特征空间太大时表现不太好 2、对于大量的分类变量无能为力 3、对于非线性特征需要做特征变换 4、依赖所有的样本数据</td>
<td>1、<strong>对于大量的观测样本，效率会很低</strong> 2、找到一个“合适”的核函数还是很tricky的</td>
<td></td>
<td>1、极易过拟合 2、无法输出score，只能给出直接的分类结果</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>  <strong><a href="https://blog.csdn.net/Alphonse_Huang/article/details/114278377">多重共线性问题</a></strong></p>
<p>  多重共线性问题就是指一个解释变量的变化引起另一个解释变量地变化。多重共<a href="https://so.csdn.net/so/search?q=线性&amp;spm=1001.2101.3001.7020">线性</a>是使用线性回归算法时经常要面对的一个问题。在其他算法中，例如决策树或者朴素贝叶斯，前者的建模过程时逐渐递进，每次都只有一个变量参与，这种机制含有抗多重共线性干扰的功能；后者假设变量之间是相互独立的。但对于回归算法来说，都要同时考虑多个预测因子，因此多重共线性不可避免。</p>
<ul>
<li>PCA等降维方法。因为在原始特征空间中变量之间相关性大，很容易想到通过降低维度的形式来去除这种共线性。</li>
<li>正则化。使用<strong>岭回归（L2</strong>）或者lasso回归（L1）或者elasticnet回归（L1+L2）</li>
<li><p>逐步回归法</p>
<p><strong><a href="https://blog.csdn.net/FrankieHello/article/details/94022594">机器学习中参数模型和非参数模型理解</a></strong></p>
<p>参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p>
</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>决策树</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测（2）Isolation Forest</title>
    <url>/posts/ZQ2GRE/</url>
    <content><![CDATA[<h2><span id="一-isolation-forest">==一、Isolation Forest==</span></h2><blockquote>
<p>  孤立森林(isolation Forest)-一个通过瞎几把乱分进行异常检测的算法 - 小伍哥聊风控的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/484495545">https://zhuanlan.zhihu.com/p/484495545</a></p>
<h5><span id="isolation-forest算法梳理isolation-forest算法梳理">Isolation Forest算法梳理🌳</span></h5></blockquote>
<h3><span id="11-概述">1.1 概述</span></h3><p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182028365.png" alt="Isolation Forest算法梳理🌳"></p>
<p><strong>异常检测 (anomaly detection)</strong>，或者又被称为“<strong>离群点检测</strong>” (outlier detection)，是机器学习研究领域中跟现实紧密联系、有广泛应用需求的一类问题。但是，什么是异常，并没有标准答案，通常因具体应用场景而异。如果要给一个比较通用的定义，很多文献通常会引用 Hawkins 在文章开头那段话。很多后来者的说法，跟这个定义大同小异。这些定义虽然笼统，但其实暗含了认定“异常”的两个标准或者说假设：</p>
<p><strong>孤立森林 <img src="https://www.zhihu.com/equation?tex=%28Isolation%5C+Forest%29" alt="[公式]"> 是一个基于 <img src="https://www.zhihu.com/equation?tex=Ensemble" alt="[公式]"> 的快速异常检测方法，具有线性时间复杂度和高精准度。</strong>其可以用于网络安全中的攻击检测，金融交易欺诈检测，疾病侦测，和噪声数据过滤等。</p>
<p>孤立森林算法的理论基础有两点：</p>
<ul>
<li>异常数据占总样本量的比列很小</li>
<li>异常点的特征值与正常点的差异很大</li>
</ul>
<h3><span id="12-itree的构建">1.2 iTREE的构建</span></h3><p> <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 是一棵随机二叉树，每一个节点要么有两个孩子，要么就是叶子节点。假设给定一堆数据集 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BD%7D" alt="[公式]"> ，这里 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BD%7D" alt="[公式]"> 的所有属性都是连续型的变量， <img src="https://www.zhihu.com/equation?tex=+iTree" alt="[公式]"> 的构建过程如下：</p>
<ol>
<li><p><strong>随机选择一个属性</strong> <img src="https://www.zhihu.com/equation?tex=Attr" alt="[公式]"> ；</p>
</li>
<li><p><strong>随机选择该属性的一个值</strong> <img src="https://www.zhihu.com/equation?tex=Value" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%5Cmin%5C%7BAttr%5C%7D%3CValue%3C%5Cmax%5C%7BAttr%5C%7D" alt="[公式]"> ;</p>
</li>
<li><p><strong>根据 <img src="https://www.zhihu.com/equation?tex=Attr+" alt="[公式]"> 对每条记录进行分类，把 <img src="https://www.zhihu.com/equation?tex=Attr" alt="[公式]"> 小于 <img src="https://www.zhihu.com/equation?tex=Value" alt="[公式]"> 的记录放在左子树，把大于等于 <img src="https://www.zhihu.com/equation?tex=Value" alt="[公式]"> 的记录放在右子树</strong>；</p>
</li>
<li><p><strong>递归构造左右子树，直到满足下列条件</strong>：（1）传入的数据集只有一条记录或者多条同样的记录；（2）树的深度达到了限定深度。</p>
</li>
</ol>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182028147.jpg" alt="img" style="zoom:50%;"></p>
<p><img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 构建完成之后，只需要追踪测试数据落在 <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 哪个叶子节点上即可评估该数据是否为异常数据，由图中 <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 的构造过程可以发现异常数据通常会很快被分配到叶子节点上，因此可以使用叶子结点到根结点的路径长度（即边的条数） <img src="https://www.zhihu.com/equation?tex=h%28x%29" alt="[公式]"> 来判断一条记录 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 是否是异常点。</p>
<h3><span id="13-iforest-构建">1.3 iForest 构建</span></h3><p>由于 <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 是随机选择属性和随机选择属性值来构建的，因此可以预见对于单棵 <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 的预测效果肯定不会很理想，因此通过引入多棵 <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 共同来预测那么从效果上看肯定会更具有说服力。 <img src="https://www.zhihu.com/equation?tex=iForest" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=Random%5C+Forest" alt="[公式]"> 的方法有些类似，都是通过随机采样，利用部分采样数据来构造每一棵树，以保证不同树之间的差异性。在构建 <img src="https://www.zhihu.com/equation?tex=iForest" alt="[公式]"> 的过程中有，采样的样本大小 <img src="https://www.zhihu.com/equation?tex=+%5Cpsi" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 的数量 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 这两个超参数需要确定，样本采样大小超过 <img src="https://www.zhihu.com/equation?tex=256" alt="[公式]"> 效果就提升不大了。</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182028401.jpg" alt="img" style="zoom:67%;"></p>
<p>通过采样数据不仅可以降低计算时间的上面的浪费，而且还能够解决一些其它的小问题：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182028205.jpg" alt="preview" style="zoom:67%;"></p>
<p>左图是原始数据，右图是经过采样了的数据，蓝色代表正常样本，红色代表异常样本。可以看出，在采样之前，正常样本和异常样本出现了重叠，因此很难分开，但通过采样之后，异常样本和正常样本可以明显的分开。 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 控制了 <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 的数量即 <img src="https://www.zhihu.com/equation?tex=Ensemble%5C+size" alt="[公式]"> ，孤立森林算法提出者通过实验发现，当 <img src="https://www.zhihu.com/equation?tex=+t%3D100" alt="[公式]"> 之前时，算法就会收敛，故通常设置 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 为默认值 <img src="https://www.zhihu.com/equation?tex=100" alt="[公式]"> ，训练一个 <img src="https://www.zhihu.com/equation?tex=+iForest" alt="[公式]"> 最差情况下的时间复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28t%5Cpsi%5E2%29" alt="[公式]"> 空间复杂度为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal++O%28t%5Cpsi%29+" alt="[公式]"> 。</p>
<h3><span id="14-评估">1.4 评估</span></h3><p>为了更好的归一化和比较，<strong>孤立森林通过引入异常值函数</strong> <img src="https://www.zhihu.com/equation?tex=s%28x%2C+n%29" alt="[公式]"> 来衡量记录 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 是否为异常点。</p>
<blockquote>
<p>  给定一个包含 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 个样本的数据集，<strong>树的平均路径长度为： c(n)</strong>。</p>
<p>  <img src="https://www.zhihu.com/equation?tex=c(n" alt="[公式]">%3D2H(n-1)-\frac{2(n-1)}{n}+\)</p>
<p>  其中， <img src="https://www.zhihu.com/equation?tex=H%28%2A%29" alt="[公式]"> 为调和数， <img src="https://www.zhihu.com/equation?tex=H%28%2A%29%3D%5Cln%28%2A%29%2B%5Cxi" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="[公式]"> 为欧拉常数，约为 <img src="https://www.zhihu.com/equation?tex=0.5772156649" alt="[公式]"> 。 <img src="https://www.zhihu.com/equation?tex=c%28n%29" alt="[公式]"> 为给定样本数 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 时，路径长度的平均值，用来标准化记录 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的路径长度 <img src="https://www.zhihu.com/equation?tex=h%28x%29" alt="[公式]"> 。</p>
</blockquote>
<p><strong>故记录 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的异常得分可以定义为：s(x, n)</strong>.其中， <strong><font color="red"> <img src="https://www.zhihu.com/equation?tex=E%28h%28x%29%29" alt="[公式]"> 为记录 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 在多个 <img src="https://www.zhihu.com/equation?tex=iTree" alt="[公式]"> 中的路径长度的期望值。</font></strong>可视化 <img src="https://www.zhihu.com/equation?tex=s%28x%2Cn%29" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=E%28h%28x%29%29" alt="[公式]"> 的关系：</p>
<p> <img src="https://www.zhihu.com/equation?tex=s%28x%2C+n%29%3D2%5E%7B-%5Cfrac%7BE%28h%28x%29%29%7D%7Bc%28n%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><img src="https://pic4.zhimg.com/80/v2-b7f0fe8132465c0b1919b7c283d1e887_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>可以得出以下结论：</p>
<ul>
<li>当 <img src="https://www.zhihu.com/equation?tex=E%28h%28x%29%29%5Crightarrow+c%28n%29" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=s%5Crightarrow+0.5" alt="[公式]"> ，即记录 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的平均长度与树的平均路径长度相近时，则不能区分是否为异常；</li>
<li>当 <img src="https://www.zhihu.com/equation?tex=E%28h%28x%29%29%5Crightarrow+0" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=s%5Crightarrow+1" alt="[公式]"> ，即记录 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的异常分数接近 <img src="https://www.zhihu.com/equation?tex=1" alt="[公式]"> 时，被判定为异常数据；</li>
<li>当 <img src="https://www.zhihu.com/equation?tex=E%28h%28x%29%29%5Crightarrow+n-1" alt="[公式]"> 时， <img src="https://www.zhihu.com/equation?tex=s%5Crightarrow+0+" alt="[公式]"> ，被判定为正常数据。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title>异常检测（1）概述</title>
    <url>/posts/WH7C02/</url>
    <content><![CDATA[<h1><span id="异常检测-anomaly-detection">异常检测 (anomaly detection)</span></h1><blockquote>
<ul>
<li>「异常检测」开源工具库推荐 - 微调的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/37132428">https://zhuanlan.zhihu.com/p/37132428</a></li>
<li><p><strong>数据挖掘中常见的「异常检测」算法有哪些？</strong> - 微调的回答 - 知乎 <a href="https://www.zhihu.com/question/280696035/answer/417091151">https://www.zhihu.com/question/280696035/answer/417091151</a></p>
</li>
<li><p>不得不推荐这门课：<a href="http://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs259d/">CS259D: Data Mining for Cyber Security</a> 虽然是网络安全方面的应用，但是方法都是通用的，看来也很有启发。<a href="https://leotsui.gitbooks.io/cs259d-notes-cn/content/">https://leotsui.gitbooks.io/cs259d-notes-cn/content/</a></p>
</li>
<li><p>中科院在读美女博士带你全面了解“异常检测”领域 - 王晋东不在家的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/260651151">https://zhuanlan.zhihu.com/p/260651151</a></p>
</li>
<li>Python 时间序列异常检测 ADTK：<a href="https://blog.csdn.net/BF02jgtRS00XKtCx/article/details/115343456">https://blog.csdn.net/BF02jgtRS00XKtCx/article/details/115343456</a></li>
<li>awesome-TS-anomaly-detection：<a href="https://github.com/rob-med/awesome-TS-anomaly-detection">https://github.com/rob-med/awesome-TS-anomaly-detection</a></li>
</ul>
</blockquote>
<h5><span id="异常检测工具">异常检测工具：</span></h5><ul>
<li><p><strong>PyOD:超过30种算法，从经典模型到深度学习模型一应俱全，和sklearn的用法一致</strong></p>
</li>
<li><p><strong>Scikit-Learn:包含了4种常见的算法，简单易用</strong></p>
</li>
<li><p><strong>TODS:与PyOD类似，包含多种时间序列上的异常检测算法</strong></p>
</li>
</ul>
<p><strong>异常检测算法</strong>：</p>
<ul>
<li><strong>线性模型</strong>：<strong>PCA</strong></li>
<li><strong>基于相似度度量的算法</strong>：<strong>KNN</strong>、LOF、HBOS</li>
<li>基于概率的算法：COPOD</li>
<li><strong>集成检测</strong>：<strong>孤立森林</strong>，<strong>XGBOD</strong></li>
<li><strong>神经网络算法</strong>：<strong>自编码器</strong></li>
</ul>
<h5><span id="评估方法">评估方法：</span></h5><ul>
<li>ROC-AUC 曲线</li>
<li>Precision Topk：top K的准确率</li>
<li>AVE Precision：平均准确率</li>
</ul>
<h2><span id="一-概述">一、概述</span></h2><h4><span id="什么是异常检测">什么是异常检测？</span></h4><p>不同于常规模式下的问题和任务，<strong>异常检测针对的是少数、不可预测或不确定、罕见的事件</strong>，它具有独特的复杂性，使得一般的机器学习和深度学习技术无效。</p>
<h4><span id="异常检测面临的挑战"><strong>异常检测面临的挑战</strong></span></h4><ul>
<li><strong>未知性</strong>：异常与许多未知因素有关，例如，具有未知的突发行为、数据结构和分布的实例。它们直到真正发生时才为人所知，比如恐怖袭击、诈骗和网络入侵等应用；</li>
<li><strong>异常类的异构性</strong>： 异常是不规则的，一类异常可能表现出与另一类异常完全不同的异常特征。例如，在视频监控中，抢劫、交通事故和盗窃等异常事件在视觉上有很大差异；</li>
<li><strong>类别不均衡</strong>：异常通常是罕见的数据实例，而正常实例通常占数据的绝大部分。<strong>因此，收集大量标了标签的异常实例是困难的，甚至是不可能的。这导致在大多数应用程序中无法获得大规模的标记数据。</strong></li>
</ul>
<h4><span id="异常的种类"><strong>异常的种类：</strong></span></h4><ul>
<li><strong>点异常</strong>（point anomalies）指的是少数个体实例是异常的，大多数个体实例是正常的，例如正常人与病人的健康指标；</li>
<li><strong>条件异常</strong>（conditional anomalies），又称上下文异常，指的是在特定情境下个体实例是异常的，在其他情境下都是正常的，例如在特定时间下的温度突然上升或下降，在特定场景中的快速信用卡交易；</li>
<li><strong>群体异常</strong>（group anomalies）指的是在群体集合中的个体实例出现异常的情况，而该个体实例自身可能不是异常，例如社交网络中虚假账号形成的集合作为群体异常子集，但子集中的个体节点可能与真实账号一样正常。</li>
</ul>
<h4><span id="异常检测数据集"><strong>异常检测数据集：</strong></span></h4><ul>
<li>统计型数据static data（文本、网络流）</li>
<li><strong>序列型数据sequential data（sensor data ）</strong></li>
<li>空间型数据spatial data（图像、视频）</li>
</ul>
<h4><span id="异常检测的应用领域-1"><strong>异常检测的应用领域 [1] ：</strong></span></h4><ul>
<li><strong>入侵检测</strong>（Intrusion detection）：通过从计算机网络或计算机系统中的若干关键点收集信息并对其执行分析，从中发觉网络或系统中能不能有违反安全策略的行为和遭到袭击的迹象，并对此做出适当反应的流程。最普遍的两种入侵检测系统包括<strong>基于主机的入侵检测系统（HIDS）</strong>、<strong>网络入侵检测系统（NIDS）</strong>。</li>
<li><strong>故障检测</strong>（Fraud detection）：主要是监控系统，在故障发生时可以识别，并且准确指出故障的种类以及出现位置。主要应用领域包括银行欺诈、移动蜂窝网络故障、保险欺诈、医疗欺诈。</li>
<li><strong>恶意软件检测</strong>（Malware Detection）</li>
<li><strong>医疗异常检测</strong>（Medical Anomaly Detection）：通过X光片、核磁共振、CT等医学图像检测疾病或量化异常，也可以通过EEG、ECG等时序信号进行疾病检测或异常预警。</li>
<li><strong>深度学习用于社交网络中的异常检测</strong>（Deep learning for Anomaly detection in Social Networks）</li>
<li><strong>日志异常检测</strong>（Log Anomaly Detection）</li>
<li><strong>物联网大数据异常检测</strong>（Internet of things (IoT) Big Data Anomaly Detection）：通过监控数据流信息检测异常设备和系统行为。</li>
<li><strong>工业异常检测</strong>（Industrial Anomalies Detection）</li>
<li><strong>时间序列中的异常检测</strong>（Anomaly Detection in TimeSeries）</li>
<li><strong>视频监控</strong>（Video Surveillance）：检测视频中的异常场景。</li>
</ul>
<h4><span id="基于标签的可获得性划分异常检测"><strong>基于标签的可获得性划分异常检测：</strong></span></h4><ul>
<li><strong>有监督异常检测</strong>：在训练集中的正常实例和异常实例都有标签，这类方法的缺点在于数据标签难以获得或数据不均衡（正常样本数量远大于异常样本数量）。</li>
<li><strong>半监督异常检测</strong>：<strong>在训练集中只有单一类别（正常实例）的实例，没有异常实例参与训练，目前很多异常检测研究都集中在半监督方法上</strong>，有很多声称是无监督异常检测方法的研究其实也是半监督的，对其解释的是该异常检测是无监督异常检测，学习特征的方式是无监督的，但是评价方式使用了半监督的方法，因此对于无监督与半监督的界定感觉没有那么规范。</li>
<li><strong>无监督异常检测</strong>：在训练集中既有正常实例也可能存在异常实例，但假设数据的比例是正常实例远大于异常实例，模型训练过程中没有标签进行校正。</li>
<li><strong>弱监督异常检测</strong>：该类我研究的少，不是特别了解，主要是针对异常实例不完全、粗粒度标签、部分实例标签错误等情况进行算法设计。</li>
</ul>
<h4><span id="基于传统方法的异常检测模型">基于传统方法的异常检测模型</span></h4><ul>
<li><strong>基于重构的方法</strong>：<strong><font color="red"> 假设异常点是不可被压缩的或不能从低维映射空间有效地被重构的。</font></strong>常见的方法有<strong>PCA</strong>、<strong>Robust PCA</strong>、random projection等降维方法 [4,5] 。</li>
<li><strong>聚类分析方法</strong>：通过聚类可以创建数据的模型，而异常点的存在可以扭曲、破坏该模型。常见的方法有Gaussian Mixture Models、 k-means、 multivariate Gaussian Models [6,7,8]。</li>
<li><strong>一类分类方法</strong>：对正常数据建立区分性边界，异常点被划分到边界外。常见的方法有<strong>OC-SVM</strong> [9,10]。</li>
</ul>
<h2><span id="二-常见异常检测算法">二、常见异常检测算法</span></h2><p><strong>一般情况下, 可以把异常检测看成是数据不平衡下的分类问题</strong>。因此, 如果数据条件允许, 优先使 用有监督的异常检测[6]。实验结果 [4]发现直接用XGBOOST进行有监督异常检测往往也能得到不错 的结果，没有思路时不妨一试。</p>
<p><strong>而在仅有少量标签的情况下, 也可采用半监督异常检测模型</strong>。比如把无监督学习作为一种特征抽取方式来辅助监督学习 $[4,8]$, 和stacking比较类似。这种方法也可以理解成通过无监督的特征工程 对数据进行预处理后, 喂给有监督的分类模型。</p>
<p>但在现实情况中, <strong>异常检测问题往往是没有标签的, 训练数据中并末标出哪些是异常点, 因此必须用无监督学习。</strong>从实用角度出发, 我们把文章的重点放在无监督学习上。</p>
<p>本文结构如下: 1. 介绍常见的无监督异常算法及实现; 2. 对比多种算法的检测能力；3. 对比多种算法的运算开销；4. 总结并归纳如何处理异常检测问题。5. 代码重现步骤。</p>
<h3><span id="21-无监督异常检测">2.1 无监督异常检测</span></h3><p>如果归类的话, 无监督异常检测模型可以大致分为:</p>
<ul>
<li><strong>统计与概率模型</strong> (statistical and probabilistic and models) ：<strong>主要是对数据的分布做出假设, 并找出假设下所定义的“异常”, 因此往往会使用极值分析Q或者假设检验</strong>。比如对最简单的一维 数据假设高斯分布 $Q$, 然后将距离均值特定范围以外的数据当做异常点。而推广到高维后, 可以 假设<strong>每个维度各自独立</strong>, <strong>并将各个维度上的异常度相加</strong>。如果考虑特征间的相关性, 也可以用马 氏距离 (mahalanobis distance) 来衡量数据的异常度[12]。不难看出, 这类方法最大的好处就 是速度一般比较快, 但因为存在比较强的”假设”, 效果不一定很好。<strong>稍微引申一点的话, 其实给每个维度做个直方图做密度估计, 再加起来就是HBOS。</strong></li>
<li><p><strong>线性模型（linear models）</strong>：假设数据在低维空间上有嵌入, 那么无法、或者在低维空间投射后 表现不好的数据可以认为是离群点 $Q$ 。</p>
<ul>
<li><strong>举个简单的例子, PCA可以用于做异常检测</strong> [10], 一种方法 就是找到 $k$ 个特征向量 (eigenvectora), 并计算每个样本再经过这 $k$ 个特征向量投射后的<strong>重建误差 (reconstruction error)</strong>, 而正常点的重建误差应该小于异常点。</li>
<li>同理, 也可以计算每个样本 到这 $k$ 个选特征向量所构成的超空间的加权欧氏距离（特征值越小权重越大）。在相似的思路下, 我们也可以直接对协方差矩阵 $Q$ 进行分析, 并把样本的马氏距离 (在考虑特征间关系时样本到分 布中心的距离) 作为样本的异常度, 而这种方法也可以被理解为一种软性 (Soft PCA) [6]。</li>
<li>同时, 另一种经典算法One-class SVM[3]也一般被归类为线性模型。</li>
</ul>
</li>
<li><p><strong>基于相似度衡量</strong>的模型 (proximity based models) : <strong>异常点因为和正常点的分布不同, 因此相似度较低, 由此衍生了一系列算法通过相似度来识别异常点</strong>。</p>
<ul>
<li>比如最简单的<strong>K近邻</strong>就可以做异常 检测,一个样本和它第k个近邻的距离就可以被当做是异常值, 显然异常点的k近邻距离更大。</li>
<li>同理, <strong>基于密度分析如LOF</strong> [1]、<strong>LOCI</strong>和<strong>LoOP主要是通过局部的数据密度来检测异常</strong>。显然, 异常点所在空间的数据点少, 密度低。</li>
<li>相似的是, <strong><font color="red"> Isolation Forest[2]通过划分超平面Q来计算”孤立” 一个样本所需的超平面数量</font></strong>（可以想象成在想吃蛋糕上的樱桃所需的最少刀数）。在密度低的空间里 (异常点所在空间中), 孤例一个样本所需要的划分次数更少。</li>
<li>另一种相似的算法<strong>ABOD</strong>[7] <strong>是计算每个样本与所有其他样本对所形成的夹角的方差</strong>, 异常点因为远离正常点, 因此方差变化 小。换句话说, 大部分异常检测算法都可以被认为是一种估计相似度, 无论是通过密度、距离、 夹角或是划分超平面。通过聚类也可以被理解为一种相似度度量, 比较常见不再赘述。</li>
</ul>
</li>
<li><strong>集成异常检测与模型融合</strong>: 在无监督学习时, 提高模型的鲁棒性很重要, 因此集成学习就大有用 武之地。比如上面提到的lsolation Forest, 就是基于构建多棵决策树实现的。最早的集成检测框 架feature bagging[9]与分类问题中的随机森林 (random forest) 很像, 先将训练数据随机划分 (每次选取所有样本的 $d / 2-d$ 个特征, $d$ 代表特征数) , 得到多个子训练集, 再在每个训练集上训 练一个独立的模型（默认为LOF）并最终合并所有的模型结果（如通过平均）。值得注意的是, 因为没有标签, 异常检测往往是通过bagging和feature bagging比较多, 而boosting比较少见。 boosting情况下的异常检测, 一般需要生成伪标签Q, 可参靠 $[13,14]$ 。集成异常检测是一个新兴但很有趣的领域, 综述文章可以参考 $[16,17,18]$ 。</li>
<li><strong>特定领域上的异常检测</strong>：比如图像异常检测 [21], 顺序及<strong>流数据异常检测（时间序列异常检测）</strong> [22], 以及高维空间上的异常检测 [23], 比如前文提到的Isolation Forest就很适合高维数据上的 异常检测。</li>
</ul>
<p><strong>不难看出, 上文提到的划分标准其实是互相交织的</strong>。比如k-近邻可以看做是概率模型非参数化后的 一种变形, 而通过马氏距离计算异常度虽然是线性模型但也对分布有假设（高斯分布）。Isolation Forest虽然是集成学习, 但其实和分析数据的密度有关, 并且适合高维数据上的异常检测。在这种 基础上, 多种算法其实是你中有我, 我中有你, <strong><font color="red"> 相似的理念都可以被推广和应用, 比如计算重建误 差不仅可以用PCA, 也可以用神经网络中的auto-encoder。</font></strong>另一种划分异常检测模型的标准可以理 解为局部算法 (local) 和全局算法 (global), 这种划分方法是考虑到异常点的特性。想要了解更多异常检测还是推荐看经典教科书Outlier Analysis [6], 或者综述文章[15]。</p>
<p><strong>虽然一直有新的算法被提出, 但因为需要采用无监督学习, 且我们对数据分布的有限了解, 模型选 择往往还是采用试错法,</strong> 因此快速迭代地尝试大量的算法就是一个必经之路。在这个回答下, 我们 会对比多种算法的预测能力、运算开销及模型特点。如无特别说明，本文中的图片、代码均来自于 开源Python异常检测工具库Pyod。文中实验所使用的17个数据集均来自于 (ODDS - Outlier Detection DataSets) 。</p>
<h3><span id="一-isolation-forest">一、Isolation Forest</span></h3><blockquote>
<p>  孤立森林(isolation Forest)-一个通过瞎几把乱分进行异常检测的算法 - 小伍哥聊风控的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/484495545">https://zhuanlan.zhihu.com/p/484495545</a></p>
<h5><span id="isolation-forest算法梳理isolation-forest算法梳理">Isolation Forest算法梳理🌳</span></h5></blockquote>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026280.png" alt="Isolation Forest算法梳理🌳"></p>
<p><strong>异常检测 (anomaly detection)</strong>，或者又被称为“<strong>离群点检测</strong>” (outlier detection)，是机器学习研究领域中跟现实紧密联系、有广泛应用需求的一类问题。但是，什么是异常，并没有标准答案，通常因具体应用场景而异。如果要给一个比较通用的定义，很多文献通常会引用 Hawkins 在文章开头那段话。很多后来者的说法，跟这个定义大同小异。这些定义虽然笼统，但其实暗含了认定“异常”的两个标准或者说假设：</p>
<h3><span id="二-hbos">二、HBOS</span></h3><p><strong>HBOS全名为：Histogram-based Outlier Score</strong>。它是一种单变量方法的组合，不能对特征之间的依赖关系进行建模，但是计算速度较快，对大数据集友好，其基本假设是数据集的每个维度相互独立，然后对<strong>每个维度进行区间(bin)划分，区间的密度越高，异常评分越低。理解了这句话，基本就理解了这个算法。</strong></p>
<h4><span id="21-hbos算法流程"><strong>2.1 HBOS算法流程</strong></span></h4><h5><span id="1-静态宽度直方图"><strong>1、静态宽度直方图</strong></span></h5><p>标准的直方图构建方法，在值范围内使用k个等宽箱，样本落入每个箱的频率（相对数量）作为密度（箱子高度）的估计，时间复杂度：O(n)</p>
<p><strong>注意：</strong>等宽分箱，每个箱中的数据宽度相同，不是指数据个数相同。例如序列[5,10,11,13,15,35,50,55,72,92,204,215]，数据集中最大值是215，最小值是5，分成3个箱，故每个箱的宽度应该为（215-5）/3=70，所以箱的宽度是70，这就要求箱中数据之差不能超过70，并且要把不超过70的数据全放在一起，最后的分箱结果如下：</p>
<p><strong>箱一：5,10，11,13,15,35,50,55,72；箱二：92；箱三：204,215</strong></p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026218.png" alt="图片" style="zoom:48%;"></p>
<h5><span id="2-动态宽度直方图"><strong>2、动态宽度直方图</strong></span></h5><p>首先对所有值进行排序，然后固定数量的N/k 个连续值装进一个箱里，其 中N是总实例数，k是箱个数，<strong>直方图中的箱面积表示实例数</strong>，因为箱的宽度是由箱中第一个值和最后一个值决定的，所有箱的面积都一样，因此每一个箱的高度都是可计算的。这意味着跨度大的箱的高度低，即密度小，只有一种情况例外，超过k个数相等，此时允许在同一个箱里超过N/k值，时间复杂度：O(n×log(n))</p>
<p>还是用序列[<strong>5,10,11,13,15</strong>,<strong>35,50,55,72</strong>,92,204,215]举例，也是假如分3箱，那么每箱都是4个，宽度为边缘之差，第一个差为15-5=10，第二差为72-35=37，第三个箱宽为215-92=123，为了保持面积相等，所以导致后面的很矮，前面的比较高，如下图所示（非严格按照规则）：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026012.png" alt="图片" style="zoom:50%;"></p>
<h5><span id="3-算法推导过程"><strong>3、算法推导过程</strong></span></h5><p><strong>对每个维度都计算了一个独立的直方图，其中每个箱子的高度表示密度的估计，然后为了使得最大高度为1（确保了每个特征与异常值得分的权重相等），对直方图进行归一化处理。</strong>最后，每一个实例的HBOS值由以下公式计算：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026886.png" alt="图片" style="zoom: 67%;"></p>
<p><strong>推导过程：</strong></p>
<p>假设样本p第 i 个特征的概率密度为pi ( p ) ，则p的概率密度可以计算为，d为总的特征的个数：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026098.png" alt="图片" style="zoom: 67%;"></p>
<p>两边取对数：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026442.png" alt="图片" style="zoom: 67%;"></p>
<p>概率密度越大，异常评分越小，为了方便评分，两边乘以“-1”：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026675.png" alt="图片" style="zoom: 67%;"></p>
<p>最后可得：</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304182026797.png" alt="图片" style="zoom: 67%;"></p>
<p>PyOD是一个可扩展的Python工具包，用于检测多变量数据中的异常值。它可以在一个详细记录API下访问大约20个离群值检测算法。</p>
<h3><span id="三-xgbod">三、 XGBOD</span></h3><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/349519844">【异常检测】<em>XGBOD</em>：用无监督表示学习改进有监督异常检测</a></p>
</blockquote>
<h4><span id="四-copod用统计机器学习检测异常">四、COPOD：用「统计」+「机器学习」检测异常</span></h4><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/338189299">COPOD：用「统计」+「机器学习」检测异常</a></p>
</blockquote>
<h3><span id="五-more-about-anomaly-detection">五、More about Anomaly Detection</span></h3><p>那这个异常检测啊,其实也是另外一门学问,那我们课堂上就没有时间讲了,异常检测不是只能用 Aauto-Encoder 这个技术,Aauto-Encoder 这个技术,只是众多可能方法里面的其中一个,我们拿它来当做 Aauto-Encoder 的作业,因为我相信,你未来有很多的机会用得上异常检测这个技术,那实际上有关异常检测更完整的介绍,我们把过去上课的录影放在这边,给大家参考,</p>
<p>Part 1: <a href="https://youtu.be/gDp2LXGnVLQ">https://youtu.be/gDp2LXGnVLQ</a></p>
<ul>
<li><h5><span id="简介">简介</span></h5></li>
</ul>
<p>Part 2: <a href="https://youtu.be/cYrNjLxkoXs">https://youtu.be/cYrNjLxkoXs</a></p>
<ul>
<li><strong>信心分数</strong></li>
</ul>
<p>Part 3: <a href="https://youtu.be/ueDlm2FkCnw">https://youtu.be/ueDlm2FkCnw</a></p>
<ul>
<li><h5><span id="异常检测系统的评估">异常检测系统的评估？</span></h5><ul>
<li>no ACC</li>
<li>cost loss设计</li>
<li>RUC</li>
</ul>
</li>
</ul>
<p>Part 4: <a href="https://youtu.be/XwkHOUPbc0Q">https://youtu.be/XwkHOUPbc0Q</a></p>
<p>Part 5: <a href="https://youtu.be/Fh1xFBktRLQ">https://youtu.be/Fh1xFBktRLQ</a></p>
<ul>
<li>无监督</li>
</ul>
<p>Part 6: <a href="https://youtu.be/LmFWzmn2rFY">https://youtu.be/LmFWzmn2rFY</a></p>
<p>Part 7: <a href="https://youtu.be/6W8FqUGYyDo">https://youtu.be/6W8FqUGYyDo</a></p>
<p>那以上就是有关 Aauto-Encoder 的部分</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（14）【Nan】LDA(主题模型)</title>
    <url>/posts/322BXWN/</url>
    <content><![CDATA[<h2><span id="ldalatent-dirichlet-allocation-主题模型"></span></h2><p><strong>同一个主题，在不同的文章中，他出现的比例(概率)是不同的</strong>，看到这里，读者可能已经发现，文档和主题之间的关系和主题和词汇的关系是多么惊人的类似！</p>
<blockquote>
<p>  LDA于2003年由 David Blei, Andrew Ng和 Michael I. Jordan提出，因为模型的简单和有效，掀起了主题模型研究的波浪。虽然说LDA模型简单，但是它的数学推导却不是那么平易近人，一般初学者会深陷数学细节推导中不能自拔。于是牛人们看不下去了，纷纷站出来发表了各种教程。国内方面rickjin有著名的《<a href="https://link.zhihu.com/?target=http%3A//www.52nlp.cn/author/rickjin">LDA数学八卦</a>》，国外的Gregor Heinrich有著名的《<a href="https://link.zhihu.com/?target=https%3A//users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf">Parameter estimation for text analysis</a>》。其实有了这两篇互补的通俗教程，大家沉住心看个4、5遍，基本就可以明白LDA为什么是简单的了。那么其实也没我什么事了，然而心中总有一种被大牛点播的豁然开朗的快感，实在是不吐不快啊。</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>概率图模型</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（8）【Nan】CRF</title>
    <url>/posts/1799EFH/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>机器学习</category>
        <category>概率图模型</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（8）【Nan】马尔可夫模型</title>
    <url>/posts/16VDGF1/</url>
    <content><![CDATA[<h2><span id="马尔可夫模型">马尔可夫模型</span></h2><blockquote>
<p>  如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？ - Scofield的回答 - 知乎 <a href="https://www.zhihu.com/question/35866596/answer/236886066">https://www.zhihu.com/question/35866596/answer/236886066</a></p>
</blockquote>
<h3><span id="一-概念">一、概念</span></h3><h4><span id="11-随机过程">1.1 随机过程</span></h4><p><strong>随机过程就是一些统计模型，利用这些统计模型可以对自然界的一些事物进行预测和处理</strong>，比如天气预报，比如股票，比如市场分析，比如人工智能。它的应用还真是多了去了。</p>
<h4><span id="12-马尔可夫链-markov-chain">1.2 马尔可夫链 （Markov Chain）</span></h4><blockquote>
<p>  马尔可夫链 （Markov Chain）是什么鬼 - 车卡门的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/26453269">https://zhuanlan.zhihu.com/p/26453269</a></p>
</blockquote>
<p>马尔可夫链就是这样一个任性的过程，它将来的状态分布只取决于现在，跟过去无关！实际上就是一个随机变量随时间按照Markov性进行变化的过程。</p>
<h4><span id="13-马尔可夫模型hidden-markov-models">1.3 马尔可夫模型（Hidden Markov Models）</span></h4><p>既是马尔可夫模型，就一定存在<a href="https://www.zhihu.com/search?q=马尔可夫链&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A&quot;64187492&quot;}">马尔可夫链</a>，该马尔可夫链服从马尔可夫性质：即无记忆性。也就是说，这一时刻的状态，受且只受前一时刻的影响，而不受更往前时刻的状态的影响。在这里我们仍然使用非常简单的天气模型来做说明。</p>
<p><img src="https://picx.zhimg.com/80/648a55725e67d718d97d6a475891d70b_1440w.png" alt="img" style="zoom:50%;"></p>
<p>在这个马尔可夫模型中，存在三个状态，Sunny， Rainy， Cloudy，同时图片上标的是各个状态间的转移概率（如果不明白什么是转移概率，那建议先去学习什么是马尔可夫再来看HMM）。</p>
<p><strong><font color="red"> 现在我们要说明什么是 HMM。既是隐形，说明这些状态是观测不到的，相应的，我们可以通过其他方式来『猜测』或是『推断』这些状态，这也是 HMM 需要解决的问题之一。</font></strong></p>
<blockquote>
<p>  举个例子，我女朋友现在在北京工作，而我还在法国读书。每天下班之后，她会根据天气情况有相应的活动：或是去商场购物，或是去公园散步，或是回家收拾房间。我们有时候会通电话，她会告诉我她这几天做了什么，而闲着没事的我呢，则要通过她的行为猜测这几天对应的天气最有可能是什么样子的。</p>
<p>  以上就是一个简单的 HMM，天气状况属于状态序列，而她的行为则属于观测序列。天气状况的转换是一个马尔可夫序列。而根据天气的不同，有相对应的概率产生不同的行为。在这里，为了简化，把天气情况简单归结为晴天和雨天两种情况。雨天，她选择去散步，购物，收拾的概率分别是0.1，0.4，0.5， 而如果是晴天，她选择去散步，购物，收拾的概率分别是0.6，0.3，0.1。而天气的转换情况如下：这一天下雨，则下一天依然下雨的概率是0.7，而转换成晴天的概率是0.3；这一天是晴天，则下一天依然是晴天的概率是0.6，而转换成雨天的概率是0.4. 同时还存在一个初始概率，也就是第一天下雨的概率是0.6， 晴天的概率是0.4.</p>
<p>  <img src="https://pic4.zhimg.com/80/792e033ff9b0418b3b6c9bbaef30fd83_1440w.png" alt="img" style="zoom:50%;"></p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>概率图模型</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（5）SVM</title>
    <url>/posts/12XTPRK/</url>
    <content><![CDATA[<h2><span id="支持向量机-svm">支持向量机 SVM</span></h2><p><strong><font color="red"> SVM 是一个非常优雅的算法，具有完善的数学理论，虽然如今工业界用到的不多，但还是决定花点时间去写篇文章整理一下。</font></strong></p>
<p><strong>本质：SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。</strong>为了对数据中的噪声有一定的容忍能力。<strong>以几何的角度，在丰富的数据理论的基础上，简化了通常的分类和回归问题。</strong></p>
<p><strong>几何意义</strong>：找到一个超平面将特征空间的正负样本分开，最大分隔（对噪音有一定的容忍能力）；</p>
<p><strong>间隔表示</strong>：划分超平面到属于不同标记的最近样本的距离之和；</p>
<p><img src="https://lzy-picture.oss-cn-beijing.aliyuncs.com/img/202304191157912.jpg" alt="【机器学习】支持向量机 SVM（非常详细）" style="zoom: 33%;"></p>
<span id="more"></span>
<h2><span id="一-支持向量">一、支持向量</span></h2><blockquote>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/52168498">https://zhuanlan.zhihu.com/p/52168498</a></p>
<p>【机器学习】支持向量机 SVM（非常详细）:<a href="https://zhuanlan.zhihu.com/p/77750026">https://zhuanlan.zhihu.com/p/77750026</a></p>
<p><strong>KKT条件</strong>：<strong>判断不等式约束问题是否为最优解的必要条件</strong></p>
</li>
</ul>
</blockquote>
<h3><span id="11-线性可分">1.1 线性可分</span></h3><p>首先我们先来了解下什么是线性可分。</p>
<p><img src="https://s2.loli.net/2023/04/17/ycmF6werxUvYLh5.jpg" alt="img" style="zoom: 33%;"></p>
<p>在二维空间上, 两类点被一条直线完全分开叫做线性可分。严格的数学定义是:<br>$D_0$ 和 $D_1$ 是 $\mathrm{n}$ 维欧氏空间中的两个点集。如果存在 $\mathrm{n}$ 维向量 $\mathrm{w}$ 和实数 $\mathrm{b}$, 使得所有属于 $D_0$ 的点 $x_i$ 都有 $w x_i+b&gt;0$ ， 而对于所有属于 $D_1$ 的点 $x_j$ 则有 $w x_j+b&lt;0$ ， 则我们称 $D_0$ 和 $D_1$ 线性可分。</p>
<h3><span id="12-最大间隔超平面">1.2 最大间隔超平面</span></h3><p>从二维扩展到多维空间中时, 将 $D_0$ 和 $D_1$ 完全正确地划分开的 $w x+b=0$ 就成了一个超平面。<br>为了使这个超平面更具鲁棒性, 我们会去找最佳超平面, 以最大间隔把两类样本分开的超平面, 也称之为最大 间隔超平面。</p>
<ul>
<li>两类样本分别分割在该超平面的两侧；</li>
<li><strong>两侧距离超平面最近的样本点到超平面的距离被最大化了。</strong>【附近点】</li>
</ul>
<h3><span id="13-支持向量-距离超平面最近的点">1.3 支持向量 【距离超平面最近的点】</span></h3><p><img src="https://s2.loli.net/2023/04/17/HDlAed2wZf8cXi1.jpg" alt="img" style="zoom: 33%;"></p>
<p>样本中距离超平面最近的一些点，这些点叫做支持向量。</p>
<h3><span id="14-svm-最优化问题">1.4 SVM 最优化问题</span></h3><p><strong>SVM 想要的就是找到各类样本点到超平面的距离最远, 也就是找到最大间隔超平面</strong>。任意超平面可以用下面这个 线性方程来描述：</p>
<script type="math/tex; mode=display">
w^T x+b=0</script><p>二维空间点 $(x, y)$ 到直线 $A x+B y+C=0$ 的距离公式是:</p>
<script type="math/tex; mode=display">
\frac{|A x+B y+C|}{\sqrt{A^2+B^2}}</script><p>扩展到 $\mathbf{n}$ 维空间后, 点 $x=\left(x_1, x_2 \ldots x_n\right)$ 到直线 $w^T x+b=0$ 的距离为:</p>
<script type="math/tex; mode=display">
\frac{\left|w^T x+b\right|}{\|w\|}</script><p>其中 $|w|=\sqrt{w_1^2+\ldots w_n^2}$ 。<br>如图所示，根据支持向量的定义我们知道, 支持向量到超平面的距离为 $d$ ，其他点到超平面的距离大于 $d$ 。</p>
<p><img src="https://s2.loli.net/2023/04/17/2yZRMfWemFcoX75.jpg" alt="img" style="zoom: 33%;"></p>
<p>于是我们有这样的一个公式：</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}
\frac{w^T x+b}{\|w\|} \geq d \quad y=1 \\
\frac{w^T x+b}{\|w\|} \leq-d \quad y=-1
\end{array}\right.</script><p>将两个方程合并, 我们可以简写为:</p>
<script type="math/tex; mode=display">
y\left(w^T x+b\right) \geq 1</script><p>至此我们就可以得到最大间隔超平面的上下两个超平面：</p>
<p><img src="https://s2.loli.net/2023/04/17/aZbmfK4zWsSjyo2.png" alt="image-20220409203050568" style="zoom:50%;"></p>
<p><strong>间隔</strong>：<strong>训练集中离划分超平面最近的样本到划分超平面距离的两倍</strong>。有了间隔的定义，划分超平面“离正负样本都比较远”这一目标可以等价描述为正负样本里划分超平面的距离尽可能远。即<strong>让离划分超平面最近的样本到划分超平面距离尽可能远</strong>。优化目标：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max _{w, b} \gamma &=\max _{w, b}\left(2 \min _{i} \frac{1}{\|w\|}\left|w^{\top} x_{i}+b\right|\right) \\
&=\max _{w, b} \min _{i} \frac{2}{\|w\|}\left|w^{\top} x_{i}+b\right|
\end{aligned}</script><p><strong>简化过程</strong>：</p>
<ul>
<li><p><strong>缩放</strong>：为了简化优化问题, 我们可以通过调整 $(w, b)$ 使得：</p>
<script type="math/tex; mode=display">
\min _{i}\left|w^{\top} x_{i}+b\right|=1 .</script></li>
<li><p><strong>标签替换绝对值</strong>：</p>
<script type="math/tex; mode=display">
s.t. \min _{i} y_{i}\left(w^{\top} x_{i}+b\right)=1</script></li>
<li><p><strong>简化约束条件</strong>【<strong>反正法</strong>】</p>
</li>
</ul>
<h4><span id="硬间隔线性svm的基本型"><strong><font color="red"> 硬间隔线性SVM的基本型：</font></strong></span></h4><script type="math/tex; mode=display">
\begin{array}{ll}
\min _{w, b} & \frac{1}{2} w^{\top} w \\
\text { s.t. } & y_{i}\left(w^{\top} x_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m
\end{array}</script><p><img src="https://s2.loli.net/2023/04/17/l4WjzV7smKBoe8f.png" alt="image-20220423163130805" style="zoom:50%;"></p>
<p><strong>二次规划是指目标函数是二次函数，约束是线性不等式约束的一类优化问题</strong> + 凸函数</p>
<h2><span id="二-硬间隔线性svm对偶型">二、硬间隔线性SVM对偶型</span></h2><p><strong>本科高等数学学的拉格朗日程数法是等式约束优化问题</strong>:</p>
<script type="math/tex; mode=display">
\begin{gathered}
\min f\left(x_1, x_2, \ldots, x_n\right) \\
\text { s.t. } \quad h_k\left(x_1, x_2, \ldots, x_n\right)=0 \quad k=1,2, \ldots, l
\end{gathered}</script><p>我们令 $L(x, \lambda)=f(x)+\sum_{k=1}^l \lambda_k h_k(x)$, 函数 $L(x, y)$ 称为 Lagrange 函数, 参数 $\lambda$ 称为 Lagrange 乘子没有非负 要求。<br>利用必要条件找到可能的极值点:</p>
<script type="math/tex; mode=display">
\begin{cases}\frac{\partial L}{\partial x_i}=0 & i=1,2, \ldots, n \\ \frac{\partial L}{\partial \lambda_k}=0 & k=1,2, \ldots, l\end{cases}</script><p>具体是否为极值点需根据问题本身的具体情况检验。这个方程组称为等式约束的极值必要条件。等式约束下的 Lagrange 乘数法引入了 $l$ 个 Lagrange 乘子, 我们将 $x_i$ 与 $\lambda_k$ 一视同仁, 把 $\lambda_k$ 也看作优化变 量, 共有 $(n+l)$ 个优化变量。</p>
<h5><span id="1写成约束优化问题的基本型">（1）写成约束优化问题的基本型</span></h5><script type="math/tex; mode=display">
\begin{array}{ll}
\min _{w, b} & \frac{1}{2} w^{\top} w \\
\text { s.t. } & 1-y_{i}\left(w^{\top} x_{i}+b\right) \leq 0, \quad i=1,2, \ldots, m
\end{array}</script><h5><span id="2-构建基本型的拉格朗日函数">（2） 构建基本型的拉格朗日函数</span></h5><script type="math/tex; mode=display">
\mathcal{L}(w, b, \alpha):=\frac{1}{2} w^{\top} w+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(w^{\top} x_{i}+b\right)\right)</script><h5><span id="3交换min-max顺序">（3）交换min, max顺序</span></h5><blockquote>
<p>  <strong>解得最优解 $u^{\star}$ 。这样两层优化问题将变为一层最大化（max）问题, 问题难度大大降低, 称为对偶问题 (Dual Problem) :</strong>【<strong>对偶问题是原问题的下界</strong>】</p>
<ul>
<li><script type="math/tex; mode=display">
\max _{\alpha}, \beta \min _{u} \mathcal{L}(u, \alpha, \beta) \leq \min _{u} \max _{\alpha}, \beta \mathcal{L}(u, \alpha, \beta)</script></li>
<li><p>硬间隔线性SVM满足<strong>Slater条件</strong>， <strong>因此原问题和对偶问题等价</strong></p>
</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\begin{array}{cl}
\max _{\alpha} \min _{w, b} & \frac{1}{2} w^{\top} w+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(w^{\top} x_{i}+b\right)\right) \\
\text { s.t. } & \alpha_{i} \geq 0, \quad i=1,2, \ldots, m .
\end{array}</script><p>首先计算 $w$ 的最优值, 令 $\frac{\partial \mathcal{L}}{\partial w}=\mathbf{0}$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial w} &=\frac{\partial}{\partial w}\left(\frac{1}{2} w^{\top} w+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(w^{\top} x_{i}+b\right)\right)\right) \\
&=w+\sum_{i=1}^{m} \alpha_{i}\left(-y_{i} x_{i}\right) \\
&=w-\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i} \\
&=\mathbf{0}
\end{aligned}</script><p>可以解得最优值$w$</p>
<script type="math/tex; mode=display">
w^{\star}=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}</script><p>然后计算 $b$ 的最优值, 令 $\frac{\partial \mathcal{L}}{\partial b}=0$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial b} &=\frac{\partial}{\partial b}\left(\frac{1}{2} w^{\top} w+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(w^{\top} x_{i}+b\right)\right)\right) \\
&=\sum_{i=1}^{m} \alpha_{i}\left(-y_{i}\right) \\
&=-\sum_{i=1}^{m} \alpha_{i} y_{i} \\
&=0
\end{aligned}</script><p>可以得到一个等式 $b^{\star}$ </p>
<script type="math/tex; mode=display">
\sum_{i=1}^{m} \alpha_{i} y_{i}=0</script><p>注意到这里并没有给出最优值 $b^{\star}$ 应该是多少, 而是一个等式, 该等式是一个约束项, 而最优值通过后面的 <strong>KKT 条件</strong>的互补松弛可以计算得到。</p>
<h4><span id="硬性间隔线性svm的对偶型"><strong><font color="red"> 硬性间隔线性SVM的对偶型：</font></strong></span></h4><script type="math/tex; mode=display">
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i}^{\top} x_{j}-\sum_{i=1}^{m} \alpha_{i} \\
\text { s.t. } & \alpha_{i} \geq 0, \quad i=1,2, \ldots, m \\
& \sum_{i=1}^{m} \alpha_{i} y_{i}=0
\end{array}</script><h5><span id="4利用kkt条件得到主问题的最优解">（4）利用KKT条件得到主问题的最优解</span></h5><p><strong>KKT 条件是指优化问题在最优处（包括基本型的最优值，对偶问题的最优值）必须满足的条件</strong>。</p>
<p>线性支持向量机的 <strong>KKT 条件</strong>:</p>
<ul>
<li><strong>主问题可行</strong>: $g<em>{i}\left(u^{\star}\right)=1-y</em>{i}\left(w^{\star \top} x_{i}+b^{\star}\right) \leq 0$ ；</li>
<li><strong>对偶问题可行</strong>: $\alpha_{i}^{\star} \geq 0$;</li>
<li><strong>主变量最优</strong>: $w^{\star}=\sum<em>{i=1}^{m} \alpha</em>{i} y<em>{i} x</em>{i}, \sum<em>{i=1}^{m} \alpha</em>{i} y_{i}=0$;</li>
<li><font color="red"> **互补松弛**: $\alpha_{i}^{\star} g_{i}\left(u^{\star}\right)=\alpha_{i}^{\star}\left(1-y_{i}\left(w^{\star \top} x_{i}+b^{\star}\right)\right)=0$ ；</font>

</li>
</ul>
<p><strong>根据 KKT 条件中的 $\alpha<em>{i}^{\star} \geq 0$, 我们可以根据 $\alpha</em>{i}^{\star}$ 的取值将训练集 $D$ 中所有的样本分成两类</strong>, 如 图 17 所示。</p>
<ul>
<li>如果 $\alpha<em>{i}^{\star}&gt;0$, <strong>对应的样本称为支持向量 (Support Vector)</strong>, 根据 $\alpha</em>{i}^{\star}\left(1-y<em>{i}\left(w^{\star \top} x</em>{i}+b^{\star}\right)\right)=0$ , 那么一定有 $y<em>{i}\left(w^{\star \top} x</em>{i}+b^{\star}\right)=1$, 该样本是距离划分超平面最近的样本, 位于最大间隔边界 （见第 $2.3$ 节）;</li>
<li>如果 $\alpha<em>{i}^{\star}=0$, 对应的样本不是非支持向量, 那么有 $y</em>{i}\left(w^{\star \top} x_{i}+b^{\star}\right) \geq 1$, 该样本不一定是距离 划分超平面最近的样本, <strong>位于最大间隔边界或之外</strong>。</li>
</ul>
<p><img src="https://s2.loli.net/2023/04/17/Gkx9AygPfvXmheq.png" alt="image-20220409212350705" style="zoom:50%;"></p>
<p><strong>结论</strong>：</p>
<ul>
<li><strong><font color="red"> 参数 w， b 仅由支持向量决定，与训练集的其他样本无关；</font></strong></li>
<li><strong><font color="red"> 对偶性是非参数模型，预测阶段不仅需要$\alpha_{i}$参数，还支持向量；</font></strong></li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&h(x):=\operatorname{sign}\left(w^{\star \top} x+b^{\star}\right) \quad \text { （硬间隔线性 SVM 的基本型的假设函数） }\\
&=\operatorname{sign}\left(\sum_{i \in S V} \alpha_{i}^{\star} y_{i} x_{i}^{\top} x+b^{\star}\right) \text {.(硬间隔线性 SVM 的对偶型的假设函数） }
\end{aligned}</script><h2><span id="三-svm优化方法">三、SVM优化方法</span></h2><p>SMO算法求解</p>
<p>我们可以看出来这是一个二次规划问题, 问题规模正比于训练样本数, 我们常用 SMO(Sequential Minimal Optimization) 算法求解。</p>
<font color="red"> SMO(Sequential Minimal Optimization), 序列最小优化算法【基于坐标下降算法】, 其核心思想非常简单: 每次只优化一个参数, 其他参数先固定住, 仅求当前这个优化参数的极值。我们来看一下 SMO 算法在 SVM 中的 应用。</font>

<p>我们刚说了 SMO 算法每次只优化一个参数, 但我们的优化目标有约束条件: $\sum_{i=1}^n \lambda_i y_i=0$, 没法一次只变动一个 参数。所以我们选择了一次选择两个参数。具体步骤为：</p>
<ol>
<li>选择两个需要更新的参数 $\lambda_i$ 和 $\lambda_j$, 固定其他参数。于是我们有以下约束:<br>这样约束就变成了:<script type="math/tex; mode=display">
\lambda_i y_i+\lambda_j y_j=c \quad \lambda_i \geq 0, \lambda_j \geq 0</script>其中 $c=-\sum_{k \neq i, j} \lambda_k y_k$, 由此可以得出 $\lambda_j=\frac{c-\lambda_i y_i}{y_j}$ ，也就是说我们可以用 $\lambda_i$ 的表达式代替 $\lambda_j$ 。这样就相当 于把目标问题转化成了仅有一个约束条件的最优化问题, 仅有的约束是 $\lambda_i \geq 0$ 。</li>
<li>对于仅有一个约束条件的最优化问题, 我们完全可以在 $\lambda<em>i$ 上对优化目标求偏导, 令导数为零, 从而求出变量 值 $\lambda</em>{i<em>{\text {new }}}$ ，然后根据 $\lambda</em>{\text {inew }}$ 求出 $\lambda<em>{j</em>{\text {new }}}$ 。</li>
<li>多次迭代直至收敛。通过 SMO 求得最优解 $\lambda^*$ 。</li>
</ol>
<h2><span id="四-软间隔线性svm">四、软间隔线性SVM</span></h2><p>在实际应用中，完全线性可分的样本是很少的，如果遇到了不能够完全线性可分的样本，我们应该怎么办？比如下面这个：</p>
<p><img src="https://s2.loli.net/2023/04/17/zDNf9QWoTs2JxVp.jpg" alt="img" style="zoom: 33%;"></p>
<p>于是我们就有了软间隔，相比于硬间隔的苛刻条件，我们允许个别样本点出现在间隔带里面，比如：</p>
<p><img src="https://s2.loli.net/2023/04/17/ha4zMrmIS9Z7jnW.jpg" alt="img" style="zoom: 33%;"></p>
<p>我们允许部分样本点不满足约束条件:</p>
<script type="math/tex; mode=display">
1-y_i\left(w^T x_i+b\right) \leq 0</script><p>为了度量这个间隔软到何种程度, 我们为每个样本引入一个松弛变量 $\xi_i$, 令 $\xi_i \geq 0$, 且 $1-y_i\left(w^T x_i+b\right)-\xi_i \leq 0$ 。对应如下图所示:</p>
<p><img src="https://s2.loli.net/2023/04/17/GlPvqnUM7FDeTyX.jpg" alt="img" style="zoom: 33%;"></p>
<p><strong>这边要注意一个问题，在间隔内的那部分样本点是不是支持向量？</strong></p>
<p>我们可以由求参数 w 的那个式子可看出，只要$\lambda &gt; 0$的点都能够影响我们的超平面，因此都是支持向量。</p>
<p><strong>硬间隔线性SVM的基本型：</strong>【凸二次规划问题， 具有全局最小值】</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{w, b} & \frac{1}{2} w^{\top} w \\
\text { s.t. } & y_{i}\left(w^{\top} x_{i}+b\right) \geq 1, \quad i=1,2, \ldots, m
\end{array}</script><p>约束中要求所有的样本都满足 $y<em>{i}\left(w^{\top} \phi\left(x</em>{i}\right)+b\right) \geq 1$, 也就是让所有的样本都满足 $y<em>{i}\left(w^{\top} \phi\left(x</em>{i}\right)+b\right)&gt;0$。<br>现在我们想对<strong>该约束进行一点放松</strong>, 我们希望在优化间隔的同时, 允许分类错误的样本出现, 但这类样本应尽可能少:</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{w, b} & \frac{1}{2} w^{\top} w+C \sum_{i=1}^{m} \mathbb{I}\left(y_{i} \neq \operatorname{sign}\left(w^{\top} \phi\left(x_{i}\right)+b\right)\right) \\
\text { s.t. } & y_{i}\left(w^{\top} \phi\left(x_{i}\right)+b\right) \geq 1, \quad \text { 若 } y_{i}=\operatorname{sign}\left(w^{\top} \phi\left(x_{i}\right)+b\right) .
\end{array}</script><p>其中, 优化目标的第一项 $\frac{1}{2} w^{\top} w$ 源自硬间隔核化 SVM 的基本型, 即优化间隔。优化目标的第二项 中的 $\mathbb{I}(\cdot)$ 是指示函数, 函数的参数通常是一个条件, 如果条件为真（True）, 则指示函数值为 1 ; 如果条件为假 (False), 则指示函数值为 0 。<br>$\sum<em>{i=1}^{m} \mathbb{I}\left(y</em>{i} \neq \operatorname{sign}\left(w^{\top} \phi\left(x_{i}\right)+b\right)\right)$ 的含义是统计训练集 $D$ 中所有<strong>预测错误的样本总数</strong>。因此, 公式 162 的目标函数是同时优化间隔和最小化训练集预测错误的样本总数, <strong>$C$ 是个可调节的超参数, 用于权衡优化间隔和出现少量分类错误的样本这两个目标。</strong></p>
<p>但是, 由于<strong>指示函数 $I(\cdot)$ 不是连续函数, 更不是凸函数, 使得优化问题不再是二次规划问题</strong>, 求解起来十分困难, 所以我们需要对其进行简化。另外<strong>指示函数没有区分预测错误的不同程度</strong>,因此, 我们<strong>引入松他变量</strong> (Slack Variable) $\xi<em>{i} \in \mathbb{R}$, 用于<strong>度量训练集 $D$ 中第 $i$ 个样本违背约束的程度</strong>。当第 $i$ 个样本<strong>违背约束的程度</strong>越大, 松弛变量 $\xi</em>{i}$ 的值越大</p>
<script type="math/tex; mode=display">
\xi_{i}:= \begin{cases}0 & \text { 若 } y_{i}\left(w^{\top} \phi\left(x_{i}\right)+b\right) \geq 1 ; \\ 1-y_{i}\left(w^{\top} \phi\left(x_{i}\right)+b\right) & \text { 否则. }\end{cases}</script><p>基于以上定义, 松弛变量 $\xi_{i}$ 的取值有以下四种情况, 如图 27 所示, 注意图 27 只是示意图, 用于理 解概念, 不表示用图中数据训练得到的分类边界一定是这样:</p>
<ul>
<li>当 $\xi<em>{i}=0$ 时, 训练集 $D$ 中第 $i$ 个样本分类正确 $h\left(x</em>{i}\right)=y<em>{i}$, 且满足大间隔约束 $y</em>{i}\left(w^{\top} \phi\left(x_{i}\right)+b\right) \geq 1$;</li>
<li>当 $0&lt;\xi<em>{i}&lt;1$ 时, 训练集 $D$ 中第 $i$ 个样本分类正确 $h\left(x</em>{i}\right)=y_{i}$, 但是不满足大间隔约束;</li>
<li>当 $\xi<em>{i}=1$ 时, 训练集 $D$ 中第 $i$ 个样本恰好位于划分超平面 $w^{\top} \phi\left(x</em>{i}\right)+b=0$ 上, 且不满足大间 隔约束;</li>
<li>当 $\xi&gt;0$ 时, 训练集 $D$ 中第 $i$ 个样本分类错误 $h\left(x<em>{i}\right) \neq y</em>{i}$, 且不满足大间隔约束。</li>
</ul>
<p><img src="https://s2.loli.net/2023/04/17/LPukQfqKiBNToV5.png" alt="image-20220409230106609" style="zoom:50%;"></p>
<h4><span id="软间隔核化svm基本型合页损失函数"><strong>软间隔核化SVM基本型：</strong>（<strong>合页损失函数</strong>）</span></h4><p><img src="https://s2.loli.net/2023/04/17/ROQk4xowVb7Fu39.png" alt="image-20220401163522598" style="zoom:50%;"></p>
<p><img src="https://s2.loli.net/2023/04/17/CvTiDaHNeWm3q6E.png" alt="image-20220401164527480" style="zoom:50%;"></p>
<p><strong>变为：</strong></p>
<p><img src="https://s2.loli.net/2023/04/17/bsWvAUQaM29RzNP.png" alt="image-20220401164354384" style="zoom:50%;"></p>
<p>令<img src="https://s2.loli.net/2023/04/17/3j4TyoqNfcnBrLW.png" alt="image-20220401164559373" style="zoom:50%;">：<strong>合页损失</strong></p>
<p><img src="https://s2.loli.net/2023/04/17/XIRKolZvQGECYAw.png" alt="image-20220401164608500" style="zoom:50%;"></p>
<blockquote>
<h5><span id="回顾对数几率回归">回顾：对数几率回归：</span></h5><p>  <img src="https://s2.loli.net/2023/04/17/ZtVDeIKydQsNhTM.png" alt="image-20220401164913393" style="zoom:50%;"></p>
</blockquote>
<p><strong>软间隔核化SVM对偶性：</strong> <strong>软间隔的对偶性是在硬间隔的对偶性对拉格朗日参数添加一个上界</strong>。</p>
<p><img src="https://s2.loli.net/2023/04/17/hMnsiJqL59xYduO.png" alt="image-20220401161707141" style="zoom:50%;"></p>
<h2><span id="五-核函数对偶性">五、核函数【对偶性】</span></h2><h3><span id="51-线性不可分">5.1 线性不可分</span></h3><blockquote>
<p>  <strong>对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。</strong></p>
</blockquote>
<p>我们刚刚讨论的<strong>硬间隔</strong>和<strong>软间隔</strong>都是在说样本的完全线性可分或者大部分样本点的线性可分。</p>
<p>但我们可能会碰到的一种情况是样本点不是线性可分的，比如：</p>
<p><img src="https://s2.loli.net/2023/04/17/BpDe2wT9zZQrJVn.jpg" alt="img" style="zoom:50%;"></p>
<p>这种情况的解决方法就是：将<strong>二维线性不可分样本映射到高维空间中，让样本点在高维空间线性可分</strong>，比如：</p>
<p><img src="https://s2.loli.net/2023/04/17/JhEDzmfuGyPK7A6.jpg" alt="img" style="zoom:50%;"></p>
<p>对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里, 再通过间隔最大化的方 式，学习得到支持向量机，就是非线性 SVM。<br>我们用 $\mathrm{x}$ 表示原来的样本点, 用 $\phi(x)$ 表示 $\mathrm{x}$ 映射到特征新的特征空间后到新向量。那么分割超平面可以表示为: $f(x)=w \phi(x)+b$ 。<br>对于非线性 SVM 的对偶问题就变成了：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \min _\lambda\left[\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j\left(\phi\left(x_i\right) \cdot \phi\left(x_j\right)\right)-\sum_{j=1}^n \lambda_i\right] \\
& \text { s.t. } \quad \sum_{i=1}^n \lambda_i y_i=0, \quad \lambda_i \geq 0, \quad C-\lambda_i-\mu_i=0
\end{aligned}</script><p>可以看到与线性 SVM 唯一的不同就是：之前的 $\left(x_i \cdot x_j\right)$ 变成了 $\left(\phi\left(x_i\right) \cdot \phi\left(x_j\right)\right)$ 。</p>
<h3><span id="52-核函数的作用">5.2 核函数的作用</span></h3><p>我们不禁有个疑问：只是做个内积运算, 为什么要有核函数的呢?</p>
<p>这是因为<font color="red"> 低维空间映射到高维空间后维度可能会很大, 如果将全部样本的点乘全部计算好, 这样的计算量太大了。</font></p>
<p>但如果我们有这样的一核函数 $k(x, y)=(\phi(x), \phi(y)) ， \quad x_i$ 与 $x_j$ 在特征空间的内积等于它们在原始样本空间中通 过函数 $k(x, y)$ 计算的结果, 我们就不需要计算高维甚至无穷维空间的内积了。<br>举个例子：假设我们有一个多项式核函数：</p>
<script type="math/tex; mode=display">
k(x, y)=(x \cdot y+1)^2</script><p>带进样本点的后:</p>
<script type="math/tex; mode=display">
k(x, y)=\left(\sum_{i=1}^n\left(x_i \cdot y_i\right)+1\right)^2</script><p>而它的展开项是:</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n x_i^2 y_i^2+\sum_{i=2}^n \sum_{j=1}^{i-1}\left(\sqrt{2} x_i x_j\right)\left(\sqrt{2} y_i y_j\right)+\sum_{i=1} n\left(\sqrt{2} x_i\right)\left(\sqrt{2} y_i\right)+1</script><p>如果没有核函数，我们则需要把向量映射成:</p>
<script type="math/tex; mode=display">
x^{\prime}=\left(x_1^2, \ldots, x_n^2, \ldots \sqrt{2} x_1, \ldots, \sqrt{2} x_n, 1\right)</script><p>然后在进行内积计算, 才能与多项式核函数达到相同的效果。可见核函数的引入一方面减少了我们计算量, 另一方面也减少了我们存储数据的内存使用量。</p>
<h3><span id="53-常见核函数">5.3 常见核函数</span></h3><p>我们常用核函数有:<br><strong>线性核函数</strong>【无映射】</p>
<script type="math/tex; mode=display">
k\left(x_i, x_j\right)=x_i^T x_j</script><ul>
<li>优点: 有加速算法库、没有特征映射、过拟合风险低</li>
<li>缺点：只能处理线性</li>
</ul>
<p><strong>多项式核函数【映射，超参数】</strong></p>
<script type="math/tex; mode=display">
k\left(x_i, x_j\right)=\left(x_i^T x_j\right)^d</script><p><strong>高斯核函数【映射，超参数】</strong></p>
<script type="math/tex; mode=display">
k\left(x_i, x_j\right)=\exp \left(-\frac{\left\|x_i-x_j\right\|}{2 \delta^2}\right)</script><ul>
<li><strong>表示能力强，但容易过拟合</strong></li>
<li><strong>高斯核没有多项核不稳定的问题</strong></li>
<li><strong>只有一个超参数</strong></li>
</ul>
<h3><span id="54-如何选择核函数">5.4 <strong>如何选择核函数？</strong></span></h3><blockquote>
<p>  <strong>其他核函数：拉普拉斯、sigmod、卡方、直方图交叉</strong></p>
<p>  可自定义和组合核函数</p>
</blockquote>
<ul>
<li>如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；</li>
<li>如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；</li>
<li>如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。</li>
</ul>
<p><img src="https://s2.loli.net/2023/04/17/uFims1jZcWSPBRx.png" alt="image-20220331203905843" style="zoom:50%;"></p>
<h3><span id="55-核方法">5.5 核方法</span></h3><h4><span id="核化lr-非线性">核化LR [非线性]</span></h4><p>正类: y = +1 负类 y= -1</p>
<p><img src="https://s2.loli.net/2023/04/17/mvX4dbSUTuzYKAq.png" alt="image-20220331205845613" style="zoom:50%;"></p>
<p><img src="https://s2.loli.net/2023/04/17/UGMhET5quOZiVLn.png" alt="image-20220331211740625" style="zoom:50%;"></p>
<p><img src="https://s2.loli.net/2023/04/17/6HjbuOyVkhNDmAL.png" alt="image-20220331211757367" style="zoom:50%;"></p>
<p><strong>梯度下降求解：</strong></p>
<p>核化LR的参数通常都非0，并且几乎用到所有训练的样本，预测效率比较低。</p>
<p><img src="https://s2.loli.net/2023/04/17/8d9UGWZyzBoAhjJ.png" alt="image-20220331212433609" style="zoom:50%;"></p>
<h3><span id="56-支持向量回归-svr-核化岭回归">5.6 支持向量回归 SVR = ？核化岭回归？</span></h3><blockquote>
<p>  <a href="https://blog.csdn.net/ch18328071580/article/details/94168411">https://blog.csdn.net/ch18328071580/article/details/94168411</a></p>
<p>  <strong>支持向量在隔代之外</strong></p>
</blockquote>
<h4><span id="svr与一般线性回归的区别">SVR与一般线性回归的区别</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>SVR</th>
<th>线性回归</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据在间隔带内则不计算损失，<strong>当且仅当f(x)与y之间的差距的绝对值大于ϵ才计算损失</strong></td>
<td>只要f(x)与y不相等时，就计算损失</td>
</tr>
<tr>
<td><strong>通过最大化间隔带的宽度与最小化总损失</strong>来优化模型</td>
<td>通过梯度下降之后求均值来优化模型</td>
</tr>
</tbody>
</table>
</div>
<p><strong>岭回归：</strong><img src="https://s2.loli.net/2023/04/17/tHg4BJuNZzcX7Yd.png" alt="image-20220401144206172" style="zoom:50%;"></p>
<h5><span id="支持向量回归我们假设fx与y之间最多有一定的偏差大于偏差才计数损失">支持向量回归：我们假设f(x)与y之间最多有一定的偏差，大于偏差才计数损失</span></h5><script type="math/tex; mode=display">
\min _{w, b} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} l_{\epsilon}\left(f\left(x_{i}\right), y_{i}\right)</script><p>其中C为正则化常数, $l_{\epsilon}$ 是图中所示的 $\epsilon$-不敏感损失 ( $\epsilon$-insensitive loss)函数:</p>
<script type="math/tex; mode=display">
l_{\epsilon}(\mathrm{z})= \begin{cases}0, & \text { if }|z| \leq \epsilon \\ |z|-\epsilon, & \text { otherwise }\end{cases}</script><p>引入松弛变量 $\xi<em>{i}$ 和 $\left(\xi</em>{i}\right)$, 可将式重写为:</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{w, b, \xi_{i}, \xi_{i}} & \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}, \widehat{\xi}_{i}\right) \\
\text { s.t. } & f\left(x_{i}\right)-y_{i} \leq \epsilon+\xi_{i} \\
& y_{i}-f\left(x_{i}\right) \leq \epsilon+\widehat{\xi}_{i} \\
& \xi_{i} \geq 0, \hat{\xi}_{i} \geq 0, i=1,2, \ldots m
\end{array}</script><p>引入拉格朗日乘子 $\mu_{i}$,</p>
<p>$L(w, b, \alpha, \hat{\alpha}, \xi, \hat{\xi}, \mu, \hat{\mu})$<br>$=\frac{1}{2}|w|^{2}+C \sum<em>{i=1}^{m}\left(\xi</em>{i}+\widehat{\xi}<em>{i}\right)-\sum</em>{i=1}^{m} \xi<em>{i} \mu</em>{i}-\sum<em>{i=1}^{m} \widehat{\xi}</em>{i} \widehat{\mu<em>{i}}$<br>$+\sum</em>{i=1}^{m} \alpha<em>{i}\left(f\left(x</em>{i}\right)-y<em>{i}-\epsilon-\xi</em>{i}\right)+\sum<em>{i=1}^{m} \widehat{\alpha</em>{i}}\left(y<em>{i}-f\left(x</em>{i}\right)-\epsilon-\widehat{\xi}_{i}\right)$</p>
<p>再令 $L(w, b, a, \hat{a}, \xi, \hat{\xi}, \mu, \mu)$ 对 $w, b, \xi<em>{i}, \hat{\xi}</em>{i}$ 的偏导为零可得:</p>
<script type="math/tex; mode=display">
w=\sum_{i=1}^{m}\left(\widehat{\alpha_{i}}-\alpha_{i}\right) x_{i}</script><p>上述过程中需满足KKT条件, 即要求:</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{c}
\alpha_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)=0 \\
\widehat{\alpha_{i}}\left(y_{i}-f\left(x_{i}\right)-\epsilon-\widehat{\xi}_{i}\right)=0 \\
\alpha_{i} \widehat{\alpha_{i}}=0, \xi_{i} \widehat{\xi}_{i}=0 \\
\left(C-\alpha_{i}\right) \xi_{i}=0,\left(C-\widehat{\alpha_{i}}\right) \widehat{\xi}_{i}=0 .
\end{array}\right.</script><h3><span id="57-多分类-svm">5.7 多分类 SVM</span></h3><h4><span id="571-多分类问题">5.7.1 多分类问题</span></h4><ul>
<li>多分类问题拆解成若干个二分类问题，对于每个二分类训练一个分类器。<ul>
<li><strong>one vs one 拆解</strong>：K(K-1)/2 个分类器。</li>
<li><strong>one vs Rest 拆解</strong>：K个分类器</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2023/04/17/OJqX539ByWxSDVN.png" alt="image-20220401192909167" style="zoom:50%;"></p>
<ul>
<li><p><strong>根据模型特点设计：多分类线性SVM</strong></p>
<ul>
<li><p><strong>层次支持向量机</strong></p>
</li>
<li><p><strong>回顾二分类</strong>；</p>
<p><img src="https://s2.loli.net/2023/04/17/YJLzqnSQGTyXFc3.png" alt="image-20220401193645250" style="zoom:50%;"></p>
</li>
<li><p><strong>多分类线性SVM</strong>：</p>
<p><img src="https://s2.loli.net/2023/04/17/kZ19DcBfzWuMAaG.png" alt="image-20220401194059380" style="zoom:50%;"></p>
</li>
</ul>
</li>
</ul>
<h2><span id="六-优缺点">六、优缺点</span></h2><h3><span id="61-优点">6.1 优点</span></h3><ul>
<li>有严格的<strong>数学理论支持</strong>，<strong>可解释性强</strong>，<strong>不依靠统计方法</strong>，从而<strong>简化了通常的分类和回归问题</strong>；</li>
<li>能找出对任务至关重要的<strong>关键样本</strong>（即：<strong>支持向量</strong>）；</li>
<li>采用<strong>核技巧</strong>之后，<strong>可以处理非线性分类/回归任务</strong>；</li>
<li><strong>最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。</strong></li>
</ul>
<h3><span id="62-缺点">6.2 缺点</span></h3><ul>
<li><strong>训练时间长</strong>：当采用 SMO 算法时，由于每次都需要挑选一对参数，因此时间复杂度为$O(N^2)$ ，其中 N 为训练样本的数量；</li>
<li><strong>存储空间大</strong>：当采用核技巧时，如果需要存储核矩阵，则空间复杂度为$O(N^2)$ ；</li>
<li><strong>预测时间长</strong>：模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。</li>
</ul>
<p><strong>因此支持向量机目前只适合小批量样本的任务，无法适应百万甚至上亿样本的任务。</strong></p>
<h2><span id="svm-qampa">SVM Q&amp;A</span></h2><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/93715996">https://zhuanlan.zhihu.com/p/93715996</a></p>
</blockquote>
<h4><span id="1-原理">1、原理：</span></h4><ul>
<li>简单介绍SVM（详细原理）：从分类平面，到求两类间的最大间隔，到转化为求间隔分之一，等优化问题，然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题， 最后再利用SMO（序列最小优化）来解决这个对偶问题。<strong>svm里面的超参数c有啥用：软间隔SVM去权衡优化目标和少量分错样本的目标。</strong></li>
<li><p>SVM的推导，解释原问题和对偶问题，<strong>SVM原问题和对偶问题的关系</strong>，<strong>KKT限制条件</strong>，<strong>KKT条件用哪些</strong>，完整描述；软间隔问题，解释支持向量、核函数（哪个地方引入、画图解释高维映射，高斯核可以升到多少维，如何选择核函数），引入拉格朗日的优化方法的原因，最大的特点，损失函数解释</p>
<ul>
<li><strong>KKT限制</strong>：主问题可行、对偶问题可行、主变量最优、<strong>互补松弛</strong></li>
</ul>
</li>
<li><p><strong>为什么要把原问题转换为对偶问题？</strong></p>
<ul>
<li>因为原问题是凸二次规划问题，转换为对偶问题更加高效。为什么求解对偶问题更加高效？因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0.alpha系数有多少个？样本点的个数</li>
</ul>
</li>
</ul>
<h4><span id="2-svm与lr最大区别lr和svm对于outlier的敏感程度分析逻辑回归与svm的区别">2、SVM与LR最大区别，LR和SVM对于outlier的敏感程度分析，逻辑回归与SVM的区别？</span></h4><h4><span id="3-svm如何解决多分类问题-可以做回归吗怎么做">3、SVM如何解决多分类问题、可以做回归吗，怎么做？</span></h4><h4><span id="4-机器学习有很多关于核函数的说法核函数的定义和作用是什么">4、机器学习有很多关于核函数的说法，核函数的定义和作用是什么？</span></h4><p><a href="https://www.zhihu.com/question/24627666">https://www.zhihu.com/question/24627666</a></p>
<h4><span id="5-linear-svm-和-lr-有什么异同">5、Linear SVM 和 LR 有什么异同？</span></h4><h5><span id="svm和lr相同点">SVM和LR相同点：</span></h5><ul>
<li>SVM和LR都属于机器学习的监督学习中的<strong>判别式模型</strong>（判别式模型对$p(y|x)$进行建模或直接基于x预测y，生成模型：$p(x|y)$和$p(y)$进行建模,预测后验概率）。</li>
<li>SVM和LR都是线性二分类模型，<strong>分类边界为一个超平面</strong>。</li>
<li>线性SVM和对数几率回归都可以基于表示定理和<strong>核技巧处理非线性可分问题</strong>。</li>
<li><strong>SVM的基本型和对数几率函数都属于参数模型。SVM的对偶性和核化对数几率回归都属于非参数模型</strong>。</li>
<li>SVM和LR优化目标都表示成：经验风险+结构风险（正则项）的形式；均是0，1损失的替代函数。风险结构都是L2正则化。</li>
<li><strong>SVM和LR都是凸优化问题，都能收敛到全局最优解。</strong></li>
<li>SVM和对数几率函数的优化目标相似，性能相当。</li>
<li><strong>SVM多分类：多分类SVM；LR多分类：Softmax回归。</strong></li>
</ul>
<h5><span id="svm和lr的区别"><strong><font color="red"> SVM和LR的区别：</font></strong></span></h5><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>SVM</th>
<th>LR</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong></td>
<td><strong>SVM 想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面</strong>。</td>
<td><strong>逻辑回归</strong>是使用线性回归模型的预测值逼近分类任务真实标记的对数几率。</td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td><strong>非概率方法</strong>；</td>
<td><strong>概率方法</strong>；需要对$p(y</td>
<td>x)$进行假设，具有概率意义。</td>
</tr>
<tr>
<td><strong>经验损失函数</strong></td>
<td><strong>合页损失函数</strong>；有一段平的零区域、使得SVM的对偶性有稀疏性。</td>
<td><strong>交叉熵损失函数</strong></td>
</tr>
<tr>
<td><strong>训练样本</strong></td>
<td>支持向量（少数样本），SVM的参数和假设函数只和支持向量有关。</td>
<td>全样本</td>
</tr>
<tr>
<td><strong>优化方法</strong></td>
<td>次梯度下降和坐标梯度下降 【<strong>SMO算法</strong>】</td>
<td><strong>梯度下降</strong></td>
</tr>
<tr>
<td>多分类</td>
<td><strong>多分类SVM</strong></td>
<td><strong>Softmax回归</strong></td>
</tr>
<tr>
<td><strong>敏感程度</strong></td>
<td>SVM考虑分类边界线附近的样本（决定分类超平面的样本）。在支持向量外添加或减少任何样本点对分类决策面没有任何影响；</td>
<td>LR受所有数据点的影响。直接依赖数据分布，每个样本点都会影响决策面的结果。如果训练数据不同类别严重不平衡。</td>
</tr>
</tbody>
</table>
</div>
<p><a href="https://www.zhihu.com/question/26768865">https://www.zhihu.com/question/26768865</a></p>
<h4><span id="6-支持向量机svm是否适合大规模数据速度">6、支持向量机(SVM)是否适合大规模数据？【速度】</span></h4><p><a href="https://www.zhihu.com/question/19591450">https://www.zhihu.com/question/19591450</a></p>
<h4><span id="7-svm和逻辑斯特回归对同一样本a进行训练如果某类中增加一些数据点那么原来的决策边界分别会怎么变化">7、SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？</span></h4><p><a href="https://www.zhihu.com/question/30123068">https://www.zhihu.com/question/30123068</a></p>
<h4><span id="8-各种机器学习的应用场景分别是什么例如k近邻贝叶斯决策树svm逻辑斯蒂回归和最大熵模型">8、各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型。</span></h4><p><a href="https://www.zhihu.com/question/26726794">https://www.zhihu.com/question/26726794</a></p>
<h4><span id="9-svm与感知器的联系和优缺点比较">9、SVM与感知器的联系和优缺点比较</span></h4><p><strong>感知机用误分类样本点的几何距离之和</strong>来表示模型的损失函数，用梯度下降算法优化，直至没有误分类点。</p>
<script type="math/tex; mode=display">
L(w, b)=-\sum_{x^{(i)} \in M} y^{(i)}\left(w^{T} x^{(i)}+b\right)</script><h5><span id="感知机与svm区别"><strong><font color="red"> 感知机与SVM区别</font></strong></span></h5><p><strong>SVM可以视为对感知器的二阶改进</strong>：第一阶改进是加入了 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="[公式]"> 获得hinge loss，从而具备了产生大间隔的潜质；第二阶改进是加入了权向量的L2正则化项，从而避免产生无意义的大函数间隔，而是产生大的几何间隔。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>感知机</th>
<th>SVM</th>
</tr>
</thead>
<tbody>
<tr>
<td>思想</td>
<td>分离超平面基于误分类的损失函数 $\min <em>{w, b} \frac{1}{n} \sum</em>{i=1}^n \max \left(0,-y_i\left(w^T x_i+b\right)\right.$</td>
<td>$\min <em>{w, b} \frac{1}{n} \sum</em>{i=1}^n \max \left(0,1-y_i\left(w^T x_i+b\right)\right)+\alpha\</td>
<td>w\</td>
<td>_2^2$</td>
</tr>
<tr>
<td>超平面</td>
<td>因采用的初值不同而得到不同的超平面</td>
<td>让离划分超平面最近的样本到划分超平面距离尽可能远</td>
</tr>
<tr>
<td>关键样本</td>
<td>每步的分错样本</td>
<td><strong>支持向量</strong></td>
</tr>
<tr>
<td>非线性问题</td>
<td>无</td>
<td>核化</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>支持向量机</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习-损失函数</title>
    <url>/posts/8Q5WCT/</url>
    <content><![CDATA[<p>机器学习中的监督学习本质上是给定一系列训练样本 $\left(x_i, y_i\right)$, 尝试学习 $x \rightarrow y$ 的映射关系, 使得给定一个 $x$ , 即便这个 $x$ 不在训练样本中, 也能够得到尽量接近真实 $y$ 的输出 $\hat{y}$ 。<strong>损失函数 (Loss Function) 则是这个过程中关键的一个组成部分, 用来衡量模型的输出 $\hat{y}$ 与真实的 $y$ 之间的差距, 给模型的优化指明方向。</strong></p>
<p>本文将介绍机器学习、深度学习中分类与回归常用的几种损失函数, 包括<strong>均方差损失 Mean Squared Loss、平 均绝对误差损失 Mean Absolute Error Loss、Huber Loss、分位数损失 Quantile Loss、交叉樀损失函数 Cross Entropy Loss、Hinge 损失 Hinge Loss</strong>。主要介绍各种损失函数的基本形式、原理、特点等方面。</p>
<span id="more"></span>
<h3><span id="前言">前言</span></h3><p>在正文开始之前, 先说下关于 Loss Function、Cost Function 和 Objective Function 的区别和联系。在机器学习 的语境下这三个术语经常被交叉使用。</p>
<ul>
<li><strong>损失函数</strong> Loss Function 通常是<strong>针对单个训练样本而言,</strong> 给定一个模型输出 $\hat{y}$ 和一个真实 $y$, 损失函数输 出一个实值损失 $L=f\left(y_i, \hat{y_i}\right)$</li>
<li><strong>代价函数</strong> Cost Function 通常是<strong>针对整个训练集</strong>（或者在使用 mini-batch gradient descent 时一个 minibatch）的总损失 $J=\sum_{i=1}^N f\left(y_i, \hat{y}_i\right)$</li>
<li><strong>目标函数</strong> Objective Function 是一个更通用的术语, 表示任意希望被优化的函数, 用于机器学习领域和非机 器学习领域 (比如运筹优化)</li>
</ul>
<p>一句话总结三者的关系就是：<font color="red"> <strong>A loss function is a part of a cost function which is a type of an objective function.</strong></font></p>
<p><img src="https://s2.loli.net/2023/04/17/BRyjpDGtWUXcJ2S.png" alt="img" style="zoom: 67%;"></p>
<p>由于损失函数和代价函数只是在针对样本集上有区别，因此在本文中统一使用了损失函数这个术语，但下文的相关公式实际上采用的是代价函数 Cost Function 的形式，请读者自行留意。</p>
<h3><span id="结构风险函数">结构风险函数</span></h3><p>损失函数（loss function）是用来估量模型的预测值f(x)与真实值$Y$不一致的程度，它是一个非负实数值函数，通常使用$L(Y,f(x))$来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下的式子：</p>
<p><img src="https://s2.loli.net/2023/04/17/TaUDLMBO5upXZEA.png" alt="image-20220821223950768" style="zoom:50%;"></p>
<p>前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty term）,它可以是L1，也可以是L2等其他的正则函数。整个式子表示的意思是找到使目标函数最小时的θ值。下面列出集中常见的损失函数。</p>
<h3><span id="一-对数损失函数逻辑回归mle-交叉熵损失函数">一、 对数损失函数（逻辑回归）MLE  【交叉熵损失函数】</span></h3><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/52100927">https://zhuanlan.zhihu.com/p/52100927</a></p>
<p>  <a href="https://blog.csdn.net/tsyccnh/article/details/79163834">一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉</a></p>
<p>  <a href="https://zhuanlan.zhihu.com/p/35709485">损失函数｜交叉熵损失函数</a></p>
</blockquote>
<p>有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。<strong>平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到</strong>，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，<strong>它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数</strong>，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：</p>
<p><strong>最小化负的似然函数</strong>（即$maxF(y,f(x))—&gt;min−F(y,f(x))$)。从损失函数的视角来看，它就成了<strong>log损失函数了。</strong></p>
<h4><span id="原理解释1条件概率下方便计算极大似然估计">原理解释1：<strong>条件概率下方便计算极大似然估计</strong></span></h4><p>Log损失函数的标准形式：</p>
<p>$L(Y,P(Y|X))=−logP(Y|X)$</p>
<p>刚刚说到，<strong>取对数是为了方便计算极大似然估计</strong>，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数$L(Y.P(Y|X))$表达的是样本在分类$Y$的情况下，使概率$P(Y|X)$达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以$logP(Y|X)$也会达到最大值，因此在前面加上负号之后，最大化$P(Y|X)$就等价于最小化$L$了。</p>
<p><strong>logistic回归</strong>的$P(y|x)$表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：</p>
<p><img src="https://s2.loli.net/2023/04/17/kYQmSNU389DaEgW.png" alt="image-20220322202521355" style="zoom:50%;"></p>
<p>将上面的公式合并在一起，可得到第i个样本正确预测的概率：</p>
<p><img src="https://s2.loli.net/2023/04/17/H9K4sYIapf7lZqC.png" alt="image-20220322202548296" style="zoom:50%;"></p>
<p>上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布为：</p>
<p><img src="https://s2.loli.net/2023/04/17/9juth32ZNkznoC6.png" alt="image-20220322202618734" style="zoom:50%;"></p>
<p>将上式代入到对数损失函数中，得到最终的损失函数为：</p>
<p><img src="https://s2.loli.net/2023/04/17/Xzdpa6StP4KgNkJ.png" alt="image-20220322202653661" style="zoom:50%;"></p>
<h4><span id="原理解释2相对熵kl散度推理">原理解释2：相对熵（KL散度）推理</span></h4><blockquote>
<p>  相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异.$DKL$的值越小，表示q分布和p分布越接近.</p>
</blockquote>
<h5><span id="相对熵">相对熵:</span></h5><p><img src="https://s2.loli.net/2023/04/17/Hs19MdiWOfokNuP.png" alt="image-20220330133351613" style="zoom:50%;"></p>
<h5><span id="相对熵-信息熵-交叉熵">相对熵 = 信息熵 + 交叉熵 ：</span></h5><p><img src="https://s2.loli.net/2023/04/17/WBZsoc9JwQjkG27.png" alt="image-20220330134202064" style="zoom:50%;"></p>
<p><strong>【对数损失函数（Log loss function）】和【交叉熵损失函数（Cross-entroy loss funtion）】在很多文献内是一致的，因为他们的表示式的本质是一样的。</strong></p>
<h3><span id="二-平方损失函数线性回归gbdt最小二乘法ordinary-least-squaresmse">二、 平方损失函数（线性回归，GBDT，最小二乘法，Ordinary Least Squares）MSE</span></h3><p>最小二乘法是线性回归的一种，OLS 将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central limit theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。换言之，OLS是基于<strong>距离</strong>的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：</p>
<ul>
<li>简单，计算方便；</li>
<li>欧氏距离是一种很好的相似性度量标准；</li>
<li>在不同的表示域变换后特征性质不变。</li>
</ul>
<p>平方损失（Square loss）的标准形式如下：$L(Y,f(X))=(Y−f(x))^2$当样本个数为n时，此时的损失函数变为：</p>
<p><img src="https://s2.loli.net/2023/04/17/XuAymnSOw6RhQ9a.png" alt="image-20220322202912962" style="zoom:50%;"></p>
<p>$Y−f(X)$ 表示的是<strong>残差</strong>，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。</p>
<p>而在实际应用中，通常会使用<strong>均方差</strong>（MSE）作为一项衡量指标，公式如下：</p>
<p><img src="https://s2.loli.net/2023/04/17/dye1NVAtOUw2BoY.png" alt="image-20220322202957484" style="zoom:50%;"></p>
<h3><span id="三-指数损失函数adaboost">三、 指数损失函数（Adaboost）</span></h3><blockquote>
<p>  Adaboost训练误差以指数下降。所以说，指数损失本身并没有带来优化上的特殊，优点在于计算和表达简单。</p>
</blockquote>
<p>学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到$fm(x)$:</p>
<p><img src="https://s2.loli.net/2023/04/17/fZ8APgDerkSjBIu.png" alt="image-20220322203050695" style="zoom:50%;"></p>
<p><strong>Adaboost</strong>每次迭代时的目的是为了找到最小化下列式子时的参数 $a$ 和 $G$：</p>
<p><img src="https://s2.loli.net/2023/04/17/iphlobfSa5m1cXI.png" alt="image-20220322203141435" style="zoom:50%;"></p>
<p>而指数损失函数(exp-loss）的标准形式如下:</p>
<p><img src="https://s2.loli.net/2023/04/17/hq6iCpQVFeGHnOA.png" alt="image-20220322203221432" style="zoom:50%;"></p>
<p>可以看出，Adaboost的目标式子就是指数损失，在给定N个样本的情况下，Adaboost的损失函数为：</p>
<p><img src="https://s2.loli.net/2023/04/17/ECdNvKtrjZM6o2h.png" alt="image-20220322203238853" style="zoom:50%;"></p>
<h3><span id="四-hinge-合页损失函数svmadvgan">四、 Hinge 合页损失函数（SVM，advGAN）</span></h3><p><img src="https://s2.loli.net/2023/04/17/7x8WAuHyCcMs6I5.png" alt="image-20220401165315551" style="zoom:50%;"></p>
<p>线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数：</p>
<p><img src="https://s2.loli.net/2023/04/17/UXvLs7pH9rj2qoi.png" alt="image-20220322205741804" style="zoom:50%;"></p>
<p>目标函数的第一项是经验损失或经验风险函数：</p>
<p><img src="https://s2.loli.net/2023/04/17/G4hjnsLVavBS9YR.png" alt="image-20220322205801232" style="zoom:50%;"></p>
<p>称为<strong>合页损失函数</strong>（hinge loss function）。下标”+”表示以下取正值的函数：</p>
<p><img src="https://s2.loli.net/2023/04/17/YLEMueb3kBpWxvA.png" alt="image-20220322205844003" style="zoom:50%;"></p>
<p>这就是说，当样本点$(xi,yi)$被正确分类且函数间隔（确信度）$yi(w·xi+b)$大于1时，损失是0，否则损失是$1−yi(w·xi+b)$。目标函数的第二项是系数为 $λ$ 的 $w$ 的 $L2$ 范数，是正则化项。</p>
<p>接下来证明线性支持向量机原始最优化问题：</p>
<p><img src="https://s2.loli.net/2023/04/17/B8nxPKcVfCsGJ92.png" alt="image-20220322210000477" style="zoom:50%;"></p>
<p><img src="https://s2.loli.net/2023/04/17/w1SQdeAEi6qb7c2.png" alt="image-20220322210121285" style="zoom:50%;"></p>
<p>先令$[1−yi(w·xi+b)]+=ξi$，则$ξi≥0$，第二个约束条件成立；由$[1−yi(w·xi+b)]+=ξi$，当$1−yi(w·xi+b)&gt;0$时，有$yi(w·xi+b)=1−ξi$;当$1−yi(w·xi+b)≤0$时，$ξi=0$，有$yi(w·xi+b)≥1−ξi$，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作</p>
<p><img src="https://s2.loli.net/2023/04/17/Sb7qPknjhDrmAvI.png" alt="image-20220322210943775" style="zoom:50%;"></p>
<p>若取 $λ=1/2C$ 则:</p>
<p><img src="https://s2.loli.net/2023/04/17/VpbFxCN62ArYZyJ.png" alt="image-20220322211012150" style="zoom:50%;"></p>
<h3><span id="五-softmax函数和sigmoid函数的区别与联系">五、Softmax函数和Sigmoid函数的区别与联系</span></h3><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/356976844">https://zhuanlan.zhihu.com/p/356976844</a></p>
</blockquote>
<h4><span id="51-分类任务">5.1 分类任务</span></h4><h5><span id="sigmoid">sigmoid</span></h5><blockquote>
<p>  Sigmoid =<strong>多标签分类问题</strong>=多个正确答案=非独占输出（例如胸部X光检查、住院）。构建分类器，解决有多个正确答案的问题时，用Sigmoid函数分别处理各个原始输出值。</p>
<p>  Softmax =<strong>多类别分类问题</strong>=只有一个正确答案=互斥输出（例如手写数字，鸢尾花）。构建分类器，解决只有唯一正确答案的问题时，用Softmax函数处理各个原始输出值。Softmax函数的分母综合了原始输出值的所有因素，这意味着，Softmax函数得到的不同概率之间相互关联。</p>
</blockquote>
<p><strong>Sigmoid函数</strong>是一种logistic函数，它将任意的值转换到 [0, 1] 之间，如图1所示，函数表达式为:$Sigmoid(x) = \frac{1}{1-e^{-x}}$ 。它的导函数为：$Sigmoid^{‘}(x)=Sigmoid(x)\cdot(1-Sigmoid(x))$。</p>
<p><img src="https://s2.loli.net/2023/04/17/vqpA79snRahWw1F.png" alt="img" style="zoom:50%;"></p>
<p><strong>优点</strong>：</p>
<ol>
<li>Sigmoid函数的输出在(0,1)之间，输出范围有限，优化稳定，可以用作<strong>输出层</strong>。</li>
<li>连续函数，便于<strong>求导</strong>。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><p>最明显的就是<strong>饱和性</strong>，从上图也不难看出其两侧导数逐渐趋近于0，容易造成<strong>梯度消失</strong>。</p>
<p>2.激活函数的偏移现象。Sigmoid函数的输出值均大于0，使得输出不是0的均值，这会导致后一层的神经元将得到上一层非0均值的信号作为输入，这会对梯度产生影响。</p>
</li>
<li><p>计算复杂度高，因为Sigmoid函数是指数形式。</p>
</li>
</ol>
<h5><span id="softmax">Softmax</span></h5><p><strong>Softmax函数</strong>，又称<strong>归一化指数函数</strong>，函数表达式为： $\operatorname{Softmax}(x)=\frac{e^{x<em>i}}{\sum</em>{j=1}^n e^{x_j}}$ 。</p>
<p><img src="https://s2.loli.net/2023/04/17/QVYJzvwPZtnpoBE.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>Softmax函数是二分类函数Sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。</strong>如图2所示，Softmax直白来说就是将原来输出是3,1,-3通过Softmax函数一作用，就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标。</p>
<p>由于Softmax函数先拉大了输入向量元素之间的差异（通过指数函数），然后才归一化为一个概率分布，在应用到分类问题时，它使得各个类别的概率差异比较显著，最大值产生的概率更接近1，这样输出分布的形式更接近真实分布。</p>
<p><strong>Softmax可以由三个不同的角度来解释。从不同角度来看softmax函数，可以对其应用场景有更深刻的理解：</strong></p>
<ol>
<li><strong>softmax可以当作argmax的一种平滑近似</strong>，与arg max操作中暴力地选出一个最大值（产生一个one-hot向量）不同，softmax将这种输出作了一定的平滑，即将one-hot输出中最大值对应的1按输入元素值的大小分配给其他位置。</li>
<li><strong>softmax将输入向量归一化映射到一个类别概率分布</strong>，即 n 个类别上的概率分布（前文也有提到）。这也是为什么在深度学习中常常将softmax作为MLP的最后一层，并配合以交叉熵损失函数（对分布间差异的一种度量)。</li>
<li>从<strong>概率图模型</strong>的角度来看，softmax的这种形式可以理解为一个概率无向图上的联合概率。因此你会发现，条件最大熵模型与softmax回归模型实际上是一致的，诸如这样的例子还有很多。由于概率图模型很大程度上借用了一些热力学系统的理论，因此也可以从物理系统的角度赋予softmax一定的内涵。</li>
</ol>
<h4><span id="52-总结">5.2 总结</span></h4><ol>
<li>如果模型输出为非互斥类别，且可以同时选择多个类别，则采用Sigmoid函数计算该网络的原始输出值。</li>
<li>如果模型输出为<strong>互斥类别</strong>，且只能选择一个类别，则采用Softmax函数计算该网络的原始输出值。</li>
<li><strong>Sigmoid函数</strong>可以用来解决<strong>多标签问题</strong>，<strong>Softmax</strong>函数用来解决<strong>单标签问题</strong>。</li>
<li>对于某个分类场景，当Softmax函数能用时，Sigmoid函数一定可以用。</li>
</ol>
<h3><span id="六-损失函数qampa">六、损失函数Q&amp;A</span></h3><h4><span id="平方误差损失函数和交叉熵损失函数分别适合什么场景">平方误差损失函数和交叉熵损失函数分别适合什么场景？</span></h4><p>一般还说，平方损失函数更适合输出为连续，并且最后一层不含sigmod或softmax激活函数的神经网络；交叉熵损失函数更适合二分类或多分类的场景。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>理论基础</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习-模型融合</title>
    <url>/posts/3ZEPE93/</url>
    <content><![CDATA[<h2><span id="融合机器学习模型一种提升预测能力的方法"></span></h2><p>没有哪个机器学习模型可以常胜，如何找到当前问题的最优解是一个永恒的问题。</p>
<p>幸运的是，<strong>结合/融合/整合 (integration/ combination/ fusion)多个机器学习模型往往可以提高整体的预测能力。</strong>这是一种非常有效的提升手段，在多分类器系统(multi-classifier system)和集成学习(ensemble learning)中，融合都是最重要的一个步骤。</p>
<p>一般来说，<strong>模型融合或多或少都能提高的最终的预测能力，且一般不会比最优子模型差</strong>。举个实用的例子，Kaggle比赛中常用的stacking方法就是模型融合，通过结合多个各有所长的子学习器，我们实现了更好的预测结果。基本的理论假设是：<strong>不同的子模型在不同的数据上有不同的表达能力，我们可以结合他们擅长的部分，得到一个在各个方面都很“准确”的模型</strong>。当然，最基本的假设是子模型的误差是互相独立的，这个一般是不现实的。但即使子模型间的误差有相关性，适当的结合方法依然可以各取其长，从而达到提升效果。</p>
<p>我们今天介绍几种简单、有效的模型结合方法。</p>
<span id="more"></span>
<h3><span id="1-案例分析"><strong>1. 案例分析</strong></span></h3><p>让我们给出一个简单的分析。假设我们有天气数据X和对应的标签y，现在希望实现一个可以预测明天天气的模型 <img src="https://www.zhihu.com/equation?tex=%5Cpsi" alt="[公式]"> 。但我们并不知道用什么算法效果最好，于是尝试了十种算法，包括</p>
<ul>
<li>算法1: 逻辑回归 - <img src="https://www.zhihu.com/equation?tex=C_%7B1%7D" alt="[公式]"></li>
<li>算法2：支持向量机（SVM）-  <img src="https://www.zhihu.com/equation?tex=C_%7B2%7D" alt="[公式]"> </li>
<li>…</li>
<li>算法10：随机森林 -  <img src="https://www.zhihu.com/equation?tex=C_%7B10%7D" alt="[公式]"> </li>
</ul>
<p>结果发现他们表现都一般，在验证集上的误分率比较高。我们现在期待找到一种方法，可以有效提高最终预测结果。</p>
<h3><span id="2-平均法投票法"><strong>2. 平均法/投票法</strong></span></h3><p>一种比较直白的方法就是对让10个算法模型同时对需要预测的数据进行预测，并对结果取平均数/众数。假设10个分类器对于测试数据 <img src="https://www.zhihu.com/equation?tex=X_%7Bt%7D" alt="[公式]"> 的预测结果是<img src="https://www.zhihu.com/equation?tex=%5BC_%7B1%7D%28X_t%29%2CC_%7B2%7D%28X_t%29%2C...%2CC_%7B10%7D%28X_t%29%5D%3D%5B0%2C1%2C1%2C1%2C1%2C1%2C0%2C1%2C1%2C0%5D" alt="[公式]"> ，那很显然少数服从多数，我们应该选择1作为 <img src="https://www.zhihu.com/equation?tex=X_%7Bt%7D" alt="[公式]"> 的预测结果。如果取平均值的话也可以那么会得到0.7，高于阈值0.5，因此是等价的。</p>
<p>但这个时候需要有几个注意的地方：</p>
<p><strong>首先，不同分类器的输出结果取值范围不同</strong>，不一定是[0,1]，而可以是无限定范围的值。举例，逻辑回归的输出范围是0-1（概率），而k-近邻的输出结果可以是大于0的任意实数，其他算法的输出范围可能是负数。<strong>因此整合多个分类器时，需要注意不同分类器的输出范围，并统一这个取值范围</strong>。</p>
<ul>
<li>比如可以先转化为如<strong>二分类结果</strong>，把输出的范围统一后再进行整合。但这种方法的问题在于我们丢失了很多信息，0.5和0.99都会被转化为1，但明显其可靠程度差别很大。</li>
<li>也可以转化为排序（ranking），再对不同的ranking进行求平均。</li>
<li>更加稳妥的方法是对每个分类器的输出结果做标准化，也就是调整到正态分布上去。之后就可以对多个调整后的结果进行整合。同理，用归一化也可以有类似的效果。</li>
</ul>
<p><strong>其次，就是整合稳定性的问题</strong>。采用平均法的另一个风险在于可能被极值所影响。正态分布的取值是 <img src="https://www.zhihu.com/equation?tex=%5B-%5Cinfty%2C%2B%5Cinfty%5D" alt="[公式]"> ，在少数情况下平均值会受到少数极值的影响。一个常见的解决方法是，用中位数（median)来代替平均数进行整合。</p>
<p><strong>同时，模型整合面临的另一个问题是子模型良莠不齐</strong>。如果10个模型中有1个表现非常差，那么会拖累最终的效果，适得其反。==因此，简单、粗暴的把所有子模型通过平均法整合起来效果往往一般。==</p>
<h3><span id="3-寻找优秀的子模型准而不同">3. 寻找优秀的子模型准而不同</span></h3><p>不难看出，一个较差的子模型会拖累整体的集成表现，那么这就涉及到另一个问题？什么样的子模型是优秀的。</p>
<p>一般来说，我们希望子模型：<strong>准而不同 -&gt; accurate but diversified</strong>。好的子模型应该首先是准确的，这样才会有所帮助。其次不同子模型间应该有差别，比如独立的误差，这样作为一个整体才能起到<strong>互补作用</strong>。</p>
<p>因此，如果想实现良好的结合效果，就必须对子模型进行筛选，去粗取精。在这里我们需要做出一点解释，我们今天说的融合方法和bagging还有boosting中的思路不大相同。==bagging和boosting中的子模型都是<strong>很简单的且基数庞大</strong>，而我们今天的模型融合是<strong>结合少量但较为复杂的模型</strong>。==</p>
<h3><span id="4-筛选方法赋予不同子模型不同的权重"><strong>4. 筛选方法：赋予不同子模型不同的权重</strong></span></h3><p>因此我们不能再简单的取平均了，而应该给优秀的子模型更大的权重。在这种前提下，一个比较直白的方法就是根据子<strong>模型的准确率给出一个参考权重</strong> <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> ，子模型越准确那么它的权重就更大，对于最终预测的影响就更强： <img src="https://www.zhihu.com/equation?tex=w_%7Bi%7D%3D%5Cfrac%7BAcc%28C_%7Bi%7D%29%7D%7B%5Csum_%7B1%7D%5E%7B10%7D%7BAcc%28C_%7Bj%7D%29%7D%7D" alt="[公式]"> 。简单取平均是这个方法的一个特例，即假设子模型准确率一致。</p>
<h3><span id="5-更进一步学习分类器权重"><strong>5. 更进一步：学习分类器权重</strong></span></h3><p>在4中提到的方法在一定程度上可以缓解问题，但效果有限。那么另一个思路是，我们是否可以学习每个分类器的权重呢？</p>
<p>答案是肯定，这也就是Stacking的核心思路。通过增加一层来学习子模型的权重。</p>
<p><img src="https://pic3.zhimg.com/v2-13396e65c2bcc1c270ca536310686d07_720w.jpg?source=d16d100b" alt="img"></p>
<p><strong>图片来源</strong>：<a href="https://www.quora.com/What-is-stacking-in-machine-learning">https://www.quora.com/What-is-stacking-in-machine-learning</a></p>
<p>更多有关于stacking的讨论可以参考我最近的文章：<a href="https://zhuanlan.zhihu.com/p/32896968">「Stacking」与「神经网络」</a>。简单来说，就是加一层逻辑回归或者SVM，把子模型的输出结果当做训练数据，来自动赋予不同子模型不同的权重。</p>
<p>==<strong>一般来看，这种方法只要使用得当，效果应该比简单取平均值、或者根据准确度计算权重的效果会更好。</strong>==</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>理论基础</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习-模型评估</title>
    <url>/posts/7N6QMR/</url>
    <content><![CDATA[<h2><span id="六-ab-测试">六、A/B 测试</span></h2><blockquote>
<p>  【AB测试最全干货】史上最全知识点及常见面试题（上篇） - 数据分析狗一枚的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/375902281">https://zhuanlan.zhihu.com/p/375902281</a></p>
</blockquote>
<h4><span id="引言">引言</span></h4><p>科学家门捷列夫说「没有测量，就没有科学」，在AI场景下我们同样需要定量的数值化指标来指导我们更好地应用模型对数据进行学习和建模。</p>
<p>事实上，在机器学习领域，对模型的测量和评估至关重要。选择与问题相匹配的评估方法，能帮助我们快速准确地发现在模型选择和训练过程中出现的问题，进而对模型进行优化和迭代。本文我们系统地讲解一下机器学习模型评估相关知识。</p>
<span id="more"></span>
<h4><span id="61-模型评估的目标">6.1 模型评估的目标</span></h4><p><strong>模型评估的目标是选出泛化能力强的模型完成机器学习任务</strong>。实际的机器学习任务往往需要进行大量的实验，经过反复调参、使用多种模型算法（甚至多模型融合策略）来完成自己的机器学习问题，并观察哪种模型算法在什么样的参数下能够最好地完成任务。</p>
<p>但是我们无法提前获取「未知的样本」，因此我们会基于已有的数据进行切分来完成模型训练和评估，借助于切分出的数据进行评估，可以很好地判定模型状态（过拟合 or 欠拟合），进而迭代优化。</p>
<p>在建模过程中，为了获得泛化能力强的模型，我们需要一整套方法及评价指标。</p>
<ul>
<li><strong>评估方法</strong>：为保证客观地评估模型，对数据集进行的有效划分实验方法。</li>
<li><strong>性能指标</strong>：量化地度量模型效果的指标。</li>
</ul>
<h4><span id="62-离线与在线实验方法">6.2 离线与在线实验方法</span></h4><p>进行评估的实验方法可以分为「离线」和「在线」两种。</p>
<h4><span id="离线实验方法">离线实验方法：</span></h4><blockquote>
<p>  在<strong>离线评估</strong>中，经常使用<strong>准确率（Accuracy）、查准率（Precision）、召回率（Recall）、ROC、AUC、PRC</strong>等指标来评估模型。</p>
</blockquote>
<p><strong>模型评估通常指离线试验</strong>。原型设计（Prototyping）阶段及离线试验方法，包含以下几个过程：</p>
<ul>
<li>使用历史数据训练一个适合解决目标任务的一个或多个机器学习模型。</li>
<li>对模型进行验证（Validation）与离线评估（Offline Evaluation）。</li>
<li>通过评估指标选择一个较好的模型。</li>
</ul>
<h4><span id="在线实验方法">在线实验方法：</span></h4><blockquote>
<p>  <strong>在线评估</strong>与离线评估所用的评价指标不同，一般使用一些商业评价指标，如<strong>用户生命周期值（Customer Lifetime value）、广告点击率（Click Through Rate）、用户流失率</strong>（Customer Churn Rate）等标。</p>
</blockquote>
<p>除了离线评估之外，其实还有一种在线评估的实验方法。由于模型是在老的模型产生的数据上学习和验证的，而线上的数据与之前是不同的，因此离线评估并不完全代表线上的模型结果。因此我们需要在线评估，来验证模型的有效性。</p>
<p><img src="https://www.zhihu.com/equation?tex=A%2FB%20%5Cquad%20Test" alt="公式"> <strong>是目前在线测试中最主要的方法</strong>。<img src="https://www.zhihu.com/equation?tex=A%2FB%20%5Cquad%20Test" alt="公式"> 是为同一个目标制定两个方案让一部分用户使用 <img src="https://www.zhihu.com/equation?tex=A" alt="公式"> 方案，另一部分用户使用 <img src="https://www.zhihu.com/equation?tex=B" alt="公式"> 方案，记录下用户的使用情况，看哪个方案更符合设计目标。如果不做AB实验直接上线新方案，新方案甚至可能会毁掉你的产品。</p>
<h4><span id="63-模型离线评估后为什么要进行ab测试"><strong><font color="red"> 6.3 模型离线评估后，为什么要进行ab测试？</font></strong></span></h4><ul>
<li><strong>离线评估无法消除过拟合的影响</strong>，因此离线评估结果无法代替线上的评估效果</li>
<li><strong>离线评估过程中无法模拟线上的真实环境，例如数据丢失、样本反馈延迟</strong></li>
<li>线上的<strong>某些商业指标例如收益、留存等无法通过离线计算</strong></li>
</ul>
<h4><span id="64-如何进行线上ab测试">6.4 <strong>如何进行线上ab测试？</strong></span></h4><p>进行ab测试的主要手段时对用户进行分桶，即将<strong>用户分成实验组和对照组</strong>。实验组使用新模型，对照组使用base模型。<strong>分桶过程中需要保证样本的独立性和采样的无偏性</strong>，确保每个用户只划分到一个桶中，分桶过程中需要保证user id是一个<a href="https://www.zhihu.com/search?q=随机数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;440144351&quot;}">随机数</a>，才能保证数据无偏的。</p>
<h2><span id="七-模型评估">七、模型评估</span></h2><h4><span id="71-holdout">7.1 holdout</span></h4><p><strong>留出法是机器学习中最常见的评估方法之一，它会从训练数据中保留出验证样本集，这部分数据不用于训练，而用于模型评估</strong>。</p>
<h4><span id="72-交叉验证">7.2 交叉验证</span></h4><p><strong>留出法的数据划分，可能会带来偏差</strong>。在机器学习中，另外一种比较常见的评估方法是交叉验证法—— <img src="https://www.zhihu.com/equation?tex=K" alt="公式"> <strong>折交叉验证对 <img src="https://www.zhihu.com/equation?tex=K" alt="公式"> 个不同分组训练的结果进行平均来减少方差</strong>。</p>
<h4><span id="73-自助法">7.3 自助法</span></h4><p>Bootstrap 是一种用小样本估计总体值的一种非参数方法，在进化和生态学研究中应用十分广泛。<strong>Bootstrap通过有放回抽样生成大量的伪样本，通过对伪样本进行计算，获得统计量的分布，从而估计数据的整体分布</strong>。</p>
<h2><span id="八-超参数调优">八、超参数调优</span></h2><p>神经网咯是有许多超参数决定的，例如网络深度，学习率，正则等等。如何寻找最好的超参数组合，是一个老人靠经验，新人靠运气的任务。</p>
<h4><span id="81-网格搜索">8.1 网格搜索</span></h4><h4><span id="82-随机搜索">8.2 随机搜索</span></h4><h4><span id="83-贝叶斯优化">==8.3 贝叶斯优化==</span></h4><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/390373572"><em>贝叶斯优化</em>(原理+代码解读)</a></p>
<p>  <a href="https://zhuanlan.zhihu.com/p/27916208">LightGBM调参指南(带贝叶斯优化代码)</a></p>
<ul>
<li>贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息</li>
<li>贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸</li>
<li><p>贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部最优</p>
<h4><span id="可用的贝叶斯优化框架">可用的贝叶斯优化框架</span></h4></li>
</ul>
<ol>
<li>BayesianOptimization：<a href="https://link.zhihu.com/?target=https%3A//github.com/fmfn/BayesianOptimization">https://github.com/fmfn/BayesianOptimization</a></li>
<li>清华开源的openbox：<a href="https://link.zhihu.com/?target=https%3A//open-box.readthedocs.io/zh_CN/latest/index.html">https://open-box.readthedocs.io/zh_CN/latest/index.html</a></li>
<li>华为开源的HEBO：<a href="https://link.zhihu.com/?target=https%3A//github.com/huawei-noah/HEBO">https://github.com/huawei-noah/HEBO</a></li>
<li><strong>Hyperopt</strong>：<a href="https://link.zhihu.com/?target=http%3A//hyperopt.github.io/hyperopt/">http://hyperopt.github.io/hype</a></li>
</ol>
</blockquote>
<h4><span id="贝叶斯优化什么黑盒优化">贝叶斯优化什么?【黑盒优化】</span></h4><p>求助 gradient-free 的优化算法了，这类算法也很多了，<strong>贝叶斯优化就属于无梯度优化算法</strong>中的一种，它希望在尽可能少的试验情况下去尽可能获得优化命题的全局最优解。</p>
<p><img src="https://pic1.zhimg.com/v2-75933a225ab1875a27188013ce0d8740_b.jpg" alt="img" style="zoom: 33%;"></p>
<ul>
<li>目标函数 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> 及其导数未知，否则就可以用梯度下降等方法求解。</li>
<li>计算目标函数时间成本大，意味着像蚁群算法、遗传算法这种方法也失效了，因为计算一次要花费很多时间。</li>
</ul>
<h4><span id="概述">概述</span></h4><p>贝叶斯优化，是一种使用<strong>贝叶斯定理来指导搜索以找到目标函数的最小值或最大值的方法</strong>，就是在每次迭代的时候，利用之前观测到的历史信息（先验知识）来进行下一次优化，通俗点讲，<strong>就是在进行一次迭代的时候，先回顾下之前的迭代结果，结果太差的 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 附近就不去找了，尽量往结果好一点的 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 附近去找最优解，</strong>这样一来搜索的效率就大大提高了，这其实和人的思维方式也有点像，每次在学习中试错，并且在下次的时候根据这些经验来找到最优的策略。</p>
<h4><span id="贝叶斯优化过程">贝叶斯优化过程</span></h4><p>首先，假设有一个这样的函数 <img src="https://www.zhihu.com/equation?tex=c%28x%29" alt="[公式]"> ，我们需要找到他的最小值，如下图所示，这也是我们所需要优化的目标函数，但是我们并不能够知道他的具体形状以及表达形式是怎么样的。</p>
<p><img src="https://pic3.zhimg.com/v2-ddb9527a36ddcdb539d573fa5124d576_b.jpg" alt="img"></p>
<p>贝叶斯优化是通过一种叫做代理优化的方式来进行的，就是不知道真实的目标函数长什么样，我们就用一个<strong>代理函数（surrogate function）来代替目标函数</strong>，<strong>而这个代理函数就可以通过先采样几个点，再通过这几个点来给他拟合出来</strong>，如下图虚线所示：</p>
<p><img src="https://pic2.zhimg.com/v2-53769125b87835a74445a472415c22a1_b.jpg" alt="img"></p>
<p>基于构造的代理函数，<strong>我们就可以在可能是最小值的点附近采集更多的点</strong>，或者在还没有采样过的区域来采集更多的点，有了更多点，就可以<strong>更新代理函数</strong>，使之更逼近真实的目标函数的形状，这样的话也更容易找到目标函数的最小值，这个采样的过程同样可以通过构建一个采集函数来表示，也就是知道了当前代理函数的形状，如何选择下一个 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 使得收益最大。</p>
<p><strong>然后重复以上过程，最终就可以找到函数的最小值点了，这大致就是贝叶斯优化的一个过程：</strong></p>
<ol>
<li><strong>初始化一个代理函数的先验分布</strong></li>
<li><strong>选择数据点 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，使得采集函数 <img src="https://www.zhihu.com/equation?tex=a%28x%29" alt="[公式]"> 取最大值</strong></li>
<li><strong>在目标函数 <img src="https://www.zhihu.com/equation?tex=+c%28x%29" alt="[公式]"> 中评估数据点 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 并获取其结果 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"></strong> </li>
<li><strong>使用新数据 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="[公式]"> 更新代理函数，得到一个后验分布（作为下一步的先验分布)</strong></li>
<li>重复2-4步，直到达到最大迭代次数</li>
</ol>
<p>举个例子，如图所示，一开始只有两个点（t=2），代理函数的分布是紫色的区域那块，然后根据代理函数算出一个采集函数（绿色线），取采集函数的最大值所在的 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> （红色三角处），算出 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> ，然后根据新的点 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="[公式]"> 更新代理函数和采集函数（t=3），继续重复上面步骤，选择新的采集函数最大值所在的 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，算出 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> ，再更新代理函数和采集函数，然后继续迭代。</p>
<p><img src="https://pic1.zhimg.com/v2-82ed7dc24fca93f3c2c5820ab519a5f8_b.jpg" alt="img" style="zoom: 67%;"></p>
<p>问题的核心就在于代理函数和采集函数如何构建，常用的代理函数有：</p>
<ol>
<li><strong>高斯过程（Gaussian processes）</strong></li>
<li><strong>Tree Parzer Estimator</strong></li>
<li><strong>概率随机森林：针对类别型变量</strong></li>
</ol>
<p>采集函数则需要兼顾两方面的性质：</p>
<ol>
<li>利用当前已开发的区域（Exploitation）：即在当前最小值附近继续搜索</li>
<li>探索尚未开发的区域（Exploration）：即在还没有搜索过的区域里面搜索，可能那里才是全局最优解</li>
</ol>
<p><strong>常用的采集函数有：</strong></p>
<ol>
<li>Probability of improvement（PI）</li>
<li>Expected improvement（EI）</li>
<li>Confidence bound criteria，包括LCB和UCB</li>
</ol>
<h4><span id="84-hyperopt">8.4 Hyperopt</span></h4><p>Hyperopt 是一个强大的 Python 库，用于超参数优化，由 jamesbergstra 开发。Hyperopt 使用贝叶斯优化的形式进行参数调整，允许你为给定模型获得最佳参数。它可以在大范围内优化具有数百个参数的模型。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>理论基础</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习-理论基础</title>
    <url>/posts/52716/</url>
    <content><![CDATA[<h3><span id="一-机器学习中参数模型和非参数模型理解">一、 机器学习中参数模型和非参数模型理解</span></h3><blockquote>
<p>  参考：<a href="https://blog.csdn.net/FrankieHello/article/details/94022594">https://blog.csdn.net/FrankieHello/article/details/94022594</a></p>
</blockquote>
<p><strong>参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型</strong>；非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p>
<p><strong>参数模型</strong>：线性回归、逻辑回归、感知机、基本型的SVM</p>
<p><strong>非参数模型</strong>：决策树、对偶型的SVM、朴素贝叶斯、神经网络</p>
<span id="more"></span>
<h3><span id="二-判别模型-vs-生成模型">二、 判别模型 VS 生成模型</span></h3><blockquote>
<p>  判别模型与生成模型，概率模型与非概率模型、参数模型与非参数模型总结 - Eureka的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/37821985">https://zhuanlan.zhihu.com/p/37821985</a></p>
<p>  <strong>机器学习中的判别式模型和生成式模型</strong> - Microstrong的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/74586507">https://zhuanlan.zhihu.com/p/74586507</a></p>
</blockquote>
<p><img src="https://s2.loli.net/2023/04/17/AYyZUiraIdN5Dn1.png" alt="image-20230417171758407" style="zoom:50%;"></p>
<p><strong>判别模型：感知机、逻辑斯特回归、支持向量机、神经网络、k近邻都属于判别学习模型。判别模型分为两种:</strong></p>
<ul>
<li>直接对输入空间到输出空间的映射进行建模, 也就是学习函数 $h$ : </li>
</ul>
<script type="math/tex; mode=display">
h: X \rightarrow Y, s . t . y=h(x)</script><ul>
<li>对条件概率 $P(y \mid x)$ 进行建模, 然后根据贝叶斯风险最小化的准则进行分类: </li>
</ul>
<script type="math/tex; mode=display">
y=\arg \max _{y \in\{-1,1\}} P(y \mid x)</script><p><strong>生成模型：</strong></p>
<p>生成模型是间接地, 先对 $P(x, y)$ 进行建模, 再根据贝叶斯公式:$P(y \mid x)=\frac{P(x \mid y) P(y)}{P(x)}$</p>
<script type="math/tex; mode=display">
P(y \mid x)=\frac{P(x \mid y) P(y)}{P(x)}</script><p>算出 $P(y \mid x)$, 最后根据 $\arg \max _{y \in{-1,1}} P(y \mid x)$ 来做分类 (由此可知, 判别模型实际上不需要对 $P(x, y)$ 进行建模)。</p>
<h3><span id="三-概率模型-vs-非概率模型">三、概率模型 vs 非概率模型</span></h3><h4><span id="31-概率模型">3.1 概率模型</span></h4><blockquote>
<p>  <strong>线性回归（高斯分布）、LR（伯努利分布）、高斯判别分析、朴素贝叶斯</strong></p>
</blockquote>
<p><strong>概率模型指出了学习的目的是学出 $P(x, y)$ 或 $P(y \mid x)$, 但最后都是根据 $\arg \max _{y \in{-1,1}} P(y \mid x)$ 来做判别归类</strong>。对于 $P(x, y)$ 的估计, 一般是根据乘法公式 $P(x, y)=P(x \mid y) P(y)$ 将其拆解成 $P(x \mid y), P(y)$ 分别进行估计。无论是对 $P(x \mid y), P(y)$ 还是 $P(y \mid x)$ 的估计, 都是会先假设分布的形式, 例如逻辑斯特回归就假设了 $Y \mid X$ 服从伯努利分 布。分布形式固定以后, 剩下的就是分布参数的估计问题。<strong>常用的估计有极大似然估计(MLE)和极大后验概率估计 (MAP)等</strong>。其中, 极大后验概率估计涉及到分布参数的先验概率, 这为我们注入先验知识提供了途径。逻辑斯特回 归、高斯判别分析、朴素贝叶斯都属于概率模型。</p>
<p>在一定的条件下，非概率模型与概率模型有以下对应关系:</p>
<p><img src="https://s2.loli.net/2023/04/17/AYyZUiraIdN5Dn1.png" alt="image-20230417171758407" style="zoom:50%;"></p>
<h4><span id="32-非概率模型">3.2 非概率模型</span></h4><blockquote>
<p>  <strong>感知机、支持向量机、神经网络、k近邻都属于非概率模型</strong>。线性支持向量机可以显式地写出损失函数——hinge损失。神经网络也可以显式地写出损失函数——平方损失。</p>
</blockquote>
<p>非概率模型指的是直接学习输入空间到输出空间的映射 $h$, 学习的过程中基本不涉及概率密度的估计, 概率密度 的积分等操作, 问题的关键在于最优化问题的求解。通常, 为了学习假设 $h(x)$, 我们会先根据一些先验知识 (prior knowledge) 来选择一个特定的假设空间 $H(x)$ (函数空间), 例如一个由所有线性函数构成的空间, 然后在 这个空间中找出泛化误差最小的假设出来, |</p>
<script type="math/tex; mode=display">
h^*=\arg \min _{h \in H} \varepsilon(h)=\arg \min _{h \in H} \sum_{x, y} l(h(x), y) P(x, y)</script><p>其中 $l(h(x), y)$ 是我们选取的损失函数, 选择不同的损失函数, 得到假设的泛化误差就会不一样。由于我们并不知 道 $P(x, y)$, 所以即使我们选好了损失函数, 也无法计算出假设的泛化误差, 更别提找到那个给出最小泛化误差的 假设。于是，我们转而去找那个使得经验误差最小的假设</p>
<script type="math/tex; mode=display">
g=\arg \min _{h \in H} \hat{\varepsilon}(h)=\arg \min _{h \in H} \frac{1}{m} \sum_{i=1}^{m} l\left(h\left(x^{(i)}\right), y^{(i)}\right)</script><p><font color="red"> 这种学习的策略叫经验误差最小化(ERM)，理论依据是大数定律：当训练样例无穷多的时候，假设的经验误差会依概率收敛到假设的泛化误差。</font>要想成功地学习一个问题，必须在学习的过程中注入先验知识。前面，我们根据先验知识来选择假设空间，其实，在选定了假设空间后，先验知识还可以继续发挥作用，这一点体现在为我们的优化问题加上正则化项上，例如常用的$L1$正则化， $L2$正则化等。</p>
<script type="math/tex; mode=display">
g=\arg \min _{h \in H} \hat{\varepsilon}(h)=\arg \min _{h \in H} \frac{1}{m} \sum_{i=1}^{m} l\left(h\left(x^{(i)}\right), y^{(i)}\right)+\lambda \Omega(h)</script><h3><span id="四-过拟合和欠拟合">四、 过拟合和欠拟合</span></h3><blockquote>
<p>  欠拟合、过拟合及如何防止过拟合 - G-kdom的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/72038532">https://zhuanlan.zhihu.com/p/72038532</a></p>
</blockquote>
<h4><span id="41-欠拟合">4.1 欠拟合</span></h4><p><strong>欠拟合是指模型不能在训练集上获得足够低的误差</strong>。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。</p>
<h4><span id="42-欠拟合解决方法">4.2 欠拟合解决方法</span></h4><p>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过<strong>增加网络复杂度</strong>或者在模型中<strong>增加特征</strong>，这些都是很好解决欠拟合的方法。</p>
<h4><span id="43-过拟合">4.3 过拟合</span></h4><p>过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，<strong>模型在训练集上表现很好，但在测试集上却表现很差</strong>。模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，<strong>泛化能力差</strong>。</p>
<p>造成原因主要有以下几种：<br>1、<strong>训练数据集样本单一，样本不足</strong>。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。<br>2、<strong>训练数据中噪声干扰过大</strong>。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。<br>3、<strong>模型过于复杂。</strong>模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。</p>
<h4><span id="44-如何防止过拟合">4.4 如何防止过拟合</span></h4><p>要想解决过拟合问题，就要显著减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。</p>
<h5><span id="1-使用正则化regularization方法">1、使用正则化（Regularization）方法。</span></h5><p>那什么是<a href="https://www.zhihu.com/search?q=正则化&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;72038532&quot;}">正则化</a>呢？<strong>正则化是指修改学习算法，使其降低泛化误差而非训练误差</strong>。</p>
<p>常用的正则化方法根据具体的使用策略不同可分为：（1）直接提供正则化约束的参数正则化方法，如L1/L2正则化；（2）通过工程上的技巧来实现更低泛化误差的方法，如提前终止(Early stopping)和Dropout；（3）不直接提供约束的隐式正则化方法，如数据增强等。</p>
<p><strong>L2正则化起到使得权重参数 $w$变小的效果，为什么能防止过拟合呢？</strong>因为更小的权重参数$w$意味着模型的复杂度更低，对训练数据的拟合刚刚好，不会过分拟合训练数据，从而提高模型的泛化能力。</p>
<h5><span id="2-获取和使用更多的数据数据集增强解决过拟合的根本性方法">2、获取和使用更多的数据（数据集增强）——解决过拟合的根本性方法</span></h5><p>让机器学习或深度学习模型泛化能力更好的办法就是使用更多的数据进行训练。但是，在实践中，我们拥有的数据量是有限的。解决这个问题的一种方法就是<strong>创建“假数据”并添加到训练集中——数据集增强</strong>。通过增加训练集的额外副本来增加训练集的大小，进而改进模型的泛化能力。</p>
<p>我们以图像数据集举例，能够做：旋转图像、缩放图像、随机裁剪、加入随机噪声、平移、镜像等方式来增加数据量。另外补充一句，在物体分类问题里，<strong>CNN在图像识别的过程中有强大的“不变性”规则，即待辨识的物体在图像中的形状、姿势、位置、图像整体明暗度都不会影响分类结果</strong>。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。</p>
<h5><span id="3-采用合适的模型控制模型的复杂度">3. 采用合适的模型（控制模型的复杂度）</span></h5><p>过于复杂的模型会带来过拟合问题。对于模型的设计，目前公认的一个深度学习规律”deeper is better”。国内外各种大牛通过实验和竞赛发现，对于CNN来说，层数越多效果越好，但是也更容易产生过拟合，并且计算所耗费的时间也越长。</p>
<p>根据<strong>奥卡姆剃刀法则</strong>：在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。对于模型的设计而言，我们应该<strong>选择简单、合适的模型解决复杂的问题</strong>。</p>
<h5><span id="4-降低特征的数量">4. 降低特征的数量</span></h5><p>对于一些特征工程而言，可以降低特征的数量——删除冗余特征，人工选择保留哪些特征。这种方法也可以解决过拟合问题。</p>
<h5><span id="5-dropout">5. Dropout</span></h5><p>Dropout是在训练网络时用的一种技巧（trike），相当于在隐藏单元增加了噪声。<strong>Dropout 指的是在训练过程中每次按一定的概率（比如50%）随机地“删除”一部分隐藏单元（神经元）。</strong>所谓的“删除”不是真正意义上的删除，其实就是将该部分神经元的激活函数设为0（激活函数的输出为0），让这些神经元不计算而已。</p>
<p><strong>Dropout为什么有助于防止过拟合呢？</strong></p>
<p>（a）在训练过程中会产生不同的训练模型，不同的训练模型也会产生不同的的计算结果。随着训练的不断进行，计算结果会在一个范围内波动，但是均值却不会有很大变化，因此可以把最终的训练结果看作是不同模型的平均输出。</p>
<p>（b）它消除或者减弱了神经元节点间的联合，降低了网络对单个神经元的依赖，从而增强了泛化能力。</p>
<h5><span id="6-early-stopping提前终止">6. Early stopping（提前终止）</span></h5><p>对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如<a href="https://www.zhihu.com/search?q=梯度下降&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;72038532&quot;}">梯度下降</a>（Gradient descent）。<strong>Early stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合</strong>。</p>
<p>为了获得性能良好的神经网络，训练过程中可能会经过很多次<a href="https://www.zhihu.com/search?q=epoch&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;72038532&quot;}">epoch</a>（遍历整个数据集的次数，一次为一个epoch）。如果epoch数量太少，网络有可能发生欠拟合；如果epoch数量太多，则有可能发生过拟合。Early <a href="https://www.zhihu.com/search?q=stopping&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;72038532&quot;}">stopping</a>旨在解决epoch数量需要手动设置的问题。具体做法：<strong>每个epoch（或每N个epoch）结束后，在验证集上获取测试结果，随着epoch的增加，如果在验证集上发现测试误差上升，则停止训练，将停止之后的权重作为网络的最终参数。</strong></p>
<p><strong>为什么能防止过拟合？</strong>当还未在神经网络运行太多迭代过程的时候，w参数接近于0，因为随机初始化<a href="https://www.zhihu.com/search?q=w值&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;72038532&quot;}">w值</a>的时候，它的值是较小的随机值。当你开始迭代过程，w的值会变得越来越大。到后面时，w的值已经变得十分大了。所以early stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。</p>
<p><strong>Early Stopping缺点：没有采取不同的方式来解决优化损失函数和过拟合这两个问题</strong>，而是用一种方法同时解决两个问题 ，结果就是要考虑的东西变得更复杂。之所以不能独立地处理，因为如果你停止了优化损失函数，你可能会发现损失函数的值不够小，同时你又不希望过拟合。</p>
<h3><span id="五-损失函数loss与评价指标metric的区别">五、损失函数(loss)与评价指标(metric)的区别？</span></h3><p><strong>当建立一个学习算法时，我们希望最大化一个给定的评价指标matric（比如说准确度），但算法在学习过程中会尝试优化一个不同的损失函数loss（比如说MSE/Cross-entropy）。</strong></p>
<p><strong><font color="red"> 那为什么不把评价指标matric作为学习算法的损失函数loss呢？</font></strong></p>
<ul>
<li><p>一般来说，我认为你应该尝试优化一个与你最关心的评价指标相对应的损失函数。例如，在做分类时，我认为你需要给我一个很好的理由，让我不要优化交叉熵。也就是说，交叉熵并不是一个非常直观的指标，所以一旦你完成了训练，你可能还想知道你的分类准确率有多高，以了解你的模型是否真的能在现实世界中发挥作用，总之，在每个epoch训练完后，你都会有多个评估指标。这样作的主要原因是为了了解你的模型在做什么。这意味着你想要最大化指标A，以便得到一个接近最大化指标B的解决方案。</p>
</li>
<li><p>通常情况下，MSE/交叉熵比精度更容易优化，因为它们对模型参数是可微的，在某些情况下甚至是凸的，这使得它更容易。</p>
</li>
</ul>
<h3><span id="六-标准化和归一化">六、标准化和归一化</span></h3><blockquote>
<p>  PCA、k-means、SVM、回归模型、<strong>神经网络</strong></p>
</blockquote>
<h4><span id="61-定义">6.1 定义</span></h4><p><strong>归一化和标准化</strong>都是对<strong>数据做变换</strong>的方式，将原始的一列数据转换到某个范围，或者某种形态，具体的：</p>
<blockquote>
<p>  <strong>归一化(Normalization)</strong>：将一列数据变化到某个固定区间(范围)中，通常，这个区间是[0, 1]，广义的讲，可以是各种区间，比如映射到[0，1]一样可以继续映射到其他范围，图像中可能会映射到[0,255]，其他情况可能映射到[-1,1]；</p>
<p>  <strong>标准化(Standardization)</strong>：将数据变换为均值为0，标准差为1的分布切记，<strong>并非一定是正态的；</strong></p>
<p>  <strong>中心化</strong>：另外，还有一种处理叫做中心化，也叫零均值处理，就是将每个原始数据减去这些数据的均值。</p>
</blockquote>
<h4><span id="62-差异">6.2 差异</span></h4><blockquote>
<p>  <strong>归一化：对处理后的数据范围有严格要求;</strong></p>
<p>  <strong>标准化:  数据不为稳定，存在极端的最大最小值;  涉及距离度量、协方差计算的时候;</strong></p>
</blockquote>
<ul>
<li><strong>归一化会严格的限定变换后数据的范围</strong>，比如按之前最大最小值处理的，它的范围严格在[ 0 , 1 ]之间；而<strong>标准化</strong>就没有严格的区间，变换后的数据没有范围，只是其均值是0，标准差为1。</li>
<li><strong>归一化的缩放比例仅仅与极值有关</strong>，容易受到异常值的影响。</li>
</ul>
<h4><span id="63-用处">6.3 用处</span></h4><ul>
<li>回归模型，自变量X的量纲不一致导致了<strong>回归系数无法直接解读</strong>或者错误解读；需要将X都处理到统一量纲下，这样才可比；</li>
<li>机器学习任务和统计学任务中有很多地方要用到<strong>“距离”的计算</strong>，比如PCA，比如KNN，比如kmeans等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；</li>
<li>参数估计时使用<strong>梯度下降</strong>，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即<strong>提升模型的收敛速度</strong>。</li>
</ul>
<p><strong>其他：log、sigmod、softmax 变换</strong></p>
<h3><span id="七-回归-vs-分类">七、回归 vs 分类</span></h3><p>回归问题可以理解为是定量输出的问题，是一个连续变量预测；分类问题可以理解为是定性输出的问题，是一个离散变量预测。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>理论基础</category>
      </categories>
      <tags>
        <tag>理论基础</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（13）EM算法</title>
    <url>/posts/1D9QYCW/</url>
    <content><![CDATA[<h1><span id="em期望最大-概率模型">EM——期望最大 [概率模型]</span></h1><blockquote>
<p>  <strong>EM 算法通过引入隐含变量，使用 MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM 算法首先会固定其中的第一个参数，然后使用 MLE 计算第二个变量值；接着通过固定第二个变量，再使用 MLE 估测第一个变量值，依次迭代，直至收敛到局部最优解。</strong></p>
<ol>
<li><a href="https://www.zhihu.com/question/27976634">怎么通俗易懂地解释 EM 算法并且举个例子?</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/zouxy09/article/details/8537620">从最大似然到 EM 算法浅解</a></li>
<li><h5><span id="em算法"></span></h5></li>
</ol>
</blockquote>
<p><strong><font color="red"> EM 算法，全称 Expectation Maximization Algorithm。期望最大算法是一种迭代算法，用于含有隐变量（Hidden Variable）的概率参数模型的最大似然估计或极大后验概率估计。</font></strong></p>
<p>本文思路大致如下：先简要介绍其思想，然后举两个例子帮助大家理解，有了感性的认识后再进行严格的数学公式推导。</p>
<h2><span id="1-思想">1. 思想</span></h2><p>EM 算法的核心思想非常简单，分为两步：<strong>Expection-Step</strong> 和 <strong>Maximization-Step</strong>。<strong>E-Step 主要通过观察数据和现有模型来估计参数</strong>，然后用这个估计的参数值来计算似然函数的期望值；而 M-Step 是寻找似然函数最大化时对应的参数。由于算法会保证在每次迭代之后<strong>似然函数都会增加</strong>，所以函数最终会收敛。</p>
<p><img src="https://www.zhihu.com/equation?tex=EM" alt="[公式]"> <strong>算法一句话总结就是</strong>： <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步固定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 优化 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步固定 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 优化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 。</p>
<h3><span id="2-例子">2 例子</span></h3><h4><span id="21-例子-a">2.1 例子 A</span></h4><p>假设有两枚硬币 A 和 B，他们的随机抛掷的结果如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-4e19d89b47e21cf284644b0576e9af0f_1440w.jpg" alt="img"></p>
<p>我们很容易估计出两枚硬币抛出正面的概率：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_A+%3D+24%2F30+%3D0.8+%5C%5C%5Ctheta_B+%3D+9%2F20+%3D0.45++%5C%5C" alt="[公式]"></p>
<p>现在我们加入<strong>隐变量</strong>，抹去每轮投掷的硬币标记：</p>
<p><img src="https://pic1.zhimg.com/80/v2-caa896173185a8f527c037c122122258_1440w.jpg" alt="img"></p>
<p>碰到这种情况，我们该如何估计 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值？</p>
<p>我们多了一个隐变量 <img src="https://www.zhihu.com/equation?tex=Z%3D%28z_1%2C+z_2%2C+z_3%2C+z_4%2C+z_5%29" alt="[公式]"> ，代表每一轮所使用的硬币，我们需要知道每一轮抛掷所使用的硬币这样才能估计 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值，但是估计隐变量 Z 我们又需要知道 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 的值，才能用极大似然估计法去估计出 Z。这就陷入了一个鸡生蛋和蛋生鸡的问题。</p>
<p>其解决方法就是先<strong>随机初始化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"></strong> ，然后用去估计 Z， 然后基于 Z 按照最大似然概率去估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> ，循环至收敛。</p>
<h4><span id="212-计算"><strong>2.1.2 计算</strong></span></h4><p>随机初始化 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A%3D0.6" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B%3D0.5" alt="[公式]"></p>
<p>对于第一轮来说，如果是硬币 A，得出的 5 正 5 反的概率为： <img src="https://www.zhihu.com/equation?tex=0.6%5E5%2A0.4%5E5" alt="[公式]"> ；如果是硬币 B，得出的 5 正 5 反的概率为： <img src="https://www.zhihu.com/equation?tex=0.5%5E5%2A0.5%5E5" alt="[公式]"> 。我们可以算出使用是硬币 A 和硬币 B 的概率分别为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P_A%3D%5Cfrac%7B0.6%5E5+%2A+0.4%5E5%7D%7B%280.6%5E5+%2A+0.4%5E5%29+%2B+%280.5%5E5+%2A+0.5%5E5%29%7D+%3D+0.45%5C%5C+P_B%3D%5Cfrac%7B0.5%5E5+%2A+0.5%5E5%7D%7B%280.6%5E5+%2A+0.4%5E5%29+%2B+%280.5%5E5+%2A+0.5%5E5%29%7D+%3D+0.55+%5C%5C" alt="[公式]"></p>
<p><img src="https://pic4.zhimg.com/80/v2-b325de65a5bcac196fc0939f346410d7_1440w.jpg" alt="img"></p>
<p>从期望的角度来看，对于第一轮抛掷，使用硬币 A 的概率是 0.45，使用硬币 B 的概率是 0.55。同理其他轮。这一步我们实际上是<strong>估计出了 Z 的概率分布</strong>，这部就是 <strong>E-Step</strong>。</p>
<p>结合硬币 A 的概率和上一张投掷结果，我们利用期望可以求出硬币 A 和硬币 B 的贡献。以第二轮硬币 A 为例子，计算方式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=H%3A+0.80%2A9+%3D7.2+%5C%5C+T%3A+0.80%2A1%3D0.8+%5C%5C" alt="[公式]"></p>
<p>于是我们可以得到：</p>
<p><img src="https://pic1.zhimg.com/80/v2-9b6e8c50c0761c6ac19909c26e0a71d4_1440w.jpg" alt="img"></p>
<p>然后用极大似然估计来估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_A" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_B" alt="[公式]"> 。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_A+%3D+%5Cfrac%7B21.3%7D%7B21.3%2B8.6%7D+%3D+0.71+%5C%5C+%5Ctheta_B+%3D+%5Cfrac%7B11.7%7D%7B11.7+%2B+8.4%7D+%3D+0.58+%5C%5C" alt="[公式]"></p>
<p>这步就对应了 M-Step，重新估计出了参数值。如此反复迭代，我们就可以算出最终的参数值。</p>
<p>上述讲解对应下图：</p>
<p><img src="https://pic3.zhimg.com/v2-6cac968d6500cbca58fc90347c288466_r.jpg" alt="preview" style="zoom:50%;"></p>
<h4><span id="22-例子-b">2.2 例子 B</span></h4><p>如果说例子 A 需要计算你可能没那么直观，那就举更一个简单的例子：</p>
<p>现在一个班里有 50 个男生和 50 个女生，且男女生分开。我们假定男生的身高服从正态分布： <img src="https://www.zhihu.com/equation?tex=N%28%5Cmu_1%2C+%5Csigma%5E2_1+%29" alt="[公式]"> ，女生的身高则服从另一个正态分布： <img src="https://www.zhihu.com/equation?tex=N%28%5Cmu_2%2C+%5Csigma%5E2_2+%29" alt="[公式]"> 。这时候我们可以用极大似然法（MLE），分别通过这 50 个男生和 50 个女生的样本来估计这两个正态分布的参数。</p>
<p>但现在我们让情况复杂一点，就是这 50 个男生和 50 个女生混在一起了。我们拥有 100 个人的身高数据，却不知道这 100 个人每一个是男生还是女生。</p>
<p>这时候情况就有点尴尬，因为通常来说，我们只有知道了精确的男女身高的正态分布参数我们才能知道每一个人更有可能是男生还是女生。但从另一方面去考量，我们只有知道了每个人是男生还是女生才能尽可能准确地估计男女各自身高的正态分布的参数。</p>
<p>这个时候有人就想到我们必须从某一点开始，并用迭代的办法去解决这个问题：<strong>==我们先设定男生身高和女生身高分布的几个参数（初始值），然后根据这些参数去判断每一个样本（人）是男生还是女生，之后根据标注后的样本再反过来重新估计参数。之后再多次重复这个过程，直至稳定。这个算法也就是 EM 算法。==</strong></p>
<h3><span id="3-推导">3. 推导</span></h3><p>给定数据集，假设样本间相互独立，我们想要拟合模型 <img src="https://www.zhihu.com/equation?tex=p%28x%3B%5Ctheta%29" alt="[公式]"> 到数据的参数。根据分布我们可以得到如下<strong>似然函数</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+L%28%5Ctheta%29+%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dlog+p%28x_i%3B%5Ctheta%29++%5C%5C+%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dlog+%5Csum_%7Bz%7Dp%28x_i%2C+z%3B%5Ctheta%29+%5Cend%7Baligned%7D+%5C%5C" alt="[公式]"></p>
<p>第一步是<strong>对极大似然函数取对数</strong>，第二步是对每个样本的每个可能的类别 z 求<strong>联合概率分布之和</strong>。如果这个 z 是已知的数，那么使用极大似然法会很容易。但如果 z 是隐变量，我们就需要用 EM 算法来求。<strong>事实上，隐变量估计问题也可以通过梯度下降等优化算法，但事实由于求和项将随着隐变量的数目以指数级上升，会给梯度计算带来麻烦；而 EM 算法则可看作一种非梯度优化方法。</strong></p>
<h4><span id="31-求解含有隐变量的概率模型">3.1 求解含有隐变量的概率模型</span></h4><p><strong>为了求解含有隐变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 的概率模型</strong> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D%5Chat%7B%5Ctheta%7D%3D%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%5Cend%7Baligned%7D" alt="[公式]"> <strong>需要一些特殊的技巧</strong>，通过引入隐变量 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%28i%29%7D" alt="[公式]"> 的概率分布为 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> ，<strong>==因为 <img src="https://www.zhihu.com/equation?tex=%5Clog+%28x%29" alt="[公式]"> 是凹函数故结合凹函数形式下的詹森不等式进行放缩处理==</strong><br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7D+Q_i%28z%5E%7B%28i%29%7D%29%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C+%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+%5Cmathbb%7BE%7D%28%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5C%5C+%26%5Cge%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Cmathbb%7BE%7D%5B%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5D%5C%5C+%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>其中由概率分布的充要条件 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%3D1%E3%80%81Q_i%28z%5E%7B%28i%29%7D%29%5Cge0" alt="[公式]"> 可看成下述关于 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 函数分布列的形式：</p>
<p><img src="https://pic2.zhimg.com/v2-cb7ddb5cdc34761ec70d63c97189b102_720w.jpg?source=d16d100b" alt="img" style="zoom:50%;"></p>
<p><strong>这个过程可以看作是对 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 求了下界</strong>，假设 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 已经给定那么 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的值就取决于 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%29" alt="[公式]"> 了，因此可以通过调整这两个概率使下界不断上升，以逼近 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的真实值，当不等式变成等式时说明调整后的概率能够等价于 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> ，所以必须找到使得等式成立的条件，即寻找<br> <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5B%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29%5D%3D%5Clog+%5Cmathbb%7BE%7D%5B%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5D%5C%5C" alt="[公式]"><br>由期望得性质可知当<br> <img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%3DC%2C%5C+%5C+%5C+%5C+%5C+C%5Cin%5Cmathbb%7BR%7D+%5C+%5C+%5C+%5C+%5C+%28%2A%29%5C%5C" alt="[公式]"><br>等式成立，对上述等式进行变形处理可得<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DCQ_i%28z%5E%7B%28i%29%7D%29%5C%5C+%26%5CLeftrightarrow+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DC%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%3DC%5C%5C+%26%5CLeftrightarrow+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%3DC+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28%2A%2A%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>把 <img src="https://www.zhihu.com/equation?tex=%28%2A%2A%29" alt="[公式]"> 式带入 <img src="https://www.zhihu.com/equation?tex=%28%2A%29" alt="[公式]"> 化简可知<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+Q_i%28z%5E%7B%28i%29%7D%29%26%3D%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7B%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7Dp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%5C%5C+%26%3D%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7Bp%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%5C%5C+%26%3Dp%28z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>至此，可以推出<strong>在固定参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 后</strong>， <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 的<strong>计算公式就是后验概率</strong>，解决了 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 如何选择得问题。这一步称为 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步，建立 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 得下界；接下来得 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步，就是在给定 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 后，调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 去极大化 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29" alt="[公式]"> 的下界即<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Clog+p%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Cleft%5B%5Clog+p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29-%5Clog+Q_i%28z%5E%7B%28i%29%7D%29%5Cright%5D%5C%5C+%26%5CLeftrightarrow+%5Cmathop%7B%5Carg%5Cmax%7D_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Clog+p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>因此EM算法的迭代形式为：</p>
<p><img src="https://pic2.zhimg.com/80/v2-8a4f41596e78bfeb1b4044212b259524_1440w.jpg?source=d16d100b" alt="img" style="zoom:50%;"></p>
<p><img src="https://pic3.zhimg.com/80/v2-2f7fc5ca144d2f85f14d46e88055dd86_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>这张图的意思就是：<strong>首先我们固定</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>使下界</strong> <img src="https://www.zhihu.com/equation?tex=J%28z%2CQ%29" alt="[公式]"> <strong>上升至与</strong> <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="[公式]"> <strong>在此点</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>处相等（绿色曲线到蓝色曲线），然后固定</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>使下界</strong> <img src="https://www.zhihu.com/equation?tex=J%28z%2CQ%29" alt="[公式]"> <strong>达到最大值（</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta_t" alt="[公式]"> <strong>到</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D" alt="[公式]"> <strong>），然后再固定</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> <strong>，调整</strong> <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]"> <strong>，一直到收敛到似然函数</strong> <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="[公式]"> <strong>的最大值处的</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 。</p>
<p><strong><font color="red"> EM 算法通过引入隐含变量，使用 MLE（极大似然估计）进行迭代求解参数。通常引入隐含变量后会有两个参数，EM 算法首先会固定其中的第一个参数，然后使用 MLE 计算第二个变量值；接着通过固定第二个变量，再使用 MLE 估测第一个变量值，依次迭代，直至收敛到局部最优解。</font></strong></p>
<h4><span id="32-em算法的收敛性">3.2 EM算法的收敛性</span></h4><p>不妨假设 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 是EM算法第 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 次迭代和第 <img src="https://www.zhihu.com/equation?tex=k%2B1" alt="[公式]"> 次迭代的结果，要确保 <img src="https://www.zhihu.com/equation?tex=EM" alt="[公式]"> 算法收敛那么等价于证明 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%5Cle%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29" alt="[公式]"> 也就是说极大似然估计单调增加，那么算法最终会迭代到极大似然估计的最大值。在选定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 后可以得到 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步 <img src="https://www.zhihu.com/equation?tex=Q_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%3Dp%28z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> ，这一步保证了在给定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 时，詹森不等式中的等式成立即<br> <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C%5C" alt="[公式]"><br>然后再进行 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步，固定 <img src="https://www.zhihu.com/equation?tex=Q_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 并将 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%29%7D" alt="[公式]"> 视作变量，对上式 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> 求导后得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 因此有如下式子成立<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29%26%3D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28a%29%5C%5C+%26%5Cle+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%5E%7B%28k%29%7D%28z%5E%7B%28i%29%7D%29%5Clog+%5Cdfrac%7Bp%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%3B%5Ctheta%5E%7B%28k%29%7D%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28b%29%5C%5C+%26%5Cle%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%28c%29+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>首先 <img src="https://www.zhihu.com/equation?tex=%28a%29" alt="[公式]"> 式是前面 <img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 步所保证詹森不等式中的等式成立的条件， <img src="https://www.zhihu.com/equation?tex=%28a%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 步的定义，<img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28c%29" alt="[公式]">对任意参数都成立，而其等式的条件是固定 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 并调整好 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 时成立，<img src="https://www.zhihu.com/equation?tex=%28b%29" alt="[公式]"> 到 <img src="https://www.zhihu.com/equation?tex=%28c%29" alt="[公式]">只是固定 <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> ，在得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28k%2B1%29%7D" alt="[公式]"> 时，只是最大化 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%29%7D%29" alt="[公式]"> ，也就是 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%5E%7B%28k%2B1%29%7D%29" alt="[公式]"> 的一个下界而没有使等式成立。</p>
<h3><span id="4-另一种理解">4. 另一种理解</span></h3><p>坐标上升法（Coordinate ascent）：</p>
<p><img src="https://pic4.zhimg.com/80/v2-b28bfe68513ff86d9643fec10786b827_1440w.jpg" alt="img"></p>
<p>途中直线为迭代优化路径，因为每次只优化一个变量，所以可以看到它没走一步都是平行与坐标轴的。</p>
<p>EM 算法类似于坐标上升法，E 步：固定参数，优化 Q；M 步：固定 Q，优化参数。交替将极值推向最大。</p>
<h4><span id="5-应用">5. 应用</span></h4><p>EM 的应用有很多，比如、混合高斯模型、聚类、HMM 等等。其中 <strong>EM 在 K-means 中的用处</strong>，我将在介绍 K-means 中的给出。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>贝叶斯分类器</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（7）朴素贝叶斯</title>
    <url>/posts/25V46VQ/</url>
    <content><![CDATA[<h2><span id="朴素贝叶斯">朴素贝叶斯</span></h2><p><a href="https://scikit-learn.org/dev/modules/naive_bayes.html#naive-bayes">https://scikit-learn.org/dev/modules/naive_bayes.html#naive-bayes</a></p>
<ul>
<li><p><a href="https://plushunter.github.io/2017/02/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">FREE WILL 机器学习算法系列（10）：朴素贝叶斯</a></p>
</li>
<li><h5><span id="最大似然估计-最大后验估计-贝叶斯估计的对比"></span></h5></li>
</ul>
<h3><span id="一-朴素贝叶斯的学习与分类">一、朴素贝叶斯的学习与分类</span></h3><blockquote>
<p>  朴素贝叶斯（Naive Bayes）是基于<strong>贝叶斯定理</strong>与<strong>特征条件假设</strong>的<strong>分类</strong>方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯是<strong>选出各个分类类别后验概率最大</strong>的作为最终分类。</p>
<ul>
<li><strong>优点</strong>：对小规模的数据表现很好，适合多分类任务，<strong>适合增量式训练</strong>。</li>
<li><strong>缺点</strong>：对输入数据的表达形式很敏感<strong>（离散、连续，值极大极小之类的）</strong>。</li>
</ul>
</blockquote>
<h4><span id="11贝叶斯定理">1.1贝叶斯定理</span></h4><p><strong>条件概率:</strong></p>
<p>$P(A|B)$  表示事件B已经发生的前提下，事件B已经发生的前提下，事件A发生的概率，叫做事件 $B$<br>发生下事件 $A$ 的条件概率。其基本求解公式为</p>
<p><img src="image-20220325172121116.png" alt="image-20220325172121116" style="zoom:50%;"></p>
<p><strong>贝叶斯定理</strong>便是基于<strong>条件概率</strong>，通过$P(A|B)$来求$P(B|A)$：【通过<strong>先验概率</strong>计算<strong>后验概率</strong>】</p>
<p><img src="image-20220325172351101.png" alt="image-20220325172351101" style="zoom:50%;"></p>
<p>顺便提一下，上式中的分母，可以根据<strong>全概率公式</strong>分解为：</p>
<p><img src="image-20220325172306652.png" alt="image-20220325172306652" style="zoom:50%;"></p>
<h4><span id="12-特征条件独立假设">1.2 特征条件独立假设</span></h4><p>这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件<strong>独立假设</strong>。给定训练数据集$(X,Y)$，其中每个样本$X$都包括 $n$ 维特征，即$x=(x1,x2,···,xn)$，类标记集合含有$K$种类别，即$y=(y1,y2,···,yk)$.</p>
<p>如果现在来了一个新样本 $x$ 我们要怎么判断它的类别?从概率的角度来看，这个问题就是给定$x$，它属于哪个类别的概率更大。那么问题就转化为求解 $P(y1|x),P(y2|x),P(yk|x)P(y1|x),P(y2|x),P(yk|x)$ 中最大的那个，即求<strong>后验概率最大</strong>的输出：==$arg max_{y_k}P(y_k|x)$==</p>
<p><img src="image-20220325203314162.png" alt="image-20220325203314162" style="zoom:50%;"></p>
<p>根据<strong>全概率公式</strong>，可以进一步分解上式中的分母：</p>
<p><img src="image-20220325203335825.png" alt="image-20220325203335825" style="zoom:50%;"></p>
<ul>
<li><p><img src="image-20220325205350669.png" alt="image-20220325205350669" style="zoom:50%;">：<strong>先验概率</strong> 【训练集计算】</p>
</li>
<li><p><img src="image-20220325205444531.png" alt="image-20220325205444531" style="zoom:50%;">：<strong>条件概率</strong>，它的参数规模是<strong>指数</strong>数量级别的。假设第i维特征xi可取值的个数有Si个，类别取值个数为k个，那么参数个数为$k∏S_j$。</p>
</li>
<li><p><strong>独立性的假设</strong>：通俗地讲就是说假设各个维度的特征互相独立，这样<strong>参数规模</strong>就降到了$∑S_ik$, 【积-&gt;和】</p>
<p><img src="image-20220325211846297.png" alt="image-20220325211846297" style="zoom:50%;"></p>
</li>
<li><p>代入公式1得出：</p>
<p><img src="image-20220325213643901.png" alt="image-20220325213643901" style="zoom:50%;"></p>
</li>
<li><p>于是朴素贝叶斯分类器可表示为：</p>
<p><img src="image-20220325213705861.png" alt="image-20220325213705861" style="zoom:50%;"></p>
</li>
<li><p>由于分母值都是一样的：<strong>==极大后验概率估计==</strong></p>
</li>
</ul>
<p><img src="image-20220325213802190.png" alt="image-20220325213802190" style="zoom:50%;"></p>
<h4><span id="13-朴素贝叶斯法的参数估计求解">1.3 朴素贝叶斯法的参数估计【求解】</span></h4><p>朴素贝叶斯要学习的东西就是：<img src="image-20220325215221520.png" alt="image-20220325215221520" style="zoom:50%;"> 和 <img src="image-20220325215238733.png" alt="image-20220325215238733" style="zoom:50%;">【极大似然函数 + 拉格朗日乘数法】</p>
<ul>
<li><p><strong>先验概率</strong>$P(Y=ck)$的极大似然估计是, <strong>样本在$c_k$出现的次数除以样本容量</strong>：</p>
<p><img src="image-20220325215849572.png" alt="image-20220325215849572" style="zoom:50%;"></p>
</li>
<li><p>$设第 j 个特征x(j)可能取值的集合为a<em>{j1},a</em>{j2},···,a<em>{jl}, 条件概率P(X_j=a</em>{jl} |Y=ck)的极大似然估计是$：</p>
<p><img src="image-20220325221320810.png" alt="image-20220325221320810" style="zoom:50%;"></p>
</li>
</ul>
<h4><span id="14-贝叶斯估计缺失值处理拉普拉斯平滑">1.4 贝叶斯估计【缺失值处理】【拉普拉斯平滑】</span></h4><p><strong>先验概率</strong>的贝叶斯估计：</p>
<p><img src="image-20220325222259483.png" alt="image-20220325222259483" style="zoom:50%;"></p>
<p><strong>条件概率</strong>的贝叶斯估计：【<strong>离散型</strong>】</p>
<p><img src="image-20220325222226643.png" alt="image-20220325222226643" style="zoom:50%;"></p>
<h4><span id="15-朴素贝叶斯有什么优缺点"><strong><font color="red"> 1.5 朴素贝叶斯有什么优缺点？</font></strong></span></h4><h5><span id="优点数学理论-缺失异常不敏感-快-增量式训练">优点：【数学理论、缺失异常不敏感、快、增量式训练】</span></h5><ul>
<li>朴素贝叶斯模型<strong>发源于古典数学理论</strong>，有稳定的分类效率。</li>
<li><strong>对缺失数据和异常数据不太敏感</strong>，算法也比较简单，常用于文本分类。</li>
<li><strong>分类准确度高，速度快</strong>。</li>
<li><strong>对小规模的数据表现很好，能处理多分类任务，适合增量式训练，当数据量超出内存时，我们可以一批批的去增量训练</strong>(朴素贝叶斯在训练过程中只需要计算各个类的概率和各个属性的类条件概率，这些概率值可以快速地根据增量数据进行更新，无需重新全量计算)。</li>
</ul>
<h5><span id="缺点">缺点：</span></h5><ul>
<li><strong>对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）</strong>。</li>
<li><strong>对训练数据的依赖性很强</strong>，如果训练数据误差较大，那么预测出来的效果就会不佳。</li>
<li>理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。 但是在实际中，因为朴素贝叶斯“朴素，”的特点，<strong>导致在属性个数比较多或者属性之间相关性较大时，分类效果不好。</strong>而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。</li>
<li>需要知道<strong>先验概率</strong>，且先验概率很多时候是基于假设或者已有的训练数据所得的，这在某些时候可能会因为假设先验概率的原因出现分类决策上的错误。</li>
</ul>
<h2><span id="二-高斯贝叶斯模型">二、高斯贝叶斯模型</span></h2><blockquote>
<p>  classifier = naive_bayes.MultinomialNB()</p>
</blockquote>
<h4><span id="21-朴素贝叶斯连续型数据处理">2.1 朴素贝叶斯(连续型数据处理)</span></h4><ul>
<li>每一个连续的<strong>数据离散化</strong>，然后用相应的离散区间替换连续数值。这种方法对于划分离散区间的粒度要求较高，不能太细，也不能太粗。</li>
<li>假设<strong>连续数据服从某个概率分布</strong>，<strong>使用训练数据估计分布参数</strong>，通常我们用<strong>高斯分布</strong>来表示<strong>连续数据的类条件概率分布</strong>。</li>
</ul>
<p><strong>==GaussianNB 的条件概率密度计算：其中均值和方差可以通过极大似然估计得出。==</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
&p\left(X^{(j)}=a_{j l} \mid y=c_{k}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{j k}} e^{-\frac{\left(a_{j l}-\mu_{j k}\right)^{2}}{2 \sigma_{j k}^{2}}}
\end{aligned}</script><h2><span id="三-贝叶斯网络">三、贝叶斯网络</span></h2><h4><span id="31-概率图模型">3.1 概率图模型</span></h4><p>概率图模型分为<strong>贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）</strong>两大类。贝叶斯网络可以用一个有向图结构表示，马尔可夫网络可以表示成一个无向图的网络结构。更详细地说，<strong>概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型</strong>等，在机器学习的诸多场景中都有着广泛的应用。</p>
<ul>
<li><strong>贝叶斯网络</strong> — 结点与结点之间是以有向箭头相连接，代表是这个结点会影响下一个结点</li>
<li><strong>马尔可夫网络</strong> — 结点与结点之间是以无向箭头相连接，代表是结点与结点之间会相互影响</li>
</ul>
<h2><span id="四-最大似然估计-最大后验估计-贝叶斯估计的对比">四、最大似然估计、最大后验估计、贝叶斯估计的对比</span></h2><h4><span id="41-贝叶斯公式">4.1 <strong>贝叶斯公式</strong></span></h4><p>这三种方法都和贝叶斯公式有关，所以我们先来了解下贝叶斯公式：</p>
<script type="math/tex; mode=display">
p(\theta \mid X)=\frac{p(X \mid \theta) p(\theta)}{p(X)}</script><p>每一项的表示如下:</p>
<script type="math/tex; mode=display">
\text { posterior }=\frac{\text { likehood } * \text { prior }}{\text { evidence }}</script><ul>
<li>posterior: 通过样本X得到参数 $\theta$ 的概率, 也就是后验概率。</li>
<li>likehood: 通过参数 $\theta$ 得到样本X的概率, 似然函数, 通常就是我们的数据集的表现。</li>
<li>prior: 参数 $\theta$ 的先验概率, 一般是根据人的先验知识来得出的。</li>
</ul>
<h4><span id="42-极大似然估计-mle">4.2 极大似然估计 (MLE)</span></h4><p>极大似然估计的核心思想是: 认为当前发生的事件是概率最大的事件。<strong>因此就可以给定的数据集, 使得该数据集发生的概率最大来求得模型中的参数</strong>。似然函数如下:</p>
<script type="math/tex; mode=display">
p(X \mid \theta)=\prod_{x 1}^{x n} p(x i \mid \theta)</script><p>为了便于计算, 我们对似然函数两边取对数, 生成新的对数似然函数（因为对数函数是单调增函数, 因此求似然函数最大化就可 以转换成对数似然函数最大化）：</p>
<script type="math/tex; mode=display">
p(X \mid \theta)=\prod_{x 1}^{x n} p(x i \mid \theta)=\sum_{x 1}^{x n} \log p(x i \mid \theta)</script><p>求对数似然函数最大化, 可以通过导数为 0 来求解。<strong><font color="red"> 极大似然估计只关注当前的样本, 也就是只关注当前发生的事情, 不考虑事情的先验情况</font></strong>。由于计算简单, 而且不需要关注先验 知识, 因此在机器学习中的应用非常广, 最常见的就是逻辑回归。</p>
<h4><span id="43-最大后验估计-map">4.3 最大后验估计 (MAP)</span></h4><p>和最大似然估计不同的是, 最大后验估计中引入了<strong>先验概率</strong>（先验分布属于贝叶斯学派引入的, 像L1, L2正则化就是对参数引入 了拉普拉斯先验分布和高斯先验分布）, 而且最大后验估计要求的是 $p(\theta \mid X)$<br>最大后验估计可以写成下面的形式:</p>
<script type="math/tex; mode=display">
\operatorname{argmaxp}(\theta \mid X)=\operatorname{argmax} \frac{p(X \mid \theta) p(\theta)}{p(X)}=\operatorname{argmaxp}(X \mid \theta) p(\theta)=\operatorname{argmax}\left(\prod_{x 1}^{x n} p(x i \mid \theta)\right) p(\theta)</script><p>在求最大后验概率时, 可以忽略分母 $p(x)$, 因为该值不影响对 $\theta$ 的估计。同样为了便于计算, 对两边取对数, 后验概率最大化就变成了:</p>
<script type="math/tex; mode=display">
\operatorname{argmax}\left(\sum_{x 1}^{x n} \operatorname{logp}(x i \mid \theta)+\log p(\theta)\right)</script><p><strong><font color="red"> 最大后验估计不只是关注当前的样本的情况，还关注已经发生过的先验知识。在朴素贝叶斯中会有最大后验概率的应用，但并没有用上最大后验估计来求参数（因为朴素贝叶斯中的θ其实就是分类的类别）。</font></strong></p>
<p><strong>最大后验估计和最大似然估计的区别：</strong>最大后验估计允许我们把先验知识加入到估计模型中，<strong>这在样本很少的时候是很有用的（因此朴素贝叶斯在较少的样本下就能有很好的表现）</strong>，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如beta分布的α，β，我们还可以调节把估计的结果“拉”向先验的幅度，α，β越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。</p>
<h2><span id="朴素贝叶斯-qampa">朴素贝叶斯 Q&amp;A</span></h2><blockquote>
<ul>
<li>朴素贝叶斯分类器原理以及公式，出现估计概率值为 0 怎么处理（拉普拉斯平滑），缺点；</li>
<li>解释贝叶斯公式和朴素贝叶斯分类。</li>
<li>贝叶斯分类，这是一类分类方法，主要代表是朴素贝叶斯，朴素贝叶斯的原理，重点在假设各个属性类条件独立。然后能根据贝叶斯公式具体推导。考察给你一个问题，如何利用朴素贝叶斯分类去分类，比如：给你一个人的特征，判断是男是女，比如身高，体重，头发长度等特征的的数据，那么你要能推到这个过程。给出最后的分类器公式。</li>
<li>那你说说贝叶斯怎么分类啊？<strong>比如说看看今天天气怎么样？</strong>我：blabla，，，利用天气的历史数据，可以知道天气类型的先验分布，以及每种类型下特征数据（比如天气数据的特征：温度啊，湿度啊）的条件分布，这样我们根据贝叶斯公式就能求得天气类型的后验分布了。。。。面试官：en（估计也比较满意吧）<strong>那你了解关于求解模型的优化方法吗？一般用什么优化方法来解？</strong></li>
<li>贝叶斯分类器的优化和特殊情况的处理</li>
</ul>
</blockquote>
<h3><span id="1-朴素贝叶斯-svm和lr的区别"><strong><font color="red"> 1、朴素贝叶斯、SVM和LR的区别？</font></strong></span></h3><p><strong>朴素贝叶斯是生成模型</strong>，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解P(Y|X)。</p>
<p><strong>LR是判别模型</strong>，根据极大化对数似然函数直接求出条件概率P(Y|X)；朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求；<strong>朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>SVM</th>
<th>LR</th>
<th>朴素贝叶斯</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong></td>
<td><strong>想要的就是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面</strong>。</td>
<td>使用线性回归模型的预测值逼近分类任务真实标记的对数几率。</td>
<td>基于<strong>贝叶斯定理</strong>与<strong>特征条件假设</strong>的<strong>分类</strong>方法。选出各个分类类别后验概率最大的作为最终分类。</td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td>判别模型、<strong>非概率方法</strong>；</td>
<td><strong>概率方法</strong>；需要对$p(y</td>
<td>x)$进行假设，具有概率意义。</td>
<td>生成模型</td>
</tr>
<tr>
<td><strong>经验损失函数</strong></td>
<td><strong>合页损失函数</strong>；有一段平的零区域、使得SVM的对偶性有稀疏性。</td>
<td><strong>交叉熵损失函数</strong></td>
<td><strong>后验概率最大</strong></td>
</tr>
<tr>
<td><strong>训练样本</strong></td>
<td><strong>支持向量</strong>（少数样本），SVM的参数和假设函数只和支持向量有关。</td>
<td>全样本</td>
<td>全样本</td>
</tr>
<tr>
<td><strong>优化方法</strong></td>
<td>次梯度下降和坐标梯度下降 【<strong>SMO算法</strong>】</td>
<td><strong>梯度下降</strong></td>
<td>无</td>
</tr>
<tr>
<td>多分类</td>
<td><strong>多分类SVM</strong></td>
<td><strong>Softmax回归</strong></td>
<td>后验概率最大</td>
</tr>
<tr>
<td><strong>敏感程度</strong></td>
<td><strong>SVM考虑分类边界线附近的样本</strong>（决定分类超平面的样本）。在支持向量外添加或减少任何样本点对分类决策面没有任何影响；【不敏感】</td>
<td><strong>LR受所有数据点的影响</strong>。直接依赖数据分布，每个样本点都会影响决策面的结果。如果训练数据不同类别严重不平衡。【敏感】</td>
<td><strong>特征值是基于频数进行统计的。</strong>一个值的异常（变成了别的数），<strong>只是贝叶斯公式里的计算概率的分子或者分母发生微小的变化，整体结果影响不大</strong>，不敏感【概率排序】</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="2-朴素贝叶斯朴素在哪里">2、<strong>朴素贝叶斯“朴素”在哪里？</strong></span></h3><p>简单来说：它假定<strong>所有的特征在数据集中的作用是同样重要和独立的</strong>，正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。</p>
<p>利用贝叶斯定理求解联合概率P(XY)时，需要计算条件概率P(X|Y)。在计算P(X|Y)时，朴素贝叶斯做了一个很强的条件独立假设（当Y确定时，X的各个分量取值之间相互独立），即P(X1=x1,X2=x2,…Xj=xj|Y=yk) = P(X1=x1|Y=yk)P(X2=x2|Y=yk)…*P(Xj=xj|Y=yk)。 多个特征全是独立的，需要分别相乘。</p>
<h3><span id="3-在估计条件概率pxy时出现概率为0的情况怎么办">3、<strong>在估计条件概率P(X|Y)时出现概率为0的情况怎么办？</strong></span></h3><p><strong>拉普拉斯平滑法</strong>是朴素贝叶斯中处理零概率问题的一种修正方式。在进行分类的时候，可能会出现某个属性在训练集中没有与某个类同时出现过的情况，如果直接基于朴素贝叶斯分类器的表达式进行计算的话就会出现<strong>零概率现象</strong>。</p>
<p>为了避免其他属性所携带的信息被训练集中未出现过的属性值“抹去”，所以才使用拉普拉斯估计器进行修正。具体的方法是：<strong>在分子上加1,对于先验概率，在分母上加上训练集中label的类别数；对于特征i 在label下的条件概率，则在分母上加上第i个属性可能的取值数（特征 i 的unique()）</strong></p>
<p><strong>先验概率</strong>的贝叶斯估计：</p>
<p><img src="image-20230129161943432.png" alt="image-20230129161943432" style="zoom:50%;"></p>
<p><strong>条件概率</strong>的贝叶斯估计：【<strong>离散型</strong>】</p>
<p><img src="image-20230129162002522.png" alt="image-20230129162002522" style="zoom:50%;"></p>
<h3><span id="4-先验概率和后验概率都是">4、<strong>先验概率和后验概率都是？</strong></span></h3><p><strong>先验概率是指根据以往经验和分析得到的概率</strong>,如全概率公式,它往往作为”由因求果”问题中的”因”出现.<strong>后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</strong></p>
<p><strong>先验概率和后验概率是相对的。</strong>如果以后还有新的信息引入，更新了现在所谓的后验概率，得到了新的概率值，那么这个新的概率值被称为后验概率。</p>
<h3><span id="5-朴素贝叶斯算法的前提假设是什么">5、<strong>朴素贝叶斯算法的前提假设是什么？</strong></span></h3><ol>
<li>特征之间相互独立</li>
<li>每个特征同等重要</li>
</ol>
<h3><span id="6-面试的时候怎么标准回答朴素贝叶斯呢">6、<strong>面试的时候怎么标准回答朴素贝叶斯呢？</strong></span></h3><p>首先朴素贝斯是一个<strong>生成模型（很重要）</strong>，其次它通过学习已知样本，计算出联合概率，再求条件概率。</p>
<h4><span id="生成模式和判别模式的区别常见"><strong>生成模式和判别模式的区别(常见)：</strong></span></h4><p><strong>生成模式</strong>：由数据学得<strong>联合概率分布，求出条件概率分布P(Y|X)的预测模型</strong>；<strong>比较在乎数据是怎么生成的</strong>；常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机。</p>
<p><strong>判别模式</strong>：由数据学得<strong>决策函数或条件概率分布作为预测模型</strong>，<strong>要关注在数据的差异分布上，而不是生成</strong>；常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场。</p>
<h3><span id="7-为什么属性独立性假设在实际情况中很难成立但朴素贝叶斯仍能取得较好的效果排序能力">7、<strong>为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?</strong>【排序能力】</span></h3><p>首先独立性假设在实际中不存在，确实会导致朴素贝叶斯不如一些其他算法，但是就算法本身而言，朴素贝叶斯也会有不错的分类效果，原因是：</p>
<ul>
<li><strong>分类问题看中的是类别的条件概率的排序</strong>，而不是具体的概率值，所以这里面对精准概率值的计算是有一定的容错的。</li>
<li>如果特征属性之间的依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。</li>
</ul>
<h3><span id="8-朴素贝叶斯中概率计算的下溢问题如何解决"><strong><font color="red"> 8、朴素贝叶斯中概率计算的下溢问题如何解决？</font></strong></span></h3><p><strong>在朴素贝叶斯的计算过程中，需要对特定分类中各个特征出现的概率进行连乘</strong>，小数相乘，越乘越小，这样就造成下溢出。在程序中，在相应小数位置进行四舍五入，计算结果可能就变成0了。</p>
<p>为了解决这个问题，<strong>对乘积结果取自然对数</strong>。将小数的乘法操作转化为取对数后的加法操作，规避了变为0的风险同时并不影响分类结果。</p>
<h3><span id="9-朴素贝叶斯分类器对异常值和缺失值敏感吗">9、<strong>朴素贝叶斯分类器对异常值和缺失值敏感吗？</strong></span></h3><p>回想朴素贝叶斯的计算过程，它在推理的时候，输入的某个特征组合，<strong>他们的特征值在训练的时候在贝叶斯公式中都是基于频数进行统计的。</strong>所以一个值的异常（变成了别的数），<strong>只是贝叶斯公式里的计算概率的分子或者分母发生微小的变化，整体结果影响不大</strong>，就算微微影响最终概率值的获得，由于<strong>分类问题只关注概率的排序而不关注概率的值，所以影响不大</strong>，保留异常值还可以提高模型的泛化性能。</p>
<p>缺失值也是一样，如果一个数据实例缺失了一个属性的数值，在建模的时将被忽略，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。</p>
<h3><span id="10-朴素贝叶斯中有没有超参数可以调">10、<strong>朴素贝叶斯中有没有超参数可以调？</strong></span></h3><p><strong>朴素贝叶斯是没有超参数可以调的，所以它不需要调参</strong>，朴素贝叶斯是根据训练集进行分类，分类出来的结果基本上就是确定了的，拉普拉斯估计器不是朴素贝叶斯中的参数，不能通过拉普拉斯估计器来对朴素贝叶斯调参。</p>
<h3><span id="11-朴素贝叶斯有哪三个模型">11、<strong>朴素贝叶斯有哪三个模型？</strong></span></h3><ul>
<li><strong>多项式模型对应于离散变量</strong>，其中离散变量指的是category型变量，也就是类别变量，比如性别；连续变量一般是数字型变量，比如年龄，身高，体重。</li>
<li><strong>高斯模型 对应于连续变量</strong>（每一维服从正态分布）</li>
<li><strong>伯努利模型</strong> <strong>对应于文本分类</strong> （特征只能是0或者1）</li>
</ul>
<h3><span id="12-朴素贝叶斯为什么适合增量计算"><strong><font color="red"> 12、朴素贝叶斯为什么适合增量计算？</font></strong></span></h3><p>朴素贝叶斯在训练过程中实际上需要<strong>计算出各个类别的概率和各个特征的条件概率</strong>，这些概率以频数统计比值（对于多项式模型而言）的形式产生概率值，<strong>可以快速根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算。</strong></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>贝叶斯分类器</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（1）评价指标</title>
    <url>/posts/27302/</url>
    <content><![CDATA[<blockquote>
<p>  一文看懂机器学习指标：准确率、精准率、召回率、F1、ROC曲线、AUC曲线:<a href="https://zhuanlan.zhihu.com/p/93107394">https://zhuanlan.zhihu.com/p/93107394</a></p>
<p>  <strong>机器学习-最全面的评价指标体系: <a href="https://zhuanlan.zhihu.com/p/359997979">https://zhuanlan.zhihu.com/p/359997979</a></strong></p>
<p>  <a href="https://github.com/HaoMood/homepage/blob/master/files/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-03-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.pdf">机器学习工程师面试宝典-03-模型评估</a></p>
<p>  <strong><a href="http://www.china-nb.cn/gongsidongtai/17-85.html">分类模型评估指标——准确率、精准率、召回率、F1、ROC曲线、AUC曲线</a></strong></p>
</blockquote>
<span id="more"></span>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/image-20220421165422230.png" alt="image-20220421165422230" style="zoom:50%;"></p>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/image-20220421165436795.png" alt="image-20220421165436795" style="zoom:50%;"></p>
<h3><span id="一-二分类问题">一、二分类问题</span></h3><blockquote>
<p>  <strong>阈值调节问题？</strong></p>
</blockquote>
<ul>
<li><strong>准确率 (Accuracy)</strong>：<strong>预测正确的概率</strong>  【<strong>(TP+TN)/(TP+TN+FP+FN)</strong>】</li>
<li><strong>精确率（查准率 Precision )：==预测为正的样本==中实际为正的样本的概率</strong> 【<strong>TP/(TP+FP)</strong>】</li>
<li>错误发现率（FDR）= 1 - 精确率 = ==预测为正的样本==中实际为负的样本的概率 【<strong>FP/(TP+FP)</strong>】</li>
<li><strong>召回率（查全率）- Recall</strong>：<strong>==实际为正的样本==中被预测为正样本的概率</strong>【<strong>TP/(TP+FN)</strong>】</li>
<li><strong>真正率（TPR） = 灵敏度（==召回率==） =</strong> <strong>TP/(TP+FN)</strong></li>
<li><strong>假正率（FPR） = 1- 特异度 =</strong> <strong>FP/(FP+TN)</strong></li>
<li><strong>F1=是准确率和召回率的==调和平均值== (2×Precision×Recall)/（Precision+Recall）</strong></li>
<li><strong>G-mean（GM）= 是准确率和召回率的==几何平均值==</strong> <img src="https://image.jiqizhixin.com/uploads/editor/c9841ee6-28df-4eb9-aace-8902a6e525a5/640.svg" alt="img"></li>
</ul>
<h3><span id="11-f12precisionrecall-precisionrecall">1.1 <strong>F1=(2×Precision×Recall) /（Precision+Recall）</strong></span></h3><p>精确率（Precision）和召回率（Recall）之间的关系用图来表达，就是下面的PR曲线。可以发现他们俩的关系是「两难全」的关系。为了综合两者的表现，在两者之间找一个平衡点，就出现了一个 F1分数。</p>
<h4><span id="f12precisionrecall-precisionrecall"><strong>F1=(2×Precision×Recall)  /（Precision+Recall）</strong></span></h4><p>P意义类似于每通过准确预测得到TP个正例需要TP+FP个预测类别为正例的样本。</p>
<p>R意义类似于每通过成功召回得到TP个正例需要TP+FN个真实类别为正例的样本。</p>
<p>F1度量了给定一批样本，对这一批样本进行预测与召回，最终得到的正例的多少。<strong>其中一半的正例是通过预测得到的，一半的正例是通过召回得到的。</strong></p>
<p>有一种把预测所需的预测类别为正例的样本和召回所需的真实类别为正例的样本看作原料，而我们的目标正例样本看作产品的感觉。<strong>所以也能解释为什么P跟R其中一者比较低的时候，F1会偏低。因为跟算术平均数不一样，两者不能互相替代，两部分各负责一半。那么加权调和平均Fbeta也可以很好的理解了。</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BF_%7B%5Cbeta%7D%7D%3D%5Cfrac%7B1%7D%7B1%2B%5Cbeta%5E%7B2%7D%7D%5Ccdot%5Cleft%28+%5Cfrac%7B1%7D%7BP%7D%2B+%5Cfrac%7B%5Cbeta%5E%7B2%7D%7D%7BR%7D%5Cright%29" alt="[公式]"></p>
<p>各自负责的比例不一样了。因此beta越大，Fbeta越着重考虑召回能力。</p>
<h3><span id="12-rocauc的概念">1.2 ROC/AUC的概念</span></h3><p><strong>1. 灵敏度，特异度，真正率，假正率</strong></p>
<p>在正式介绍ROC/AUC之前，我们还要再介绍两个指标，<strong>这两个指标的选择也正是ROC和AUC可以无视样本不平衡的原因。</strong> 这两个指标分别是：<strong>灵敏度和（1-特异度），也叫做真正率（TPR）和假正率（FPR）</strong>。其实我们可以发现<strong>灵敏度和召回率是一模一样的，只是名字换了而已</strong>。由于我们比较关心正样本，所以需要查看有多少负样本被错误地预测为正样本，所以使用（1-特异度），而不是特异度。</p>
<p><strong>真正率（TPR） = 灵敏度（==召回率==） =</strong> <strong>TP/(TP+FN)</strong></p>
<p><strong>假正率（FPR） = 1- 特异度 =</strong> <strong>FP/(FP+TN)</strong></p>
<p>下面是真正率和假正率的示意，我们发现<strong>TPR和FPR分别是基于实际表现1和0出发的，也就是说它们分别在实际的正样本和负样本中来观察相关概率问题。</strong> </p>
<blockquote>
<p>  正因为如此，所以无论样本是否平衡，都不会被影响。还是拿之前的例子，总样本中，90%是正样本，10%是负样本。我们知道用准确率是有水分的，但是用TPR和FPR不一样。这里，TPR只关注90%正样本中有多少是被真正覆盖的，而与那10%毫无关系，同理，FPR只关注10%负样本中有多少是被错误覆盖的，也与那90%毫无关系，</p>
</blockquote>
<p><strong>如果我们从实际表现的各个结果角度出发，就可以避免样本不平衡的问题了，这也是为什么选用TPR和FPR作为ROC/AUC的指标的原因。</strong></p>
<h4><span id="2-roc接受者操作特征曲线"><strong>2. ROC（接受者操作特征曲线）</strong></span></h4><blockquote>
<p>  ROC（Receiver Operating Characteristic）曲线，又称接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力，ROC曲线是基于<strong>混淆矩阵</strong>得出的。</p>
</blockquote>
<p>ROC曲线中的主要两个指标就是<strong>真正率</strong>和<strong>假正率，</strong> 上面也解释了这么选择的好处所在。其中<strong>横坐标为假正率（FPR），纵坐标为真正率（TPR）</strong>，下面就是一个标准的ROC曲线图。</p>
<h4><span id="auc的缺陷">AUC的缺陷？</span></h4><p><strong>优点</strong>：目前普遍认为接收器工作特性曲线（ROC）曲线下的面积—AUC是评估分类模型准确性的标准方法。<strong>它避免了在阈值选择过程中假定的主观性</strong>，当连续的概率得到的分数被转换为二分类标签时，通过总结整体模型表现，其衡量模型区分正负样本的性能优于通过阈值来判断的其他方法（比如准确率、召回率等）。</p>
<ul>
<li><strong>忽略了预测的概率值和模型的拟合优度</strong></li>
<li><strong>AUC反应了太过笼统的信息。无法反应召回率、精确率等在实际业务中经常关心的指标</strong></li>
<li><font color="red"> **对FPR和TPR两种错误的代价同等看待**</font></li>
<li>它没有给出模型误差的空间分布信息</li>
<li>最重要的一点，AUC的misleading的问题</li>
</ul>
<p><strong>==auc仅反应模型的排序能力==，无法反应模型的拟合优度；auc很多时候无法直接反应细粒度的和业务目标更相关的metric信息，例如 top k的准确率，召回率等等（例如同auc的模型在不同的区间的预测能力是存在差别的）；</strong></p>
<h3><span id="13-k-s曲线">1.3、K-S曲线</span></h3><blockquote>
<p>  <strong>K-S曲线</strong>，又称作洛伦兹曲线。实际上，K-S曲线的数据来源以及本质和ROC曲线是一致的，只是ROC曲线是把真正率（ <img src="https://www.zhihu.com/equation?tex=TPR" alt="[公式]"> ）和假正率（ <img src="https://www.zhihu.com/equation?tex=FPR" alt="[公式]"> ）当作横纵轴，<strong>而K-S曲线是把真正率（ <img src="https://www.zhihu.com/equation?tex=TPR" alt="[公式]"> ）和假正率（ <img src="https://www.zhihu.com/equation?tex=FPR" alt="[公式]"> )都当作是纵轴，横轴则由选定的阈值来充当。从 </strong>K-S 曲线<strong>就能衍生出 <img src="https://www.zhihu.com/equation?tex=KS" alt="[公式]"> 值， <img src="https://www.zhihu.com/equation?tex=KS+%3D+max%28TPR+-+FPR%29" alt="[公式]"> ，即是两条曲线之间的最大间隔距离。</strong></p>
</blockquote>
<p><strong>K-S曲线的画法：</strong></p>
<ol>
<li><p><strong>排序：</strong>对于二元分类器来说，模型训练完成之后每个样本都会得到一个类概率值，把样本按这个类概率值从大到小进行排序；</p>
</li>
<li><p><strong>找阈值：</strong>取排序后前 <img src="https://www.zhihu.com/equation?tex=10%5C%25%5Ctimes+k%28k%3D1%2C2%2C3%2C...%2C9%29" alt="[公式]"> 处的值（概率值）作为阈值，分别计算出不同的 <img src="https://www.zhihu.com/equation?tex=TPR" alt="[公式]"> 和<img src="https://www.zhihu.com/equation?tex=FPR" alt="[公式]"> 值，以<img src="https://www.zhihu.com/equation?tex=10%5C%25%5Ctimes+k%28k%3D1%2C2%2C3%2C...%2C9%29" alt="[公式]">为横坐标，分别以<img src="https://www.zhihu.com/equation?tex=TPR" alt="[公式]"> 和<img src="https://www.zhihu.com/equation?tex=FPR" alt="[公式]"> 值为纵坐标，就可以画出两个曲线，这就是K-S曲线，类似于下图。</p>
</li>
<li><p><strong>KS值</strong>：</p>
<p>从 <strong>K-S 曲线</strong>就能衍生出 <img src="https://www.zhihu.com/equation?tex=KS" alt="[公式]"> 值， <img src="https://www.zhihu.com/equation?tex=KS+%3D+max%28TPR+-+FPR%29" alt="[公式]"> ，即是两条曲线之间的最大间隔距离。KS值越大表示模型 的区分能力越强。</p>
</li>
</ol>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/v2-f913b42cefcd32f9fdbfa027de2dfbc8_1440w.jpg" alt="img" style="zoom: 50%;"></p>
<h3><span id="14-lift曲线">1.4 Lift曲线</span></h3><p><strong>Lift曲线它衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。实质上它强调的是投入与产出比</strong>。</p>
<p><strong>tip:</strong>理解<strong>Lift</strong>可以先看一下Quora上的一篇文章：<strong><a href="https://link.zhihu.com/?target=https%3A//www.quora.com/Whats-Lift-curve">What’s Lift curve?</a></strong></p>
<p><strong>Lift计算公式：</strong>先介绍几个相关的指标，以免混淆：</p>
<ul>
<li><strong>准确率（accuracy，ACC）</strong>：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=ACC%3D%5Cfrac%7BTP%2BTN%7D%7BFP%2BFN%2BTP%2BTN%7D%5C%5C" alt="[公式]"></p>
<ul>
<li><strong>正确率(Precision，PRE)，查准率</strong>：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=PRE+%3D+%5Cfrac%7BTP%7D%7BTP%2BFP%7D+%5C%5C" alt="[公式]"></p>
<ul>
<li><strong>真阳性率(True Positive Rate，TPR)，灵敏度(Sensitivity)，召回率(Recall)</strong>：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=TPR%3D%5Cfrac%7BTP%7D%7BTP%2BFN%7D+%5C%5C" alt="[公式]"></p>
<ul>
<li><strong>假阳性率(False Positice Rate，FPR)，误诊率( = 1 - 特异度)</strong>：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=FPR%3D%5Cfrac%7BFP%7D%7BFP%2BTN%7D%5C%5C" alt="[公式]"></p>
<p><strong>Lift计算公式：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=Lift%3D%5Cfrac%7B%5Cfrac%7BTP%7D%7BTP%2BFP%7D%7D%7B%5Cfrac%7BTP%2BFN%7D%7BTP%2BFP%2BTN%2BFN%7D%7D%3D%5Cfrac%7BPRE%7D%7B%E6%AD%A3%E4%BE%8B%E5%8D%A0%E6%AF%94%7D%5C%5C" alt="[公式]"></p>
<p>根据以上公式可知，<strong>Lift指标可以这样理解：</strong>在不使用模型的情况下，我们用先验概率估计正例的比例，即上式子分母部分，以此作为正例的命中率；利用模型后，我们不需要从整个样本中来挑选正例，只需要从我们预测为正例的那个样本的子集 <img src="https://www.zhihu.com/equation?tex=TP%2BFP" alt="[公式]"> 中挑选正例，这时正例的命中率为 <img src="https://www.zhihu.com/equation?tex=PRE" alt="[公式]"> ，后者除以前者即可得提升值<strong>Lift。</strong></p>
<h4><span id="lift曲线"><strong>Lift曲线：</strong></span></h4><p>为了作出<strong>LIft</strong>曲线，首先引入 <img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]"> 的概念：</p>
<p><img src="https://www.zhihu.com/equation?tex=depth%3D%5Cfrac%7BTP%2BFP%7D%7BTP%2BFP%2BTN%2BFN%7D%5C%5C" alt="[公式]"></p>
<p><strong>从公式可以看出</strong>，<img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]">代表的是预测为正例的样本占整个样本的比例。</p>
<p>当阈值为0时，所有的样本都被预测为正例，因此 <img src="https://www.zhihu.com/equation?tex=depth%3D1" alt="[公式]"> ，于是 <img src="https://www.zhihu.com/equation?tex=Lift%3D1" alt="[公式]"> ，模型未起提升作用。随着阈值逐渐增大，被预测为正例的样本数逐渐减少，<img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]">减小，而较少的预测正例样本中的真实正例比例逐渐增大。当阈值增大至1时，没有样本被预测为正例，此时 <img src="https://www.zhihu.com/equation?tex=depth%3D0" alt="[公式]"> ，而 <img src="https://www.zhihu.com/equation?tex=Lift%3D0" alt="[公式]"> 。由此可见， <img src="https://www.zhihu.com/equation?tex=Lift" alt="[公式]"> 与<img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]">存在相反方向变化的关系。在此基础上作出 <img src="https://www.zhihu.com/equation?tex=Lift" alt="[公式]"> 图：</p>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/v2-4cfa1e77335b91d9a47acb7238383c1e_1440w.jpg" alt="img" style="zoom: 50%;"></p>
<p>一般要求，在尽量大的 <img src="https://www.zhihu.com/equation?tex=depth" alt="[公式]"> 下得到尽量大的 <img src="https://www.zhihu.com/equation?tex=Lift" alt="[公式]">，所以 <img src="https://www.zhihu.com/equation?tex=Lift" alt="[公式]"> 曲线的右半部分应该尽量陡峭。</p>
<h3><span id="15-p-r曲线">1.5 <strong>P-R曲线</strong></span></h3><ul>
<li><p><strong>精确率（查准率）- Precision ：==预测为正的样本==中实际为正的样本的概率</strong> 【<strong>TP/(TP+FP)</strong>】</p>
</li>
<li><p><strong>召回率（查全率）- Recall</strong>：<strong>==实际为正的样本==中被预测为正样本的概率</strong>【<strong>TP/(TP+FN)</strong>】</p>
</li>
</ul>
<p>P-R曲线刻画<strong>查准率</strong>和<strong>查全率（召回率）</strong>之间的关系，查准率指的是在所有预测为正例的数据中，真正例所占的比例，查全率是指预测为真正例的数据占所有正例数据的比例。查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，查全率高时，查准率往往偏低。</p>
<p>在很多情况下，我们可以根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在后面的是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可计算当前的查全率和查准率，以查准率为y轴，以查全率为x轴，可以画出下面的P-R曲线。</p>
<p><img src="apple/Documents/Tynote/%E5%B7%A5%E4%BD%9C/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/AI%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0*/pic/v2-dc6abbb24e2dfbfefe4777408d2a8e5c_1440w.jpg" alt="img" style="zoom:67%;"></p>
<p>如果一个学习器的P-R曲线被另一个学习器的P-R曲线完全包住，则可断言后者的性能优于前者，当然我们可以根据曲线下方的面积大小来进行比较，但更常用的是<strong>平衡点</strong>或者是F1值。</p>
<ul>
<li><strong>平衡点（BEP）</strong>是查准率=查全率时的取值，如果这个值较大，则说明学习器的性能较好。F1值越大，我们可以认为该学习器的性能较好。</li>
<li><font color="red"> **F1度量**：**BEP过于简单，这个平衡点是建立在”查准率=查全率“的前提下，无法满足实际不同场景的应用。**</font>

</li>
</ul>
<p>我们先来引入加权调和平均： <img src="https://www.zhihu.com/equation?tex=F_%5Cbeta" alt="[公式]">：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+%5Cfrac+%7B1%7D%7BF_%7B%5Cbeta%7D%7D%3D%5Cfrac+%7B1%7D%7B1%2B%7B%5Cbeta%7D%5E2%7D%28%5Cfrac%7B1%7D%7BP%7D%2B%5Cfrac%7B%5Cbeta%5E2%7D%7BR%7D%29++++%5Cquad+%E5%85%AC%E5%BC%8F%281%29" alt="[公式]"></p>
<p>加权调和平均与<strong>算术平均</strong> <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BP%2BR%7D%7B2%7D" alt="[公式]"> 和<strong>几何平均</strong> <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BP%2BR%7D" alt="[公式]"> 相比，<strong>调和平均更重视较小值（这可以从倒数上看出来）</strong>。当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3D1+" alt="[公式]"> ，即F1是基于查准率和查全率的调和平均定义的，F1的公式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+%5Cfrac+%7B1%7D%7BF_%7B1%7D%7D%3D%5Cfrac+%7B1%7D%7B2%7D%28%5Cfrac%7B1%7D%7BP%7D%2B%5Cfrac%7B1%7D%7BR%7D%29" alt="[公式]"></p>
<p>我们把公式求倒数，即可得：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+F1%3D%5Cfrac%7B2%2AP%2AR%7D%7BP%2BR%7D" alt="[公式]"></p>
<p>在一些应用中，对查准率和查全率的重视程度不同。例如在商品推荐中，为了尽可能少打扰用户，更希望推荐的内容确实是用户感兴趣的，此时查准率更重要；而在罪犯信息检索或者病人检查系统中，更希望尽可能少的漏判，此时查全率更重要。F1度量的一般形式是 <img src="https://www.zhihu.com/equation?tex=F_%7B%5Cbeta%7D" alt="[公式]"> ，能让我们自定义对查准率/查全率的不同偏好：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+F_%7B%5Cbeta%7D%3D%5Cfrac%7B%281%2B%5Cbeta%5E2%29%2AP%2AR%7D%7B%28%5Cbeta%5E2%2AP%29%2BR%7D" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3E0+" alt="[公式]"> 度量了查全率对查准率的相对重要性（不明白的同学可以回看公式1）， <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3D1" alt="[公式]"> 时退化为标准F1，<img src="https://www.zhihu.com/equation?tex=%5Cbeta%3E1+" alt="[公式]">==时查全率有更大影响； <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3C1" alt="[公式]"> 时，查准率有更大影响。==</p>
<h3><span id="16-对数损失log-loss">1.6 <strong>对数损失(Log Loss)</strong></span></h3><p><strong>AUC ROC考虑用于确定模型性能的预测概率</strong>。然而，AUC ROC存在问题，它只考虑概率的顺序，因此<strong>没有考虑模型预测更可能为正样本的更高概率的能力(即考虑了大小，但没有考虑更高精度)</strong>。<strong>在这种情况下，我们可以使用对数损失，即每个实例的正例预测概率的对数的负平均值。</strong></p>
<p>对数损失（Logistic Loss，logloss）是对预测概率的似然估计，其标准形式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+logloss%3DlogP%28Y%7CX%29" alt="[公式]"></p>
<p>对数损失最小化本质是上利用样本中的已知分布，求解拟合这种分布的最佳模型参数，使这种分布出现概率最大。</p>
<p>对数损失对应的二分类的计算公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=logloss%3D-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_ilog%5Chat%7By_i%7D%2B%281-y_i%29log%281-%5Chat%7By_i%7D%29%29+%2C%5Cquad%5Cquad%5Cquad+y%5Cin%5B0%2C1%5D" alt="[公式]"></p>
<p>其中N为样本数， <img src="https://www.zhihu.com/equation?tex=%5Chat+y_i" alt="[公式]"> 为预测为1的概率。对数损失在多分类问题中也可以使用，其计算公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=logloss%3D-%5Cfrac%7B1%7D%7BN%7D%5Cfrac%7B1%7D%7BC%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Csum_%7Bj%3D1%7D%5E%7BC%7D%28y_%7Bij%7Dlog%5Chat%7By_%7Bij%7D%7D%29+%2C%5Cquad%5Cquad%5Cquad+y%5Cin%5B0%2C1%5D" alt="[公式]"></p>
<p>其中，N为样本数，C为类别数，logloss衡量的是预测概率分布和真实概率分布的差异性，取值越小越好。</p>
<h3><span id="17-多分类">1.7 多分类</span></h3><p>很多时候我们有多个<strong>二分类混淆矩阵</strong>，例如进行多次训练/测试，每次得到一个混淆矩阵；或是在多个数据集上进行训练/测试，希望估计算法的全局性能；或者是执行分类任务，每两两类别的组合都对应一个混淆矩阵；总之是在<strong>n个二分类混淆矩阵上综合考察查准率和查全率</strong>。</p>
<ul>
<li><strong>宏观</strong>：在各个混淆军阵上分别计算出查准率和查全率，记为(P1,R1)，(P2,R2),…(Pn,Rn)，在<strong>计算平均值</strong>，这样就得到“宏观查准率”(macro-P)，“宏观查全率”(macro-R)、“宏观F1”(macro-F1)：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+macro-P+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7DP_i" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+macro-R+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7DR_i" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+macro-F1%3D%5Cfrac%7B2%2Amacro-P%2Amacro-R%7D%7Bmacro-P%2Bmacro-R%7D" alt="[公式]"></p>
<ul>
<li><strong>微观</strong>：<strong>将个混淆矩阵对应的元素进行平均，得到TP、FP、TN、FN的平均值</strong>，分别记为 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTP%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BFP%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BFN%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTN%7D" alt="[公式]"> ，再基于这些平均值计算出“微观查准率”(micro-P)，“微观查全率”(micro-R)、“微观F1”(micro-F1)：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+micro-P%3D%5Cfrac%7B%5Coverline%7BTP%7D%7D%7B%5Coverline%7BTP%7D%2B%5Coverline%7BFP%7D%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+micro-R%3D%5Cfrac%7B%5Coverline%7BTP%7D%7D%7B%5Coverline%7BTP%7D%2B%5Coverline%7BFN%7D%7D" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cquad+%5Cquad+micro-F1%3D%5Cfrac%7B2%2Amicro-P%2Amicro-R%7D%7Bmicro-P%2Bmicro-R%7D" alt="[公式]"></p>
<h2><span id="二-回归问题评价指标">二、回归问题评价指标</span></h2><blockquote>
<p>  <strong>均方差损失 Mean Squared Loss、平均绝对误差损失 Mean Absolute Error Loss、Huber Loss、分位数损失 Quantile Loss</strong></p>
</blockquote>
<p>机器学习中的监督学习本质上是给定一系列训练样本 <img src="https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29" alt="[公式]"> ，尝试学习 <img src="https://www.zhihu.com/equation?tex=x%5Crightarrow+y" alt="[公式]"> 的映射关系，使得给定一个 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> ，即便这个 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 不在训练样本中，也能够得到尽量接近真实 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 的输出 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="[公式]"> 。而损失函数（Loss Function）则是这个过程中关键的一个组成部分，用来<strong>衡量模型的输出</strong> <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="[公式]"> <strong>与真实的</strong> <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> <strong>之间的差距</strong>，给模型的优化指明方向。</p>
<h3><span id="21-均方差损失-mse-l2-loss">2.1 均方差损失 MSE、L2 loss</span></h3><h5><span id="基本形式与原理"><strong>基本形式与原理</strong></span></h5><p><strong>均方差Mean Squared Error (MSE)损失是机器学习、深度学习回归任务中最常用的一种损失函数</strong>，也称为 <strong>L2 Loss</strong>。其基本形式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7BMSE%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_i+-+%5Chat%7By_i%7D%29%5E2+%5C%5C" alt="[公式]"></p>
<p>从直觉上理解均方差损失，这个损失函数的最小值为 0（当预测等于真实值时），最大值为无穷大。下图是对于真实值 <img src="https://www.zhihu.com/equation?tex=y%3D0" alt="[公式]"> ，不同的预测值 <img src="https://www.zhihu.com/equation?tex=%5B-1.5%2C+1.5%5D" alt="[公式]"> 的均方差损失的变化图。横轴是不同的预测值，纵轴是均方差损失，可以看到随着预测与真实值绝对误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y-+%5Chat%7By%7D%5Crvert" alt="[公式]"> 的增加，均方差损失呈二次方地增加。</p>
<p><img src="https://pic1.zhimg.com/80/v2-f13a4355c21d16cad8b3f30e8a24b5cc_1440w.jpg" alt="img"></p>
<blockquote>
<h4><span id="背后的假设">背后的假设</span></h4><p>  <strong>【独立同分布-中心极限定理】</strong>：<br>  如果 <img src="https://www.zhihu.com/equation?tex=%5C%7BX_n%5C%7D" alt="[公式]"> 独立同分布，且 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb+EX%3D%5Cmu%2C%5Cquad+%5Cmathbb+D+X%3D%5Csigma%5E2%3E0" alt="[公式]"> ，则n足够大时 <img src="https://www.zhihu.com/equation?tex=%5Coverline+X_n" alt="[公式]"> 近似服从正态分布 <img src="https://www.zhihu.com/equation?tex=N%5Cleft%28%5Cmu%2C%5Cfrac%7B%5Csigma%5E2%7Dn%5Cright%29" alt="[公式]"> ，即</p>
<p>  <img src="https://www.zhihu.com/equation?tex=%5Clim_%7Bn%5Cto%5Cinfty%7DP%5Cleft%28%5Cfrac%7B%5Coverline+X_n-%5Cmu%7D%7B%5Csigma%2F%5Csqrt+n%7D%3Ca%5Cright%29%3D%5CPhi%28a%29%3D%5Cint_%7B-%5Cinfty%7D%5Ea%5Cfrac1%7B%5Csqrt%7B2%5Cpi%7D%7De%5E%7B-t%5E2%2F2%7Ddt%5C%5C" alt="[公式]"></p>
<p>  实际上在一定的假设下，我们可以使用最大化似然得到均方差损失的形式。假设<strong>模型预测与真实值之间的误差服从标准高斯分布</strong>（ <img src="https://www.zhihu.com/equation?tex=%5Cmu%3D0%2C+%5Csigma%3D1" alt="[公式]"> ），则给定一个 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 模型输出真实值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 的概率为</p>
<p>  <img src="https://www.zhihu.com/equation?tex=p%28y_i%7Cx_i%29+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cmathbb%7Bexp%7D%5Cleft+%28-%5Cfrac%7B%28y_i-%5Chat%7By_i%7D%29%5E2%7D%7B2%7D%5Cright+%29+%5C%5C" alt="[公式]"></p>
<p>  <strong>进一步我们假设数据集中 N 个样本点之间相互独立，则给定所有 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 输出所有真实值 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 的概率，即似然 Likelihood</strong>，为所有 <img src="https://www.zhihu.com/equation?tex=p%28y_i+%5Cvert+x_i%29" alt="[公式]"> 的累乘</p>
<p>  <img src="https://www.zhihu.com/equation?tex=L%28x%2C+y%29+%3D+%5Cprod_%7Bi%3D1%7D%5E%7BN%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cmathbb%7Bexp%7D%5Cleft+%28-%5Cfrac%7B%28y_i-%5Chat%7By_i%7D%29%5E2%7D%7B2%7D%5Cright%29+%5C%5C" alt="[公式]"></p>
<p>  通常为了计算方便，我们通常最大化对数似然 Log-Likelihood</p>
<p>  <img src="https://www.zhihu.com/equation?tex=LL%28x%2C+y%29%3D%5Cmathbb%7Blog%7D%28L%28x%2C+y%29%29%3D-%5Cfrac%7BN%7D%7B2%7D%5Cmathbb%7Blog%7D2%5Cpi+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%28y_i-%5Chat%7By_i%7D%29%5E2+%5C%5C" alt="[公式]"></p>
<p>  去掉与 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]"> 无关的第一项，然后转化为最小化负对数似然 Negative Log-Likelihood</p>
<p>  <img src="https://www.zhihu.com/equation?tex=NLL%28x%2C+y%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_i+-+%5Chat%7By_i%7D%29%5E2+%5C%5C" alt="[公式]"></p>
<p>  可以看到这个实际上就是均方差损失的形式。也就是说<strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的</strong>，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。</p>
</blockquote>
<h3><span id="hulu-百面机器学习-平方根误差的意外"><strong><font color="red"> hulu 百面机器学习 —— 平方根误差的”意外“</font></strong></span></h3><h4><span id="95的时间区间效果很好rmse指标居高不下的原因">95%的时间区间效果很好，RMSE指标居高不下的原因？</span></h4><p><img src="https://www.zhihu.com/equation?tex=J_%7BMSE%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_i+-+%5Chat%7By_i%7D%29%5E2+%5C%5C" alt="[公式]"></p>
<p>一般情况下RSME能反应预测值与真实值的偏离程度，但是<strong>易受离群点</strong>的影响；</p>
<p><strong>解决方案</strong>：</p>
<ul>
<li>数据预处理将噪音去掉</li>
<li>将离群点的产生机制建模进去</li>
<li>更鲁棒的模型评估指标：<strong>平均绝对百分比误差</strong>（MAPE），<strong>分位数损失</strong></li>
</ul>
<h4><span id="22-平均绝对误差-mae">2.2 <strong>平均绝对误差 MAE</strong></span></h4><p><strong>平均绝对误差 Mean Absolute Error (MAE）</strong> 是另一类常用的损失函数，也称为 <strong>L1 Loss</strong>。其基本形式如下</p>
<p><img src="https://www.zhihu.com/equation?tex=+J_%7BMAE%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft+%7C+y_i+-+%5Chat%7By_i%7D+%5Cright+%7C+%5C%5C" alt="[公式]"></p>
<p>同样的我们可以对这个损失函数进行可视化如下图，MAE 损失的最小值为 0（当预测等于真实值时），最大值为无穷大。可以看到随着预测与真实值绝对误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y-+%5Chat%7By%7D%5Crvert" alt="[公式]"> 的增加，MAE 损失呈线性增长。</p>
<p><img src="https://pic3.zhimg.com/80/v2-fd248542b6b5aa9fadcab44340045dee_1440w.jpg" alt="img"></p>
<blockquote>
<h4><span id="背后的假设">背后的假设</span></h4><p>  同样的我们可以在一定的假设下通过最大化似然得到 MAE 损失的形式，假设<strong>模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution</strong>（ <img src="https://www.zhihu.com/equation?tex=%5Cmu%3D0%2C+b%3D1" alt="[公式]"> ），则给定一个 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 模型输出真实值 <img src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]"> 的概率为</p>
<p>  <img src="https://www.zhihu.com/equation?tex=p%28y_i%7Cx_i%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cmathbb%7Bexp%7D%28-%5Cleft+%7Cy_i-%5Chat%7By_i%7D%5Cright%7C%29+%5C%5C" alt="[公式]"></p>
<p>  与上面推导 MSE 时类似，我们可以得到的负对数似然实际上就是 MAE 损失的形式</p>
<p>  <img src="https://www.zhihu.com/equation?tex=L%28x%2C+y%29+%3D+%5Cprod_%7Bi%3D1%7D%5E%7BN%7D%5Cfrac%7B1%7D%7B2%7D%5Cmathbb%7Bexp%7D%28-%7Cy_i-%5Chat%7By_i%7D%7C%29%5C%5C+++LL%28x%2C+y%29+%3D+N%5Cln%7B%5Cfrac%7B1%7D%7B2%7D%7D+-+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%7Cy_i-%5Chat%7By_i%7D%7C+%5C%5C+++NLL%28x%2C+y%29+%3D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%7Cy_i-%5Chat%7By_i%7D%7C++%5C%5C" alt="[公式]"></p>
</blockquote>
<h3><span id="23-mae-与-mse-区别">2.3 MAE 与 MSE 区别</span></h3><p>MAE 和 MSE 作为损失函数的主要区别是：<strong>MSE 损失相比 MAE 通常可以更快地收敛，但 MAE 损失对于 outlier 更加健壮</strong>，即更加不易受到 outlier 影响。</p>
<ul>
<li><p><strong>MSE 通常比 MAE 可以更快地收敛</strong>。当使用梯度下降算法时，MSE 损失的梯度为 <img src="https://www.zhihu.com/equation?tex=-%5Chat%7By_i%7D" alt="[公式]"> ，而 MAE 损失的梯度为 <img src="https://www.zhihu.com/equation?tex=%5Cpm1" alt="[公式]"> ，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y_i-%5Chat%7By_i%7D+%5Crvert" alt="[公式]"> 很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的。当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了 MSE 在大部分时候比 MAE 收敛地更快。这个也是 MSE 更为流行的原因。</p>
</li>
<li><p><strong>MAE 对于异常值（outlier） 更加 robust</strong>。我们可以从两个角度来理解这一点：</p>
<ul>
<li>第一个角度是直观地理解，下图是 MAE 和 MSE 损失画到同一张图里面，由于MAE 损失与绝对误差之间是线性关系，MSE 损失与误差是平方关系，当误差非常大的时候，MSE 损失会远远大于 MAE 损失。<strong>因此当数据中出现一个误差非常大的 outlier 时，MSE 会产生一个非常大的损失，对模型的训练会产生较大的影响</strong>。<img src="https://pic2.zhimg.com/80/v2-c8edffe0406dafae41a042e412cd3251_1440w.jpg" alt="img"></li>
<li>第二个角度是从两个损失函数的假设出发，MSE 假设了误差服从高斯分布，MAE 假设了误差服从拉普拉斯分布。拉普拉斯分布本身对于 outlier 更加 robust。参考下图（来源：<a href="https://link.zhihu.com/?target=https%3A//www.cs.ubc.ca/~murphyk/MLbook/">Machine Learning: A Probabilistic Perspective</a> 2.4.3 The Laplace distribution Figure 2.8），当右图右侧出现了 outliers 时，拉普拉斯分布相比高斯分布受到的影响要小很多。因此以拉普拉斯分布为假设的 MAE 对 outlier 比高斯分布为假设的 MSE 更加 robust。<img src="https://pic1.zhimg.com/80/v2-93ad65845f5b0dc0327fde4ded661804_1440w.jpg" alt="img" style="zoom: 67%;"></li>
</ul>
</li>
</ul>
<h3><span id="24-huber-loss">2.4 Huber Loss</span></h3><blockquote>
<ul>
<li>在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定</li>
<li>在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。</li>
</ul>
</blockquote>
<p>上文我们分别介绍了 MSE 和 MAE 损失以及各自的优缺点，MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Huber_loss">Huber Loss</a> 则是一种将 MSE 与 MAE 结合起来，取两者优点的损失函数，也被称作 Smooth Mean Absolute Error Loss 。其原理很简单，就是在误差接近 0 时使用 MSE，误差较大时使用 MAE，公式为</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7Bhuber%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%5Cmathbb%7BI%7D_%7B%7C+y_i+-+%5Chat%7By_i%7D%7C+%5Cleq+%5Cdelta%7D+%5Cfrac%7B%28y_i+-+%5Chat%7By_i%7D%29%5E2%7D%7B2%7D%2B+%5Cmathbb%7BI%7D_%7B%7C+y_i+-+%5Chat%7By_i%7D%7C+%3E+%5Cdelta%7D+%28%5Cdelta+%7Cy_i+-+%5Chat%7By_i%7D%7C+-+%5Cfrac%7B1%7D%7B2%7D%5Cdelta%5E2%29+%5C%5C" alt="[公式]"></p>
<p>上式中 <img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="[公式]"> 是 Huber Loss 的一个超参数，<img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="[公式]"> 的值是 MSE 和 MAE 两个损失连接的位置。上式等号右边第一项是 MSE 的部分，第二项是 MAE 部分，在 MAE 的部分公式为 <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%5Clvert+y_i+-+%5Chat%7By_i%7D%5Crvert+-+%5Cfrac%7B1%7D%7B2%7D%5Cdelta%5E2" alt="[公式]"> 是为了保证误差 <img src="https://www.zhihu.com/equation?tex=%5Clvert+y+-+%5Chat%7By%7D%5Crvert%3D%5Cpm+%5Cdelta" alt="[公式]"> 时 MAE 和 MSE 的取值一致，进而保证 Huber Loss 损失连续可导。</p>
<p>下图是 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D1.0" alt="[公式]"> 时的 Huber Loss，可以看到在 <img src="https://www.zhihu.com/equation?tex=%5B-%5Cdelta%2C+%5Cdelta%5D" alt="[公式]"> 的区间内实际上就是 MSE 损失，在 <img src="https://www.zhihu.com/equation?tex=%28-%5Cinfty%2C+%5Cdelta%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%28%5Cdelta%2C+%5Cinfty%29" alt="[公式]"> 区间内为 MAE损失。</p>
<p><img src="https://pic4.zhimg.com/80/v2-b4260d38f70dd920fa46b8717596bda7_1440w.jpg" alt="img"></p>
<h3><span id="25-分位数损失-quantile-loss">2.5 分位数损失 Quantile Loss</span></h3><blockquote>
<p>  <strong>MAE 中分别用不同的系数控制高估和低估的损失，进而实现分位数回归</strong></p>
</blockquote>
<p><strong>分位数回归 Quantile Regression 是一类在实际应用中非常有用的回归算法</strong>，通常的回归算法是拟合目标值的期望或者中位数，而分位数回归可以通过给定不同的分位点，<strong>拟合目标值的不同分位数</strong>。</p>
<p><img src="https://pic1.zhimg.com/80/v2-8eb8ecfcdd8031a16a471905217934a0_1440w.jpg" alt="img"></p>
<p>分位数回归是通过使用分位数损失 Quantile Loss 来实现这一点的，分位数损失形式如下，式中的 r 分位数系数。</p>
<p><img src="https://www.zhihu.com/equation?tex=J_%7Bquant%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D+%5Cmathbb%7BI%7D_%7B%5Chat%7By_i%7D%5Cgeq+y_i%7D%281-r%29%7Cy_i+-+%5Chat%7By_i%7D%7C+%2B+%5Cmathbb%7BI%7D_%7B%5Chat%7By_i%7D%3C+y_i%7Dr%7Cy_i-%5Chat%7By_i%7D%7C+%5C%5C" alt="[公式]"></p>
<p>我们如何理解这个损失函数呢？这个损失函数是一个分段的函数 ，将 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%5Cgeq+y_i" alt="[公式]"> （高估） 和 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%3C+y_i" alt="[公式]"> （低估） 两种情况分开来，并分别给予不同的系数。当 <img src="https://www.zhihu.com/equation?tex=r%3E0.5" alt="[公式]"> 时，低估的损失要比高估的损失更大，反过来当 <img src="https://www.zhihu.com/equation?tex=r+%3C+0.5" alt="[公式]"> 时，高估的损失比低估的损失大；分位数损失实现了<strong>分别用不同的系数控制高估和低估的损失，进而实现分位数回归</strong>。特别地，当 <img src="https://www.zhihu.com/equation?tex=r%3D0.5" alt="[公式]"> 时，分位数损失退化为 MAE 损失，从这里可以看出 MAE 损失实际上是分位数损失的一个特例 — 中位数回归。</p>
<p>下图是取不同的分位点 0.2、0.5、0.6 得到的三个不同的分位损失函数的可视化，可以看到 0.2 和 0.6 在高估和低估两种情况下损失是不同的，而 0.5 实际上就是 MAE。</p>
<p><img src="https://pic4.zhimg.com/80/v2-f8ed385f32a517c784bce841e6da1daf_1440w.jpg" alt="img"></p>
<h3><span id="26-平均绝对百分误差-mape">2.6  平均绝对百分误差 MAPE</span></h3><p>虽然平均绝对误差能够获得一个评价值，但是你并不知道这个值代表模型拟合是优还是劣，只有通过对比才能达到效果。当需要以相对的观点来衡量误差时，则使用MAPE。</p>
<p><strong>平均绝对百分误差</strong>（<strong>Mean Absolute Percentage Error，MAPE</strong>）是对 MAE 的一种改进，考虑了绝对误差相对真实值的比例。</p>
<ul>
<li><strong>优点</strong>：考虑了预测值与真实值的误差。考虑了误差与真实值之间的比例。</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=MAPE%3D%5Cfrac%7B100%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%5Cleft%20%7C%20%20%5Cfrac%7By_%7Bi%7D-f%5Cleft%28x_%7Bi%7D%5Cright%29%7D%7By_%7Bi%7D%7D%20%5Cright%20%7C" alt="公式"></p>
<blockquote>
<p>  在某些场景下，如房价从 <img src="https://www.zhihu.com/equation?tex=5K" alt="公式"> 到 <img src="https://www.zhihu.com/equation?tex=50K" alt="公式"> 之间，<img src="https://www.zhihu.com/equation?tex=5K" alt="公式"> 预测成 <img src="https://www.zhihu.com/equation?tex=10K" alt="公式"> 与 <img src="https://www.zhihu.com/equation?tex=50K" alt="公式"> 预测成 <img src="https://www.zhihu.com/equation?tex=45K" alt="公式"> 的差别是非常大的，而平均绝对百分误差考虑到了这点。</p>
</blockquote>
<h2><span id="三-相似性度量指标">三、相似性度量指标</span></h2><blockquote>
<p>  机器学习中的相似性度量方法 - 天下客的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/411876558">https://zhuanlan.zhihu.com/p/411876558</a></p>
</blockquote>
<p>描述样本之间相似度的方法有很多种，一般来说常用的有相关系数和欧式距离。本文对机器学习中常用的相似性度量方法进行了总结。<strong>在做分类时，常常需要估算不同样本之间的相似性度量（Similarity Measurement），</strong>这时通常采用的方法就是计算样本间的“距离”（distance）。采用什么样的方法计算距离是很讲究的，甚至关系到分类的正确与否。</p>
<ul>
<li><strong>欧式距离</strong>：k-means</li>
<li><strong>曼哈顿距离</strong>：</li>
<li><strong>切比雪夫距离</strong>：</li>
<li>闵可夫斯基距离</li>
<li>标准化欧氏距离</li>
<li>马氏距离</li>
<li><a href="https://www.zhihu.com/search?q=夹角余弦&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;55493039&quot;}">夹角余弦</a></li>
<li><strong>汉明距离</strong>：simhash</li>
<li><strong>杰卡德距离&amp;杰卡德相似系数</strong>: <strong>杰卡德相似系数是衡量两个集合的相似度一种指标。</strong></li>
<li>相关系数&amp;相关距离</li>
<li>信息熵</li>
</ul>
<h2><span id="四-推荐算法评价指标">四、推荐算法评价指标</span></h2><ul>
<li>推荐算法评价指标 - 一干正事就犯困的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/359528909">https://zhuanlan.zhihu.com/p/359528909</a></li>
</ul>
<h4><span id="41-ap">4.1 AP</span></h4><p><code>AP</code> 衡量的是训练好的模型在每个类别上的好坏；</p>
<p><img src="https://pic2.zhimg.com/80/v2-e8656365e7eee25065d6bdfec33368e5_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p><strong>AP总结了一个精确召回曲线，作为在每个阈值处获得的精度的加权平均值，并且与以前的阈值相比，召回率的增加用作权重</strong>：</p>
<p><img src="/Users/apple/Library/Application Support/typora-user-images/image-20220711160205051.png" alt="image-20220711160205051" style="zoom:50%;"></p>
<p>其中和分别是第n个阈值[1]时的精度和召回率。此实现未进行插值，并且与使用梯形规则计算精确调用曲线下的面积有所不同，后者使用线性插值并且可能过于乐观。</p>
<h4><span id="42-map">4.2 MAP</span></h4><p><strong>MAP（Mean Average Precision）常用于排序任务，MAP的计算涉及另外两个指标：Precision和Recall</strong></p>
<ul>
<li><strong>Precision和Precision@k</strong></li>
</ul>
<p>推荐算法中的精度precision计算如下： </p>
<p><img src="https://www.zhihu.com/equation?tex=precision%3D%5Cfrac%7B%E7%AE%97%E6%B3%95%E7%BB%93%E6%9E%9C%E4%B8%AD%E7%9B%B8%E5%85%B3%E7%9A%84item%E6%95%B0%E9%87%8F%7D%7B%E6%8E%A8%E8%8D%90%E7%9A%84item%E6%80%BB%E6%95%B0%E9%87%8F%7D+%5C%5C" alt="[公式]"></p>
<p>可以看出Precision的计算没有考虑结果列表中item的顺序，Precision@k则通过切片的方式将顺序隐含在结果中。Precision@k表示列表前k项的Precision，随着k的变化，可以得到一系列precision值，用 <img src="https://www.zhihu.com/equation?tex=P%28k%29" alt="[公式]"> 表示。</p>
<ul>
<li><strong>Recall和Recall@k</strong></li>
</ul>
<p>推荐算法中的召回率recall计算如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=+recall%3D%5Cfrac%7B%E7%AE%97%E6%B3%95%E7%BB%93%E6%9E%9C%E4%B8%AD%E7%9B%B8%E5%85%B3%E7%9A%84item%E6%95%B0%E9%87%8F%7D%7B%E6%89%80%E6%9C%89%E7%9B%B8%E5%85%B3%E7%9A%84item%E6%95%B0%E9%87%8F%7D%5C%5C" alt="[公式]"></p>
<p>与Precision@k相似，recall@k表示结果列表前k项的recall，随着k的变化，可以得到一系列的recall值，用 <img src="https://www.zhihu.com/equation?tex=r%28k%29" alt="[公式]"> 表示。</p>
<ul>
<li><h5><span id="apn">AP@N</span></h5></li>
</ul>
<p>AP（Average Precision）平均精度的计算以Precision@k为基础，可以体现出结果列表中item顺序的重要性，其计算过程如下： </p>
<p><img src="https://www.zhihu.com/equation?tex=AP%40N%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum%5EN_%7Bk%3D1%7D%28P%28k%29%5Cquad+if%5C%2C+kth%5C%2C+item%5C%2C+is%5C%2C+relevant%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum%5EN_%7Bk%3D1%7DP%28k%29%5Ccdot+rel%28k%29+%5C%5C" alt="[公式]"></p>
<p>其中，N表示要求推荐的N个item，m表示所有相关的item总数， <img src="https://www.zhihu.com/equation?tex=rel%28k%29" alt="[公式]"> 表示第k个item是否相关，相关为1，反之为0</p>
<p><strong>AP@N的值越大，表示推荐列表中相关的item数量越多以及相关item的排名越靠前</strong></p>
<ul>
<li><h5><span id="mapn">MAP@N</span></h5></li>
</ul>
<p><strong>AP@N评价了算法对单个用户的性能，MAP@N则是算法对多个用户的平均值，是平均数的平均，其计算过程如下</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=MAP%40N%3D%5Cfrac%7B1%7D%7B%7CU%7C%7D%5Csum_%7Bu%3D1%7D%5E%7B%7CU%7C%7D%28AP%40N%29u%3D%5Cfrac%7B1%7D%7B%7CU%7C%7D%5Csum%7Bu%3D1%7D%5E%7B%7CU%7C%7D%28%5Cfrac%7B1%7D%7Bm%7D%5Csum%5EN_%7Bk%3D1%7DP_u%28k%29%5Ccdot+rel_u%28k%29%29+%5C%5C" alt="[公式]"></p>
<h2><span id="五-聚类算法评价指标">五、聚类算法评价指标</span></h2><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/343667804">https://zhuanlan.zhihu.com/p/343667804</a></p>
<p>  十分钟掌握聚类算法的评估指标：<a href="https://juejin.cn/post/6997913127572471821">https://juejin.cn/post/6997913127572471821</a></p>
</blockquote>
<h4><span id="前言">前言</span></h4><p>如同之前介绍的其它算法模型一样，对于聚类来讲我们同样会通过一些评价指标来衡量聚类算法的优与劣。在聚类任务中，常见的评价指标有：<strong>纯度（Purity）</strong>、<strong>兰德系数（Rand Index, RI）</strong>、<strong>F值（F-score）</strong>和<strong>调整兰德系数（Adjusted Rand Index,ARI）</strong>。同时，这四种评价指标也是聚类相关论文中出现得最多的评价方法。下面，我们就来对这些算法一一进行介绍。</p>
<p><img src="https://pic3.zhimg.com/80/v2-e62c8b4b793c89b1cd70f2aaebf690c6_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>好的聚类算法，一般要求类簇具有：</p>
<ul>
<li><strong>簇内 (intra-cluster) 相似度高</strong></li>
<li><strong>簇间 (inter-cluster) 相似度底</strong></li>
</ul>
<p>一般来说，评估聚类质量有两个标准，内部评估评价指标和外部评估指标。</p>
<h3><span id="外部评估">【外部评估】</span></h3><h3><span id="51聚类纯度-聚类的准确率"><strong>5.1聚类纯度</strong> - 聚类的准确率</span></h3><p>在聚类结果的评估标准中，一种最简单最直观的方法就是计算它的<strong>聚类纯度</strong>（purity），别看纯度听起来很陌生，但实际上和<strong>分类问题中的准确率有着异曲同工之妙</strong>。因为聚类纯度的总体思想也<strong>用聚类正确的样本数除以总的样本数，因此它也经常被称为聚类的准确率</strong>。只是对于聚类后的结果我们并不知道每个簇所对应的真实类别，因此需要取每种情况下的最大值。具体的，纯度的计算公式定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P%3D%28%5COmega%2C%5Cmathbb%7BC%7D%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bk%7D%5Cmax_%7Bj%7D%7C%5Comega_k%5Ccap+c_j%7C+%5Cend%7Baligned%7D%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%281%29+%5C%5C" alt="[公式]"></p>
<p>其中<img src="https://www.zhihu.com/equation?tex=N" alt="[公式]">表示总的样本数；<img src="https://www.zhihu.com/equation?tex=%5COmega%3D%5C%7B%5Comega_1%2C%5Comega_2%2C...%2C%5Comega_K%5C%7D" alt="[公式]">表示一个个聚类后的簇，而<img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BC%7D%3D%5C%7Bc_1%2C_2%2C...c_J%5C%7D" alt="[公式]">表示正确的类别；<img src="https://www.zhihu.com/equation?tex=%5Comega_k" alt="[公式]">表示聚类后第<img src="https://www.zhihu.com/equation?tex=k" alt="[公式]">个簇中的所有样本，<img src="https://www.zhihu.com/equation?tex=c_j" alt="[公式]">表示第<img src="https://www.zhihu.com/equation?tex=j" alt="[公式]">个类别中真实的样本。在这里<img src="https://www.zhihu.com/equation?tex=P" alt="[公式]">的取值范围为<img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[公式]">，越大表示聚类效果越好。</p>
<h3><span id="52-兰德系数与f值-同簇混淆矩阵"><strong>5.2 兰德系数与F值</strong>  [同簇混淆矩阵]</span></h3><p>在介绍完了纯度这一评价指标后，我们再来看看兰德系数（Rand Index）和F值。虽然兰德系数听起来是一个陌生的名词，但它的计算过程却也与准确率的计算过程类似。同时，虽然这里也有一个叫做F值的指标，并且它的计算过程也和分类指标中的F值类似，但是两者却有着本质的差别。说了这么多，那这两个指标到底该怎么算呢？同分类问题中的混淆矩阵类似，这里我们也要先定义四种情况进行计数，然后再进行指标的计算。</p>
<p><strong>为了说明兰德系数背后的思想，我们还是以图1中的聚类结果为例进行说明（为了方便观察，我们再放一张图在这里）:</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-e62c8b4b793c89b1cd70f2aaebf690c6_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=TP" alt="[公式]">：表示两个<strong>同类样本点</strong>在<strong>同一个簇</strong>（布袋）中的情况数量；</li>
<li><img src="https://www.zhihu.com/equation?tex=FP" alt="[公式]">：表示两个<strong>非同类样本点</strong>在<strong>同一个簇</strong>中的情况数量；</li>
<li><img src="https://www.zhihu.com/equation?tex=TN" alt="[公式]">：表示两个<strong>非同类样本点</strong>分别在<strong>两个簇</strong>中的情况数量；</li>
<li><img src="https://www.zhihu.com/equation?tex=FN" alt="[公式]">：表示两个同类样本点分别在<strong>两个簇</strong>中的情况数量；</li>
</ul>
<p>由此，我们便能得到如下所示的对<strong>混淆矩阵（Pair Confusion Matrix）</strong>：</p>
<p><img src="https://pic3.zhimg.com/80/v2-a9e709a995b006be04d026aebc721c4e_1440w.png" alt="img" style="zoom:75%;"></p>
<p>有了上面各种情况的统计值，我们就可以定义出兰德系数和F值的计算公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=RI%3D%5Cfrac%7BTP%2BTN%7D%7BTP%2BFP%2BFN%2BTN%7D%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%283%29+%5C%5C" alt="[公式]"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+Precision%26%3D%5Cfrac%7BTP%7D%7BTP%2BFP%7D%5C%5C%5B2ex%5D+Recall%26%3D%5Cfrac%7BTP%7D%7BTP%2BFN%7D%5C%5C%5B2ex%5D+F_%7B%5Cbeta%7D%26%3D%281%2B%5Cbeta%5E2%29%5Cfrac%7BPrecision%5Ccdot+Recall%7D%7B%5Cbeta%5E2%5Ccdot+Precision%2BRecall%7D+%5Cend%7Baligned%7D%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%284%29+%5C%5C" alt="[公式]"></p>
<p>从上面的计算公式来看，<img src="https://www.zhihu.com/equation?tex=%283%29%284%29" alt="[公式]">从形式上看都非常像分类问题中的准确率与F值，但是有着本质的却别。同时，在这里<img src="https://www.zhihu.com/equation?tex=RI" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=F_%7B%5Cbeta%7D" alt="[公式]">的取值范围均为<img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[公式]">，越大表示聚类效果越好。</p>
<h4><span id="53-调整兰德系数adjusted-rand-index归一化">5.3 调整兰德系数（Adjusted Rand index）【归一化】</span></h4><p>对于随机结果，RI并不能保证分数接近零。<strong>为了实现“在聚类结果随机产生的情况下，指标应该接近零”</strong>，调整兰德系数（Adjusted rand index）被提出，它具有更高的区分度。</p>
<p>其公式为：</p>
<script type="math/tex; mode=display">
\mathrm{ARI}=\frac{\mathrm{RI}-E[\mathrm{RI}]}{\max (\mathrm{RI})-E[\mathrm{RI}]}</script><p>$A R$ 取值范围为 $[-1,1]$, 值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲, ARI衡量的是两个数据分布的吻合程度。</p>
<p>优点:</p>
<ul>
<li>对任意数量的聚类中心和样本数, 随机聚类的ARI都非常接近于 0 。</li>
<li>取值在 $[-1,1]$ 之间, 负数代表结果不好, 越接近于1越好。</li>
<li>对簇的结构不需作出任何假设：可以用于比较聚类算法。</li>
</ul>
<p>缺点:</p>
<ul>
<li>ARI 需要 ground truth classes 的相关知识, ARI需要真实标签, 而在实践中几乎不可用, 或者需要人工 标注者手动分配（如在监督学习环境中）。</li>
</ul>
<h3><span id="54-标准化互信息nmi-normalized-mutual-information">5.4 <strong><font color="red"> 标准化互信息（NMI, Normalized Mutual Information）</font></strong></span></h3><p>互信息是用来衡量两个数据分布的吻合程度。它也是一有用的信息度量，它是指两个事件集合之间的相关性。互信息越大，词条和类别的相关程度也越大。</p>
<h3><span id="内部指标">【内部指标】</span></h3><p>内部评估指标主要基于数据集的集合结构信息从紧致性、分离性、连通性和重叠度等方面对聚类划分进行评价。即基于数据聚类自身进行评估的。</p>
<h3><span id="55-轮廓系数silhouette-coefficient">5.5 <strong><font color="red"> 轮廓系数（Silhouette Coefficient）</font></strong></span></h3><p>轮廓系数适用于实际类别信息未知的情况。</p>
<p>对于单个样本，设<strong>a是与它同类别中其他样本的平均距离</strong>，<strong>b是与它距离最近不同类别中样本的平均距离</strong>，其轮廓系数为：</p>
<p>$s = \frac {b-a} {max(a, b)}$</p>
<p>对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。轮廓系数的取值范围是[-1,1]，同类别样本距离越相近，不同类别样本距离越远，值越大。当值为负数时，说明聚类效果很差。</p>
<h3><span id="56-calinski-harabaz指数calinski-harabaz-index">5.6 Calinski-Harabaz指数（Calinski-Harabaz Index）</span></h3><p>在真实的分群label不知道的情况下，Calinski-Harabasz可以作为评估模型的一个指标。</p>
<p>Calinski-Harabasz指数通过<strong>计算类中各点与类中心的距离平方和来度量类内的紧密度</strong>，通过<strong>==计算各类中心点与数据集中心点距离平方和来度量数据集的分离度==</strong>，CH指标<strong>由分离度与紧密度的比值得到</strong>。从而，CH越大代表着类自身越紧密，类与类之间越分散，即更优的聚类结果。</p>
<p><strong>优点</strong></p>
<ul>
<li>当簇的密集且分离较好时，分数更高。</li>
<li>得分计算很快，与轮廓系数的对比，最大的优势：快！相差几百倍！毫秒级。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>凸的簇的CH指数通常高于其他类型的簇。例如，通过 DBSCAN 获得基于密度的簇；所以，不适合基于密度的聚类算法（DBSCAN）。</li>
</ul>
<h3><span id="57-戴维森堡丁指数dbi-davies-bouldin-index">5.7 戴维森堡丁指数（DBI, Davies-Bouldin Index）</span></h3><p><strong>DB指数是计算任意两类别的类内距离平均距离之和除以两聚类中心距离求最大值</strong>。DB越小，意味着类内距 离越小同时类间距离越大。<strong>零是可能的最低值, 接近零的值表示更好的分区</strong>。</p>
<script type="math/tex; mode=display">
\begin{gathered}
R_{i j}=\frac{s_{i}+s_{j}}{d_{i j}} \\
D B=\frac{1}{k} \sum_{i=1}^{k} \max _{i \neq j} R_{i j}
\end{gathered}</script><p>其中, $s<em>{i}$ 表示簇的每个点与该簇的质心之间的平均距离, 也称为簇直径。 $d</em>{i j}$ 表示聚类和的质心之间的距 离。<br>算法生成的聚类结果越是朝着簇内距离最小（类内相似性最大）和笶间距离最大（类间相似性最小）变化， 那么Davies-Bouldin指数就会越小。<br><strong>缺点</strong>:</p>
<ul>
<li>因使用欧式距离, 所以对于环状分布聚类评测很差。</li>
</ul>
<h2><span id="六-评分总结sklearn">六、评分总结（sklearn）</span></h2><blockquote>
<p>  sklearn.metrics - 回归/分类模型的评估方法:<a href="https://zhuanlan.zhihu.com/p/408078074">https://zhuanlan.zhihu.com/p/408078074</a></p>
</blockquote>
<h3><span id="61-分类模型">6.1 分类模型</span></h3><h4><span id="accuracy_score"><strong>accuracy_score</strong></span></h4><p><strong>分类准确率分数是指所有分类正确的百分比</strong>。分类准确率这一衡量分类器的标准比较容易理解，但是它不能告诉你响应值的潜在分布，并且它也不能告诉你分类器犯错的类型。所以在使用的时候，一般需要搭配matplotlib等数据可视化工具来观察预测的分类情况，与实际的结果做更加直观的比较。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score  </span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]  </span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]  </span><br><span class="line">accuracy_score(y_true, y_pred)  <span class="comment"># 默认normalization = True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.5</span></span><br><span class="line">accuracy_score(y_true, y_pred, normalize=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span></span><br></pre></td></tr></table></figure>
<h4><span id="recall_score"><strong>recall_score</strong></span></h4><p>召回率 =<strong>提取出的正确信息条数 /样本中的信息条数</strong>。通俗地说，就是所有准确的条目有多少被检索出来了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">recall_score(y_true, y_pred, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>,average=<span class="string">&#x27;binary&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line">参数average : string, [<span class="literal">None</span>, ‘micro’, ‘macro’(default), ‘samples’, ‘weighted’]</span><br></pre></td></tr></table></figure>
<p>将一个二分类matrics拓展到多分类或多标签问题时，我们可以将数据看成多个二分类问题的集合，每个类都是一个二分类。接着，我们可以通过跨多个分类计算每个二分类metrics得分的均值，这在一些情况下很有用。你可以使用<strong>average参数</strong>来指定。 </p>
<ul>
<li>macro：计算二分类metrics的均值，为每个类给出相同权重的分值。</li>
<li>weighted:对于不均衡数量的类来说，计算二分类metrics的平均，通过在每个类的score上进行加权实现。 </li>
<li>micro：给出了每个样本类以及它对整个metrics的贡献的pair（sample-weight），而非对整个类的metrics求和，它会每个类的metrics上的权重及因子进行求和，来计算整个份额。</li>
<li>samples：应用在multilabel问题上。它不会计算每个类，相反，它会在评估数据中，通过计算真实类和预测类的差异的metrics，来求平均（sample_weight-weighted） </li>
<li>average：average=None将返回一个数组，它包含了每个类的得分.</li>
</ul>
<h4><span id="roc_curve"><strong>roc_curve</strong></span></h4><p>ROC曲线指受试者工作特征曲线/接收器操作特性(receiver operating characteristic，ROC)曲线,是<strong>反映灵敏性和特效性连续变量的综合指标</strong>,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界值，从而计算出一系列敏感性和特异性。ROC曲线是根据一系列不同的二分类方式（分界值或决定阈），<strong>以真正例率（也就是灵敏度）（True Positive Rate,TPR）为纵坐标，假正例率（1-特效性）（False Positive Rate,FPR）为横坐标</strong>绘制的曲线。</p>
<p>通过ROC我们可以观察到模型正确识别的正例的比例与模型错误地把负例数据识别成正例的比例之间的权衡。TPR的增加以FPR的增加为代价。ROC曲线下的面积是模型准确率的度量，<strong>AUC</strong>（Area under roc curve）。</p>
<p><strong>TPR</strong> = TP /（TP + FN） （正样本<strong>预测数</strong> / 正样本<strong>实际数</strong>）</p>
<p><strong>FPR</strong> = FP /（FP + TN） （负样本<strong>预测数</strong> /负样本<strong>实际数</strong>）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics  </span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])  </span><br><span class="line">scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])  </span><br><span class="line">fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=<span class="number">2</span>)  </span><br><span class="line">fpr  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.</span> ,  <span class="number">0.5</span>,  <span class="number">0.5</span>, <span class="number">1.</span> ])  </span><br><span class="line">tpr  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.5</span>,  <span class="number">0.5</span>,  <span class="number">1.</span> , <span class="number">1.</span> ])  </span><br><span class="line">thresholds  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">0.8</span> ,  <span class="number">0.4</span> ,  <span class="number">0.35</span>, <span class="number">0.1</span> ])  </span><br><span class="line"></span><br><span class="line"><span class="comment"># check auc score</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc   </span><br><span class="line">metrics.auc(fpr, tpr)   </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.75</span>   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以直接根据预测值+真实值来计算出auc值，略过roc的计算过程</span></span><br><span class="line">‘’‘</span><br><span class="line">sklearn.metrics.roc_auc_score(y_true, y_score, average=<span class="string">&#x27;macro&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br><span class="line">average : string, [<span class="literal">None</span>, ‘micro’, ‘macro’(default), ‘samples’, ‘weighted’]</span><br><span class="line">’‘’</span><br><span class="line"><span class="comment"># 真实值（必须是二值）、预测值（可以是0/1,也可以是proba值）</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score  </span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])  </span><br><span class="line">y_scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])  </span><br><span class="line">roc_auc_score(y_true, y_scores)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.75</span>  </span><br></pre></td></tr></table></figure>
<h4><span id="confusion-metric"><strong>confusion metric</strong></span></h4><p>混淆矩阵（confusion matrix），又称为可能性表格或是错误矩阵。它是一种特定的矩阵用来呈现算法性能的可视化效果。其每一列代表预测值，每一行代表的是实际的类别。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">confusion_matric(y_true, y_pred, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">&#x27;binary&#x27;</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h4><span id="precision_score"><strong>precision_score</strong></span></h4><p>计算精确度——precision <img src="https://www.zhihu.com/equation?tex=%3DTP%2F%28TP%2FFP%29" alt="[公式]"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">precision_score(y_true, y_pred, labels=None, pos_label=1, average=&#x27;binary&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic2.zhimg.com/v2-a3b6092e30d2eab7d2372007aec15105_r.jpg" alt="preview"></p>
<h2><span id="评价指标qampa">评价指标Q&amp;A</span></h2><h4><span id="精度指标存在的问题"><strong>精度指标存在的问题</strong>？</span></h4><ul>
<li>有倾向性的问题。比如，判断空中的飞行物是导弹还是其他飞行物，很显然为了减少损失，我们更倾向于相信是导弹而采用相应的防护措施。此时判断为导弹实际上是其他飞行物与判断为其他飞行物实际上是导弹这两种情况的重要性是不一样的；</li>
<li>样本类别数量严重不均衡的情况。比如银行客户样本中好客户990个，坏客户10个。如果一个模型直接把所有客户都判断为好客户，得到精度为99%，但这显然是没有意义的。</li>
</ul>
<h4><span id="为什么-roc-和-auc-都能应用于非均衡的分类问题"><strong>为什么 ROC 和 AUC 都能应用于非均衡的分类问题？</strong></span></h4><p><strong>ROC曲线只与横坐标 (FPR) 和 纵坐标 (TPR) 有关系</strong> 。我们可以发现TPR只是正样本中预测正确的概率，而FPR只是负样本中预测错误的概率，和正负样本的比例没有关系。因此 ROC 的值与实际的正负样本比例无关，因此既可以用于均衡问题，也可以用于非均衡问题。而 AUC 的几何意义为ROC曲线下的面积，因此也和实际的正负样本比例无关。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>理论基础</category>
      </categories>
      <tags>
        <tag>评价指标</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（14）聚类*-Kmeans</title>
    <url>/posts/26FYF9Q/</url>
    <content><![CDATA[<h2><span id="聚类算法-无监督">聚类算法 【无监督】</span></h2><blockquote>
<p>  常用聚类算法 - 小胡子的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/104355127">https://zhuanlan.zhihu.com/p/104355127</a></p>
<p>  <strong>K-means, K-medians, K-mediods and K-centers</strong> - 仲基的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/398600714">https://zhuanlan.zhihu.com/p/398600714</a></p>
</blockquote>
<p>什么是聚类算法？聚类是一种机器学习技术，它涉及到数据点的分组。给定一组数据点，我们可以使用聚类算法将每个数据点划分为一个特定的组。理论上，同一组中的数据点应该具有相似的属性和/或特征，而不同组中的数据点应该具有高度不同的属性和/或特征。<strong>聚类是一种无监督学习的方法</strong>，是许多领域中常用的统计数据分析技术。</p>
<p><strong>聚类算法主要包括以下五类：</strong></p>
<ul>
<li><strong>基于分层的聚类（hierarchical methods）</strong></li>
</ul>
<p>这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。<strong>代表算法有：<em>BIRCH算法</em>（1996）、<em>CURE算法</em>、CHAMELEON算法等。</strong></p>
<blockquote>
<p>  层次聚类通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层，树的顶层是一个聚类的根节点。</p>
<p>  <strong>最小距离的层次聚类算法</strong>通过自下而上合并创建聚类树，合并算法通过计算两类数据点间的欧式距离来计算不同类别数据点间的相似度，对所有数据点中最为相似的两个数据点进行组合，组合后，最小距离（Single Linkage）的计算方法是将两个组合数据点中距离最近的两个数据点间的距离作为这两个组合数据点的距离。并反复迭代这一过程。</p>
</blockquote>
<ul>
<li><strong>基于划分的聚类（partitioning methods）</strong></li>
</ul>
<p>给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：<strong><em>==K-means算法==</em>、<em>K-medoids算法</em>、<em>CLARANS算法</em></strong></p>
<ul>
<li><strong>基于密度的聚类（density-based methods）</strong></li>
</ul>
<p>基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有<strong>：<em>==DBSCAN（Density-Based Spatial Clustering of Applic with Noise）==算法（1996）</em>、<em>OPTICS（Ordering Points to Identify Clustering Structure）算法（1999）</em>、<em>DENCLUE算法（1998）</em>、<em>WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据）</em></strong></p>
<ul>
<li><strong>基于网格的聚类（grid-based methods）</strong></li>
</ul>
<p>这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：<strong><em>STING（Statistical Information Grid）</em>、<em>CLIQUE（Clustering In Quest）算法（1998）</em>、<em>WaveCluster算法</em>。</strong>其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。</p>
<ul>
<li><strong>基于模型的聚类（model-based methods）</strong></li>
</ul>
<p>基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。</p>
<h2><span id="一-k-means-基于划分k值的选择">一、K-means 【基于划分】[==K值的选择？==]</span></h2><p><img src="https://pic1.zhimg.com/v2-e7195b6620e2e6ec743fb77702b1d3ff_1440w.jpg?source=172ae18b" alt="【机器学习】K-means（非常详细）" style="zoom:51%;"></p>
<blockquote>
<p>  K-means 聚类的迭代算法实际上是 EM 算法。EM 算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。在 K-means 中的隐变量是每个类别所属类别。</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/20463356">K-means 笔记（三）数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//sofasofa.io/forum_main_post.php%3Fpostid%3D1000282">K-means 怎么选 K?</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/161733843">K-Means：隐变量、聚类、EM</a></li>
</ol>
</blockquote>
<p>k近邻法中，当<strong>训练集</strong>、<strong>距离度量</strong>、<strong>K值</strong>以及<strong>分类决策规则</strong>确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。</p>
<p><strong>K-均值是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为:</strong></p>
<ul>
<li>首先选择𝐾个<strong>随机</strong>的点，称为<strong>聚类中心</strong>（cluster centroids）；</li>
<li>对于数据集中的每一个数据，按照<strong>距离𝐾个中心点的距离</strong>，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</li>
<li>计算每一个组的平均值，将该组所<strong>关联的中心点移动到平均值</strong>的位置。</li>
<li>重复步骤，直至中心点不再变化。</li>
</ul>
<p>K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用 K-均值算法将数据分为三类，用于帮助确定将要生产的 T-恤衫的三种尺寸。</p>
<p><img src="https://camo.githubusercontent.com/86b1cfa2d801f27862bcc8cab59f04401e01defd5248311eb77ec75a02286c5f/687474703a2f2f7778332e73696e61696d672e636e2f6d773639302f30303633304465666c79316735623734367a776a676a33306668306337676e6a2e6a7067" alt="img" style="zoom: 50%;"></p>
<h3><span id="11-损失函数">1.1 损失函数</span></h3><p><strong>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和</strong>，因此 <strong>K-均值的代价函数（又称==畸变函数 Distortion function）==为</strong>：</p>
<p><img src="/Users/apple/Library/Application Support/typora-user-images/image-20220707191400653.png" alt="image-20220707191400653" style="zoom:50%;"></p>
<p>其中 <a href="https://camo.githubusercontent.com/c0d78bacde143a412a432d0f2c8d1a746fc34802fec6dfd079a07708e30a98f4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f25374263253545253742286929253744253744"><img src="https://camo.githubusercontent.com/c0d78bacde143a412a432d0f2c8d1a746fc34802fec6dfd079a07708e30a98f4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f25374263253545253742286929253744253744" alt="img"></a>代表与 <a href="https://camo.githubusercontent.com/b14a6424262c05d4fb45a600d8fb5b4881cee4801e8699dcc0c58ef503164f94/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253545253742286929253744"><img src="https://camo.githubusercontent.com/b14a6424262c05d4fb45a600d8fb5b4881cee4801e8699dcc0c58ef503164f94/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253545253742286929253744" alt="img"></a>最近的聚类中心点。 我们的的优化目标便是找出使得代价函数最小的 <a href="https://camo.githubusercontent.com/c48ec5ec6d35ce0993ee193bc86c0d4eddb3dbac31e30fa1f38fbf56b1401083/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f632535452537422831292537442c632535452537422832292537442c2e2e2e2c63253545253742286d29253744"><img src="https://camo.githubusercontent.com/c48ec5ec6d35ce0993ee193bc86c0d4eddb3dbac31e30fa1f38fbf56b1401083/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f632535452537422831292537442c632535452537422832292537442c2e2e2e2c63253545253742286d29253744" alt="img"></a>和 <a href="https://camo.githubusercontent.com/d287d301a22d8bb905fe7e82874a282adff45c972a158b7a2cb91f0354a4ae84/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f312c755f322c2e2e2e2c755f6b"><img src="https://camo.githubusercontent.com/d287d301a22d8bb905fe7e82874a282adff45c972a158b7a2cb91f0354a4ae84/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f755f312c755f322c2e2e2e2c755f6b" alt="img"></a>。</p>
<h3><span id="12-k值的选择-肘部法则">1.2 k值的选择 【肘部法则】</span></h3><p>在运行 K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：</p>
<ol>
<li>我们应该选择𝐾 &lt; 𝑚，即聚类中心点的个数要小于所有训练集实例的数量。</li>
<li>随机选择𝐾个训练实例，然后令𝐾个聚类中心分别与这𝐾个训练实例相等K-均值的一个问题在于，它有可能会<strong>停留在一个局部最小值</strong>处，而这取决于初始化的情况。</li>
</ol>
<p>为了解决这个问题，我们通常需要多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行 K-均值的结果，选择代价函数最小的结果。这种方法在𝐾较小的时候（2—10）还是可行的，<strong>但是如果𝐾较大，这么做也可能不会有明显地改善。</strong></p>
<p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么。有一个可能会谈及的方法叫作<strong>“肘部法则”</strong>。关 于“肘部法则”，我们所需要做的是改变𝐾值，也就是聚类类别数目的总数。我们用一个聚类来运行 K 均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后<strong>计算成本函数或者计算畸变函数</strong>𝐽。𝐾代表聚类数字。</p>
<p><a href="https://camo.githubusercontent.com/8888198abee1b3069d27a6bff19f1830a94ac4141ebe7dd300f8ee4000262c6d/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562377062616561766a3330716f3063777463392e6a7067"><img src="https://camo.githubusercontent.com/8888198abee1b3069d27a6bff19f1830a94ac4141ebe7dd300f8ee4000262c6d/687474703a2f2f7778322e73696e61696d672e636e2f6d773639302f30303633304465666c7931673562377062616561766a3330716f3063777463392e6a7067" alt="img"></a></p>
<p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。你会发现这种模式，它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用 3 个聚类来进行聚类是正确的，<strong>这是因为那个点是曲线的肘点，畸变值下降得很快，𝐾 = 3之后就下降得很慢，那么我们就选𝐾 = 3。</strong>当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p>
<h3><span id="13-knn与k-means区别">1.3 KNN与K-means区别？</span></h3><p>K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。</p>
<h4><span id="区别">区别：</span></h4><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>KNN</th>
<th>K-Means</th>
</tr>
</thead>
<tbody>
<tr>
<td>类别</td>
<td>1.KNN是<strong>分类</strong>算法 2.属于<strong>监督学习</strong> 3.训练数据集是带label的数据</td>
<td>1.K-Means是<strong>聚类</strong>算法 2.属于<strong>非监督学习</strong> 3.训练数据集是无label的数据，是杂乱无章的，经过聚类后变得有序，先无序，后有序。</td>
</tr>
<tr>
<td></td>
<td>没有明显的前期训练过程，属于memory based learning</td>
<td>有明显的前期训练过程</td>
</tr>
<tr>
<td>k值的含义</td>
<td>K的含义：一个样本x，对它进行分类，就从训练数据集中，<strong>在x附近找离它最近的K个数据点</strong>，这K个数据点，类别c占的个数最多，就把x的label设为c。</td>
<td>K的含义：<strong>K是人工固定好的数字，假设数据集合可以分为K个蔟</strong>，那么就利用训练数据来训练出这K个分类。</td>
</tr>
</tbody>
</table>
</div>
<h4><span id="相似点"><strong>相似点</strong>：</span></h4><p>都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法思想。</p>
<h3><span id="14-k-means优缺点及改进">1.4 K-Means优缺点及改进</span></h3><p>k-means：在大数据的条件下，<strong>会耗费大量的时间和内存</strong>。 优化k-means的建议：</p>
<ol>
<li><p>减少聚类的数目K。因为，每个样本都要跟类中心计算距离。</p>
</li>
<li><p>减少样本的特征维度。比如说，<strong>通过PCA等进行降维</strong>。</p>
</li>
<li><p>考察其他的聚类算法，通过选取toy数据，去测试不同聚类算法的性能。</p>
</li>
<li><p><strong>hadoop集群</strong>，K-means算法是很容易进行并行计算的。</p>
</li>
<li><p>算法可能找到局部最优的聚类，而不是全局最优的聚类。使用改进的二分k-means算法。</p>
<p>二分k-means算法：首先将整个数据集看成一个簇，然后进行一次k-means（k=2）算法将该簇一分为二，并计算每个簇的误差平方和，选择平方和最大的簇迭代上述过程再次一分为二，直至簇数达到用户指定的k为止，此时可以达到的全局最优。</p>
</li>
</ol>
<h3><span id="二-k-means的调优与改进">二、K - means的调优与改进</span></h3><p>针对 K-means 算法的缺点，我们可以有很多种调优方式：如<strong>数据预处理</strong>（去除异常点），<strong>合理选择 K 值</strong>，<strong>高维映射</strong>等。以下将简单介绍：</p>
<h3><span id="21-数据预处理">2.1 数据预处理</span></h3><p>K-means 的本质是基于欧式距离的数据划分算法，均值和方差大的维度将对数据的聚类产生决定性影响。所以<strong>未做归一化处理和统一单位的数据是无法直接参与运算和比较</strong>的。常见的数据预处理方式有：<strong>数据归一化，数据标准化</strong>。</p>
<p>此外，离群点或者噪声数据会对均值产生较大的影响，导致中心偏移，因此我们还需要对数据进行异常点检测。</p>
<h3><span id="22-合理选择-k-值">2.2 合理选择 K 值</span></h3><p>K 值的选取对 K-means 影响很大，这也是 K-means 最大的缺点，常见的选取 K 值的方法有：<strong>手肘法、Gap statistic 方法</strong>。</p>
<p><strong>【手肘法】</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-5ca4a5fe0b06b25a2b97262abb401a16_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>当 K &lt; 3 时，曲线急速下降；当 K &gt; 3 时，曲线趋于平稳，通过手肘法我们认为拐点 3 为 K 的最佳值。</p>
<p>==【<strong>Gap statistic</strong>】==</p>
<p><img src="https://www.zhihu.com/equation?tex=Gap%28K%29%3D%5Ctext%7BE%7D%28%5Clog+D_k%29-%5Clog+D_k+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=D_k" alt="[公式]"> 为损失函数，这里 <img src="https://www.zhihu.com/equation?tex=E%28logD_k%29" alt="[公式]"> 指的是 <img src="https://www.zhihu.com/equation?tex=logD_k" alt="[公式]"> 的期望。这个数值通常通过<strong>蒙特卡洛模拟</strong>产生，我们在样本里所在的区域中按照<strong>均匀分布随机产生和原始样本数一样多的随机样本</strong>，并对这个<strong>随机样本做 K-Means</strong>，从而得到一个 <img src="https://www.zhihu.com/equation?tex=D_k+" alt="[公式]"> 。如此往复多次，通常 20 次，我们可以得到 20 个 <img src="https://www.zhihu.com/equation?tex=logD_k" alt="[公式]"> 。对这 20 个数值求平均值，就得到了 <img src="https://www.zhihu.com/equation?tex=E%28logD_k%29" alt="[公式]">  的近似值。最终可以计算 Gap Statisitc。而 Gap statistic 取得最大值所对应的 K 就是最佳的 K。</p>
<p><img src="https://pic3.zhimg.com/80/v2-9a39a8dad143e5dd52a506d83c2cbb36_1440w.jpg" alt="img"></p>
<p>由图可见，当 K=3 时，Gap(K) 取值最大，所以最佳的簇数是 K=3。</p>
<p>Github 上一个项目叫 <a href="https://link.zhihu.com/?target=https%3A//github.com/milesgranger/gap_statistic">gap_statistic</a> ，可以更方便的获取建议的类簇个数。</p>
<h3><span id="23-采用核函数">2.3 采用核函数</span></h3><p><strong>基于欧式距离的 K-means 假设了了各个数据簇的数据具有一样的的先验概率并呈现球形分布</strong>，但这种分布在实际生活中并不常见。面对非凸的数据分布形状时我们可以引入核函数来优化，这时算法又称为核 K-means 算法，是核聚类方法的一种。<strong>核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。</strong>非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。</p>
<h3><span id="24-k-means">==2.4 K-means++==</span></h3><blockquote>
<p>  <strong>K-means++ 就是选择离已选中心点最远的点</strong>。这也比较符合常理，聚类中心当然是互相离得越远越好。</p>
</blockquote>
<p>我们知道初始值的选取对结果的影响很大，对初始值选择的改进是很重要的一部分。在所有的改进算法中，K-means++ 最有名。</p>
<p>K-means++ 算法步骤如下所示：</p>
<ol>
<li>随机选取一个中心点 <img src="https://www.zhihu.com/equation?tex=a_1" alt="[公式]"> ；</li>
<li>计算数据到之前 n 个聚类中心最远的距离 <img src="https://www.zhihu.com/equation?tex=D%28x%29" alt="[公式]"> ，并以一定概率 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BD%28x%29%5E2%7D%7B%5Csum%7BD%28x%29%5E2%7D%7D" alt="[公式]"> 选择新中心点 <img src="https://www.zhihu.com/equation?tex=a_i" alt="[公式]"> ；</li>
<li>重复第二步。</li>
</ol>
<p>简单的来说，就是 <strong>K-means++ 就是选择离已选中心点最远的点</strong>。这也比较符合常理，聚类中心当然是互相离得越远越好。</p>
<p>但是这个算法的缺点在于，难以并行化。所以 k-means II 改变取样策略，并非按照 k-means++ 那样每次遍历只取样一个样本，而是每次遍历取样 k 个，重复该取样过程 <img src="https://www.zhihu.com/equation?tex=log%28n+%29" alt="[公式]"> 次，则得到 <img src="https://www.zhihu.com/equation?tex=klog%28n%29" alt="[公式]"> 个样本点组成的集合，然后从这些点中选取 k 个。当然一般也不需要 <img src="https://www.zhihu.com/equation?tex=log%28n%29" alt="[公式]"> 次取样，5 次即可。</p>
<h3><span id="25-isodata">2.5 ISODATA</span></h3><p>ISODATA 的全称是<strong>迭代自组织数据分析法</strong>。它解决了 K 的值需要预先人为的确定这一缺点。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出 K 的大小。ISODATA 就是针对这个问题进行了改进，它的思想也很直观：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别。</p>
<h3><span id="三-收敛证明em算法">==三、 收敛证明【EM算法】==</span></h3><p>我们先来看一下 K-means 算法的步骤：先随机选择初始节点，然后计算每个样本所属类别，然后通过类别再跟新初始化节点。这个过程有没有想到之前介绍的 <a href="https://zhuanlan.zhihu.com/p/78311644">EM 算法</a> 。</p>
<p>我们需要知道的是 K-means 聚类的迭代算法实际上是 EM 算法。EM 算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。在 <strong>==K-means 中的隐变量是每个样本所属类别==</strong>。</p>
<p>K-means 算法迭代步骤中的每次确认中心点以后重新进行标记对应 EM 算法中的 <strong>E 步</strong>：<strong>求当前参数条件下的 Expectation</strong>。而根据标记重新求中心点 对应 EM 算法中的 <strong>M 步</strong>：<strong>求似然函数最大化时（损失函数最小时）对应的参数 。</strong></p>
<p>首先我们看一下损失函数的形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=J%3D%5Csum_%7Bi%3D1%7D%5E%7BC%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BN%7D%7Br_%7Bij%7D%5Ccdot+%7B%5Cnu%28x_j%2C%7B%5Cmu%7D_i%29%7D%7D%7D+%5C%5C" alt="[公式]"></p>
<p>其中：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cnu%28%7Bx_j%7D%2C%5Cmu_i%29%3D%7B%7C%7Cx_j-%7B%5Cmu%7D_i%7C%7C%7D%5E%7B2%7D+%2C%5Cquad++%7Br%7D_%7Bnk%7D%3D%5Cleft+%5C%7B+%5Cbegin%7Baligned%7D+%261+%5Cquad+if+%5C%3B+x_n+%5Cin+k+%5C%5C+%260+%5Cquad+else+%5Cend%7Baligned%7D+%5Cright.+%5C%5C" alt="[公式]"></p>
<p>为了求极值，我们令损失函数求偏导数且等于 0：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%7BJ%7D%7D%7B%5Cpartial+%5Cmu_k%7D%3D2+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7D%28x_i-%7B%5Cmu%7D_%7Bk%7D%29%7D%3D0+%5C%5C" alt="[公式]"></p>
<p>k 是指第 k 个中心点，于是我们有：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu_k%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7Dx_i%7D%7D%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Br_%7Bik%7D%7D%7D+%5C%5C" alt="[公式]"></p>
<p>可以看出，新的中心点就是所有该类的<strong>质心</strong>。</p>
<p><strong>EM 算法的缺点就是，容易陷入局部极小值，这也是 K-means 有时会得到局部最优解的原因。</strong></p>
<h3><span id="四-高斯混合模型gmm">四、高斯混合模型(GMM)</span></h3><h3><span id="41-gmm的思想">4.1 GMM的思想</span></h3><p>高斯混合模型（Gaussian Mixed Model，GMM）也是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。<strong>高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的</strong>，当前<strong>数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。</strong></p>
<p>第一张图是一个数据分布的样例，如果只用一个高斯分布来拟合图中的数据，图 中所示的椭圆即为高斯分布的二倍标准差所对应的椭圆。直观来说，图中的数据 明显分为两簇，因此只用一个高斯分布来拟和是不太合理的，需要推广到用多个 高斯分布的叠加来对数据进行拟合。第二张图是用两个高斯分布的叠加来拟合得到的结果。<strong>这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。</strong>理论上，高斯混合模型可以拟合出任意类型的分布。</p>
<p>高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来 的。在该假设下，每个单独的分模型都是标准高斯模型，其均值 $u_i$ 和方差 $\sum_i$ 是待估计的参数。此外，每个分模型都还有一个参数 $\pi_i$，可以理解为权重或生成数据的概 率。高斯混合模型的公式为：</p>
<p><a href="https://camo.githubusercontent.com/4055f35cd39fe2e095b0fc70f8cccb1e3cab924a2ebd6bc56ca7bdcf026c79ec/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f702878293d25354373756d5f253742693d312537442535452537426b25374425354370695f694e2878253743755f692c25354373756d5f6929"><img src="https://camo.githubusercontent.com/4055f35cd39fe2e095b0fc70f8cccb1e3cab924a2ebd6bc56ca7bdcf026c79ec/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f702878293d25354373756d5f253742693d312537442535452537426b25374425354370695f694e2878253743755f692c25354373756d5f6929" alt="img"></a></p>
<p>通常我们并不能直接得到高斯混合模型的参数，而是观察到了一系列 数据点，给出一个类别的数量K后，希望求得最佳的K个高斯分模型。因此，<strong>高斯混合模型的计算，便成了最佳的均值μ，方差Σ、权重π的寻找</strong>，这类问题通常通过最大似然估计来求解。遗憾的是，此问题中直接使用最大似然估计，得到的是一个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。</p>
<p><strong>在这种情况下，可以用EM算法。 </strong>EM算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。具体到高 斯混合模型的求解，EM算法的迭代过程如下。</p>
<p>首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。</p>
<ul>
<li>E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。</li>
<li>M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。</li>
</ul>
<blockquote>
<p>  高斯混合模型是一个生成式模型。可以这样理解数据的生成过程，假设一个最简单的情况，即只有两个一维标准高斯分布的分模型<em>N</em>(0,1)和<em>N</em>(5,1)，其权重分别为0.7和0.3。那么，在生成第一个数据点时，先按照权重的比例，随机选择一个分布，比如选择第一个高斯分布，接着从<em>N</em>(0,1)中生成一个点，如−0.5，便是第一个数据点。在生成第二个数据点时，随机选择到第二个高斯分布<em>N</em>(5,1)，生成了第二个点4.7。如此循环执行，便生成出了所有的数据点。</p>
</blockquote>
<p>也就是说，我们并不知道最佳的K个高斯分布的各自3个参数，也不知道每个 数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不 变，获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。</p>
<h3><span id="42-gmm与k-means相比">4.2 GMM与K-Means相比</span></h3><p>高斯混合模型与K均值算法的相同点是：</p>
<ul>
<li><strong>都需要指定K值</strong>；</li>
<li><strong>都是使用EM算法来求解</strong>；</li>
<li>都往往只能收敛于局部最优。</li>
</ul>
<p>而它相比于K 均值算法的优点是，可以给出一个样本属于某类的<strong>概率</strong>是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；<strong>并且可以用于生成新的样本点</strong>。</p>
<h3><span id="五-聚类算法如何评估">五、聚类算法如何评估</span></h3><p>由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。例 如，K均值聚类可以用<strong>误差平方</strong>和来评估，但是基于密度的数据簇可能不是球形， 误差平方和则会失效。在许多情况下，判断聚类算法结果的好坏强烈依赖于主观解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。</p>
<p>聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结 果的质量。这一过程又分为三个子任务。</p>
<ol>
<li><p><strong>估计聚类趋势。</strong></p>
<p>这一步骤是检测数据分布中是否存在非随机的簇结构。如果数据是基本随机 的，那么聚类的结果也是毫无意义的。我们可以观察聚类误差是否随聚类类别数 量的增加而单调变化，如果数据是基本随机的，即不存在非随机簇结构，那么聚 类误差随聚类类别数量增加而变化的幅度应该较不显著，并且也找不到一个合适 的K对应数据的真实簇数。</p>
</li>
<li><p><strong>判定数据簇数。</strong></p>
<p>确定聚类趋势之后，我们需要找到与真实数据分布最为吻合的簇数，据此判定聚类结果的质量。数据簇数的判定方法有很多，例如<strong>手肘法</strong>和<strong>Gap Statistic</strong>方 法。需要说明的是，用于评估的最佳数据簇数可能与程序输出的簇数是不同的。 例如，有些聚类算法可以自动地确定数据的簇数，但可能与我们通过其他方法确定的最优数据簇数有所差别。</p>
</li>
<li><p><strong>测定聚类质量。</strong></p>
<p>在无监督的情况下，我们可以通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。定义评估指标可以展现面试者实际解决和分析问题的能力。事实上测量指标可以有很多种，以下列出了几种常用的度量指标，更多的指标可以阅读相关文献。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>聚类</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（15）聚类*-DBSCAN</title>
    <url>/posts/1TSFG2S/</url>
    <content><![CDATA[<h2><span id="二-dbscan算法基于密度">二、DBSCAN算法【基于密度】</span></h2><blockquote>
<p>  （3）聚类算法之DBSCAN算法 - GISer.Wang的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/77043965">https://zhuanlan.zhihu.com/p/77043965</a></p>
</blockquote>
<p>密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。其代表算法为<strong>DBSCAN算法</strong>和<strong>密度最大值</strong>算法。</p>
<h3><span id="21-dbscan算法原理">2.1 DBSCAN算法原理</span></h3><p><strong><font color="red"> DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。</font></strong>与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并<strong>可在有“噪声”的数据中发现任意形状的聚类</strong>。</p>
<h3><span id="22-若干概念">2.2 若干概念</span></h3><p><strong>DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数 <img src="https://www.zhihu.com/equation?tex=%28%CF%B5%2C+MinPts%29" alt="[公式]"> 用来描述邻域的样本分布紧密程度</strong>。其中， <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 描述了某一数据点的<strong>邻域距离阈值（半径）</strong>， <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 描述了数据点<strong>半径为</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> <strong>的邻域</strong>中数据点个数的最小个数。下面是与密度聚类相关的定义（假设我的样本集是 <img src="https://www.zhihu.com/equation?tex=D%3D%5C%7Bx_1%2Cx_2%2C...%2Cx_m%5C%7D" alt="[公式]"> )：</p>
<ul>
<li><p><strong>对象的ε领域</strong>：给定对象在半径<strong>ε</strong>内的区域；对于 <img src="https://www.zhihu.com/equation?tex=x_j%E2%88%88D" alt="[公式]"> ，其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域包含样本集 <img src="https://www.zhihu.com/equation?tex=D" alt="[公式]"> 中与 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 的距离不大于 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 的子样本集。即 <img src="https://www.zhihu.com/equation?tex=N_%CF%B5%28x_j%29%3D%5C%7Bx_i%E2%88%88D%7Cdistance%28x_i%2Cx_j%29%E2%89%A4%CF%B5%5C%7D" alt="[公式]"> , 这个子样本集的个数记为 <img src="https://www.zhihu.com/equation?tex=%7CN_%CF%B5%28x_j%29%7C" alt="[公式]"> 。 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域是一个集合</p>
</li>
<li><p><strong>核心对象</strong>：对于任一样本 <img src="https://www.zhihu.com/equation?tex=x_j%E2%88%88D" alt="[公式]"> ，如果其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域对应的 <img src="https://www.zhihu.com/equation?tex=N_%CF%B5%28x_j%29" alt="[公式]"> 至少包含 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 个样本，即如果 <img src="https://www.zhihu.com/equation?tex=+%7CN_%CF%B5%28x_j%29%7C%E2%89%A5MinPts" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 是核心对象。</p>
</li>
<li><p><strong>直接密度可达</strong>：如果 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 位于 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域中，且 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 是核心对象，则称 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 密度直达。反之不一定成立，即此时不能说 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 密度直达, 除非 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 也是核心对象，<strong>即密度直达不满足对称性</strong>。如图ε=1,m=5，q是一个核心对象，从对象q出发到对象p是<strong>直接密度可达</strong>的。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afb2323b988730.jpg" alt="2019-05-18-061126.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>密度可达</strong>：对于 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> ,如果存在样本样本序列 <img src="https://www.zhihu.com/equation?tex=p_1%2Cp_2%2C...%2Cp_T" alt="[公式]"> ,满足 <img src="https://www.zhihu.com/equation?tex=p1%3Dx_i%2Cp_T%3Dx_j" alt="[公式]"> , 且 <img src="https://www.zhihu.com/equation?tex=p_%7Bt%2B1%7D" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=p_t" alt="[公式]"> 密度直达，则称 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 由 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本 <img src="https://www.zhihu.com/equation?tex=p_1%2Cp_2%2C...%2Cp_%7BT%E2%88%921%7D" alt="[公式]"><strong>均为核心对象</strong>，因为只有核心对象才能使其他样本密度直达。<strong>密度可达也不满足对称性</strong>，这个可以由密度直达的不对称性得出。</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afbfa514735525.jpg" alt="2019-05-18-061154.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>密度相连</strong>：对于 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> ,如果存在核心对象样本 <img src="https://www.zhihu.com/equation?tex=x_k" alt="[公式]"> ，使<strong><img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 均由 <img src="https://www.zhihu.com/equation?tex=x_k" alt="[公式]"> 密度可达</strong>，则称 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=x_j" alt="[公式]"> 密度相连。<strong>密度相连关系满足对称性</strong>。</li>
</ul>
<p><img src="https://i.loli.net/2019/08/01/5d42afd32fc7340149.jpg" alt="2019-05-18-061202.jpg" style="zoom:50%;"></p>
<ul>
<li><p><strong>==簇：一个基于密度的簇是最大的密度相连对象的集合。==</strong></p>
</li>
<li><p><strong>噪声</strong>：不包含在任何簇中的对象称为噪声。</p>
</li>
</ul>
<p>从下图可以很容易看出理解上述定义，图中 <img src="https://www.zhihu.com/equation?tex=MinPts%3D5" alt="[公式]"> ，红色的点都是核心对象，因为其 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域至少有 <img src="https://www.zhihu.com/equation?tex=5" alt="[公式]"> 个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的圆内，如果不在圆内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列，此序列是一个簇集。在这些密度可达的样本序列的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域内所有的样本相互都是密度相连的 <strong>(注意，此图中有两个簇集)</strong>。</p>
<p><img src="https://pic2.zhimg.com/80/v2-7d15fc871942e0287be42a12d6d615dd_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h3><span id="23-dbscan密度聚类思想">2.3 DBSCAN密度聚类思想</span></h3><p><strong>DBSCAN的聚类定义很简单</strong>： <strong>由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。（注意是密度相连的集合）</strong>，簇里面可以有一个或者多个核心对象。<strong>如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> <strong>-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的</strong> <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -<strong>邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达</strong>。这些核心对象的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域里所有的样本的集合组成的一个DBSCAN聚类簇。</p>
<p>那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够<strong>密度可达</strong>的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找<strong>密度可达</strong>的样本集合，这样就得到另一个聚类簇 <strong>（这样的得到都肯定是密度相连的）</strong>。一直运行到<strong>所有核心对象都有类别为止。</strong></p>
<p>基本上这就是DBSCAN算法的主要内容了，是不是很简单？<strong>但是我们还是有三个问题没有考虑。</strong></p>
<ul>
<li><strong>异常点问题：</strong>一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。</li>
<li><strong>距离度量问题</strong>：<strong><font color="red"> 即如何计算某样本和核心对象样本的距离</font></strong>。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量<strong>样本距离，比如欧式距离、曼哈顿距离</strong>等。</li>
<li><strong>数据点优先级分配问题</strong>：例如某些样本可能到两个核心对象的距离都小于 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，<strong>此时 DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说DBSCAN的算法不是完全稳定的算法。</strong></li>
</ul>
<h3><span id="24-算法步骤">2.4 算法步骤</span></h3><p><strong>输入：样本集 <img src="https://www.zhihu.com/equation?tex=D%3D%5C%7Bx_1%2Cx_2%2C...%2Cx_m%5C%7D" alt="[公式]"> ，邻域参数 <img src="https://www.zhihu.com/equation?tex=%28%CF%B5%2CMinPts%29" alt="[公式]"></strong></p>
<ol>
<li>初始化核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9%3D%E2%88%85%2C" alt="[公式]"> 初始化类别 <img src="https://www.zhihu.com/equation?tex=k%3D0" alt="[公式]"></li>
<li>遍历 <img src="https://www.zhihu.com/equation?tex=D" alt="[公式]"> 的元素，如果是核心对象，则将其加入到核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中</li>
<li>如果核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中元素都已经被<strong>访问</strong>，<strong>则算法结束</strong>，<strong>否则转入步骤4</strong>.</li>
<li>在核心对象集合 <img src="https://www.zhihu.com/equation?tex=%CE%A9" alt="[公式]"> 中，随机选择一个<strong>未访问</strong>的核心对象 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> ，首先将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 标记为<strong>已访问</strong>，然后将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 标记类别 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> ，最后将 <img src="https://www.zhihu.com/equation?tex=o" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> -邻域中<strong>未访问</strong>的数据，存放到种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds" alt="[公式]"> 中。</li>
<li>如果种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds%3D%E2%88%85" alt="[公式]"> ，则当前聚类簇 <img src="https://www.zhihu.com/equation?tex=C_k" alt="[公式]"> 生成完毕, 且 <img src="https://www.zhihu.com/equation?tex=k%3Dk%2B1" alt="[公式]"> ，<strong>跳转到3</strong>。否则，从种子集合 <img src="https://www.zhihu.com/equation?tex=Seeds" alt="[公式]"> 中挑选一个种子点 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> ，首先将其标记为已访问、标记类别 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> ，然后判断 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> 是否为核心对象，如果是将 <img src="https://www.zhihu.com/equation?tex=seed" alt="[公式]"> 中<strong>未访问</strong>的种子点加入到种子集合中，<strong>跳转到5</strong>。</li>
</ol>
<p><strong>从上述算法可知：</strong></p>
<ul>
<li><strong>每个簇至少包含一个核心对象</strong>；</li>
<li>非核心对象可以是簇的一部分，构成了簇的边缘（edge）；</li>
<li>包含过少对象的簇被认为是噪声；</li>
</ul>
<h3><span id="25-总结">2.5 总结</span></h3><h4><span id="优点">优点</span></h4><ol>
<li><strong>可以对任意形状的稠密数据集进行聚类</strong>，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li><strong>可以在聚类的同时发现异常点</strong>，对数据集中的异常点不敏感。</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>
<h4><span id="缺点">缺点</span></h4><ol>
<li><strong>不能处理密度差异过大（密度不均匀）的聚类</strong>：如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
<li>如果样本集较大时，聚类收敛时间较长;<strong>此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进；</strong></li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂，<strong>主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响</strong>。【OPTICS算法】</li>
<li><strong>边界点不完全确定性</strong></li>
</ol>
<h3><span id="26-optics算法">2.6 OPTICS算法:</span></h3><p><strong>OPTICS主要针对输入参数$ϵ$过敏感做的改进</strong>，OPTICS和DBSCNA的输入参数一样（ <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> ），虽然OPTICS算法中也需要两个输入参数，但该算法对 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 输入不敏感（一般将 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 固定为无穷大），同时该算法中并不显式的生成数据聚类，只是对数据集合中的对象进行排序，得到一个有序的对象列表，通过该有序列表，可以得到一个决策图，通过决策图可以不同 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 参数的数据集中检测簇集，即：<strong>先通过固定的 <img src="https://www.zhihu.com/equation?tex=MinPts" alt="[公式]"> 和无穷大的 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 得到有序列表，然后得到决策图，通过决策图可以知道当 <img src="https://www.zhihu.com/equation?tex=%CF%B5" alt="[公式]"> 取特定值时（比如 <img src="https://www.zhihu.com/equation?tex=%CF%B5%3D3" alt="[公式]"> )数据的聚类情况。</strong></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>聚类</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（16）聚类*-HDBSCAN</title>
    <url>/posts/1N8XMT6/</url>
    <content><![CDATA[<h2><span id="一-hdbscan聚类">一、HDBSCAN聚类</span></h2><blockquote>
<p>  <strong>图解HDBSCANS - Mr.g的文章</strong> - 知乎 <a href="https://zhuanlan.zhihu.com/p/412918565">https://zhuanlan.zhihu.com/p/412918565</a></p>
<p>  原文: <a href="https://link.zhihu.com/?target=https%3A//nbviewer.jupyter.org/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/How%20HDBSCAN%20Works.ipynb">How HDBSCAN works</a></p>
<p>  聚类算法(Clustering Algorithms)之层次聚类(Hierarchical Clustering) - 小玉的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/363879425">https://zhuanlan.zhihu.com/p/363879425</a></p>
</blockquote>
<p><strong>HDBSCAN 是由 Campello、Moulavi 和 Sander 开发的聚类算法。它通过将 DBSCAN 转换为层次聚类算法，然后用一种稳定的聚类技术提取出一个扁平的聚类来扩展 DBSCAN</strong>。这篇文章的目标是让你大致了解这个算法的工作原理及其背后的动机。与 HDBSCAN 的原论文不一样，我们这里将不将 DBSCAN 进行对照分析。作者这里更倾向将这算法类比成一种扁平聚类提取方法（ Robust Single Linkage ）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets <span class="keyword">as</span> data</span><br><span class="line">%matplotlib inline</span><br><span class="line">sns.set_context(<span class="string">&#x27;poster&#x27;</span>)</span><br><span class="line">sns.set_style(<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">sns.set_color_codes()</span><br><span class="line">plot_kwds = &#123;<span class="string">&#x27;alpha&#x27;</span> : <span class="number">0.5</span>, <span class="string">&#x27;s&#x27;</span> : <span class="number">80</span>, <span class="string">&#x27;linewidths&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">moons, _ = data.make_moons(n_samples=<span class="number">50</span>, noise=<span class="number">0.05</span>)</span><br><span class="line">blobs, _ = data.make_blobs(n_samples=<span class="number">50</span>, centers=[(-<span class="number">0.75</span>,<span class="number">2.25</span>), (<span class="number">1.0</span>, <span class="number">2.0</span>)], cluster_std=<span class="number">0.25</span>)</span><br><span class="line">test_data = np.vstack([moons, blobs])</span><br><span class="line">plt.scatter(test_data.T[<span class="number">0</span>], test_data.T[<span class="number">1</span>], color=<span class="string">&#x27;b&#x27;</span>, **plot_kwds)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> hdbscan</span><br><span class="line">clusterer = hdbscan.HDBSCAN(min_cluster_size=<span class="number">5</span>, gen_min_span_tree=<span class="literal">True</span>)</span><br><span class="line">clusterer.fit(test_data)</span><br></pre></td></tr></table></figure>
<p><strong>现在我们已经对数据聚类完了——但实际发生了什么我们还不知道。我们将拆解成下面5个步骤来进行分析：</strong></p>
<ol>
<li>根据 密度/稀疏度 进行<strong>空间转换</strong>。</li>
<li>构建基于加权距离图的最小生成树。</li>
<li>构建组件之间的层次簇结构。</li>
<li>用最小簇大小压缩层次聚类。</li>
<li>利用压缩好的生成树进行分类。</li>
</ol>
<h3><span id="11-空间转换">1.1 空间转换</span></h3><p><strong>聚类时我们希望在稀疏带噪声的数据中找到密度更高的族类——噪声的假设很重要</strong>：因为在真实情况下，数据都比较复杂，会有异常值的、缺失的数据和噪声等情况。算法的核心是单链聚类，它对噪声非常敏感：如果噪声数据点放在两个岛屿之间，可能就会将它们连在一起（也就是本来是两个族类的被分成一个）。显然，我们希望我们的算法对噪声具有鲁棒性，因此我们需要在运行单链算法之前找到一种方法来减少噪声。（作者用岛屿来比喻族类，海洋来表示噪声，下面“海洋”和“岛屿”代表这个意思。）</p>
<p><strong>我们要如何在不进行聚类的情况下找出“海洋”和“岛屿”？</strong>只要我们能够估算出样本集的密度，我们就可以认为密度较低的点都是“海洋”。要注意的是这里的目标不是完全区分出“海洋”和“岛屿”——现在只是聚类的初始步骤，并不是最终的输出——现在只是为了使我们的聚类中心对噪声更加鲁棒。因此，要识别出“海洋”的话，我们可以降低海平面（也就是加大容错范围）。出于实际目的，这意味着使每个“海洋”之间以及“海洋”与“岛屿”之间的距离会增加。</p>
<p>当然这只是直觉。它在实际中是如何工作的？我们需要一个计算量少的密度估计方式，简单到只要计算 k 个最近邻点的距离就可以。<strong><font color="red"> 我们可以直接从一个距离矩阵（不管怎样后面都要生成的）中地读取到这个距离；或者，如果我们的指标支持（并且维度较低），用 <a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/neighbors.html%23k-d-tree">kd-trees</a> 来做这种检索就很适合。</font></strong>下面正式将点 x 的参数 k 定义为<strong>核心距离</strong>，并表示为 <img src="https://www.zhihu.com/equation?tex=core_k%28x%29" alt="[公式]"> （与DBSCAN、LOF 和 HDBSCAN 文献一样）。现在我们需要一种降维方法来拉开点之间的距离（相对高维距离）。简单的方法是定义一种新距离公式，我们将其称为（与论文一样)<strong>相互可达距离（mutual reachability distance)。相互可达距离的定义如下：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=d_%7B%5Cmathrm%7Bmreach-%7Dk%7D%28a%2Cb%29+%3D+%5Cmax+%5C%7B%5Cmathrm%7Bcore%7D_k%28a%29%2C+%5Cmathrm%7Bcore%7D_k%28b%29%2C+d%28a%2Cb%29+%5C%7D" alt="[公式]" style="zoom: 150%;"></p>
<p><strong>其中 d(a,b) 是 a 和 b 之间的原始距离</strong>。在这个度量下，密集点（具有低核心距离）之间的距离保持不变，但稀疏的点与其他点的距离被拉远到用core距离来计算。这有效地“降低了海平面”，减少稀疏的“海”点，同时使“陆地”保持原状。需要注意的是，显然 k 取值很关键；较大的 k 值将更多的点圈到“海”中。下面用图片来解析更容易理解，先让k=5。然后对于给定的点，我们可以为核心距离绘制一个圆刚好圈到第六个临近点（包括点本身），如下所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-dbf4559853b0d81f1c5ae2205a7b97a9_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p><strong>再选择另一个点</strong>，进行同样的操作，这次选到另一组临近点集合（其中一些点可能是上一组的临近点）。</p>
<p><img src="https://pic3.zhimg.com/80/v2-308c1bb09e8f779232aac217bee2507e_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>我们再选一个点再做一遍，得到第三组临近点。</p>
<p><img src="https://pic4.zhimg.com/80/v2-0e0e83ecf594b202cea6d3c208e45f0f_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>如果我们现在想知道蓝绿两组之间的相互可达距离，我们可以像下图，先画一个箭头连接蓝绿两个圆心：它穿过蓝色圆心，但不穿过绿色圆圈——绿色的核心距离大于蓝色和绿色之间的距离。<strong>因此，我们认为蓝色和绿色之间的相互可达距离更大——也就是绿色圆圈的半径（如果我们将一端设在绿色点上，则最容易想象）。</strong></p>
<p>实际上，有<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1506.06422v2.pdf">基础理论</a>可以证明<strong>相互可达距离</strong>作为一种变换，在允许单链接聚类的情况下，更接近层次水平上的真实密度分布。</p>
<h3><span id="12-构建最小生成树"><strong>1.2 构建最小生成树</strong></span></h3><p><strong>为了从密集的数据集上找到“岛屿”，现在我们有了新的指标：相互可达性</strong>。当然密集区是相对的，不同的“岛屿”可能会有不同的密度。<strong><font color="red"> 理论上，我们要做的是：将数据当成是一个加权图，以数据点作为顶点，任意两点之间的边的权重等于这些点的相互可达距离。</font></strong></p>
<p>现在考虑一个阈值，一开始很高，然后逐渐变小。丢弃权重高于该阈值的任何边。我们删除边的同时，连接的组件从图里断开。最终，我们将拥有不同阈值级别的连接元件（从完全连接到完全断开）的层次结构。</p>
<p>实际当中，这样操纵非常耗时：有 <a href="https://zhuanlan.zhihu.com/p/412918565/edit#">n^2</a> 条边，我们不想多次计算连通组件算法。正确的做法是找到最小的边集，从这个集合中删除任何边都会导致组件的连接断开。但是我们还需要找到更多这样的边，使得找不到更小的边来连接组件。幸运的是，图论为我们提供了这样的东西：<strong>图的最小生成树</strong>。</p>
<p>我们可以通过 Prim 的算法非常有效地构建最小生成树——我们一次构建一条边，每次都把当前最小权重的边去连接一个尚未加入到树中的节点。可以看到下面HDBSCAN构造的树 ；请注意，这是相互可达距离的最小生成树，与图中的纯距离不同。在这种情况下，我们的 k 值为 5。 在这个例子中，存在一种更快的方法，例如用 <strong>Dual Tree Boruvka 来构建最小生成树。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clusterer.minimum_spanning_tree_.plot(edge_cmap=<span class="string">&#x27;viridis&#x27;</span>, </span><br><span class="line">                                      edge_alpha=<span class="number">0.6</span>, </span><br><span class="line">                                      node_size=<span class="number">80</span>, </span><br><span class="line">                                      edge_linewidth=<span class="number">2</span>)   </span><br></pre></td></tr></table></figure>
<p><img src="https://pic2.zhimg.com/80/v2-a4b387ac9c43b1f681589529c82fe68d_1440w.jpg" alt="img" style="zoom:50%;"></p>
<h3><span id="13-构建层次聚类">1.3 <strong>构建层次聚类</strong></span></h3><p><strong>给定最小生成树，下一步是将其转换为层次结构的组件</strong>。这最容易以相反的顺序完成：按距离（按递增顺序）对树的边进行排序，然后迭代，为每条边新建一个合并后的簇。这里唯一困难是识别每条边将连接在哪两个簇上，但这通过联合查找数据结构很容易。我们可以将结果视为树状图，如下所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-33a8cb27ae3ea0009e1ad77b438cf458_1440w.jpg" alt="img" style="zoom:50%;"></p>
<p>这图可以告诉我们这个鲁棒的单一链接会在哪挺下来。我们想知道；层次结构结构的聚类虽好，但我们想要的是一个扁平的聚类。我们可以通过在上图中画一条水平线并选择它穿过的聚类，来做到这一点。这实际上是 DBSCAN 里用到的操作（将只能切割成单一集群的作为噪声）。<strong>问题是，我们怎么知道在哪里画这条线？ DBSCAN 只是把它作为一个（非常不直观的）参数</strong>。更糟糕的是，我们真的要用来处理可变密度的聚类，并希望任何切割线都是通过相互可达距离选出来的，并且今后固定在一个密度水平上。理想情况下，我们希望能够在不同的地方切割树，来选择我们的聚类。这是 HDBSCAN 下一步开始的地方，并与鲁棒的单一链接产生差异。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>聚类</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（11）CatBoost</title>
    <url>/posts/2ZWAT5R/</url>
    <content><![CDATA[<h2><span id="深入理解catboost">深入理解CatBoost</span></h2><blockquote>
<p>  深入理解CatBoost - Microstrong的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/102540344">https://zhuanlan.zhihu.com/p/102540344</a></p>
</blockquote>
<p><strong>本文主要内容概览：</strong></p>
<p><img src="https://pic1.zhimg.com/v2-f6a9520c6db0ba77ad620800cb36c054_b.jpg" alt="img"></p>
<h3><span id="一-catboost简介"><strong>一、CatBoost简介</strong></span></h3><p>CatBoost是俄罗斯的搜索巨头Yandex在2017年开源的机器学习库，是Boosting族算法的一种。CatBoost和XGBoost、LightGBM并称为GBDT的三大主流神器，都是在GBDT算法框架下的一种改进实现。XGBoost被广泛的应用于工业界，LightGBM有效的提升了GBDT的计算效率，而Yandex的CatBoost号称是比XGBoost和LightGBM在算法准确率等方面表现更为优秀的算法。</p>
<p>CatBoost是一种基于对称决策树（oblivious trees）为基学习器实现的参数较少、支持类别型变量和高准确性的GBDT框架，主要解决的痛点是高效合理地处理类别型特征，这一点从它的名字中可以看出来，<strong>CatBoost是由Categorical和Boosting组成。此外，CatBoost还解决了梯度偏差（Gradient Bias）以及预测偏移（Prediction shift）的问题，从而减少过拟合的发生，进而提高算法的准确性和泛化能力。</strong></p>
<p><strong>与XGBoost、LightGBM相比，CatBoost的创新点有：</strong></p>
<ul>
<li><strong>嵌入了自动将类别型特征处理为数值型特征的创新算法。首先对categorical features做一些统计，计算某个类别特征（category）出现的频率，之后加上超参数，生成新的数值型特征（numerical features）。</strong></li>
<li><strong>Catboost还使用了组合类别特征，可以利用到特征之间的联系，这极大的丰富了特征维度</strong>。</li>
<li>采用排序提升的方法对抗训练集中的噪声点，从而避免梯度估计的偏差，进而解决预测偏移的问题。</li>
<li>采用了<strong>完全对称树作为基模型</strong>。</li>
</ul>
<h3><span id="二-类别型特征">二、<strong>类别型特征</strong></span></h3><p><strong>所谓类别型特征，即这类特征不是数值型特征，而是离散的集合</strong>，比如省份名（山东、山西、河北等），城市名（北京、上海、深圳等），学历（本科、硕士、博士等）。在梯度提升算法中，最常用的是将这些类别型特征转为数值型来处理，一般类别型特征会转化为一个或多个数值型特征。</p>
<p><strong>类别型特征基数比较低（low-cardinality features）</strong>，即该特征的所有值去重后构成的集合元素个数比较少，一般利用One-hot编码方法将特征转为数值型。One-hot编码可以在数据预处理时完成，也可以在模型训练的时候完成，从训练时间的角度，后一种方法的实现更为高效，CatBoost对于基数较低的类别型特征也是采用后一种实现。</p>
<p><strong>高基数类别型特征（high cardinality features）</strong>当中，比如 <code>user ID</code>，这种编码方式会产生大量新的特征，造成维度灾难。一种折中的办法是可以将类别分组成有限个的群体再进行One-hot编码。<strong>一种常被使用的方法是根据目标变量统计（Target Statistics，以下简称TS）进行分组</strong>，目标变量统计用于估算每个类别的目标变量期望值。甚至有人直接用TS作为一个新的数值型变量来代替原来的类别型变量。<strong><font color="red"> 重要的是，可以通过对TS数值型特征的阈值设置，基于对数损失、基尼系数或者均方差，得到一个对于训练集而言将类别一分为二的所有可能划分当中最优的那个。</font></strong></p>
<p>在LightGBM当中，类别型特征用每一步梯度提升时的梯度统计（Gradient Statistics，以下简称GS）来表示。虽然为建树提供了重要的信息，但是这种方法有以下两个缺点：</p>
<ul>
<li>增加计算时间，因为需要对每一个类别型特征，在迭代的每一步，都需要对GS进行计算；</li>
<li>增加存储需求，对于一个类别型变量，需要存储每一次分离每个节点的类别；</li>
</ul>
<p><strong>为了克服这些缺点，LightGBM以损失部分信息为代价将所有的长尾类别归为一类</strong>，作者声称这样处理高基数类别型特征时比One-hot编码还是好不少。不过如果采用TS特征，那么对于每个类别只需要计算和存储一个数字。</p>
<p>因此，采用TS作为一个新的数值型特征是最有效、信息损失最小的处理类别型特征的方法。TS也被广泛应用在点击预测任务当中，这个场景当中的类别型特征有用户、地区、广告、广告发布者等。接下来我们着重讨论TS，暂时将One-hot编码和GS放一边。</p>
<h4><span id="21-目标变量统计target-statistics">2.1 <strong>目标变量统计（Target Statistics）</strong></span></h4><p><strong><font color="red"> CatBoost算法的设计初衷是为了更好的处理GBDT特征中的categorical features</font></strong>。在处理 GBDT特征中的categorical features的时候，最简单的方法是用 categorical feature 对应的标签的平均值来替换。在决策树中，标签平均值将作为节点分裂的标准。<strong>这种方法被称为 Greedy Target-based Statistics , 简称 Greedy TS</strong>，用公式来表达就是：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx%7D_k%5Ei%3D%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Bx_%7Bj%2Ck%7D%3Dx_%7Bi%2Ck%7D%5D%5Ccdot+Y_i%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Bx_%7Bj%2Ck%7D%3Dx_%7Bi%2Ck%7D%5D%7D+%5C%5C" alt="[公式]"></p>
<p>这种方法有一个显而易见的缺陷，就是通常特征比标签包含更多的信息，<strong><font color="red"> 如果强行用标签的平均值来表示特征的话，当训练数据集和测试数据集数据结构和分布不一样的时候会出条件偏移问题。</font></strong></p>
<p>一个标准的改进 Greedy TS的方式是添加先验分布项，这样可以减少噪声和低频率类别型数据对于数据分布的影响：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx%7D_k%5Ei%3D%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E%7Bp-1%7D%7B%5Bx_%7B%5Csigma_%7Bj%2Ck%7D%7D%3Dx_%7B%5Csigma_%7Bp%2Ck%7D%7D%5D%7DY_%7B%5Csigma_%7Bj%7D%7D%2Ba%5Ccdot+p%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bp-1%7D%7B%5Bx_%7B%5Csigma_%7Bj%2Ck%7D%7D%3Dx_%7B%5Csigma_%7Bp%2Ck%7D%7D%5D%7D%2Ba%7D+%5C%5C" alt="[公式]"></p>
<p> 其中$p$是添加的先验项， $a$通常是大于 <img src="https://www.zhihu.com/equation?tex=+0+" alt="[公式]"> 的权重系数。添加先验项是一个普遍做法，针对类别数较少的特征，它可以减少噪声数据。对于回归问题，一般情况下，先验项可取数据集label的均值。对于二分类，先验项是正例的先验概率。利用多个数据集排列也是有效的，但是，如果直接计算可能导致过拟合。CatBoost利用了一个比较新颖的计算叶子节点值的方法，这种方式(oblivious trees，对称树)可以避免多个数据集排列中直接计算会出现过拟合的问题。</p>
<p>当然，在论文《CatBoost: unbiased boosting with categorical features》中，还提到了其它几种改进Greedy TS的方法，分别有：Holdout TS、Leave-one-out TS、Ordered TS。我这里就不再翻译论文中的这些方法了，感兴趣的同学可以自己翻看一下原论文。</p>
<h4><span id="22-特征组合">2.2 <strong>特征组合</strong></span></h4><p>值得注意的是几个类别型特征的任意组合都可视为新的特征。例如，在音乐推荐应用中，我们有两个类别型特征：用户ID和音乐流派。如果有些用户更喜欢摇滚乐，将用户ID和音乐流派转换为数字特征时，根据上述这些信息就会丢失。结合这两个特征就可以解决这个问题，并且可以得到一个新的强大的特征。然而，组合的数量会随着数据集中类别型特征的数量成指数增长，因此不可能在算法中考虑所有组合。为当前树构造新的<a href="https://www.zhihu.com/search?q=分割点&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">分割点</a>时，CatBoost会采用贪婪的策略考虑组合。对于树的第一次分割，不考虑任何组合。对于下一个分割，CatBoost将当前树的所有组合、类别型特征与数据集中的所有类别型特征相结合，并将新的组合类别型特征动态地转换为数值型特征。CatBoost还通过以下方式生成数值型特征和类别型特征的组合：树中选定的所有分割点都被视为具有两个值的类别型特征，并像类别型特征一样被进行组合考虑。</p>
<h4><span id="23-catboost处理categorical-features总结">2.3  <strong>CatBoost处理Categorical features总结</strong></span></h4><ul>
<li><strong>首先计算一些数据的statistics。计算某个category出现的频率，加上超参数，生成新的numerical features</strong>。这一策略要求同一标签数据不能排列在一起（即先全是0之后全是1这种方式），训练之前需要打乱数据集。</li>
<li>使用数据的不同排列（实际上是4个）。在每一轮建立树之前，先扔一轮骰子，决定使用哪个排列来生成树。</li>
<li>考虑使用categorical features的不同组合。例如颜色和种类组合起来，可以构成类似于blue dog这样的特征。当需要组合的categorical features变多时，CatBoost只考虑一部分<a href="https://www.zhihu.com/search?q=combinations&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">combinations</a>。在选择第一个节点时，只考虑选择一个特征，例如A。在生成第二个节点时，考虑A和任意一个categorical feature的组合，选择其中最好的。就这样使用贪心算法生成combinations。</li>
<li><strong>除非向gender这种维数很小的情况，不建议自己生成One-hot编码向量，最好交给算法来处理。</strong></li>
</ul>
<h2><span id="三-catboostqampa">三、CatboostQ&amp;A</span></h2><h4><span id="31-catboost与xgboost-lightgbm的联系与区别">3.1 <strong>CatBoost与XGBoost、LightGBM的联系与区别？</strong></span></h4><p>（1）2014年3月XGBoost算法首次被<a href="https://www.zhihu.com/search?q=陈天奇&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;102540344&quot;}">陈天奇</a>提出，但是直到2016年才逐渐著名。2017年1月微软发布LightGBM第一个稳定版本。2017年4月Yandex开源CatBoost。自从XGBoost被提出之后，很多文章都在对其进行各种改进，CatBoost和LightGBM就是其中的两种。</p>
<p>（2）<strong>CatBoost处理类别型特征十分灵活，可直接传入类别型特征的列标识，模型会自动将其使用One-hot编码，还可通过设置 one_hot_max_size参数来限制One-hot特征向量的长度</strong>。如果不传入类别型特征的列标识，那么CatBoost会把所有列视为数值特征。对于One-hot编码超过设定的one_hot_max_size值的特征来说，CatBoost将会使用一种高效的encoding方法，与mean encoding类似，但是会降低过拟合。处理过程如下：</p>
<ul>
<li>将输入样本集随机排序，并生成多组随机排列的情况；</li>
<li>将浮点型或属性值标记转化为整数；</li>
<li>将所有的类别型特征值结果都根据以下公式，转化为数值结果；</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=avg%5C_target+%3D+%5Cfrac%7BcountInClass+%2B+prior%7D%7BtotalCount+%2B+1%7D+%5C%5C" alt="[公式]"> </p>
<p>其中 countInClass 表示在当前类别型特征值中有多少样本的标记值是1；prior 是分子的初始值，根据初始参数确定。totalCount 是在所有样本中（包含当前样本）和当前样本具有相同的类别型特征值的样本数量。</p>
<p>LighGBM 和 CatBoost 类似，也可以通过使用特征名称的输入来处理类别型特征数据，它没有对数据进行独热编码，因此速度比独热编码快得多。LighGBM 使用了一个特殊的算法来确定属性特征的分割值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = lgb.Dataset(data, label=label, feature_name=[<span class="string">&#x27;c1&#x27;</span>, <span class="string">&#x27;c2&#x27;</span>, <span class="string">&#x27;c3&#x27;</span>], categorical_feature=[<span class="string">&#x27;c3&#x27;</span>])</span><br><span class="line"><span class="comment"># 注意，在建立适用于 LighGBM 的数据集之前，需要将类别型特征变量转化为整型变量，此算法不允许将字符串数据传给类别型变量参数。</span></span><br></pre></td></tr></table></figure>
<p>（3）XGBoost 和 CatBoost、 LighGBM 算法不同，XGBoost 本身无法处理类别型特征，而是像随机森林一样，只接受数值数据。因此在将类别型特征数据传入 XGBoost 之前，必须通过各种编码方式：例如序号编码、独热编码和二进制编码等对数据进行处理。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成学习</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（12）降维</title>
    <url>/posts/16HRCXQ/</url>
    <content><![CDATA[<h2><span id="降维">降维</span></h2><blockquote>
<ul>
<li>数据降维算法: <a href="https://www.zhihu.com/column/c_1194552337170214912">https://www.zhihu.com/column/c_1194552337170214912</a></li>
</ul>
</blockquote>
<p><img src="https://pic2.zhimg.com/v2-e47296e78fff3d97eea11d0657ddcb81_1440w.jpg?source=172ae18b" alt="【机器学习】降维——PCA（非常详细）" style="zoom:51%;"></p>
<h2><span id="一-pca">一、PCA</span></h2><blockquote>
<p>  <strong><font color="red"> 降维问题的优化目标：将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。</font></strong></p>
<p>  要找的 <strong>==P 是能让原始协方差矩阵对角化的 P==</strong>。换句话说，优化目标变成了<strong>寻找一个矩阵 P，满足</strong> <img src="https://www.zhihu.com/equation?tex=PCP%5ET" alt="[公式]"> <strong>是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件</strong>。</p>
</blockquote>
<p><strong>PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量</strong>。PCA 的数学推导可以从<strong>==最大可分型==</strong>和<strong>最近重构性</strong>两方面进行，前者的优化条件为划分后方差最大，后者的优化条件为点到划分平面距离最小，这里我将从最大可分性的角度进行证明。</p>
<h3><span id="1-向量表示与基变换">1. 向量表示与基变换</span></h3><p>我们先来介绍些线性代数的基本知识。</p>
<h3><span id="11-内积">1.1 内积</span></h3><p><strong>两个向量的 A 和 B 内积</strong>我们知道形式是这样的：</p>
<p><img src="https://www.zhihu.com/equation?tex=%28a_1%2Ca_2%2C%5Ccdots%2Ca_n%29%5Ccdot+%28b_1%2Cb_2%2C%5Ccdots%2Cb_n%29%5E%5Cmathsf%7BT%7D%3Da_1b_1%2Ba_2b_2%2B%5Ccdots%2Ba_nb_n+%5C%5C" alt="[公式]"></p>
<p>内积运算将两个向量映射为实数，其计算方式非常容易理解，但我们无法看出其物理含义。接下来我们从几何角度来分析，为了简单起见，我们假设 A 和 B 均为二维向量，则：</p>
<p><img src="https://www.zhihu.com/equation?tex=A%3D%28x_1%2Cy_1%29%EF%BC%8CB%3D%28x_2%2Cy_2%29+%5C+A+%5Ccdot+B+%3D+%7CA%7C%7CB%7Ccos%28%5Calpha%29+%5C%5C" alt="[公式]"></p>
<p>其几何表示见下图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-cf4c0041c8459d2894b9a57d8f679a0a_1440w.jpg" alt="img"></p>
<p>我们看出 A 与 B 的内积等于 <strong>A 到 B 的投影长度乘以 B 的模</strong>。</p>
<p>如果假设 B 的模为 1，即让 <img src="https://www.zhihu.com/equation?tex=%7CB%7C%3D1" alt="[公式]"> ，那么就变成了：</p>
<p><img src="https://www.zhihu.com/equation?tex=A%5Ccdot+B%3D%7CA%7Ccos%28a%29+%5C%5C" alt="[公式]"></p>
<p>也就是说，<strong>A 与 B 的内积值等于 A 向 B 所在直线投影的标量大小。</strong></p>
<h3><span id="12-基">1.2 基</span></h3><p>在我们常说的坐标系种，向量 (3,2) 其实隐式引入了一个定义：以 x 轴和 y 轴上正方向长度为 1 的向量为标准。向量 (3,2) 实际是说在 x 轴投影为 3 而 y 轴的投影为 2。<strong>注意投影是一个标量，所以可以为负。</strong></p>
<p>所以，对于向量 (3, 2) 来说，如果我们想求它在 <img src="https://www.zhihu.com/equation?tex=%281%2C0%29%2C%280%2C1%29" alt="[公式]"> 这组基下的坐标的话，分别内积即可。当然，内积完了还是 (3, 2)。</p>
<p>所以，我们大致可以得到一个结论，我们<strong>要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了</strong>。为了方便求坐标，我们希望这组基向量模长为 1。因为向量的内积运算，当模长为 1 时，内积可以直接表示投影。然后还需要这组基是线性无关的，我们一般用正交基，非正交的基也是可以的，不过正交基有较好的性质。</p>
<h3><span id="13-基变换的矩阵表示">1.3 基变换的矩阵表示</span></h3><p>这里我们先做一个练习：对于向量 (3,2) 这个点来说，在 <img src="https://www.zhihu.com/equation?tex=%28%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%2C%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%28-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%2C+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 这组基下的坐标是多少？</p>
<p>我们拿 (3,2) 分别与之内积，得到 <img src="https://www.zhihu.com/equation?tex=%28%5Cfrac%7B5%7D%7B%5Csqrt%7B2%7D%7D%2C-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%29" alt="[公式]"> 这个新坐标。</p>
<p>我们可以用矩阵相乘的形式简洁的表示这个变换：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D++1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+3+%5C%5C+2+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+5%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D++%5Cend%7Bpmatrix%7D++%5C%5C" alt="[公式]"></p>
<p>左边矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。推广一下，如果我们有 m 个二维向量，只要将二维向量按列排成一个两行 m 列矩阵，然后用“基矩阵”乘以这个矩阵就可以得到了所有这些向量在新基下的值。例如对于数据点 <img src="https://www.zhihu.com/equation?tex=%281%2C1%29%EF%BC%8C%282%2C2%29%EF%BC%8C%283%2C3%29" alt="[公式]"> 来说，想变换到刚才那组基上，则可以这样表示：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+1+%26+2+%26+3+%5C%5C+1+%26+2+%26+3+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+2%2F%5Csqrt%7B2%7D+%26+4%2F%5Csqrt%7B2%7D+%26+6%2F%5Csqrt%7B2%7D+%5C%5C+0+%26+0+%26+0+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>我们可以把它写成通用的表示形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+p_1+%5C%5C+p_2+%5C%5C+%5Cvdots+%5C%5C+p_R+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+a_1+%26+a_2+%26+%5Ccdots+%26+a_M+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+p_1a_1+%26+p_1a_2+%26+%5Ccdots+%26+p_1a_M+%5C%5C+p_2a_1+%26+p_2a_2+%26+%5Ccdots+%26+p_2a_M+%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C+p_Ra_1+%26+p_Ra_2+%26+%5Ccdots+%26+p_Ra_M+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 是一个行向量，表示第 i 个基， <img src="https://www.zhihu.com/equation?tex=a_j" alt="[公式]"> 是一个列向量，表示第 j 个原始数据记录。实际上也就是做了一个向量矩阵化的操作。</p>
<p>==上述分析给矩阵相乘找到了一种物理解释：<strong>两个矩阵相乘的意义是将右边矩阵中的每一列向量</strong> <img src="https://www.zhihu.com/equation?tex=a_i" alt="[公式]"> <strong>变换到左边矩阵中以每一行行向量为基所表示的空间中去。</strong>也就是说一个矩阵可以表示一种线性变换。==</p>
<h3><span id="2-最大可分性">2. 最大可分性</span></h3><p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，<strong>如果基的数量少于向量本身的维数，则可以达到降维的效果</strong>。</p>
<p><strong>但是我们还没回答一个最关键的问题：如何选择基才是最优的。或者说，如果我们有一组 N 维向量，现在要将其降到 K 维（K 小于 N），那么我们应该如何选择 K 个基才能最大程度保留原有的信息？</strong></p>
<p>一种直观的看法是：<strong><font color="red"> 希望投影后的投影值尽可能分散，因为如果重叠就会有样本消失。当然这个也可以从熵的角度进行理解，熵越大所含信息越多。</font></strong></p>
<h4><span id="21-方差">2.1 方差</span></h4><p>我们知道数值的分散程度，可以用数学上的方差来表述。<strong>一个变量的方差可以看做是每个元素与变量均值的差的平方和的均值</strong>，即：</p>
<p><img src="https://www.zhihu.com/equation?tex=Var%28a%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu%29%5E2%7D+%5C%5C" alt="[公式]"></p>
<p><strong>为了方便处理，我们将每个变量的均值都化为 0</strong> ，因此方差可以直接用每个元素的平方和除以元素个数表示：</p>
<p><img src="https://www.zhihu.com/equation?tex=Var%28a%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%5C%5C" alt="[公式]"></p>
<p>于是上面的问题被形式化表述为：<strong>寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</strong></p>
<h4><span id="22-协方差">2.2 协方差</span></h4><p>在一维空间中我们可以用方差来表示数据的分散程度。而对于高维数据，我们用协方差进行约束，<strong>协方差可以表示两个变量的相关性</strong>。<strong><font color="red"> 为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性</font></strong>，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。</p>
<p>协方差公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm-1%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu_a%29%28b_i-%5Cmu_b%29%7D+%5C%5C" alt="[公式]"></p>
<p>由于均值为 0，所以我们的协方差公式可以表示为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C" alt="[公式]"></p>
<p>当样本数较大时，不必在意其是 m 还是 m-1，为了方便计算，我们分母取 m。</p>
<p><strong><font color="red"> 协方差为 0 时，表示两个变量完全不相关</font></strong>。为了让协方差为 0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。</p>
<p>（<strong>补充</strong>：协方差为 0 时，两个变量只是线性不相关。完全独立是有问题的，才疏学浅，还望见谅。）</p>
<p><strong><font color="red"> 至此，我们得到了降维问题的优化目标：将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。</font></strong></p>
<h4><span id="23-协方差矩阵">2.3 协方差矩阵</span></h4><p>针对我们给出的优化目标，接下来我们将从数学的角度来给出优化目标。我们看到，最终要达到的目的与<strong>变量内方差及变量间协方差</strong>有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们有：</p>
<p>假设我们只有 a 和 b 两个变量，那么我们将它们按行组成矩阵 X：</p>
<p><img src="https://www.zhihu.com/equation?tex=X%3D%5Cbegin%7Bpmatrix%7D++a_1+%26+a_2+%26+%5Ccdots+%26+a_m+%5C%5C+b_1+%26+b_2+%26+%5Ccdots+%26+b_m++%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>然后：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D%3D+%5Cbegin%7Bpmatrix%7D++%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Bb_i%5E2%7D++%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D++Cov%28a%2Ca%29+%26+Cov%28a%2Cb%29+%5C%5C++Cov%28b%2Ca%29+%26+Cov%28b%2Cb%29+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>我们可以看到这个矩阵对角线上的分别是两个变量的方差，而其它元素是 a 和 b 的协方差。两者被统一到了一个矩阵里。</p>
<p><strong>设我们有 m 个 n 维数据记录，将其排列成矩阵</strong> <img src="https://www.zhihu.com/equation?tex=X_%7Bn%2Cm%7D" alt="[公式]"> <strong>，设</strong> <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5ET" alt="[公式]"> <strong>，则 C 是一个对称矩阵，其对角线分别对应各个变量的方差，而第 i 行 j 列和 j 行 i 列元素相同，表示 i 和 j 两个变量的协方差</strong>。</p>
<h4><span id="24-矩阵对角化">2.4 矩阵对角化</span></h4><p>根据我们的优化条件，<strong>我们需要将除对角线外的其它元素化为 0，并且在对角线上将元素按大小从上到下排列（变量方差尽可能大）</strong>，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系。</p>
<p>设原始数据矩阵 X 对应的协方差矩阵为 C，而 P 是一组基按行组成的矩阵，设 Y=PX，则 Y 为 X 对 P 做基变换后的数据。设 Y 的协方差矩阵为 D，我们推导一下 D 与 C 的关系：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++D+%26+%3D++%5Cfrac%7B1%7D%7Bm%7DYY%5ET+%5C%5C++%26+%3D+%5Cfrac%7B1%7D%7Bm%7D%28PX%29%28PX%29%5ET+%5C%5C+%26+%3D+%5Cfrac%7B1%7D%7Bm%7DPXX%5ETP%5ET+%5C%5C++%26+%3D+P%28%5Cfrac%7B1%7D%7Bm%7DXX%5ET%29P%5ET+%5C%5C++%26+%3D+PCP%5ET++%5Cend%7Baligned%7D++%5C%5C" alt="[公式]"></p>
<p>这样我们就看清楚了，我们要找的 <strong>==P 是能让原始协方差矩阵对角化的 P==</strong>。换句话说，优化目标变成了<strong>寻找一个矩阵 P，满足</strong> <img src="https://www.zhihu.com/equation?tex=PCP%5ET" alt="[公式]"> <strong>是一个对角矩阵，并且对角元素按从大到小依次排列，那么 P 的前 K 行就是要寻找的基，用 P 的前 K 行组成的矩阵乘以 X 就使得 X 从 N 维降到了 K 维并满足上述优化条件</strong>。</p>
<p>至此，我们离 PCA 还有仅一步之遥，我们还需要完成对角化。</p>
<p><strong>由上文知道，协方差矩阵 C 是一个是对称矩阵，在线性代数中实对称矩阵有一系列非常好的性质：</strong></p>
<ol>
<li><strong>实对称矩阵不同特征值对应的特征向量必然正交</strong>。</li>
<li><strong>设特征向量 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> 重数为 r，则必然存在 r 个线性无关的特征向量对应于 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> ，因此可以将这 r 个特征向量单位正交化。</strong></li>
</ol>
<p><strong>由上面两条可知，一个 n 行 n 列的实对称矩阵一定可以找到 n 个单位正交特征向量，设这 n 个特征向量为 <img src="https://www.zhihu.com/equation?tex=e_1%2Ce_2%2C%5Ccdots%2Ce_n" alt="[公式]"> ，我们将其按列组成矩阵： <img src="https://www.zhihu.com/equation?tex=E%3D%28e_1+%2C+e_2+%2C+%5Ccdots+%2C+e_n+%29" alt="[公式]"> 。</strong></p>
<p>则对协方差矩阵 C 有如下结论：</p>
<p><img src="https://www.zhihu.com/equation?tex=E%5ETCE%3D%5CLambda%3D%5Cbegin%7Bpmatrix%7D+%5Clambda_1+%26+%26+%26+%5C%5C+%26+%5Clambda_2+%26+%26+%5C%5C+%26+%26+%5Cddots+%26+%5C%5C+%26+%26+%26+%5Clambda_n+%5Cend%7Bpmatrix%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。到这里，我们发现我们已经找到了需要的矩阵 P： <img src="https://www.zhihu.com/equation?tex=P%3DE%5E%5Cmathsf%7BT%7D" alt="[公式]"> 。</p>
<p><strong>P 是协方差矩阵的特征向量单位化后按行排列出的矩阵</strong>，其中每一行都是 C 的一个特征向量。如果设 P 按照 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 中特征值的从大到小，将特征向量从上到下排列，则用 P 的前 K 行组成的矩阵乘以原始数据矩阵 X，就得到了我们需要的降维后的数据矩阵 Y。</p>
<blockquote>
<p>  <strong>拉格朗日乘子法证明</strong>:<strong>方差就是协方差矩阵的特征值</strong></p>
</blockquote>
<h4><span id="25-最近重构性-思路">2.5 最近重构性-思路</span></h4><p>以上的证明思路主要是基于最大可分性的思想，<strong>通过一条直线使得样本点投影到该直线上的方差最大</strong>。除此之外，我们还可以<strong>将其转换为线型回归问题，其目标是求解一个线性函数使得对应直线能够更好地拟合样本点集合</strong>。这就<strong>使得我们的优化目标从方差最大转化为平方误差最小</strong>，因为映射距离越短，丢失的信息也会越小。区别于最大可分性，这是从最近重构性的角度进行论证。</p>
<h3><span id="3-求解步骤">==3. 求解步骤==</span></h3><h4><span id="总结一下-pca-的算法步骤设有-m-条-n-维数据">总结一下 PCA 的算法步骤：<strong>设有 m 条 n 维数据。</strong></span></h4><ol>
<li><strong>将原始数据按列组成 n 行 m 列矩阵 X；</strong></li>
<li><strong>将 X 的每一行进行==零均值化==，即减去这一行的均值</strong>；【<strong>零均值化</strong>】【<strong>方差、协方差好计算</strong>】</li>
<li><strong>==求出协方差矩阵==</strong> <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D" alt="[公式]"> ；</li>
<li><strong>求出协方差矩阵的特征值及对应的特征向量</strong>；</li>
<li><strong>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P</strong>；</li>
<li><img src="https://www.zhihu.com/equation?tex=Y%3DPX" alt="[公式]"> <strong>即为降维到 k 维后的数据</strong>。</li>
</ol>
<h4><span id="4-性质维度灾难-降噪-过拟合-特征独立">4. 性质【维度灾难、降噪、过拟合、特征独立】</span></h4><ol>
<li><strong>==缓解维度灾难==</strong>：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；</li>
<li><strong>==降噪==</strong>：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；</li>
<li><strong>==过拟合==</strong>：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；</li>
<li><strong>==特征独立==</strong>：PCA 不仅将数据压缩到低维，它也使得<strong>降维之后的数据各特征相互独立</strong>；</li>
</ol>
<h3><span id="5-细节">5. 细节</span></h3><h4><span id="51-零均值化">5.1 零均值化</span></h4><p>当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。==而<strong>对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来</strong>，不能使用验证集或者测试集的中心向量。==</p>
<p>其原因也很简单，因为我们的训练集时可观测到的数据，测试集不可观测所以不会知道其均值，而验证集再大部分情况下是在处理完数据后再从训练集中分离出来，一般不会单独处理。如果真的是单独处理了，不能独自求均值的原因是和测试集一样。</p>
<p>另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现 Variance Shift 的问题。</p>
<h4><span id="52-svd-的对比">==5.2 SVD 的对比==</span></h4><p>这是两个不同的数学定义。我们先给结论：<strong>特征值和特征向量是针对方阵</strong>才有的，而<strong>对任意形状的矩阵都可以做奇异值分解</strong>。</p>
<p><strong>PCA</strong>：<strong>方阵的特征值分解</strong>，对于一个方阵 A。其中，Q 是这个矩阵 A 的特征向量组成的矩阵， <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列)。也就是说矩阵 A 的信息可以由其特征值和特征向量表示。</p>
<p><strong>SVD</strong>：<strong>矩阵的奇异值分解其实就是对于矩阵 A 的协方差矩阵 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 做特征值分解推导出来的</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=A_%7Bm%2Cn%7D%3DU_%7Bm%2Cm%7D%5CLambda_%7Bm%2Cn%7DV%5ET_%7Bn%2Cn%7D+%5Capprox+U_%7Bm%2Ck%7D%5CLambda_%7Bk%2Ck%7DV%5ET_%7Bk%2Cn%7D+%5C%5C" alt="[公式]"></p>
<p>其中：U V 都是正交矩阵，有 <img src="https://www.zhihu.com/equation?tex=U%5ETU%3DI_m%2C+V%5ETV%3DI_n" alt="[公式]"> 。这里的约等于是因为 <img src="https://www.zhihu.com/equation?tex=%5CLambda" alt="[公式]"> 中有 n 个奇异值，但是由于排在后面的很多接近 0，所以我们可以仅保留比较大的 k 个奇异值。</p>
<p><img src="https://www.zhihu.com/equation?tex=A%5ETA%3D%28U+%5CLambda+V%5ET%29%5ETU+%5CLambda+V%5ET+%3DV+%5CLambda%5ET+U%5ETU+%5CLambda+V%5ET++%3D+V%5CLambda%5E2+V%5ET+%5C%5C+AA%5ET%3DU+%5CLambda+V%5ET%28U+%5CLambda+V%5ET%29%5ET+%3DU+%5CLambda+V%5ETV+%5CLambda%5ET+U%5ET+%3D+U%5CLambda%5E2+U%5ET++%5C%5C" alt="[公式]"></p>
<p>所以，V U 两个矩阵分别是 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 的特征向量，中间的矩阵对角线的元素是 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=AA%5ET" alt="[公式]"> 的特征值。我们也很容易看出 A 的奇异值和 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 的特征值之间的关系。</p>
<p>PCA 需要对协方差矩阵 <img src="https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5ET" alt="[公式]"> 。进行特征值分解； SVD 也是对 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 进行特征值分解。如果取 <img src="https://www.zhihu.com/equation?tex=A%3D%5Cfrac%7BX%5ET%7D%7B%5Csqrt%7Bm%7D%7D" alt="[公式]"> 则两者基本等价。所以 PCA 问题可以转换成 SVD 求解。</p>
<p><strong>==而实际上 Sklearn 的 PCA 就是用 SVD 进行求解的==</strong>，原因有以下几点：</p>
<ol>
<li>当样本维度很高时，协方差矩阵计算太慢；</li>
<li>方阵特征值分解计算效率不高；</li>
<li><strong>==SVD 除了特征值分解这种求解方式外，还有更高效更准确的迭代求解方式，避免了 <img src="https://www.zhihu.com/equation?tex=A%5ETA" alt="[公式]"> 的计算；==</strong></li>
<li><strong>其实 PCA 与 SVD 的右奇异向量的压缩效果相同</strong>。</li>
</ol>
<blockquote>
<ol>
<li>《机器学习》周志华</li>
<li><a href="https://link.zhihu.com/?target=https%3A//blog.codinglabs.org/articles/pca-tutorial.html">PCA 的数学原理</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm">Singular Value Decomposition (SVD) tutorial</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">机器学习中的数学（4）——线性判别分析（LDA）, 主成分分析（PCA）</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//my.oschina.net/findbill/blog/535044">从SVD到PCA——奇妙的数学游戏</a></li>
<li>scikit-learn：降维算法PCA和SVD <a href="https://blog.csdn.net/HHG20171226/article/details/102981822">https://blog.csdn.net/HHG20171226/article/details/102981822</a></li>
</ol>
</blockquote>
<h2><span id="二-线性判别分析lda监督">二、线性判别分析（LDA）【监督】</span></h2><blockquote>
<p>  ==<strong>“投影后类内方差最小，类间方差最大”</strong>==</p>
<ul>
<li><a href="https://blog.csdn.net/liuweiyuxiang/article/details/78874106">https://blog.csdn.net/liuweiyuxiang/article/details/78874106</a></li>
</ul>
</blockquote>
<h3><span id="21-概念">2.1 概念</span></h3><p><strong>线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。</strong></p>
<p><strong>LDA分类思想简单总结如下：</strong></p>
<ol>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。</li>
<li>对于训练数据，设法将多维数据投影到一条直线上，<strong>同类数据的投影点尽可能接近，异类数据点尽可能远离</strong>。</li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</li>
</ol>
<p><strong><font color="red"> 如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</font></strong></p>
<p>假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="image-20220526135646769.png" alt="image-20220526135646769"></p>
<p> 从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。 以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3><span id="22-原理">2.2 原理</span></h3><p> LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(<a href="http://en.wikipedia.org/wiki/Linear_classifier">Linear Classifier</a>)：因为LDA是一种线性分类器。对于<strong>K-分类的一个分类问题，会有K个线性函数</strong>：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455464493.png" alt="image"></p>
<p>当满足条件：对于所有的j，都有Yk &gt; Yj,的时候，我们就说x属于类别k。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的就是所属的分类了。</p>
<p>上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455475507.gif" alt="clip_image002" style="zoom:67%;"></p>
<p>红色的方形的点为0类的原始点、蓝色的方形点为1类的原始点，经过原点的那条线就是投影的直线，从图上可以清楚的看到，红色的点和蓝色的点被<strong>原点</strong>明显的分开了，这个数据只是随便画的，如果在高维的情况下，看起来会更好一点。下面我来推导一下二分类LDA问题的公式：假设用来区分二分类的直线（投影函数)为：</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455471885.png" alt="image"></p>
<p> <strong>LDA分类的一个目标是使得==不同类别==之间的距离越远越好，==同一类别==之中的距离越近越好</strong>，所以我们需要定义几个关键的值。</p>
<ul>
<li><p><strong>==类别i的原始中心点为==</strong>：（Di表示属于类别i的点)</p>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455478264.png" alt="image"></p>
</li>
<li><p>类别i投影后的中心点为：</p>
</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455488231.png" alt="image"></p>
<ul>
<li><strong>==衡量类别i投影后，类别点之间的分散程度（方差）为==</strong>：</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455487326.png" alt="image"></p>
<ul>
<li><strong>==最终我们可以得到一个下面的公式，表示LDA投影到w后的损失函数==</strong>：</li>
</ul>
<p><img src="https://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101081455484785.png" alt="image"></p>
<p>我们<strong>分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。</strong>分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。</p>
<p> 我们定义一个<strong>投影前的==各类别分散程度的矩阵==</strong>，这个矩阵看起来有一点麻烦，其实意思是，如果某一个分类的<strong>输入点集Di里面的点距离这个分类的中心店mi越近</strong>，则Si里面元素的值就越小，如果分类的点都紧紧地围绕着mi，则Si里面的元素值越更接近0.</p>
<script type="math/tex; mode=display">
S_{i}=\sum_{x \in D_{i}}\left(x-m_{i}\right)\left(x-m_{i}\right)^{T}</script><p>带入 $\mathrm{Si}$, 将 $\mathrm{J}(\mathrm{w})$ 分母化为:</p>
<p>$\tilde{s}<em>{i}=\sum</em>{x \in D<em>{i}}\left(w^{T} x-w^{T} m</em>{i}\right)^{2}=\sum<em>{x \in D</em>{i}} w^{T}\left(x-m<em>{i}\right)\left(x-m</em>{i}\right)^{T} w=w^{T} S_{i} w$</p>
<script type="math/tex; mode=display">{\tilde{S_{1}}}^{2}+{\tilde{S_{2}}}^{2}=w^{T}\left(S_{1}+S_{2}\right) w=w^{T} S_{w} w</script><p>同样的将 $\mathrm{J}(\mathrm{w})$ 分子化为:</p>
<script type="math/tex; mode=display">
\left|\widetilde{m_{1}}-\widetilde{m_{2}}\right|^{2}=w^{T}\left(m_{1}-m_{2}\right)\left(m_{1}-m_{2}\right)^{T} w=w^{T} S_{B} w</script><p>这样<strong>损失函数</strong>可以化成下面的形式:</p>
<script type="math/tex; mode=display">
J(w)=\frac{w^{T} S_{B} w}{w^{T} S_{w} w}</script><p>这样就可以用最喜欢的<strong>==拉格朗日乘子法==</strong>了, 但是还有一个问题, 如果分子、分母是都可以取任意值的, 那就会 使得有无穷解, 我们将分母限制为长度为 1, 并作为拉格朗日乘子法的限制条件, 带入得到:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&c(w)=w^{T} S_{B} w-\lambda\left(w^{T} S_{w} w-1\right) \\
&\Rightarrow \frac{d c}{d w}=2 S_{B} w-2 \lambda S_{w} w=0 \\
&\Rightarrow S_{B} w=\lambda S_{w} w
\end{aligned}</script><p><strong>==这样的式子就是一个求特征值的问题了。==</strong><br>对于 $N(N&gt;2)$ 分类的问题, 我就直接写出下面的结论了:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&S_{W}=\sum_{i=1}^{c} S_{i} \\
&S_{B}=\sum_{i=1}^{c} n_{i}\left(m_{i}-m\right)\left(m_{i}-m\right)^{T} \\
&S_{B} w_{i}=\lambda S_{w} w_{i}
\end{aligned}</script><p>这同样是一个求特征值的问题，我们求出的第i大的特征向量，就是对应的Wi了。</p>
<blockquote>
<p>  这里想多谈谈特征值，特征值在纯数学、量子力学、固体力学、计算机等等领域都有广泛的应用，特征值表示的是矩阵的性质，当我们取到矩阵的前N个最大的特征值的时候，我们可以说提取到的矩阵主要的成分（这个和之后的PCA相关，但是不是完全一样的概念）。在机器学习领域，不少的地方都要用到特征值的计算，比如说图像识别、pagerank、LDA、还有之后将会提到的PCA等等。</p>
</blockquote>
<p><strong>优缺点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>优缺点</th>
<th>简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>1. 可以使用类别的先验知识； 2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr>
<td>缺点</td>
<td>1. LDA不适合对非高斯分布样本进行降维； 2. <strong>LDA降维最多降到分类数k-1维</strong>； 3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好； 4. LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="三-t-sne-高维数据可视化">三、t-SNE 高维数据可视化</span></h2><blockquote>
<p>  高维数据可视化之t-SNE算法🌈:<a href="https://zhuanlan.zhihu.com/p/57937096">https://zhuanlan.zhihu.com/p/57937096</a></p>
</blockquote>
<p><strong>T-SNE算法是用于可视化的算法中效果最好的算法之一</strong>，相信大家也对T-SNE算法略有耳闻，本文参考T-SNE作者<strong>Laurens van der Maaten</strong>给出的源代码自己实现T-SNE算法代码，以此来加深对T-SNE的理解。先简单介绍一下T-SNE算法，T-SNE将数据点变换映射到概率分布上。</p>
<h4><span id="31-t-sne数据算法的目的">3.1 t-SNE数据算法的目的</span></h4><p><strong>主要是将数据从高维数据转到低维数据，并在低维空间里也保持其在高维空间里所携带的信息（比如高维空间里有的清晰的分布特征，转到低维度时也依然存在）。</strong></p>
<p><strong>==t-SNE将欧氏距离距离转换为条件概率，来表达点与点之间的相似度，再优化两个分布之间的距离-KL散度，从而保证点与点之间的分布概率不变。==</strong></p>
<h4><span id="32-sne原理">3.2 SNE原理</span></h4><p>$S N E$ 是<strong>通过仿射变换将数据点映射到相应概率分布上</strong>, 主要包括下面两个步骤:</p>
<ol>
<li>通过在高维空间中构建数据点之间的概率分布 $P$, 使得相似的数据点有更高的概率被选择, 而 不相似的数据点有较低的概率被选择;</li>
<li>然后在低维空间里重构这些点的概率分布 $Q$, 使得这两个概率分布尽可能相似。</li>
</ol>
<p>令输入空间是 $X \in \mathbb{R}^{n}$, 输出空间为 $Y \in \mathbb{R}^{t}(t \ll n)$ 。不妨假设含有 $m$ 个样本数据 $\left{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\right}$, 其中 $x^{(i)} \in X$, 降维后的数据为 $\left{y^{(1)}, y^{(2)}, \cdots, y^{(m)}\right}, y^{(i)} \in Y$ 。 $S N E$ 是<strong>先将欧几里得距离转化为条件概率来表达点与点之间的相似度</strong>, 即首先是计算条件概 率 $p<em>{j \mid i}$, 其正比于 $x^{(i)}$ 和 $x^{(j)}$ 之间的相似度, $p</em>{j \mid i}$ 的计算公式为:</p>
<script type="math/tex; mode=display">
p_{j \mid i}=\frac{\exp \left(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^{2}}{2 \sigma_{i}^{2}}\right)}{\sum_{k \neq i} \exp \left(-\frac{\left\|x^{(i)}-x^{(k)}\right\|^{2}}{2 \sigma_{i}^{2}}\right)}</script><p>在这里引入了一个参数 $\sigma<em>{i}$, 对于不同的数据点 $x^{(i)}$ 取值亦不相同, 因为我们关注的是不同数据 点两两之间的相似度, 故可设置 $p</em>{i \mid i}=0$ 。对于低维度下的数据点 $y^{(i)}$, 通过条件概率 $q<em>{j \mid i}$ 来 刻画 $y^{(i)}$ 与 $y^{(j)}$ 之间的相似度, $q</em>{j \mid i}$ 的计算公式为:</p>
<script type="math/tex; mode=display">
q_{j \mid i}=\frac{\exp \left(-\left\|y^{(i)}-y^{(j)}\right\|^{2}\right)}{\sum_{k \neq i} \exp \left(-\left\|y^{(i)}-y^{(k)}\right\|^{2}\right)}</script><p>同理, 设置 $q<em>{i \mid i}=0$ 。<br>如果降维的效果比较好, 局部特征保留完整, 那么有 $p</em>{i \mid j}=q_{i \mid j}$ 成立, 因此通过优化两个分布之 间的 <strong>$K L$ 散度构造出的损失函数为</strong>:</p>
<script type="math/tex; mode=display">
C\left(y^{(i)}\right)=\sum_{i} K L\left(P_{i} \| Q_{i}\right)=\sum_{i} \sum_{j} p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}}</script><p>这里的 $P<em>{i}$ 表示在给定高维数据点 $x^{(i)}$ 时, 其他所有数据点的条件概率分布; $Q</em>{i}$ 则表示在给定 低维数据点 $y^{(i)}$ 时, 其他所有数据点的条件概率分布。从损失函数可以看出, 当 $p<em>{j \mid i}$ 较大 $q</em>{j \mid i}$ 较小时, 惩罚较高; 而 $p<em>{j \mid i}$ 较小 $q</em>{j \mid i}$ 较大时, 惩罚较低。换句话说就是高维空间中两个数据点距 离较近时, 若映射到低维空间后距离较远, 那么将得到一个很高的惩罚; 反之, 高维空间中两个数 据点距离较远时, 若映射到低维空间距离较近, 将得到一个很低的惩罚值。也就是说, <strong>$S N E$ 的 损失函数更关注于局部特征, 而忽视了全局结构</strong>。</p>
<h4><span id="33-目标函数求解">3.3 目标函数求解</span></h4><h4><span id="34-对称性-sne">3.4 对称性-SNE</span></h4><p><strong>优化 <img src="https://www.zhihu.com/equation?tex=KL%28P%5CVert+Q%29" alt="[公式]"> 的一种替换思路是使用联合概率分布来替换条件概率分布</strong>，即 <img src="https://www.zhihu.com/equation?tex=P" alt="[公式]"> 是高维空间里数据点的联合概率分布， <img src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 是低维空间里数据点的联合概率分布，此时的损失函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=C%28y%5E%7B%28i%29%7D%29%3DKL%28P%5CVert+Q%29%3D%5Csum%5Climits_i%5Csum%5Climits_jp_%7Bij%7D%5Clog%5Cdfrac%7Bp_%7Bij%7D%7D%7Bq_%7Bij%7D%7D%5C%5C" alt="[公式]"></p>
<p>同样的 <img src="https://www.zhihu.com/equation?tex=p_%7Bii%7D%3Dq_%7Bii%7D%3D0" alt="[公式]"> ，这种改进下的 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 称为对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> ，因为它的先验假设为对 <img src="https://www.zhihu.com/equation?tex=%5Cforall+i" alt="[公式]"> 有 <img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3Dp_%7Bji%7D%2Cq_%7Bij%7D%3Dq_%7Bji%7D" alt="[公式]"> 成立，故概率分布可以改写成：</p>
<p><img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3D%5Cdfrac%7Bexp%28-%5Cfrac%7B%5CVert+x%5E%7B%28i%29%7D-x%5E%7B%28j%29%7D%5CVert%5E2%7D%7B2%5Csigma%5E2%7D%29%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7Dexp%28-%5Cfrac%7B%5CVert+x%5E%7B%28k%29%7D-x%5E%7B%28l%29%7D+%5CVert%5E2%7D%7B2%5Csigma%5E2%7D%29%7D%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+q_%7Bij%7D%3D%5Cdfrac%7Bexp%28-%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7Dexp%28-%5CVert+y%5E%7B%28k%29%7D-y%5E%7B%28l%29%7D+%5CVert%5E2%29%7D%5C%5C" alt="[公式]"></p>
<p>这种改进方法使得表达式简洁很多，但是容易受到异常点数据的影响，为了解决这个问题通过对联合概率分布定义修正为： <img src="https://www.zhihu.com/equation?tex=p_%7Bij%7D%3D%5Cfrac%7Bp_%7Bj%7Ci%7D%2Bp_%7Bi%7Cj%7D%7D%7B2%7D" alt="[公式]"> ，这保证了 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_jp_%7Bij%7D+%5Cgt+%5Cfrac%7B1%7D%7B2m%7D" alt="[公式]"> ，使得每个点对于损失函数都会有贡献。对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 最大的优点是简化了梯度计算，梯度公式改写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpartial+C%28y%5E%7B%28i%29%7D%29%7D%7B%5Cpartial+y%5E%7B%28i%29%7D%7D%3D4%5Csum%5Climits_j%28p_%7Bij%7D-q_%7Bij%7D%29%28y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D%29%5C%5C" alt="[公式]"></p>
<p>研究表明，对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 的效果差不多，有时甚至更好一点。</p>
<h4><span id="35-t-sne">3.5 t-SNE</span></h4><p><img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7DSNE" alt="[公式]"> 在对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 的改进是，首先<strong>通过在高维空间中使用高斯分布将距离转换为概率分布，然后在低维空间中，使用更加偏重长尾分布的方式来将距离转换为概率分布</strong>，使得高维度空间中的中低等距离在映射后能够有一个较大的距离。</p>
<p><img src="https://pic4.zhimg.com/80/v2-928a3ada308128f26b719d510a728fbb_1440w.jpg" alt="img"></p>
<p>从图中可以看到，在没有异常点时， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布与高斯分布的拟合结果基本一致。而在第二张图中，出现了部分异常点，由于高斯分布的尾部较低，对异常点比较敏感，为了照顾这些异常点，高斯分布的拟合结果偏离了大多数样本所在位置，方差也较大。<strong>相比之下， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布的尾部较高，对异常点不敏感，保证了其鲁棒性，因此拟合结果更为合理，较好的捕获了数据的全局特征。</strong></p>
<p>使用 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布替换高斯分布之后 <img src="https://www.zhihu.com/equation?tex=q_%7Bij+%7D" alt="[公式]"> 的变化如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=q_%7Bij%7D%3D%5Cdfrac%7B%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%7D%7B%5Csum%5Climits_%7Bk%5Cneq+l%7D%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%7D%5C%5C" alt="[公式]"></p>
<p>此外，随着自由度的逐渐增大， <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布的密度函数逐渐接近标准正态分布，因此在计算梯度方面会简单很多，优化后的梯度公式如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpartial+C%28y%5E%7B%28i%29%7D%29%7D%7B%5Cpartial+y%5E%7B%28i%29%7D%7D%3D4%5Csum%5Climits_%7Bj%7D%28p_%7Bij%7D-q_%7Bij%7D%29%28y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D%29%281%2B%5CVert+y%5E%7B%28i%29%7D-y%5E%7B%28j%29%7D+%5CVert%5E2%29%5E%7B-1%7D%5C%5C" alt="[公式]"></p>
<p>总的来说， <img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7DSNE" alt="[公式]"> 的梯度更新具有以下两个优势：</p>
<ul>
<li><strong>对于低维空间中不相似的数据点，用一个较小的距离会产生较大的梯度让这些数据点排斥开来</strong>；</li>
<li><strong>这种排斥又不会无限大，因此避免了不相似的数据点距离太远</strong>。</li>
</ul>
<h4><span id="总结">总结：</span></h4><p><img src="https://www.zhihu.com/equation?tex=t%5Ctext%7B-%7D+SNE" alt="[公式]"> 算法其实就是在 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 算法的基础上增加了两个改进：</p>
<ul>
<li>把 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 修正为对称 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> ，提高了计算效率，效果稍有提升；</li>
<li>在低维空间中采用了 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 分布替换原来的高斯分布，解决了高维空间映射到低维空间所产生的拥挤问题，优化了 <img src="https://www.zhihu.com/equation?tex=SNE" alt="[公式]"> 过于关注局部特征而忽略全局特征的问题 。</li>
</ul>
<h2><span id="四-autoencoder">四、AutoEncoder</span></h2><blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/80377698">【全】一文带你了解自编码器（<em>AutoEncoder</em>）</a></li>
</ul>
</blockquote>
<p>理解为：（下图）高维数据（左测蓝色）通过某种网络变成低位数据（中间红色）后，又经过某种网络变回高维数据（右侧蓝色）。数据经过该模型前后没有变化，而中间的低维数据完全具有输入输出的高维数据的全部信息，所以可以用<a href="https://www.zhihu.com/search?q=低维数据&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;157482881&quot;}">低维数据</a>代表高维数据。</p>
<p>之所以叫AutoEncoder，而不叫AutoEncoderDecoder，是因为训练好之后只有encoder部分有用，<a href="https://www.zhihu.com/search?q=decoder&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;157482881&quot;}">decoder</a>部分就不用了。</p>
<p><img src="https://pic2.zhimg.com/v2-47f6429e5ffb379205ba0bcb0db399d1_b.jpg" alt="img"></p>
<p>进入深度学习的思路之后，编码的网络是开放的，可以自由设计的。一个思路是端到端，将网络的输出设为你任务要的结果（如类别、序列等），<strong>过程中的某层嵌入都可以作为降维的低维结果</strong>。当然，这种低维结果其实是模型的副产品，因为任务已经解决。比如bert模型得到（中文的）字嵌入。</p>
<h4><span id="优点">优点：</span></h4><ul>
<li>能够学习到非线性特性</li>
<li>降低数据维度</li>
</ul>
<h4><span id="缺点">缺点：</span></h4><ul>
<li>训练的<strong>计算成本高</strong></li>
<li><strong>可解释性较差</strong></li>
<li>背后的数学知识复杂</li>
<li>容易产生<strong>过度拟合</strong>的问题，尽管可以通过引入正则化策略缓解</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>降维与度量学习</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（11）LGB*</title>
    <url>/posts/YFRZTY/</url>
    <content><![CDATA[<h2><span id="参考链接">参考链接</span></h2><ul>
<li><p><strong>XGBoost官方文档</strong>：<a href="https://xgboost.readthedocs.io/en/latest/index.html">https://xgboost.readthedocs.io/en/latest/index.html</a></p>
</li>
<li><p>LightGBM算法梳理：<a href="https://zhuanlan.zhihu.com/p/78293497">https://zhuanlan.zhihu.com/p/78293497</a></p>
</li>
<li><p>详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）：<a href="https://zhuanlan.zhihu.com/p/366234433">https://zhuanlan.zhihu.com/p/366234433</a></p>
</li>
<li><p>【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）：<a href="https://zhuanlan.zhihu.com/p/87885678">https://zhuanlan.zhihu.com/p/87885678</a></p>
</li>
<li><p>xgboost面试题整理: <a href="https://xiaomindog.github.io/2021/06/22/xgb-qa/">https://xiaomindog.github.io/2021/06/22/xgb-qa/</a></p>
</li>
</ul>
<h2><span id="机器学习决策树下xgboost-lightgbm">【机器学习】决策树（下）——XGBoost、LightGBM</span></h2><p><img src="https://pic2.zhimg.com/80/v2-358e4bfce928d0460bd5e8b4cab8f715_1440w.jpg" alt="img"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Boosting 算法</th>
<th>GBDT</th>
<th>XGBoost</th>
<th>LightGBM</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong><img src="image-20220315210756470.png" alt="image-20220315210756470" style="zoom:25%;"></td>
<td>回归树、梯度迭代、缩减（Shrinkage）;<strong>GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0</strong></td>
<td><strong>二阶导数、线性分类器、正则化</strong>、缩减、<strong>列抽样、并行化</strong></td>
<td><strong>更快的训练速度和更低的内存使用</strong></td>
</tr>
<tr>
<td>目标函数</td>
<td><img src="image-20220315213233284.png" alt="image-20220315213233284" style="zoom: 25%;"></td>
<td><img src="image-20220315213503054.png" alt="image-20220315213503054" style="zoom: 67%;"><img src="image-20220315213608526.png" alt="image-20220315213608526" style="zoom: 33%;"></td>
<td>同上</td>
</tr>
<tr>
<td>损失函数</td>
<td>最小均方损失函数、<strong>绝对损失或者 Huber 损失函数</strong></td>
<td>【线性】最小均方损失函数、==sigmod和softmax==</td>
<td><strong>复杂度模型</strong>：<img src="image-20220315215849417.png" alt="image-20220315215849417" style="zoom: 25%;"></td>
</tr>
<tr>
<td>基模型</td>
<td>CART模型</td>
<td>CART模型/ ==回归模型==</td>
<td>CART模型/ ==回归模型==</td>
</tr>
<tr>
<td>抽样算法</td>
<td>无</td>
<td><strong>列抽样</strong>：借鉴了<strong>随机森林</strong>的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</td>
<td><strong>单边梯度抽样算法；</strong>根据样本梯度来对梯度小的这边样本进行采样，一部分大梯度和随机分布</td>
</tr>
<tr>
<td><strong>切分点算法</strong></td>
<td>CART模型</td>
<td><strong>预排序</strong>、<strong>贪心算法</strong>、<strong>近似算法（</strong>加权分位数缩略图<strong>）</strong></td>
<td><strong>直方图算法</strong>：内存消耗降低，计算代价减少；（不需要记录特征到样本的索引）</td>
</tr>
<tr>
<td><strong>缺失值算法</strong></td>
<td>CART模型</td>
<td><strong>稀疏感知算法</strong>：选择增益最大的枚举项即为最优<strong>缺省方向</strong>。【<strong><font color="red"> 稀疏数据优化不足</font></strong>】【<strong>gblinear 补0</strong>】</td>
<td><strong>互斥特征捆绑算法</strong>：<strong>互斥</strong>指的是一些特征很少同时出现非0值。<strong>稀疏感知算法</strong>；【<strong>gblinear 补0</strong>】</td>
</tr>
<tr>
<td><strong>建树策略</strong></td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Leaf-wise</strong>：每次分裂增益最大的叶子节点，直到达到停止条件。</td>
</tr>
<tr>
<td><strong>正则化</strong></td>
<td>无</td>
<td>L1 和 L2 正则化项</td>
<td>L1 和 L2 正则化项</td>
</tr>
<tr>
<td><strong>Shrinkage（缩减）</strong></td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>类别特征优化</td>
<td>无</td>
<td>无</td>
<td><strong>类别特征最优分割</strong>：<strong>many-vs-many</strong></td>
</tr>
<tr>
<td>并行化设计</td>
<td>无</td>
<td><strong>块结构设计</strong>、</td>
<td><strong>特征并行</strong>、 <strong>数据并行</strong>、<strong>投票并行</strong></td>
</tr>
<tr>
<td>==缓存优化==</td>
<td>无</td>
<td>为每个线程分配一个连续的缓存区、<strong>“核外”块计算</strong></td>
<td>1、所有的特征都采用相同的方法获得梯度；2、其次，因为不需要存储特征到样本的索引，降低了存储消耗</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>对异常点敏感；</td>
<td><strong>预排序</strong>：仍需要遍历数据集；==不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。==</td>
<td><strong>内存更小</strong>： 索引值、特征值边bin、互斥特征捆绑; <strong>速度更快</strong>：遍历直方图；单边梯度算法过滤掉梯度小的样本；基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；特征并行、数据并行方法加速计算</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="一-lightgbm">一、LightGBM</span></h2><blockquote>
<ul>
<li>《<a href="https://www.zhihu.com/search?q=Lightgbm%3A+A+highly+efficient+gradient+boosting+decision+tree&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">Lightgbm: A highly efficient gradient boosting decision tree</a>》</li>
<li>《A communication-efficient <a href="https://www.zhihu.com/search?q=parallel+algorithm+for+decision+tree&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">parallel algorithm for decision tree</a>》</li>
</ul>
</blockquote>
<p>LightGBM 由微软提出，主要用于解决 GDBT 在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中。从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有<strong>训练速度快、内存占用低</strong>的特点。下图分别显示了 XGBoost、XGBoost_hist（利用梯度直方图的 XGBoost） 和 LightGBM 三者之间针对不同数据集情况下的内存和训练时间的对比：</p>
<p><img src="https://pic1.zhimg.com/80/v2-e015e3c4018f44787d74a47c9e0cd040_1440w.jpg" alt="img"></p>
<p>那么 LightGBM 到底如何做到<strong>更快的训练速度和更低的内存</strong>使用的呢？</p>
<h4><span id="lightgbm-为了解决这些问题提出了以下几点解决方案"><strong><font color="red"> LightGBM 为了解决这些问题提出了以下几点解决方案：</font></strong></span></h4><ol>
<li><p><strong>【减小内存、最优分类点】直方图算法</strong>；【特征离散化 + 内存占用 + 方差减少】</p>
</li>
<li><p><strong>【样本维度】 单边梯度抽样算法</strong>；【<strong>根据样本梯度来对梯度小的这边样本进行采样</strong>，一部分大梯度和随机分布】</p>
<blockquote>
<p>  <strong>一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。</strong></p>
</blockquote>
</li>
<li><p><strong>【特征维度】互斥特征捆绑算法</strong>；【特征稀疏行优化 +分箱 】</p>
</li>
<li><p><strong>【分裂算法】基于最大深度的 Leaf-wise 的垂直生长算法</strong>；【深度限制的最大分裂收益的叶子】</p>
</li>
<li><p><strong>类别特征最优分割</strong>；</p>
</li>
<li><p><strong>特征并行和数据并行</strong>；</p>
</li>
<li><p><strong>缓存优化。</strong></p>
</li>
</ol>
<h3><span id="11-数学原理">1.1 数学原理</span></h3><h3><span id="111-直方图算法"><strong>1.1.1 直方图算法</strong></span></h3><h4><span id="1-直方图算法"><strong>(1) 直方图算法</strong></span></h4><p><strong><font color="red"> 直方图算法的基本思想是将连续的特征离散化为 k （默认256 1字节）个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。</font></strong></p>
<p>我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）：</p>
<ul>
<li><strong>内存占用更小：</strong>XGBoost 需要用 <strong>32 位的浮点数去存储特征值，并用 32 位的整形去存储排序索引</strong>，而 LightGBM 只需要用 8 位去存储直方图，<strong>相当于减少了 1/8</strong>；</li>
<li><strong>计算代价更小：</strong>计算特征分裂增益时，XGBoost 需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k 次，直接将时间复杂度从代价是O( feature <em> <strong>distinct_values_of_the_feature</strong>); 而 histogram 只需要计算 bins次, 代价是( feature </em> <strong>bins</strong>)。<strong>distinct_values_of_the_feature &gt;&gt; bins</strong></li>
</ul>
<p><img src="image-20220625171413733.png" alt="image-20220625171413733"></p>
<ol>
<li><strong>直方图优化算法需要在训练前预先把特征值转化为bin value</strong>，也就是对每个特征的取值做个分段函数，将所有样本在该特征上的取值划分到某一段（bin）中。最终把特征取值从连续值转化成了离散值。需要注意得是：feature value对应的bin value在整个训练过程中是不会改变的。</li>
<li><strong>最外面的 for 循环表示的意思是对当前模型下所有的叶子节点处理</strong>，需要遍历所有的特征，来找到增益最大的特征及其划分值，以此来分裂该叶子节点。</li>
<li>在某个叶子上，第二个 for 循环就开始遍历所有的特征了。<strong>对于每个特征，首先为其创建一个直方图 (new Histogram() )</strong>。这个直方图存储了两类信息，分别是<strong><font color="red"> 每个bin中样本的梯度之和 $H[ f.bins[i] ].g$ </font></strong>，还有就是<strong>每个bin中样本数量</strong>$（H[f.bins[i]].n）$</li>
<li>第三个 for 循环遍历所有样本，累积上述的两类统计值到样本所属的bin中。即直方图的每个 bin 中包含了一定的样本，在此计算每个 bin 中的样本的梯度之和并对 bin 中的样本记数。</li>
<li>最后一个for循环, 遍历所有bin, 分别以当前bin作为分割点, 累加其左边的bin至当前bin的梯度和（ $\left.S<em>{L}\right)$ 以及样本数量 $\left(n</em>{L}\right)$, 并与父节点上的总梯度和 $\left(S<em>{p}\right)$ 以及总样本数量 $\left(n</em>{p}\right)$ 相减, 得到右边 所有bin的梯度和 $\left(S<em>{R}\right)$ 以及样本数量 $\left(n</em>{R}\right)$, 带入公式, 计算出增益, 在遍历过程中取最大的增 益, 以此时的特征和bin的特征值作为分裂节点的特征和分裂特征取值。</li>
</ol>
<h4><span id="2-源码分析">(2) 源码分析</span></h4><blockquote>
<p>  <a href="https://blog.csdn.net/anshuai_aw1/article/details/83040541">https://blog.csdn.net/anshuai_aw1/article/details/83040541</a></p>
<p>  <strong><font color="red"> 『我爱机器学习』集成学习（四）LightGBM</font></strong>：<a href="https://www.hrwhisper.me/machine-learning-lightgbm/">https://www.hrwhisper.me/machine-learning-lightgbm/</a></p>
</blockquote>
<p>问题一：<strong>如何将特征映射到bin呢？即如何分桶？对于连续特征和类别特征分别怎么样处理？</strong></p>
<p>问题二：<strong>如何构建直方图？直方图算法累加的g是什么？难道没有二阶导数h吗？</strong></p>
<h4><span id="特征分桶">特征分桶：</span></h4><blockquote>
<p>  <strong>特征分桶的源码</strong>在<strong>bin.cpp</strong>文件和<strong>bin.h</strong>文件中。由于LGBM可以处理类别特征，因此对连续特征和类别特征的处理方式是不一样的。</p>
</blockquote>
<h4><span id="连续特征">连续特征:</span></h4><p>在<strong>bin.cpp</strong>中，我们可以看到<strong>GreedyFindBin</strong>函数和<strong>FindBinWithZeroAsOneBin</strong>函数，这两个函数得到了数值型特征取值（负数，0，正数）的各个bin的切分点，即bin_upper_bound。</p>
<h4><span id="greedyfindbin-数值型根据特征不同取值的个数划分类别型">GreedyFindBin: 数值型根据特征不同取值的个数划分，类别型？？</span></h4><ul>
<li><em>特征取值计数的数组</em>、<em>特征的不同的取值的数组</em>、<em>特征有多少个不同的取值</em></li>
<li><strong>bin_upper_bound就是记录桶分界的数组</strong></li>
<li>特征取值数比max_bin数量少，直接取distinct_values的中点放置</li>
<li>特征取值数比max_bin来得大，说明几个特征值要共用一个bin<ul>
<li>如果一个特征值的数目比mean_bin_size大，那么这些特征需要单独一个bin</li>
<li>剩下的特征取值的样本数平均每个剩下的bin：mean size for one bin</li>
</ul>
</li>
</ul>
<h4><span id="构建直方图">构建直方图：</span></h4><p>给定一个特征的值，我们现在已经可以转化为对应的bin了。现在我们就可以构建直方图了。</p>
<h4><span id="constructhistogram"><strong>ConstructHistogram</strong>：</span></h4><ul>
<li><strong>累加了一阶、二阶梯度和还有==个数==</strong></li>
<li>当然还有其它的版本，当is_constant_hessianis_constant_hessian为true的时候是不用二阶梯度的</li>
</ul>
<h4><span id="寻找最优切分点-缺失值处理-gain和xgb一样">寻找最优切分点 : 缺失值处理 + Gain和XGB一样</span></h4><h4><span id="3直方图算法优点"><strong><font color="red"> （3）直方图算法优点：</font></strong></span></h4><ul>
<li><p><strong>内存消耗降低</strong>。预排序算法需要的内存约是训练数据的两倍（2x样本数x维度x4Bytes），它需要用32位浮点来保存特征值，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位的存储空间。对于 直方图算法，则只需要(1x样本数x维 度x1Bytes)的内存消耗，仅为预排序算法的1/8。因为直方图算法仅需要存储特征的 bin 值(离散化后的数值)，不需要原始的特征值，也不用排序，而bin值用8位整型存储就足够了。</p>
</li>
<li><p><strong>算法时间复杂度大大降低</strong>。决策树算法在节点分裂时有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，在“寻找分割点”时，预排序算法对于深度为$k$的树的时间复杂度：对特征所有取值的排序为$O(NlogN)$，$N$为样本点数目，若有$D$维特征，则$O(kDNlogN)$，而直方图算法需要$O(kD \times bin)$ (bin是histogram 的横轴的数量，一般远小于样本数量$N$)。</p>
</li>
<li><p><strong>直方图算法还可以进一步加速</strong>【<strong>==两个维度==</strong>】。一个容易观察到的现象：<strong>一个叶子节点的直方图可以直接由父节点的直方图和兄弟节点的直方图做差得到（分裂时左右集合）</strong>。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的$k$个bin。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>
<p><img src="https://pic1.zhimg.com/80/v2-86919e4fc187a11fe3fdb72780709c98_1440w.jpg" alt="img" style="zoom:67%;"></p>
</li>
<li><p><strong>缓存优化</strong>：上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。<strong><font color="red"> LightGBM 所使用直方图算法对 Cache 天生友好所有的特征都采用相同的方法获得梯度，构建直方图时bins字典同步记录一阶导、二阶导和个数，大大提高了缓存命中</font></strong>；因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</p>
</li>
<li><p><strong>数据并行优化</strong>，用 histgoram 可以大幅降低通信代价。用 pre-sorted 算法的话，通信代价是非常大的（几乎是没办法用的）。所以 xgoobst 在并行的时候也使用 histogram 进行通信。</p>
</li>
</ul>
<h4><span id="4直方图算法缺点">（4）直方图算法缺点：</span></h4><p><strong>当然，直方图算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。</strong>但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；<strong>较粗的分割点也有正则化的效果，可以有效地防止过拟合</strong>；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（GradientBoosting）的框架下没有太大的影响。</p>
<h3><span id="112-单边梯度抽样算法"><strong>1.1.2 单边梯度抽样算法</strong></span></h3><font color="red"> **直方图算法仍有优化的空间**，建立直方图的复杂度为O(**feature × data**)，如果能**降低特征数**或者**降低样本数**，训练的时间会大大减少。</font>

<p><strong>GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法</strong>（Gradient-based One-Side Sampling, GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算锅中只需关注梯度高的样本，极大的减少了计算量。</p>
<ol>
<li>根据<strong>梯度的绝对值</strong>将样本进行<strong>降序</strong>排序</li>
<li>选择前a×100%的样本，这些样本称为A</li>
<li>剩下的数据(1−a)×100的数据中，随机抽取b×100%的数据，这些样本称为B</li>
<li>在计算增益的时候，放大样本B中的梯度 (1−a)/b 倍</li>
<li>关于g，在具体的实现中是一阶梯度和二阶梯度的乘积，见Github的实现（LightGBM/src/boosting/goss.hpp）</li>
</ol>
<blockquote>
<p>  a%（大梯度）+ (1-a)/ b * b % 的大梯度</p>
</blockquote>
<p><strong>使用GOSS进行采样, 使得训练算法更加的关注没有充分训练(under-trained)的样本, 并且只会稍微的改变原有的数据分布</strong>。原有的在特征值为 $\mathrm{d}$ 处分数据带来的增益可以定义为：</p>
<script type="math/tex; mode=display">
V_{j \mid O}(d)=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in O: x_{i j} \leq d} g_{i}\right)^{2}}{n_{l \mid O}^{j}(d)}+\frac{\left(\sum_{x_{i} \in O: x_{i j}>d} g_{i}\right)^{2}}{n_{r \mid O}^{j}(d)}\right)</script><p>其中:</p>
<ul>
<li>O为在决策树待分裂节点的训练集</li>
<li>$n<em>{o}=\sum I\left(x</em>{i} \in O\right)$</li>
<li>$n<em>{l \mid O}^{j}(d)=\sum I\left[x</em>{i} \in O: x<em>{i j} \leq d\right]$ and $n</em>{r \mid O}^{j}(d)=\sum I\left[x<em>{i} \in O: x</em>{i j}&gt;d\right]$</li>
</ul>
<p><strong>而使用GOSS后, 增益定义为：</strong></p>
<script type="math/tex; mode=display">
V_{j \mid O}(d)=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in A_{l}} g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}} g_{i}\right)^{2}}{n_{l}^{j}(d)}+\frac{\left(\sum_{x_{i} \in A_{r}} g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}} g_{r}\right)^{2}}{n_{r}^{j}(d)}\right)</script><p>其中:</p>
<ul>
<li>$A<em>{l}=\left{x</em>{i} \in A: x<em>{i j} \leq d\right}, A</em>{r}=\left{x<em>{i} \in A: x</em>{i j}&gt;d\right}$</li>
<li>$B<em>{l}=\left{x</em>{i} \in B: x<em>{i j} \leq d\right}, B</em>{r}=\left{x<em>{i} \in B: x</em>{i j}&gt;d\right}$</li>
</ul>
<p>实验表明，该做法并没有降低模型性能，反而还有一定提升。究其原因，应该是采样也会增加弱学习器的多样性，从而潜在地提升了模型的泛化能力，稍微有点像深度学习的dropout。</p>
<h3><span id="113-互斥特征捆绑算法冲突小的特征可能与多个特征包组合特征集合"><strong>1.1.3 互斥特征捆绑算法</strong>【冲突小的特征可能与多个特征包组合】[特征集合]</span></h3><blockquote>
<p>  <strong>==互斥==指的是一些特征很少同时出现非0值</strong>【<strong>类似one-hot特征</strong>】</p>
<p>  <a href="https://zhuanlan.zhihu.com/p/366234433">详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）</a></p>
</blockquote>
<p><strong><font color="red"> 互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行合并，则可以降低特征数量。</font></strong>高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。</p>
<p><strong>1）首先介绍如何判定哪些特征应该捆绑在一起？</strong></p>
<p>EFB算法采用<strong>构图（build graph）</strong>的思想，将特征作为节点，不互斥的特征之间进行连边，然后从图中找出所有的捆绑特征集合。其实学过数据结构里的图算法就了解过，这个问题基本就是<a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%9B%BE%E7%9D%80%E8%89%B2%E9%97%AE%E9%A2%98/8928655%3Ffr%3Daladdin">图着色问题</a>。但是图着色问题是一个<strong>NP-hard问题</strong>，不可能在多项式时间里找到最优解。</p>
<p>因此<strong>EFB采用了一种近似的贪心策略解决办法。它允许特征之间存在少数的样本点并不互斥</strong>（比如某些对应的样本点之间并不同时为非0），并设置一个最大冲突阈值 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 。我们选择合适的 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 值，可以在准确度和训练效率上获得很好的trade-off（均衡)。</p>
<p>==<strong>下面给出EFB的特征捆绑的贪心策略流程：</strong>==</p>
<blockquote>
<p>  （1）将特征作为图的顶点，对于<strong>不互斥的特征进行相连</strong>（存在同时不为0的样本），特征同时不为0的样本个数作为边的权重；<br>  （2）根据顶点的度对特征进行降序排序，度越大表明特征与其他特征的冲突越大（越不太可能与其他特征进行捆绑）；【<strong>入度排序，转化为非零值个数排序</strong>】<br>  （3）设置<strong>最大冲突阈值K</strong>，外层循环先对每一个上述排序好的特征，遍历已有的特征捆绑簇，如果发现该特征加入到该特征簇中的冲突数不会超过最大阈值K，则将该特征加入到该簇中。否则新建一个特征簇，将该特征加入到新建的簇中。</p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-743681d9fd6cebee11f0dcc607f2f687_1440w.jpg" alt="img" style="zoom: 33%;"></p>
<p>上面时间的复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28n%5E2%29" alt="[公式]"> ，n为特征的数量，时间其实主要花费在建图上面，两两特征计算互斥程度的时间较长（2层for循环）。对于百万级别的特征数量来说，该复杂度仍是<strong>不可行的</strong>。==为了提高效率，可以不再构建图，将特征直接按照非零值个数排序，将特征<strong>非零值个数</strong>类比为节点的度（即冲突程度)，因为更多的非零值更容易引起冲突。只是改进了排序策略，不再构建图，下面的for循环是一样的。==</p>
<p><strong>2）如何将特征捆绑簇里面的所有特征捆绑（合并）为一个特征？</strong>【<strong>直方图偏移</strong>】</p>
<p>如何进行合并，最关键的是如何能将原始特征从合并好的特征进行分离出来。EFB采用的是加入一个<strong>偏移常量</strong>（offset）来解决。</p>
<blockquote>
<p>  举个例子，我们绑定两个特征A和B，A取值范围为[0, 10)，B取值范围为[0, 20)。则我们可以加入一个偏移常量10，即将B的取值范围变为[10,30），然后合并后的特征范围就是[0, 30)，并且能很好的分离出原始特征~</p>
</blockquote>
<p>因为lgb中<strong>直方图算法</strong>对特征值进行了<strong>分桶</strong>（bin）操作，导致合并互斥特征变得更为简单。从上面伪码看到偏移常量offset直接对每个特征桶的数量累加就行，然后放入偏移常数数组（binRanges）中。</p>
<h3><span id="114-带深度限制的-leaf-wise-算法"><strong>1.1.4 带深度限制的 Leaf-wise 算法</strong></span></h3><h4><span id="level-wise">Level-wise</span></h4><p>大多数GBDT框架使用的按层生长 (level-wise) 的决策树生长策略，Level-wise遍历一次数据可以同时分裂同一层的叶子，容易进行<strong>多线程优化</strong>，也好<strong>控制模型复杂度，不容易过拟合</strong>。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
<h4><span id="leaf-wise">Leaf-wise</span></h4><p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<p><img src="https://pic2.zhimg.com/80/v2-76f2f27dd24fc452a9a65003e5cdd305_1440w.jpg" alt="img"></p>
<h3><span id="115-lightgbm类别特征最优分割">==<strong>1.1.5 LightGBM类别特征最优分割</strong>==</span></h3><blockquote>
<p>  LightGBM中只需要提前将类别映射到非负整数即可(<code>integer-encoded categorical features</code>)</p>
</blockquote>
<p><strong>我们知道，LightGBM可以直接处理类别特征，而不需要对类别特征做额外的one-hot encoding。那么LGB是如何实现的呢？</strong></p>
<p>类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足, LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。<strong>LightGBM 采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分</strong>。假设某维 特征有 k 个类别，则有 <img src="https://www.zhihu.com/equation?tex=2%5E%7B%28k-1%29%7D-1" alt="[公式]"> 种可能, 时间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%5Cleft%282%5E%7Bk%7D%5Cright%29%2C" alt="[公式]"> LightGBM 基于 Fisher的 《<a href="https://www.zhihu.com/search?q=On+Grouping+For+Maximum+Homogeneity&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">On Grouping For Maximum Homogeneity</a>》论文实现了 O(klogk) 的<a href="https://www.zhihu.com/search?q=时间复杂度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;165627712&quot;}">时间复杂度</a>。</p>
<p><strong>算法流程如下图所示</strong>，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序; 然后按照排序的结果依次枚举最优分割点。从下图可以看到, <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BS+u+m%28y%29%7D%7B%5Coperatorname%7BCount%7D%28y%29%7D" alt="[公式]"> 为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。</p>
<p><img src="https://pic1.zhimg.com/v2-0f1b7024e9da8f09c75b7f8e436a5d24_b.jpg" alt="img" style="zoom:67%;"></p>
<p><strong>在Expo数据集上的实验结果表明，相比0/1展开的方法，使用LightGBM支持的类别特征可以使训练速度加速8倍，并且精度一致。</strong>更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。</p>
<h3><span id="12-工程实现-并行计算">1.2 工程实现 - 并行计算</span></h3><h3><span id="121-特征并行优化-最优划分点"><strong>1.2.1 特征并行</strong>【优化 最优划分点】</span></h3><p>传统的特征并行算法在于对数据进行垂直划分，然后使用<strong>不同机器找到不同特征的最优分裂点</strong>，<strong>基于通信整合得到最佳划分点</strong>，然后基于通信告知其他机器划分结果。==在本小节中，<strong>工作的节点称为worker</strong>==</p>
<h4><span id="传统">==<strong>传统：</strong>==</span></h4><ul>
<li>垂直划分数据<strong>（对特征划分）</strong>，<strong>不同的worker有不同的特征集</strong></li>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li><strong>具有全局最佳切分点的worker进行节点分裂，然后广播切分后的结果</strong>（<strong>左右子树的instance indices</strong>）</li>
<li>其它worker根据收到的instance indices也进行划分</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-b0d10c5cd832402e4503e2c1220f7376_r.jpg" alt="preview" style="zoom: 67%;"></p>
<p><strong>传统的特征并行方法有个很大的缺点</strong>：</p>
<ul>
<li><strong>需要告知每台机器最终划分结果，增加了额外的复杂度</strong>（因为对数据进行垂直划分，每台机器所含数据不同，划分结果需要通过通信告知）；</li>
<li>无法加速split的过程，该过程复杂度为O(#data)O(#data)，当数据量大的时候效率不高；</li>
</ul>
<h4><span id="lightgbm"><strong>==LightGBM==</strong></span></h4><p><strong>LightGBM 则不进行数据垂直划分，每台机器都有训练集完整数据</strong>，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。</p>
<ul>
<li>每个workers找到局部最佳的切分点{feature, threshold}</li>
<li>workers使用点对点通信，找到全局最佳切分点</li>
<li>每个worker根据全局最佳切分点进行节点分裂</li>
</ul>
<p>缺点：</p>
<ul>
<li>split过程的复杂度仍是O(#data)，当数据量大的时候效率不高</li>
<li><strong>每个worker保存所有数据，存储代价高</strong></li>
</ul>
<h3><span id="122-数据并行"><strong>1.2.2 数据并行</strong></span></h3><h4><span id="传统方法">传统方法：</span></h4><p>数据并行目标是并行化整个决策学习的过程：</p>
<ul>
<li>水平切分数据，<strong>不同的worker拥有部分数据</strong></li>
<li>每个worker根据本地数据构建局部直方图</li>
<li>合并所有的局部直方图得到全部直方图</li>
<li>根据全局直方图找到最优切分点并进行分裂</li>
</ul>
<p><img src="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-data-parallelization.png" alt="LightGBM-data-parallelization"></p>
<p>在第3步中，有两种合并的方式：</p>
<ul>
<li>采用点对点方式(point-to-point communication algorithm)进行通讯，每个worker通讯量为O(#machine∗#feature∗#bin)</li>
<li>采用collective communication algorithm(如“<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html">All Reduce</a>”)进行通讯（相当于有一个中心节点，通讯后在返回结果），每个worker的通讯量为O(2∗#feature∗#bin)</li>
</ul>
<h4><span id="lightgbm中的数据并行">LightGBM中的数据并行</span></h4><ol>
<li><strong>使用“Reduce Scatter”将不同worker的不同特征的直方图合并，然后workers在局部合并的直方图中找到局部最优划分，最后同步全局最优划分。</strong></li>
<li>前面提到过，可以通过直方图作差法得到兄弟节点的直方图，因此只需要通信一个节点的直方图。</li>
</ol>
<p>传统的数据并行策略主要为水平划分数据，然后本地构建直方图并整合成全局直方图，最后在全局直方图中找出最佳划分点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 <img src="https://www.zhihu.com/equation?tex=O%28%5C%23machine+%2A+%5C%23feature+%2A%5C%23bin+%29" alt="[公式]"> ；如果使用集成的通信，则通讯开销为 <img src="https://www.zhihu.com/equation?tex=O%282+%2A+%5C%23feature+%2A%5C%23bin+%29" alt="[公式]"> ，</p>
<p><strong>LightGBM 采用分散规约（Reduce scatter）的方式将直方图整合的任务分摊到不同机器上，从而降低通信代价，并通过直方图做差进一步降低不同机器间的通信。</strong></p>
<h3><span id="123-投票并行"><strong>1.2.3 投票并行</strong></span></h3><p>LightGBM采用一种称为<strong>PV-Tree</strong>的算法进行投票并行(Voting Parallel)，其实这本质上也是一种<strong>数据并行</strong>。PV-Tree和普通的决策树差不多，只是在寻找最优切分点上有所不同。</p>
<p>其算法伪代码描述如下：</p>
<p><img src="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-pv-tree.png" alt="LightGBM-pv-tree"></p>
<ol>
<li>水平切分数据，不同的worker拥有部分数据。</li>
<li>Local voting: <strong>每个worker构建直方图，找到top-k个最优的本地划分特征</strong></li>
<li>Global voting: <strong>中心节点聚合得到最优的top-2k个全局划分特征（top-2k是看对各个worker选择特征的个数进行计数，取最多的2k个）</strong></li>
<li><strong>Best Attribute Identification</strong>： <strong>中心节点向worker收集这top-2k个特征的直方图，并进行合并，然后计算得到全局的最优划分</strong></li>
<li>中心节点将全局最优划分广播给所有的worker，worker进行本地划分。</li>
</ol>
<p><img src="https://www.hrwhisper.me/images/machine-learning-lightgbm/LightGBM-voting-parallelization.png" alt="LightGBM-voting-parallelization"></p>
<p><strong>可以看出，PV-tree将原本需要#feature×#bin#feature×#bin 变为了2k×#bin2k×#bin，通信开销得到降低。此外，可以证明，当每个worker的数据足够多的时候，top-2k个中包含全局最佳切分点的概率非常高。</strong></p>
<h3><span id="124-缓存优化"><strong>1.2.4 缓存优化</strong></span></h3><p>上边说到 XGBoost 的预排序后的特征是通过索引给出的样本梯度的统计值，因其索引访问的结果并不连续，XGBoost 提出缓存访问优化算法进行改进。</p>
<p>而 LightGBM 所使用直方图算法对 Cache 天生友好：</p>
<ol>
<li>首先，<strong>所有的特征都采用相同的方法获得梯度</strong>（区别于不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中；</li>
<li>其次，因为<strong>不需要存储特征到样本的索引</strong>，降低了存储消耗，而且也不存在 Cache Miss的问题。</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成学习</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（17）KNN</title>
    <url>/posts/171MAZN/</url>
    <content><![CDATA[<h2><span id="1-什么是knnkd树-siftbbf算法">1. 什么是KNN【KD树 + SIFT+BBF算法】</span></h2><blockquote>
<p>  KNN与KD树：<a href="https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272">https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272</a></p>
<p>  【数学】kd 树算法之详细篇 - 椰了的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/23966698">https://zhuanlan.zhihu.com/p/23966698</a></p>
<p>  <strong>KNN是生成式模型还是判别式的</strong>，为什么？ - 风控算法小白的回答 - 知乎 <a href="https://www.zhihu.com/question/475072467/answer/2027766449">https://www.zhihu.com/question/475072467/answer/2027766449</a></p>
</blockquote>
<h3><span id="11-knn的通俗解释">1.1 KNN的通俗解释</span></h3><p>何谓K近邻算法，即K-Nearest Neighbor algorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。</p>
<p>用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，<strong>在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。</strong></p>
<p>​                                                                     <a href="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067"><img src="https://camo.githubusercontent.com/f0eabe33161ae7f977f082590a3690be147319df428ad1695c79127ad406729d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936333437323839353636302e6a7067" alt="img"></a></p>
<p>​                                                                <a href="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67"><img src="https://camo.githubusercontent.com/a8942eed547ab4f08494126b0d2e5480fdfc795252e4a34a956c2b959cb5c6b8/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353238333936363635343430333634362e706e67" alt="img"></a></p>
<p>如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），KNN就是解决这个问题的。</p>
<p>如果<strong>K=3</strong>，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>红色</strong>的三角形一类。</p>
<p>如果<strong>K=5</strong>，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于<strong>蓝色</strong>的正方形一类。</p>
<p><strong>于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</strong></p>
<h3><span id="12-近邻的距离度量">1.2 近邻的距离度量</span></h3><p>我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。</p>
<p><strong>有哪些距离度量的表示法</strong>(普及知识点，可以跳过)：</p>
<ol>
<li><p><strong>==欧氏距离==</strong>，最常见的两点之间或多点之间的距离表示法，又称之为<strong>欧几里得度量</strong>，它定义于欧几里得空间中，如点 x = (x1,…,xn) 和 y = (y1,…,yn) 之间的距离为：</p>
<p><a href="https://camo.githubusercontent.com/0699ac91c0bb89297e4cf418ad7d46cac175652d1a1de30fccc66a16a0eef254/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6428782c79293d2535437371727425374228785f312d795f3129253545322b28785f322d795f3229253545322b2e2e2e2b28785f6e2d795f6e29253545322537443d2535437371727425374225354373756d5f253742693d312537442535452537426e25374428785f692d795f692925354532253744"><img src="https://camo.githubusercontent.com/0699ac91c0bb89297e4cf418ad7d46cac175652d1a1de30fccc66a16a0eef254/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6428782c79293d2535437371727425374228785f312d795f3129253545322b28785f322d795f3229253545322b2e2e2e2b28785f6e2d795f6e29253545322537443d2535437371727425374225354373756d5f253742693d312537442535452537426e25374428785f692d795f692925354532253744" alt="img"></a></p>
<ul>
<li><p>二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：</p>
<p><a href="https://camo.githubusercontent.com/b21b48fd9bce0b9f47d16d8e5505a6b600a02e1032632df73a77d5ba43012e2a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f322925354532253744"><img src="https://camo.githubusercontent.com/b21b48fd9bce0b9f47d16d8e5505a6b600a02e1032632df73a77d5ba43012e2a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f322925354532253744" alt="img"></a></p>
</li>
<li><p>三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：</p>
<p><a href="https://camo.githubusercontent.com/ebf7fbc4dd6e6eb0d80576e7e5cd19d38ea7a8b9cd801680fc9b39f242367515/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f3229253545322b287a5f312d7a5f322925354532253744"><img src="https://camo.githubusercontent.com/ebf7fbc4dd6e6eb0d80576e7e5cd19d38ea7a8b9cd801680fc9b39f242367515/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228785f312d785f3229253545322b28795f312d795f3229253545322b287a5f312d7a5f322925354532253744" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：</p>
<p><a href="https://camo.githubusercontent.com/62f0123f5d52ed280b23c7972c60f635d52a02d0b4958bd5bf0a7c61328f3225/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374225354373756d5f2537426b3d312537442535452537426e25374428785f253742316b2537442d785f253742326b2537442925354532253744"><img src="https://camo.githubusercontent.com/62f0123f5d52ed280b23c7972c60f635d52a02d0b4958bd5bf0a7c61328f3225/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374225354373756d5f2537426b3d312537442535452537426e25374428785f253742316b2537442d785f253742326b2537442925354532253744" alt="img"></a></p>
<p>也可以用表示成向量运算的形式：</p>
<p><a href="https://camo.githubusercontent.com/8e7528374c22f456904f868c6619a94ba0e51180896315eb3b019f9d7b358742/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228612d622928612d622925354554253744"><img src="https://camo.githubusercontent.com/8e7528374c22f456904f868c6619a94ba0e51180896315eb3b019f9d7b358742/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d2535437371727425374228612d622928612d622925354554253744" alt="img"></a></p>
</li>
</ul>
</li>
<li><p><strong>曼哈顿距离</strong>，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在<strong>==欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和==</strong>。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为： <a href="https://camo.githubusercontent.com/eeacd6c593ac3eadb18eebfdaf9e27626a54b8f8bdb9f53bc11a90713e8b0bf8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253743785f312d785f322537432b253743795f312d795f32253743"><img src="https://camo.githubusercontent.com/eeacd6c593ac3eadb18eebfdaf9e27626a54b8f8bdb9f53bc11a90713e8b0bf8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253743785f312d785f322537432b253743795f312d795f32253743" alt="img"></a>，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。</p>
<p>通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，此即曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。</p>
<ul>
<li><p>二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>
<p><a href="https://camo.githubusercontent.com/c914feff78b9ccb3f920a67c8dba69b110d641abde5f2533f184a656780cb94d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d253743785f312d785f322537432b253743795f312d795f32253743"><img src="https://camo.githubusercontent.com/c914feff78b9ccb3f920a67c8dba69b110d641abde5f2533f184a656780cb94d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d253743785f312d785f322537432b253743795f312d795f32253743" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离</p>
<p><a href="https://camo.githubusercontent.com/eb4a219d2fbe50979339a7e1a83464ec27957562f106da7a3b08da8cc3394795/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d25354373756d5f2537426b3d312537442535452537426e253744253743785f253742316b2537442d785f253742326b253744253743"><img src="https://camo.githubusercontent.com/eb4a219d2fbe50979339a7e1a83464ec27957562f106da7a3b08da8cc3394795/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d25354373756d5f2537426b3d312537442535452537426e253744253743785f253742316b2537442d785f253742326b253744253743" alt="img"></a></p>
</li>
</ul>
</li>
<li><p><strong>切比雪夫距离</strong>，若二个向量或二个点p 、and q，其座标分别为Pi及qi，则两者之间的切比雪夫距离定义如下：</p>
<p><a href="https://camo.githubusercontent.com/3a305bb0cf8cdcd6a77f9e2e2cc1085542ae46f8c70c5b43c67be9728d3d99d6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686562797368657625374428702c71293d6d61785f6928253743705f692d715f6925374329"><img src="https://camo.githubusercontent.com/3a305bb0cf8cdcd6a77f9e2e2cc1085542ae46f8c70c5b43c67be9728d3d99d6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686562797368657625374428702c71293d6d61785f6928253743705f692d715f6925374329" alt="img"></a></p>
<p>这也等于以下Lp度量的极值： <a href="https://camo.githubusercontent.com/8fe6945bfb5795e12cfe28b72758c59a598e0c5a89b1ffb874f01cc8ff77a7ba/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d392d32345f32322d31392d34312e706e67"><img src="https://camo.githubusercontent.com/8fe6945bfb5795e12cfe28b72758c59a598e0c5a89b1ffb874f01cc8ff77a7ba/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d392d32345f32322d31392d34312e706e67" alt="img"></a>，因此切比雪夫距离也称为L∞度量。</p>
<p>以数学的观点来看，切比雪夫距离是由一致范数（uniform norm）（或称为上确界范数）所衍生的度量，也是超凸度量（injective metric space）的一种。</p>
<p>在平面几何中，若二点p及q的直角坐标系坐标为(x1,y1)及(x2,y2)，则切比雪夫距离为：</p>
<p><a href="https://camo.githubusercontent.com/2c68232279b3d8c857a5ebf5bcb91f6792abeb47daedeb22214391611680dd97/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686573732537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329"><img src="https://camo.githubusercontent.com/2c68232279b3d8c857a5ebf5bcb91f6792abeb47daedeb22214391611680dd97/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f445f25374243686573732537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329" alt="img"></a></p>
<p><strong>玩过国际象棋的朋友或许知道，国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？。你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。</strong></p>
<ul>
<li><p>二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离 ：</p>
<p><a href="https://camo.githubusercontent.com/9387d181258b2a722bcabf891beb2d616a2ec297f35bade87679099641f27f09/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329"><img src="https://camo.githubusercontent.com/9387d181258b2a722bcabf891beb2d616a2ec297f35bade87679099641f27f09/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d617828253743785f322d785f312537432c253743795f322d795f3125374329" alt="img"></a></p>
</li>
<li><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离：</p>
<p><a href="https://camo.githubusercontent.com/ea4a630eab19efb4c0270f3808dde70fec7bb5f21d37f76ab016316277fbb730/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d61785f6928253743785f25374231692537442d785f253742326925374425374329"><img src="https://camo.githubusercontent.com/ea4a630eab19efb4c0270f3808dde70fec7bb5f21d37f76ab016316277fbb730/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f645f25374231322537443d6d61785f6928253743785f25374231692537442d785f253742326925374425374329" alt="img"></a></p>
</li>
</ul>
</li>
</ol>
<p><strong>==简单说来，各种“距离”的应用场景简单概括为：==</strong></p>
<ul>
<li><strong>空间：欧氏距离</strong>，</li>
<li><strong>路径：曼哈顿距离，国际象棋国王：切比雪夫距离</strong>，</li>
<li>以上三种的统一形式:闵可夫斯基距离，</li>
<li>加权：标准化欧氏距离，</li>
<li>排除量纲和依存：马氏距离，</li>
<li>向量差距：夹角余弦，</li>
<li><strong>编码差别：汉明距离</strong>，</li>
<li>集合近似度：杰卡德类似系数与距离，</li>
<li>相关：相关系数与相关距离。</li>
</ul>
<h3><span id="13-k值选择">1.3 K值选择</span></h3><ol>
<li>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></li>
<li>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且<strong>K值的增大就意味着整体的模型变得简单。</strong></li>
<li>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</li>
</ol>
<p>在实际应用中，K值一般取一个比较小的数值，<strong>例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</strong></p>
<h3><span id="14-knn最近邻分类算法的过程">1.4 KNN最近邻分类算法的过程</span></h3><ol>
<li>计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前 k 个最小距离的样本；</li>
<li>根据这 k 个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<h2><span id="关于knn的一些问题">关于KNN的一些问题</span></h2><ol>
<li><p>在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用<strong>曼哈顿距离</strong>？</p>
<p><strong>答：</strong>我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向的运动。</p>
</li>
<li><p>KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?</p>
<p>答：极大的节约了时间成本．点线距离如果 &gt;　最小点，无需回溯上一层，如果&lt;,则再上一层寻找。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成学习</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（10）集成学习*</title>
    <url>/posts/12GKN6G/</url>
    <content><![CDATA[<h2><span id="机器学习决策树中random-forest-adaboost-gbdt">【机器学习】决策树（中）——Random Forest、Adaboost、GBDT</span></h2><blockquote>
<p>  <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble</a></p>
<p>  机器学习算法中GBDT与Adaboost的区别与联系是什么？ - Frankenstein的回答 - 知乎 <a href="https://www.zhihu.com/question/54626685/answer/140610056">https://www.zhihu.com/question/54626685/answer/140610056</a></p>
<p>  GBDT学习笔记 - 许辙的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/169382376">https://zhuanlan.zhihu.com/p/169382376</a></p>
<p>  GBDT - 王多鱼的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/38057220">https://zhuanlan.zhihu.com/p/38057220</a></p>
</blockquote>
<p><img src="https://pic2.zhimg.com/v2-1ac553d300784e8d158bcc686e7cf66d_1440w.jpg?source=172ae18b" alt="【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细）"></p>
<p>本文主要介绍基于集成学习的决策树，其主要通过不同学习框架生产基学习器，并综合所有基学习器的预测结果来改善单个基学习器的识别率和泛化性。</p>
<p>==模型的准确度可由偏差和方差共同决定：==</p>
<p>$\text { Error }=\text { bias }^{2}+\operatorname{var}+\xi$</p>
<p><strong>模型总体期望：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
E(F) &=E\left(\sum_{i}^{m} r_{i} f_{i}\right) \\
&=\sum_{i}^{m} r_{i} E\left(f_{i}\right)
\end{aligned}</script><p><strong>模型总体方差</strong>:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{Var}(F) &=\operatorname{Var}\left(\sum_{i}^{m} r_{i} f_{i}\right) \\
&=\sum_{i}^{m} \operatorname{Var}\left(r_{i} f_{i}\right)+\sum_{i \neq j}^{m} \operatorname{Cov}\left(r_{i} f_{i}, r_{j} f_{j}\right) \\
&=\sum_{i}^{m} r_{i}{ }^{2} \operatorname{Var}\left(f_{i}\right)+\sum_{i \neq j}^{m} \rho r_{i} r_{j} \sqrt{\operatorname{Var}\left(f_{i}\right)} \sqrt{\operatorname{Var}\left(f_{j}\right)} \\
&=m r^{2} \sigma^{2}+m(m-1) \rho r^{2} \sigma^{2} \\
&=m r^{2} \sigma^{2}(1-\rho)+m^{2} r^{2} \sigma^{2} \rho
\end{aligned}</script><div class="table-container">
<table>
<thead>
<tr>
<th>集成学习</th>
<th>Bagging</th>
<th>Boosting</th>
<th>Stacking</th>
</tr>
</thead>
<tbody>
<tr>
<td>思想</td>
<td>对训练集进行<strong>有放回抽样</strong>得到子训练集</td>
<td>基模型的训练是有<strong>顺序</strong>的，每个基模型都会在前一个基模型学习的基础上进行学习；基于贪心策略的前向加法</td>
<td><strong>预测值</strong>将作为训练样本的特征值，进行训练得到最终预测结果。</td>
</tr>
<tr>
<td>样本抽样</td>
<td>有放回地抽取数据集</td>
<td>训练集不变</td>
<td></td>
</tr>
<tr>
<td>样本权重</td>
<td>样本权重相等</td>
<td>不断调整样本的权重</td>
<td></td>
</tr>
<tr>
<td>优化目标</td>
<td>减小的是方差</td>
<td>减小的是偏差</td>
<td></td>
</tr>
<tr>
<td>基模型</td>
<td><strong>强模型（偏差低，方差高）</strong></td>
<td><strong>弱模型（偏差高，方差低）</strong>而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</td>
<td><strong>强模型（偏差低，方差高）</strong></td>
</tr>
<tr>
<td>相关性</td>
<td></td>
<td>对于 Boosting 来说，由于基模型共用同一套训练集，所以基模型间具有强相关性，故模型间的相关系数近似等于 1</td>
<td></td>
</tr>
<tr>
<td>模型偏差</td>
<td><strong>整体模型的偏差与基模型近似</strong>。(<script type="math/tex">\mu</script>)</td>
<td>基于贪心策略的前向加法，随着基模型数的增多，偏差减少。</td>
<td></td>
</tr>
<tr>
<td>模型方差</td>
<td>随着<strong>模型的增加可以降低整体模型的方差</strong>，故其基模型需要为强模型；(<script type="math/tex">\frac{\sigma^{2}(1-\rho)}{m}+\sigma^{2} \rho</script>)</td>
<td><strong>整体模型的方差与基模型近似</strong>（<script type="math/tex">\sigma^{2}</script>）</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="1-集成学习">1. 集成学习</span></h2><p>常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。三种集成学习框架在基学习器的产生和综合结果的方式上会有些区别，我们先做些简单的介绍。</p>
<h3><span id="11-bagging">1.1 Bagging</span></h3><p>Bagging 全称叫 <strong>Bootstrap aggregating</strong>，，==每个基学习器都会对训练集进行<strong>有放回抽样</strong>得到子训练集==，比较著名的采样法为 0.632 自助法（<strong>Bootstrap</strong>）。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是<strong>投票法</strong>，票数最多的类别为预测类别。</p>
<p><img src="https://pic1.zhimg.com/80/v2-a0a3cb02f629f3db360fc68b4c2153c0_1440w.jpg" alt="img"></p>
<h3><span id="12-boosting">1.2 Boosting</span></h3><p><strong>Boosting 训练过程为阶梯状，基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果</strong>，用的比较多的综合方式为加权法。</p>
<p><img src="https://pic3.zhimg.com/80/v2-3aab53d50ab65e11ad3c9e3decf895c2_1440w.jpg" alt="img"></p>
<h3><span id="13-stacking">1.3 Stacking</span></h3><p><strong>Stacking 是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行的预测，==其预测值将作为训练样本的特征值==，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终预测结果。</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-f6787a16c23950d129a7927269d5352a_1440w.jpg" alt="img"></p>
<p>==那么，为什么集成学习会好于单个学习器呢？原因可能有三：==</p>
<ul>
<li><p>训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；</p>
</li>
<li><p>假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；</p>
</li>
<li><p>可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。</p>
</li>
</ul>
<h3><span id="14-stacking-vs-神经网络">==<strong>1.4 Stacking</strong> vs <strong>神经网络</strong>==</span></h3><blockquote>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/32896968">https://zhuanlan.zhihu.com/p/32896968</a></p>
<p><strong>本文的核心观点是提供一种对于stacking的理解，即与神经网络对照来看。</strong>当然，在<a href="https://www.zhihu.com/question/59769987/answer/269367049">阿萨姆：为什么做stacking之后，准确率反而降低了？</a>中我已经说过stacking不是万能药，但往往很有效。通过与神经网络的对比，读者可以从另一个角度加深对stacking的理解。</p>
</li>
</ul>
</blockquote>
<h4><span id="141-stacking是一种表示学习representation-learning">1.4.1 Stacking是一种表示学习(representation learning)</span></h4><p><strong>表示学习指的是模型从原始数据中自动抽取有效特征的过程</strong>，比如深度学习就是一种表示学习的方法。关于表示学习的理解可以参考：<a href="https://www.zhihu.com/question/264417928/answer/283087276">阿萨姆：人工智能（AI）是如何处理数据的？</a></p>
<p>原始数据可能是杂乱无规律的。在stacking中，通过第一层的多个学习器后，有效的特征被学习出来了。从这个角度来看，stacking的第一层就是特征抽取的过程。在[1]的研究中，上排是未经stacking的数据，下排是经过stacking(多个无监督学习算法)处理后的数据，我们显著的发现红色和蓝色的数据在下排中分界更为明显。<strong>==数据经过了压缩处理。这个小例子说明了，有效的stacking可以对原始数据中的特征有效的抽取==</strong>。</p>
<p><img src="https://pic1.zhimg.com/80/v2-92ff83c7c6acc0dea6bc53ffe815e8bc_1440w.jpg" alt="img" style="zoom:80%;"></p>
<h4><span id="142-stacking和神经网络从某种角度看有异曲同工之妙神经网络也可以被看作是集成学习">1.4.2  <strong>Stacking和神经网络从某种角度看有异曲同工之妙，神经网络也可以被看作是集成学习</strong></span></h4><p>承接上一点，stacking的学习能力主要来自于对于特征的表示学习，这和神经网络的思路是一致的。这也是为什么我说“第一层”，“最后一层”。</p>
<p>而且神经网络也可以被看做是一种集成学习，主要取决于不同神经元、层对于不同特征的理解不同。从浅层到深层可以理解为一种从具体到抽象的过程。</p>
<p><strong>Stacking中的第一层可以等价于神经网络中的前 n-1层，而stacking中的最终分类层可以类比于神经网络中最后的输出层。</strong>不同点在于，<strong>stacking中不同的分类器通过异质来体现对于不同特征的表示</strong>，神经网络是从同质到异质的过程且有分布式表示的特点(distributed representation)。Stacking中应该也有分布式的特点，主要表现在多个分类器的结果并非完全不同，而有很大程度的相同之处。</p>
<p>但同时这也提出了一个挑战，多个分类器应该尽量在保证效果好的同时尽量不同，stacking集成学习框架的对于基分类器的两个要求：</p>
<ul>
<li>差异化(diversity)要大</li>
<li>准确性(accuracy)要高</li>
</ul>
<h4><span id="143-stacking-的输出层为什么用逻辑回归"><strong>1.4.3 Stacking 的输出层为什么用逻辑回归？</strong></span></h4><blockquote>
<p>  <strong>表示学习的过拟合问题</strong>：</p>
<ul>
<li>仅包含学习到的特征</li>
<li>交叉验证</li>
<li>简单模型：<strong>逻辑回归</strong></li>
</ul>
</blockquote>
<p>如果你看懂了上面的两点，你应该可以理解stacking的有效性主要来自于特征抽取。<strong>而表示学习中，如影随形的问题就是过拟合，试回想深度学习中的过拟合问题。</strong></p>
<p>在[3]中，周志华教授也重申了stacking在使用中的过拟合问题。因为第二层的特征来自于对于第一层数据的学习，那么第二层数据中的特征中不该包括原始特征，<strong>以降低过拟合的风险</strong>。举例：</p>
<ul>
<li>第二层数据特征：仅包含学习到的特征</li>
<li>第二层数据特征：包含学习到的特征 + 原始特征</li>
</ul>
<p>另一个例子是，stacking中一般都用交叉验证来避免过拟合，足可见这个问题的严重性。</p>
<p>为了降低过拟合的问题，第二层分类器应该是较为简单的分类器，广义线性如逻辑回归是一个不错的选择。<strong>在特征提取的过程中，我们已经使用了复杂的非线性变换，因此在输出层不需要复杂的分类器</strong>。这一点可以对比神经网络的激活函数或者输出层，都是很简单的函数，一点原因就是不需要复杂函数并能控制复杂度。</p>
<h4><span id="144-stacking是否需要多层第一层的分类器是否越多越好"><strong>1.4.4 Stacking是否需要多层？第一层的分类器是否越多越好？</strong></span></h4><p>通过以上分析，stacking的表示学习不是来自于多层堆叠的效果，而是<strong>来自于不同学习器对于不同特征的学习能力</strong>，并有效的结合起来。一般来看，2层对于stacking足够了。多层的stacking会面临更加复杂的过拟合问题，且收益有限。</p>
<p>第一层分类器的数量对于特征学习应该有所帮助，<strong>经验角度看越多的基分类器越好。即使有所重复和高依赖性，我们依然可以通过特征选择来处理</strong>，问题不大。</p>
<h2><span id="2-偏差与方差">2. 偏差与方差</span></h2><p>上节介绍了集成学习的基本概念，这节我们主要介绍下如何从偏差和方差的角度来理解集成学习。</p>
<h4><span id="21-集成学习的偏差与方差">2.1 集成学习的偏差与方差</span></h4><p><strong>==偏差（Bias）描述的是预测值和真实值之差==</strong>；<strong>==方差（Variance）描述的是预测值作为随机变量的离散程==度</strong>。放一场很经典的图：</p>
<p><img src="https://pic2.zhimg.com/80/v2-60c942f91d33d9dedf9dd2c7d482af5d_1440w.jpg" alt="img"></p>
<p><strong>模型</strong>的<strong>偏差</strong>与<strong>方差</strong></p>
<ul>
<li><strong>偏差：</strong>描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的 High Variance，点会很分散。低偏差对应的点都打在靶心附近，所以喵的很准，但不一定很稳；</li>
<li><strong>方差：</strong>描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图 High Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。</li>
</ul>
<p>我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型，<strong>但并不是所有集成学习框架中的基模型都是弱模型</strong>。<strong>Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting 中的基模型为弱模型（偏差高，方差低）</strong>。</p>
<h4><span id="22-bagging-的偏差与方差">2.2 Bagging 的偏差与方差</span></h4><ul>
<li><strong>整体模型的期望等于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。</strong></li>
<li><strong>整体模型的方差小于等于基模型的方差，当且仅当相关性为 1 时取等号，随着基模型数量增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。</strong>但是，模型的准确度一定会无限逼近于 1 吗？并不一定，当基模型数增加到一定程度时，方差公式第一项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。</li>
</ul>
<h4><span id="23-boosting-的偏差与方差">2.3 Boosting 的偏差与方差</span></h4><ul>
<li>整体模型的方差等于基模型的方差，如果基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting 框架中的基模型必须为弱模型。</li>
<li>此外 Boosting 框架中采用基于贪心策略的前向加法，整体模型的期望由基模型的期望累加而成，所以随着基模型数的增多，整体模型的期望值增加，整体模型的准确度提高。</li>
</ul>
<h4><span id="24-小结">2.4 小结</span></h4><ul>
<li>我们可以使用<strong>==模型的偏差和方差来近似描述模型的准确度==</strong>；</li>
<li>对于 Bagging 来说，整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型；</li>
<li>对于 Boosting 来说，整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</li>
</ul>
<h2><span id="3-random-forestbagging">3. Random Forest（Bagging）</span></h2><blockquote>
<ol>
<li>随机森林具有<strong>防止过拟合能力</strong>，精度比大多数单个算法要好；<ol>
<li>随机森林分类器可以<strong>处理缺失值</strong>；</li>
</ol>
</li>
<li><strong>==于有袋外数据(OOB)==，可以在模型生成过程中取得真实误差的无偏估计，且不损失训练数据量在训练过程中，能够检测到feature间的互相影响，且可以得出feature的重要性，具有一定参考意义；</strong><ol>
<li>每棵树可以独立、同时生成，容易做成<strong>并行化方法</strong>；</li>
</ol>
</li>
<li>具有一定的特征选择能力。</li>
</ol>
</blockquote>
<p><strong>Random Forest（随机森林），用随机的方式建立一个森林。RF 算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。</strong></p>
<p>对于分类问题，其输出的类别是由个别树输出的众数所决定的。在回归问题中，把每一棵决策树的输出进行平均得到最终的回归结果。</p>
<h3><span id="31-思想">3.1 思想</span></h3><p>Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：</p>
<ul>
<li><strong>样本随机：</strong>假设训练数据集共有 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 个对象的数据，从样本数据中采取有放回（<strong>Boostrap</strong>）随机抽取 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 个样本（因为是有放回抽取，有些数据可能被选中多次，有些数据可能不被选上)，每一次取出的样本不完全相同，这些样本组成了决策树的训练数据集；</li>
<li><strong>特征随机：</strong>假设每个样本数据都有 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 个特征，从所有特征中随机地选取 <img src="https://www.zhihu.com/equation?tex=k%28k%3C%3DK%29" alt="[公式]"> 个特征，选择最佳分割属性作为节点建立CART决策树，决策树成长期间 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 的大小始终不变（<strong>在Python中构造随机森林模型的时候，默认取特征的个数 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 的平方根，即 <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BK%7D" alt="[公式]"></strong> )；</li>
<li>重复前面的步骤，建立 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 棵CART树，这些树都要完全的成长且不被修剪，这些树形成了森林；</li>
<li>根据这些树的预测结果进行投票，决定样本的最后预测类别。（针对回归模型，是根据这些决策树模型的平均值来获取最终的结果）</li>
</ul>
<p>随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；<strong>==随机选择特征是指在每个节点在分裂过程中都是随机选择特征的==</strong>（区别与每棵树随机选择一批特征）。</p>
<blockquote>
<p>  这种随机性导致随机森林的==偏差会有稍微的增加==（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的==方差减小==，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。</p>
</blockquote>
<p>随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算==不剪枝也不会出现过拟合==。</p>
<h3><span id="32-处理缺失值的方法">3.2 处理缺失值的方法</span></h3><ul>
<li>方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是<strong>分类变量(categorical var)缺失，用众数补上</strong>，如果是<strong>连续型变量(numerical var)缺失，用中位数补</strong>。</li>
<li>方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用<strong>proximity矩阵进行加权平均的方法补缺失值</strong>。然后迭代4-6次，这个补缺失值的思想和KNN有些类似。</li>
</ul>
<h3><span id="33-优缺点">3.3 优缺点</span></h3><h4><span id="优点"><strong>优点</strong>：</span></h4><ol>
<li><strong>模型准确率高</strong>：随机森林既可以处理分类问题，也可以处理回归问题，即使存在部分数据缺失的情况，随机森林也能保持很高的分类精度。</li>
<li><strong>能够处理数量庞大的高维度的特征</strong>，且不需要进行降维（因为特征子集是随机选择的）；</li>
<li><strong>易于并行化</strong>，在大数据集上有很大的优势；</li>
<li><p><strong>可解释性</strong>：可以生成树状结构，判断各个特征的重要性；</p>
<blockquote>
<p>  在sklearn中，随机森林<strong>基于每棵树分裂时的GINI指数下降量</strong>来判断各个特征的重要性。但是这种方法存在一个问题：当特征是连续的时候或者是类别很多的离散特征时，该方法会将这些特征的重要性增加。</p>
<p>  解决方法：对特征编码，使得特征的取值数量相近。</p>
</blockquote>
</li>
<li><strong>对异常值、缺失值不敏感；</strong></li>
<li><strong>随机森林有袋外数据（OOB），因此不需要单独划分交叉验证集</strong>。</li>
</ol>
<h4><span id="缺点">缺点：</span></h4><ul>
<li>随机森林解决回归问题的效果不如分类问题；（因为它的预测不是天生连续的，在解决回归问题时，随机森林并不能为训练数据以外的对象给出答案）</li>
<li><strong>树之间的相关性越大，错误率就越大</strong>；</li>
<li><strong>当训练数据噪声较大时，容易产生过拟合现象。</strong></li>
</ul>
<h3><span id="34-基学习期的选择">==3.4 基学习期的选择？==</span></h3><h4><span id="为什么集成学习的基分类器通常是决策树还有什么">为什么集成学习的基分类器通常是决策树？还有什么？</span></h4><p>基分类器通常是决策树：样本权重、方便调节、随机性；</p>
<ul>
<li><strong>==决策树可以较方便地将样本权重整合到训练过程中，而不需要通过过采样来调整样本权重。==</strong></li>
<li>树的表达能力和泛化能力，<strong>方便调节</strong>（可以通过树的层数来调节）</li>
<li>样本的扰动对决策树的影响较大，<strong><font color="red"> 因此不同子样本集合生成的决策树基分类器随机性较大。这样的不稳定的分类器更适合作为基分类器。</font></strong>此外树节点分类时随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li>
</ul>
<h4><span id="可以将随机森林的基分类器由决策树替换成线性分类器或k-nn吗">可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></h4><p>Bagging主要好处是集成后的方差，比基分类器小。bagging采用的基分类，最好是本身对样本分布较为敏感。而线性分类器和K-NN都是较为稳定的分类器（参数模型？）甚至<strong>可能因为采样，而导致他们再训练中更难收敛，从而增大了集成分类器的偏差。</strong></p>
<h2><span id="4-adaboost-boosting-样本权重更新">4 Adaboost (Boosting) 样本权重更新?</span></h2><p>AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：<strong>前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的==足够小的错误率或达到预先指定的最大迭代次数==。</strong></p>
<h3><span id="41-思想">4.1 思想</span></h3><p><strong>==Adaboost 迭代算法有三步：==</strong></p>
<ul>
<li>初始化训练样本的权值分布，每个样本具有相同权重；</li>
<li>训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；</li>
<li>将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，<strong>加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重</strong>。</li>
</ul>
<h3><span id="42-细节">4.2 细节</span></h3><h5><span id="421-损失函数">==<strong>4.2.1 损失函数 ???</strong>==</span></h5><p>Adaboost 模型是<strong>加法模型</strong>，学习算法为<strong>前向分步学习算法</strong>，损失函数为<strong>指数函数的分类问题</strong>。</p>
<p><strong>加法模型</strong>：最终的强分类器是由若干个弱分类器<strong>加权平均</strong>得到的。</p>
<p><strong>前向分布学习算法</strong>：算法是通过一轮轮的弱学习器学习，<strong>利用前一个弱学习器的结果来更新后一个弱学习器的训练集权重</strong>。第 k 轮的强学习器为：</p>
<script type="math/tex; mode=display">
F_{k}(x)=\sum_{i=1}^{k} \alpha_{i} f_{i}(x)=F_{k-1}(x)+\alpha_{k} f_{k}(x)</script><p><strong>定义损失函数为 n 个样本的指数损失函数</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28y%2CF%29+%3D+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bn%7Dexp%28-y_iF_%7Bk%7D%28x_i%29%29++%5C%5C" alt="[公式]"></p>
<p>利用前向分布学习算法的关系可以得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++L%28y%2C+F%29+%26%3D+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B%28-y_i%29+%28F_%7Bk-1%7D%28x_i%29+%2B+%5Calpha_k+f_k%28x_i%29%29%5D++%5C%5C+%26%3D++%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B-y_i+F_%7Bk-1%7D%28x_i%29+-y_i++%5Calpha_k+f_k%28x_i%29%5D+%5C%5C+%26%3D++%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dexp%5B-y_i+F_%7Bk-1%7D%28x_i%29+%5D+exp%5B-y_i++%5Calpha_k+f_k%28x_i%29%5D+++%5Cend%7Balign%7D++%5C%5C" alt="[公式]"></p>
<p><strong>因为 <img src="https://www.zhihu.com/equation?tex=F_%7Bk-1%7D%28x%29" alt="[公式]"> 已知==，所以令 <img src="https://www.zhihu.com/equation?tex=w_%7Bk%2Ci%7D+%3D+exp%28-y_iF_%7Bk-1%7D%28x_i%29%29" alt="[公式]"> ，==随着每一轮迭代而将这个式子带入损失函数，损失函数转化为：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=L%28y%2C+F%28x%29%29+%3D%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bk%2Ci%7Dexp%5B-y_i%5Calpha_k+f_k%28x_i%29%5D+%5C%5C" alt="[公式]"></p>
<p>我们求 <img src="https://www.zhihu.com/equation?tex=f_k%28x%29" alt="[公式]"> ，可以得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=f_k%28x%29+%3Dargmin%5C%3B+%5Csum_%5Climits%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bk%2Ci%7DI%28y_i+%5Cneq+f_k%28x_i%29%29+%5C%5C" alt="[公式]"></p>
<p>将 <img src="https://www.zhihu.com/equation?tex=f_k%28x%29" alt="[公式]"> 带入损失函数，并对 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 求导，使其等于 0，则就得到了：</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Calpha_k+%3D+%5Cfrac%7B1%7D%7B2%7Dlog%5Cfrac%7B1-e_k%7D%7Be_k%7D++%5C%5C" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=e_k" alt="[公式]"> 即为我们前面的<strong>分类误差率</strong>。</p>
<p><img src="https://www.zhihu.com/equation?tex=+e_k+%3D+%5Cfrac%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7D%5E%7B%E2%80%99%7DI%28y_i+%5Cneq+f_k%28x_i%29%29%7D%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7D%5E%7B%E2%80%99%7D%7D+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dw_%7Bki%7DI%28y_i+%5Cneq+f_k%28x_i%29%29+%5C%5C" alt="[公式]"></p>
<p><strong>最后看样本权重的更新</strong>。利用 <img src="https://www.zhihu.com/equation?tex=F_%7Bk%7D%28x%29+%3D+F_%7Bk-1%7D%28x%29+%2B+%5Calpha_kf_k%28x%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=w_%7Bk%2B1%2Ci%7D%3Dw_%7Bk%2Ci%7Dexp%5B-y_i%5Calpha_kf_k%28x%2Ci%29%5D" alt="[公式]"> ，即可得：</p>
<p><img src="https://www.zhihu.com/equation?tex=w_%7Bk%2B1%2Ci%7D+%3D+w_%7Bki%7Dexp%5B-y_i%5Calpha_kf_k%28x_i%29%5D+%5C%5C" alt="[公式]"></p>
<p>这样就得到了样本权重更新公式。</p>
<h4><span id="422-正则化"><strong>4.2.2 正则化</strong></span></h4><p><strong>为了防止 Adaboost 过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长</strong>（learning rate）。对于前面的弱学习器的迭代,加上正则化项 <img src="https://www.zhihu.com/equation?tex=%5Cmu+" alt="[公式]"> 我们有：</p>
<script type="math/tex; mode=display">
F_{k}(x)=F_{k-1}(x)+\mu \alpha_{k} f_{k}(x)</script><p><img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 的取值范围为 <img src="https://www.zhihu.com/equation?tex=0%3C%5Cmu%5Cleq1" alt="[公式]"> 。对于同样的训练集学习效果，较小的 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>
<h3><span id="43-优缺点">4.3 优缺点</span></h3><p><strong>4.3.1 优点</strong></p>
<ol>
<li>分类精度高；</li>
<li>可以<strong>用各种回归分类模型来构建弱学习器，非常灵活</strong>；</li>
<li>不容易发生过拟合。</li>
</ol>
<p><strong>4.3.2 缺点</strong></p>
<ol>
<li>对异常点敏感，异常点会获得较高权重。</li>
</ol>
<h2><span id="5-gbdt">5. GBDT</span></h2><blockquote>
<p>  <a href="https://www.cnblogs.com/modifyrong/p/7744987.html">https://www.cnblogs.com/modifyrong/p/7744987.html</a></p>
</blockquote>
<p><strong>GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，该算法由多棵决策树组成，从名字中我们可以看出来它是属于 Boosting 策略。GBDT 是被公认的泛化能力较强的算法。</strong></p>
<h3><span id="51-思想">5.1 思想</span></h3><p><strong>GBDT是boosting算法的一种，按照boosting的思想，在GBDT算法的每一步，用一棵决策树去拟合当前学习器的残差，获得一个新的弱学习器。将这每一步的决策树组合起来，就得到了一个强学习器</strong>。GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和 Shrinkage（一个重要演变）</p>
<h4><span id="511-回归树regression-decision-tree"><strong>5.1.1 回归树（Regression Decision Tree）</strong></span></h4><p>如果认为 GBDT 由很多分类树那就大错特错了（虽然调整后也可以分类）。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。GBDT 的核心在于累加所有树的结果作为最终结果，所以 GBDT 中的树都是<strong>回归树</strong>，不是分类树，这一点相当重要。</p>
<p><strong><font color="red"> 回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。</font></strong></p>
<h4><span id="512-梯度迭代gradient-boosting"><strong>5.1.2 梯度迭代（Gradient Boosting）</strong></span></h4><p>上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，GBDT 的每一棵树都是以之前树得到的<strong>残差【负梯度】</strong>来更新目标值，这样每一棵树的<strong>值加起来</strong>即为 GBDT 的预测值。</p>
<p>模型的预测值可以表示为：</p>
<script type="math/tex; mode=display">
F_{k}(x)=\sum_{i=1}^{k} f_{i}(x)</script><p><img src="https://www.zhihu.com/equation?tex=f_%7Bi%7D%28x%29+" alt="[公式]"> 为<strong>基模型与其权重的乘积</strong>，模型的训练目标是使预测值 <img src="https://www.zhihu.com/equation?tex=F_k%28x%29" alt="[公式]"> 逼近真实值 y，也就是说要让每个基模型的预测值逼近各自要预测的部分真实值。<strong>==贪心==</strong>的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：</p>
<script type="math/tex; mode=display">
F_{k}(x)=F_{k-1}(x)+f_{k}(x)</script><p>其实很简单，其<strong>==残差其实是最小均方损失函数关于预测值的反向梯度(划重点)==</strong>：<strong>用负梯度的解作为样本新的真实值</strong>。基于残差 GBDT 容易对异常值敏感。</p>
<script type="math/tex; mode=display">
-\frac{\partial\left(\frac{1}{2}\left(y-F_{k}(x)\right)^{2}\right)}{\partial F_{k}(x)}=y-F_{k}(x)</script><p>很明显后续的模型会对第 4 个值关注过多，这不是一种好的现象，所以一般回归类的损失函数会用<strong>绝对损失或者 Huber 损失函数</strong>来代替平方损失函数。</p>
<script type="math/tex; mode=display">
L(y, F)=|y-F|</script><script type="math/tex; mode=display">
L(y, F)= \begin{cases}\frac{1}{2}(y-F)^{2} & |y-F| \leq \delta \\ \delta(|y-F|-\delta / 2) & |y-F|>\delta\end{cases}</script><p>GBDT 的 Boosting 不同于 Adaboost 的 Boosting，<strong>==GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0==</strong>，这样后面的树就能专注于那些被分错的样本。</p>
<blockquote>
<p>  <strong><font color="red"> 最后补充一点拟合残差的问题，无论损失函数是什么形式，每个决策树拟合的都是负梯度。只有当损失函数是均方损失时，负梯度刚好是残差</font></strong>，也就是说<strong>拟合残差只是针对均方损失的特例</strong>，并不能说GBDT的迭代的过程是拟合残差。</p>
</blockquote>
<h4><span id="513-缩减shrinkage添加权重-基数增大"><strong>5.1.3 缩减（Shrinkage）</strong>添加权重、基数增大</span></h4><blockquote>
<p>  <strong><font color="red"> gbdt中的步长和参数中的学习率作用是什么？详细讲一讲？</font></strong></p>
<ul>
<li>参数中的学习率用于梯度下降</li>
</ul>
</blockquote>
<p>Shrinkage 的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。</p>
<p>Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。<strong>本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大</strong>。</p>
<h3><span id="52-优缺点">5.2 优缺点</span></h3><h4><span id="优点"><strong>优点</strong></span></h4><ol>
<li>可以自动进行特征组合，拟合非线性数据；在稠密数据集上泛化能力和表达能力很好。</li>
<li>可以灵活处理各种类型的数据，不需要对数据预处理和归一化。</li>
<li>预测可以并行，计算数据很快。</li>
</ol>
<h4><span id="缺点"><strong>缺点</strong></span></h4><ol>
<li>对异常点敏感。</li>
</ol>
<h3><span id="53-gbdt-与-adaboost-的对比">5.3 GBDT 与 Adaboost 的对比</span></h3><h4><span id="相同"><strong>相同：</strong></span></h4><ol>
<li>都是 Boosting 家族成员，使用弱分类器；</li>
<li>都使用前向分布算法；</li>
</ol>
<h4><span id="不同"><strong>不同：</strong></span></h4><ol>
<li><strong>迭代思路不同</strong>：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；</li>
<li><strong>损失函数不同</strong>：AdaBoost 采用的是<strong>指数损失</strong>，GBDT 使用的是<strong>绝对损失</strong>或者 <strong>Huber 损失函数</strong>；</li>
</ol>
<h3><span id="54-gbdt算法用于分类问题-二分类1维-多分类k维one-vs-all"><strong><font color="red"> 5.4 GBDT算法用于分类问题、二分类1维、多分类k维（one vs all）</font></strong></span></h3><blockquote>
<p>  <a href="https://zhuanlan.zhihu.com/p/46445201">https://zhuanlan.zhihu.com/p/46445201</a></p>
</blockquote>
<p>将GBDT应用于回归问题，相对来说比较容易理解。因为<strong>回归问题的损失函数一般为平方差损失函数</strong>，这时的残差，恰好等于预测值与实际值之间的差值。每次拿一棵决策树去拟合这个差值，使得残差越来越小，这个过程还是比较intuitive的。</p>
<p><strong><font color="red"> 将GBDT用于分类问题，类似于逻辑回归、FM模型用于分类问题，其实是在用一个线性模型或者包含交叉项的非线性模型，去拟合所谓的对数几率 <img src="https://www.zhihu.com/equation?tex=%5Cln+%5Cfrac%7Bp%7D%7B1-p%7D" alt="[公式]"> 。</font></strong>而GBDT也是一样，只是用一系列的梯度提升树去拟合这个对数几率，实际上最终得到的是一系列CART回归树。其分类模型可以表达为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-+%5Csum_%7Bm%3D0%7D%5EM+h_m%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p>其中<img src="https://www.zhihu.com/equation?tex=h_m%28x%29" alt="[公式]"> 就是学习到的决策树。清楚了这一点之后，我们便可以参考逻辑回归，单样本 <img src="https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29" alt="[公式]"> 的损失函数可以表达为<strong>交叉熵</strong>：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss%28x_i%2C+y_i%29+%3D+-y_i+%5Clog+%5Chat%7By_i%7D+-+%281-y_i%29+%5Clog%281-%5Chat%7By_i%7D%29%5C%5C" alt="[公式]"></p>
<p>假设第k步迭代之后当前学习器为 <img src="https://www.zhihu.com/equation?tex=F%28x%29+%3D+%5Csum_%7Bm%3D0%7D%5Ek+h_m%28x%29" alt="[公式]"> ，将 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D" alt="[公式]"> 的表达式带入之后， 可将损失函数写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss%28x_i%2C+y_i%7CF%28x%29%29+%3D+y_i+%5Clog+%5Cleft%28+1%2Be%5E%7B-F%28x_i%29%7D+%5Cright%29+%2B+%281-y_i%29+%5Cleft%5B+F%28x_i%29+%2B+%5Clog+%5Cleft%28+1%2Be%5E%7B-F%28x_i%29%7D+%5Cright%29+%5Cright%5D%5C%5C" alt="[公式]"></p>
<p><strong>可以求得损失函数相对于当前学习器的负梯度为：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+F%28x%29%7D%7C_%7Bx_i%2Cy_i%7D+%3D+y_i+-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-F%28x_i%29%7D%7D%3D+y_i+-+%5Chat%7By_i%7D%5C%5C" alt="[公式]"></p>
<p>可以看到，同回归问题很类似，下一棵决策树的训练样本为： <img src="https://www.zhihu.com/equation?tex=%5C%7B+x_i%2C+y_i+-+%5Chat%7By_i%7D+%5C%7D_%7Bi%3D1%7D%5En" alt="[公式]"> ，其所需要拟合的残差为真实标签与预测概率之差。于是便有下面GBDT应用于二分类的算法：</p>
<ul>
<li><p><img src="https://www.zhihu.com/equation?tex=F_0%28x%29+%3D+h_0%28x%29+%3D+%5Clog+%5Cfrac%7Bp_1%7D%7B1-p_1%7D" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=p_1" alt="[公式]"> 是训练样本中y=1的比例，利用先验信息来初始化学习器</p>
</li>
<li><p>For <img src="https://www.zhihu.com/equation?tex=m+%3D+1%2C2%2C...%2CM" alt="[公式]"> ：</p>
<ul>
<li>计算 <img src="https://www.zhihu.com/equation?tex=g_i+%3D+%5Chat%7By_i%7D+-+y_i" alt="[公式]"> ，并使用训练集 <img src="https://www.zhihu.com/equation?tex=%5C%7B%28x_i%2C+-g_i%29%5C%7D_%7Bi%3D1%7D%5En" alt="[公式]"> <strong>训练一棵回归树</strong> <img src="https://www.zhihu.com/equation?tex=t_m%28x%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By_i%7D+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-F_%7Bm-1%7D%28x%29%7D%7D" alt="[公式]"></li>
<li>通过一维最小化损失函数找到树的最优权重： <img src="https://www.zhihu.com/equation?tex=%5Crho_m+%3D+%5Cmathop%7B%5Cmathrm%7Bargmin%7D%7D%5Climits_%7B%5Crho%7D+%5Csum_i+loss%28x_i%2C+y_i%7CF_%7Bm-1%7D%28x%29%2B%5Crho+t_m%28x%29%29" alt="[公式]"></li>
<li><strong>考虑shrinkage</strong>，可得这一轮迭代之后的学习器 <img src="https://www.zhihu.com/equation?tex=F_m%28x%29+%3D+F_%7Bm-1%7D%28x%29+%2B+%5Calpha+%5Crho_m+t_m%28x%29" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 为学习率</li>
</ul>
</li>
<li><p>得到最终学习器为： <img src="https://www.zhihu.com/equation?tex=F_M%28x%29" alt="[公式]"></p>
</li>
</ul>
<p>以上就是将GBDT应用于二分类问题的算法流程。类似地，对于多分类问题，则需要考虑以下softmax模型：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_1%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3D2%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_2%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=...+...%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%3Dk%7Cx%29+%3D+%5Cfrac%7Be%5E%7BF_k%28x%29%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Ek+e%5E%7BF_i%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><strong>其中 <img src="https://www.zhihu.com/equation?tex=F_1" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=F_k" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 个不同的tree ensemble。每一轮的训练实际上是训练了 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 棵树去拟合softmax的每一个分支模型的负梯度</strong>【one-hot中的一维】。softmax模型的单样本损失函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=loss+%3D+-%5Csum_%7Bi%3D1%7D%5Ek+y_i+%5Clog+P%28y_i%7Cx%29+%3D+-%5Csum_%7Bi%3D1%7D%5Ek+y_i+%5Clog+%5Cfrac%7Be%5E%7BF_i%28x%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5Ek+e%5E%7BF_j%28x%29%7D%7D%5C%5C" alt="[公式]"></p>
<p><strong>这里的 <img src="https://www.zhihu.com/equation?tex=y_i%5C+%28i%3D1...k%29" alt="[公式]"> 是样本label在k个类别上作one-hot编码之后的取值，只有一维为1，其余都是0。由以上表达式不难推导：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+F_q%7D+%3D+y_q+-+%5Cfrac%7Be%5E%7BF_q%28x%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5Ek+e%5E%7BF_j%28x%29%7D%7D+%3D+y_q+-+%5Chat%7By_q%7D%5C%5C" alt="[公式]"></p>
<p>可见，这k棵树同样是拟合了样本的真实标签与预测概率之差，与二分类的过程非常类似。</p>
<h2><span id="6-集成学习qampa">6 集成学习Q&amp;A</span></h2><h4><span id="61-为什么gbdt和随机森林稍好点都不太适用直接用高维稀疏特征训练集"><strong><font color="red"> 6.1 为什么gbdt和随机森林(稍好点)都不太适用直接用高维稀疏特征训练集？</font></strong></span></h4><h5><span id="原因">原因：</span></h5><p>gbdt这类boosting或者rf这些bagging集成分类器模型的算法，是典型的贪心算法，在当前节点总是选择对当前数据集来说最好的选择</p>
<p>一个6层100树的模型，要迭代2^(5 4 3 2 1 0)<em>100次<em>*,每次都根据当前节点最大熵或者最小误差分割来选择变量</em></em></p>
<p><strong>那么，高维稀疏数据集里很多“小而美”的数据就被丢弃了</strong>，因为它对当前节点来说不是最佳分割方案(比如，关联分析里，支持度很低置信度很高的特征)</p>
<p>但是高维数据集里面，对特定的样本数据是有很强预测能力的，比如你买叶酸，买某些小的孕妇用品品类，对应这些人6个月后买奶粉概率高达40%，但叶酸和孕妇用品销量太小了，用户量全网万分之一都不到，这种特征肯定是被树算法舍弃的，哪怕这些特征很多很多。。它仍是被冷落的份。。。</p>
<h5><span id="方法lightgbm-互斥捆绑算法">方法：【LightGBM 互斥捆绑算法】</span></h5><h5><span id="选择svm和lr这种能提供最佳分割平面的算法可能会更好">选择svm和lr这种能提供最佳分割平面的算法可能会更好；</span></h5><p>但如果top.特征已经能够贡献很大的信息量了，比如刚才孕妇的案例，你用了一个孕妇用品一级类目的浏览次数购买金额购买次数这样的更大更强的特征包含了这些高维特征的信息量，那可能gbdt会更好</p>
<p>实际情况的数据集是，在数据仓库里的清洗阶段，你可以选择把它做成高维的特征，也可以选择用算法把它做成低维的特征，一般有</p>
<p>1-在数据清洗阶段，或用类目升级(三级类目升级到二三级类目)范围升级的方式来做特征，避免直接清洗出来高维特征</p>
<p>2-在特征生成后，<strong>利用数据分析结论简单直接的用多个高维特征合并</strong>(<a href="https://www.zhihu.com/search?q=加减乘除&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A574865175}">加减乘除</a>逻辑判断都行，随你合并打分)的方式来做特征，前提你hold得住工作量判断量，但这个如果业务洞察力强效果有可能特别好</p>
<p>3-在特征工程的特征处理阶段，我们可以用<strong>PCA因子构建等降维算法做特征整合</strong>，对应训练集，也这么搞，到时候回归或预测的时候，就用这个因子或者主成分的值来做特征</p>
<h4><span id="61-为什么集成学习的基分类器通常是决策树还有什么">6.1 为什么集成学习的基分类器通常是决策树？还有什么？</span></h4><p>基分类器通常是决策树：样本权重、方便调节、随机性；</p>
<ul>
<li><strong>==决策树可以较方便地将样本权重整合到训练过程中，而不需要通过过采样来调整样本权重。==</strong></li>
<li>树的表达能力和泛化能力，<strong>方便调节</strong>（可以通过树的层数来调节）</li>
<li>样本的扰动对决策树的影响较大，<strong><font color="red"> 因此不同子样本集合生成的决策树基分类器随机性较大。这样的不稳定的分类器更适合作为基分类器。</font></strong>此外树节点分类时随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。</li>
</ul>
<h4><span id="62-可以将随机森林的基分类器由决策树替换成线性分类器或k-nn吗">6.2 可以将随机森林的基分类器，由决策树替换成线性分类器或K-NN吗？</span></h4><p>Bagging主要好处是集成后的方差，比基分类器小。bagging采用的基分类，最好是本身对样本分布较为敏感。而线性分类器和K-NN都是较为稳定的分类器（参数模型？）甚至可能因为采样，而导致他们再训练中更难收敛，从而增大了集成分类器的偏差。</p>
<h4><span id="63-为什么可以利用gbdt算法实现特征组合和筛选gbdtlr"><strong><font color="red"> 6.3 为什么可以利用GBDT算法实现特征组合和筛选？【GBDT+LR】</font></strong></span></h4><p>GBDT模型是有一组有序的树模型组合起来的，前面的树是由对大多数样本有明显区分度的特征分裂构建而成，经过前面的树，仍然存在少数残差较大的样本，后面的树主要由能对这些少数样本有区分度的特征分裂构建。优先选择对整体有区分度的特征，然后再选择对少数样本有区分度的特征，这样才更加合理，所以<strong>GBDT子树节点分裂是一个特征选择的过程，而子树的多层结构则对特征组合的过程，最终实现特征的组合和筛选。</strong></p>
<p><strong>GBDT+LR融合方案：</strong></p>
<p>（1）利用GBDT模型训练数据，最终得到一系列弱分类器的cart树。</p>
<p>（2）<strong>生成新的训练数据。将原训练数据重新输入GBDT模型，对于每一个样本，都会经过模型的一系列树，对于每棵树，将样本落到的叶子节点置为1，其他叶子为0，然后将叶子节点的数字从左至右的拼接起来，形成该棵树的特征向量，最后将所有树的特征向量拼接起来，形成新的数据特征，之后保留原样本标签形成新的训练数据。</strong></p>
<p>（3）将上一步得到的训练数据作为输入数据输入到LR模型中进行训练</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成学习</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（19）【TODO】FP-growth</title>
    <url>/posts/2406X25/</url>
    <content><![CDATA[<h2><span id="fp-growth">FP-growth</span></h2><blockquote>
<ul>
<li>数据挖掘随笔（一）频繁模式挖掘与关联规则挖掘以及Apriori算法（python实现）：<a href="https://zhuanlan.zhihu.com/p/410019734">https://zhuanlan.zhihu.com/p/410019734</a></li>
<li>数据挖掘随笔（二）FP-growth算法——一种用于频繁模式挖掘的模式增长方式(Python实现)：<a href="https://zhuanlan.zhihu.com/p/411594391">https://zhuanlan.zhihu.com/p/411594391</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>频繁项挖掘</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习（11）XGB*</title>
    <url>/posts/3HRFFWP/</url>
    <content><![CDATA[<h2><span id="参考链接">参考链接</span></h2><ul>
<li><p><strong>XGBoost官方文档</strong>：<a href="https://xgboost.readthedocs.io/en/latest/index.html">https://xgboost.readthedocs.io/en/latest/index.html</a></p>
</li>
<li><p>LightGBM算法梳理：<a href="https://zhuanlan.zhihu.com/p/78293497">https://zhuanlan.zhihu.com/p/78293497</a></p>
</li>
<li><p>详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）：<a href="https://zhuanlan.zhihu.com/p/366234433">https://zhuanlan.zhihu.com/p/366234433</a></p>
</li>
<li><p>【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）：<a href="https://zhuanlan.zhihu.com/p/87885678">https://zhuanlan.zhihu.com/p/87885678</a></p>
</li>
<li><p>xgboost面试题整理: <a href="https://xiaomindog.github.io/2021/06/22/xgb-qa/">https://xiaomindog.github.io/2021/06/22/xgb-qa/</a></p>
</li>
</ul>
<h2><span id="机器学习决策树下xgboost-lightgbm">【机器学习】决策树（下）——XGBoost、LightGBM</span></h2><p><img src="https://pic2.zhimg.com/80/v2-358e4bfce928d0460bd5e8b4cab8f715_1440w.jpg" alt="img"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Boosting 算法</th>
<th>GBDT</th>
<th>XGBoost</th>
<th>LightGBM</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>思想</strong><img src="image-20220315210756470.png" alt="image-20220315210756470" style="zoom:25%;"></td>
<td>回归树、梯度迭代、缩减（Shrinkage）;<strong>GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0</strong></td>
<td><strong>二阶导数、线性分类器、正则化</strong>、缩减、<strong>列抽样、并行化</strong></td>
<td><strong>更快的训练速度和更低的内存使用</strong></td>
</tr>
<tr>
<td>目标函数</td>
<td><img src="image-20220315213233284.png" alt="image-20220315213233284" style="zoom: 25%;"></td>
<td><img src="image-20220315213503054.png" alt="image-20220315213503054" style="zoom: 67%;"><img src="image-20220315213608526.png" alt="image-20220315213608526" style="zoom: 33%;"></td>
<td>同上</td>
</tr>
<tr>
<td>损失函数</td>
<td>最小均方损失函数、<strong>绝对损失或者 Huber 损失函数</strong></td>
<td>【线性】最小均方损失函数、==sigmod和softmax==</td>
<td><strong>复杂度模型</strong>：<img src="image-20220315215849417.png" alt="image-20220315215849417" style="zoom: 25%;"></td>
</tr>
<tr>
<td>基模型</td>
<td>CART模型</td>
<td>CART模型/ ==回归模型==</td>
<td>CART模型/ ==回归模型==</td>
</tr>
<tr>
<td>抽样算法</td>
<td>无</td>
<td><strong>列抽样</strong>：借鉴了<strong>随机森林</strong>的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</td>
<td><strong>单边梯度抽样算法；</strong>根据样本梯度来对梯度小的这边样本进行采样，一部分大梯度和随机分布</td>
</tr>
<tr>
<td><strong>切分点算法</strong></td>
<td>CART模型</td>
<td><strong>预排序</strong>、<strong>贪心算法</strong>、<strong>近似算法（</strong>加权分位数缩略图<strong>）</strong></td>
<td><strong>直方图算法</strong>：内存消耗降低，计算代价减少；（不需要记录特征到样本的索引）</td>
</tr>
<tr>
<td><strong>缺失值算法</strong></td>
<td>CART模型</td>
<td><strong>稀疏感知算法</strong>：选择增益最大的枚举项即为最优<strong>缺省方向</strong>。【<strong><font color="red"> 稀疏数据优化不足</font></strong>】【<strong>gblinear 补0</strong>】</td>
<td><strong>互斥特征捆绑算法</strong>：<strong>互斥</strong>指的是一些特征很少同时出现非0值。<strong>稀疏感知算法</strong>；【<strong>gblinear 补0</strong>】</td>
</tr>
<tr>
<td><strong>建树策略</strong></td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Level-wise</strong>：基于层进行生长，直到达到停止条件；</td>
<td><strong>Leaf-wise</strong>：每次分裂增益最大的叶子节点，直到达到停止条件。</td>
</tr>
<tr>
<td><strong>正则化</strong></td>
<td>无</td>
<td>L1 和 L2 正则化项</td>
<td>L1 和 L2 正则化项</td>
</tr>
<tr>
<td><strong>Shrinkage（缩减）</strong></td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>类别特征优化</td>
<td>无</td>
<td>无</td>
<td><strong>类别特征最优分割</strong>：<strong>many-vs-many</strong></td>
</tr>
<tr>
<td>并行化设计</td>
<td>无</td>
<td><strong>块结构设计</strong>、</td>
<td><strong>特征并行</strong>、 <strong>数据并行</strong>、<strong>投票并行</strong></td>
</tr>
<tr>
<td>==缓存优化==</td>
<td>无</td>
<td>为每个线程分配一个连续的缓存区、<strong>“核外”块计算</strong></td>
<td>1、所有的特征都采用相同的方法获得梯度；2、其次，因为不需要存储特征到样本的索引，降低了存储消耗</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>对异常点敏感；</td>
<td><strong>预排序</strong>：仍需要遍历数据集；==不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。==</td>
<td><strong>内存更小</strong>： 索引值、特征值边bin、互斥特征捆绑; <strong>速度更快</strong>：遍历直方图；单边梯度算法过滤掉梯度小的样本；基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；特征并行、数据并行方法加速计算</td>
</tr>
</tbody>
</table>
</div>
<h2><span id="一-xgboost-线性模型多分类增量训练xgb_直方图分布式部署">一、XGBoost [线性模型？多分类？增量训练？XGB_直方图，分布式部署]</span></h2><blockquote>
<p>  <strong>增量学习</strong>：XGBoost提供两种增量训练的方式，一种是在当前迭代树的基础上增加新树，原树不变；另一种是当前迭代树结构不变，重新计算叶节点权重，同时也可增加新树。</p>
<p>  <strong>线性模型</strong>：xgboost通过泰勒公式的二阶展开迭代的残差是1导/2导，线性回归迭代的是标签，xgboost需要串行多个线性回归，预测结果为多个象形线性回归的累积值……，除了用到了线性回归的原理方程式，他们两的损失函数，下降梯度都不一样，几乎没有什么共同点</p>
<p>  <strong>XGBoost 用泰勒展开优势在哪？</strong>：<a href="https://www.zhihu.com/question/61374305">https://www.zhihu.com/question/61374305</a></p>
<ul>
<li><strong>xgboost是以mse为基础推导出来的</strong>，在mse的情况下，xgboost的目标函数展开就是一阶项+二阶项的形式，而其他类似logloss这样的目标函数不能表示成这种形式。为了后续推导的统一，所以将<strong>目标函数进行二阶泰勒展开，就可以直接自定义损失函数了，只要二阶可导即可，增强了模型的扩展性</strong>。</li>
<li><p><strong>二阶信息能够让梯度收敛的更快，类似牛顿法比SGD收敛更快</strong>。一阶信息描述梯度变化方向，二阶信息可以描述梯度变化方向是如何变化的。</p>
<p><strong>==深入理解XGBoost==</strong>：<a href="https://bailingnan.github.io/post/shen-ru-li-jie-xgboost/">https://bailingnan.github.io/post/shen-ru-li-jie-xgboost/</a></p>
</li>
</ul>
</blockquote>
<p>XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包，比常见的工具包快 10 倍以上。Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是<strong>目标函数</strong>的定义。故本文将从数学原理和工程实现上进行介绍，并在最后介绍下 Xgboost 的优点。</p>
<h3><span id="11-数学原理">1.1 数学原理</span></h3><p><strong>1.1.1 目标函数</strong></p>
<p>我们知道 XGBoost 是由$k$个基模型组成的一个加法运算式：</p>
<script type="math/tex; mode=display">
\hat{y}_{i}=\sum_{t=1}^{k} f_{t}\left(x_{i}\right)</script><p><strong>损失函数：</strong></p>
<script type="math/tex; mode=display">
L=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)</script><p>我们知道模型的预测精度由模型的<strong>偏差</strong>和<strong>方差</strong>共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的<strong>损失函数$L$</strong>与抑<strong>制模型复杂度的正则项 <script type="math/tex">\Omega</script></strong>组成。支持<strong>决策树</strong>也支持<strong>线性模型</strong>。</p>
<script type="math/tex; mode=display">
O b j=\sum_{i=1}^{n} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{t=1}^{k} \Omega\left(f_{t}\right)</script><p><strong>Boosting模型是向前加法：</strong></p>
<script type="math/tex; mode=display">
\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)</script><p>目标函数就可以写成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
O b j^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\
&=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right)
\end{aligned}</script><p>求此时最优化目标函数，就相当于求解 <script type="math/tex">f_{t}\left(x_{i}\right)</script>。根据泰勒展开式：</p>
<script type="math/tex; mode=display">
f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}</script><p><strong>我们==把<script type="math/tex">\hat{y}_{i}^{t-1}</script>,视为x， <script type="math/tex">f_{t}\left(x_{i}\right)</script>视为<script type="math/tex">\Delta x</script>==，故可以将目标函数写成</strong>：</p>
<script type="math/tex; mode=display">
O b j^{(t)}=\sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{t-1}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t} \Omega\left(f_{i}\right)</script><p>由于<strong>第一项为常数，对优化没有影响，所以我们只需要求出每一步损失函数的一阶导和二阶导的值</strong>【==前t-1的结果和标签求==】，然后最优化目标函数，就可以得到每一步的f(x),最后根据加法模型得到一个整体模型。</p>
<script type="math/tex; mode=display">
O b j^{(t)} \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t} \Omega\left(f_{i}\right)</script><blockquote>
<p>  以<strong>平方损失函数</strong>【绝对值、hubor损失】为例（GBDT 残差）：</p>
<p>  <img src="image-20220404141858337.png" alt="image-20220404141858337" style="zoom:50%;"></p>
<p>  其中 <img src="https://www.zhihu.com/equation?tex=g_%7Bi%7D" alt="[公式]"> 为损失函数的一阶导， <img src="https://www.zhihu.com/equation?tex=h_%7Bi%7D" alt="[公式]"> 为损失函数的二阶导，<strong>注意这里的导是对 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D_i%5E%7Bt-1%7D" alt="[公式]"> 求导</strong>。</p>
<script type="math/tex; mode=display">
  \begin{aligned}
  &g_{i}=\frac{\partial\left(\hat{y}^{t-1}-y_{i}\right)^{2}}{\partial \hat{y}^{t-1}}=2\left(\hat{y}^{t-1}-y_{i}\right) \\
  &h_{i}=\frac{\partial^{2}\left(\hat{y}^{t-1}-y_{i}\right)^{2}}{\hat{y}^{t-1}}=2
  \end{aligned}</script></blockquote>
<h4><span id="112-基于决策树的目标函数"><strong>1.1.2 基于决策树的目标函数</strong></span></h4><p>我们知道 Xgboost 的基模型<strong>不仅支持决策树，还支持线性模型</strong>，这里我们主要介绍基于决策树的目标函数。</p>
<p>我们可以将决<strong>策树定义为<script type="math/tex">f_{t}(x)=w_{q(x)}</script></strong>，x为某一样本，这里的 <img src="https://www.zhihu.com/equation?tex=q%28x%29" alt="[公式]"> 代表了该样本在哪个叶子结点上，而 <img src="https://www.zhihu.com/equation?tex=w_q" alt="[公式]"> 则代表了叶子结点取值 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> ，所以 <img src="https://www.zhihu.com/equation?tex=w_%7Bq%28x%29%7D" alt="[公式]"> 就代表了每个样本的取值 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> （即预测值)。</p>
<p><strong>决策树的复杂度</strong>可由<strong>叶子数 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"></strong> 组成，叶子节点越少模型越简单，此外<strong>叶子节点也不应该含有过高的权重</strong> <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> （类比 LR 的每个变量的权重)，所以目标函数的正则项可以定义为：</p>
<script type="math/tex; mode=display">
\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}</script><p>即<strong>决策树模型的复杂度</strong>由生成的所有<strong>决策树的叶子节点数量</strong>，和所有<strong>节点权重所组成的向量的 <img src="https://www.zhihu.com/equation?tex=L_2" alt="[公式]"> 范式</strong>共同决定。</p>
<p><img src="https://pic1.zhimg.com/80/v2-e0ab9287990a6098e4cdbc5a8cff4150_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>我们设 <img src="https://www.zhihu.com/equation?tex=I_j%3D+%5C%7B+i+%5Cvert+q%28x_i%29%3Dj+%5C%7D" alt="[公式]"> 为第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 个叶子节点的样本集合，故我们的目标函数可以写成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
O b j^{(t)} & \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\
&=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\
&=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T
\end{aligned}</script><p>第二步是遍历所有的样本后求每个样本的损失函数，但样本最终会落在叶子节点上，所以我们也可以遍历叶子节点，然后获取叶子节点上的样本集合，最后在求损失函数。即我们之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi+%5Cin+I_j%7Dg_i" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi+%5Cin+I_j%7Dh_i" alt="[公式]"> 这两项， <img src="https://www.zhihu.com/equation?tex=w_j" alt="[公式]"> 为第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 个叶子节点取值。</p>
<p><img src="image-20220314162051124.png" alt="image-20220314162051124" style="zoom: 25%;"></p>
<p><strong><font color="red"> 这里我们要注意 <img src="https://www.zhihu.com/equation?tex=G_j" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=H_j" alt="[公式]"> 是前 <img src="https://www.zhihu.com/equation?tex=t-1" alt="[公式]"> 步得到的结果，其值已知可视为常数，只有最后一棵树的叶子节点 <img src="https://www.zhihu.com/equation?tex=w_j" alt="[公式]"> 不确定，那么将目标函数对 <img src="https://www.zhihu.com/equation?tex=w_j" alt="[公式]"> 求一阶导，并令其等于 <img src="https://www.zhihu.com/equation?tex=0" alt="[公式]"> ，则可以求得叶子结点 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 对应的权值：</font></strong></p>
<p><img src="https://www.zhihu.com/equation?tex=w_j%5E%2A%3D-%5Cfrac%7BG_j%7D%7BH_j%2B%5Clambda%7D++%5C%5C" alt="[公式]"></p>
<p>所以<strong>目标函数可以化简为：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=Obj+%3D+-%5Cfrac12+%5Csum_%7Bj%3D1%7D%5ET+%5Cfrac%7BG_j%5E2%7D%7BH_j%2B%5Clambda%7D+%2B+%5Cgamma+T+%5C%5C" alt="[公式]"></p>
<p><img src="https://pic2.zhimg.com/80/v2-f6db7af6c1e683192cb0ccf48eafaf99_1440w.jpg" alt="img" style="zoom: 67%;"></p>
<p>上图给出目标函数计算的例子，求每个节点每个样本的一阶导数 <img src="https://www.zhihu.com/equation?tex=g_i" alt="[公式]"> 和二阶导数 <img src="https://www.zhihu.com/equation?tex=h_i" alt="[公式]"> ，然后针对每个节点对所含样本求和得到的 <img src="https://www.zhihu.com/equation?tex=G_j" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=H_j" alt="[公式]"> ，最后遍历决策树的节点即可得到<strong>目标函数</strong>。</p>
<h4><span id="113-最优切分点划分算法"><strong>1.1.3 最优切分点划分算法</strong></span></h4><p><strong><font color="red"> 在决策树的生长过程中，一个非常关键的问题是如何找到叶子的节点的最优切分点，</font></strong>Xgboost 支持两种分裂节点的方法——<strong>贪心算法</strong>和<strong>近似算法</strong>。</p>
<p><strong>1）贪心算法</strong></p>
<ol>
<li><strong>从深度为 <img src="https://www.zhihu.com/equation?tex=0" alt="[公式]"> 的树开始，对每个叶节点枚举所有的可用特征</strong>；</li>
<li>针对每个特征，把属于该节点的训练样本根据该特征值进行<strong>升序排列</strong>，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；</li>
<li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点<strong>关联对应的样本集</strong>？？</li>
<li>回到第 1 步，递归执行到满足特定条件为止。（树的深度、gamma）</li>
</ol>
<h5><span id="那么如何计算每个特征的分裂收益呢">那么如何计算每个特征的分裂收益呢？</span></h5><p>假设我们在某一节点完成特征分裂，则分列前的目标函数可以写为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Obj_%7B1%7D+%3D-%5Cfrac12+%5B%5Cfrac%7B%28G_L%2BG_R%29%5E2%7D%7BH_L%2BH_R%2B%5Clambda%7D%5D+%2B+%5Cgamma++%5C%5C" alt="[公式]"></p>
<p>分裂后的目标函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=Obj_2+%3D++-%5Cfrac12+%5B+%5Cfrac%7BG_L%5E2%7D%7BH_L%2B%5Clambda%7D+%2B+%5Cfrac%7BG_R%5E2%7D%7BH_R%2B%5Clambda%7D%5D+%2B2%5Cgamma+%5C%5C" alt="[公式]"></p>
<p>则对于目标函数来说，分裂后的收益为：<strong>MAX</strong>【<strong>obj1 - obj2 （分裂后越小越好）</strong>】</p>
<p><img src="https://www.zhihu.com/equation?tex=Gain%3D%5Cfrac12+%5Cleft%5B+%5Cfrac%7BG_L%5E2%7D%7BH_L%2B%5Clambda%7D+%2B+%5Cfrac%7BG_R%5E2%7D%7BH_R%2B%5Clambda%7D+-+%5Cfrac%7B%28G_L%2BG_R%29%5E2%7D%7BH_L%2BH_R%2B%5Clambda%7D%5Cright%5D+-+%5Cgamma+%5C%5C" alt="[公式]"></p>
<p>注意<strong>该特征收益也可作为特征重要性输出的重要依据</strong>。</p>
<p>我们可以发现对于所有的分裂点 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> ，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 <img src="https://www.zhihu.com/equation?tex=G_L" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=G_R" alt="[公式]"> 。然后用上面的公式计算每个分割方案的分数就可以了。<font color="red">观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入<strong>新叶子的惩罚项（gamma)</strong>，也就是说引入的分割带来的<strong>增益如果小于一个阀值</strong>的时候，我们可以剪掉这个分割。 </font></p>
<p><strong>2）近似算法</strong>【<strong>加权分位划分点</strong>】</p>
<p><strong>贪婪算法可以的到最优解，但当数据量太大时则无法读入内存进行计算</strong>，近似算法主要针对贪婪算法这一缺点给出了近似最优解。</p>
<p>对于每个特征，只考察分位点可以减少计算复杂度。该算法会首先根据<strong>特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中</strong>，然后聚合统计信息找到所有区间的最佳分裂点。在提出候选切分点时有两种策略：</p>
<ul>
<li><strong>Global</strong>：<strong>学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割</strong>；</li>
<li><strong>Local</strong>：每次分裂前将重新提出候选切分点。</li>
</ul>
<p><strong>下图给出近似算法的具体例子，以三分位为例：</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-5d1dd1673419599094bf44dd4b533ba9_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的 <img src="https://www.zhihu.com/equation?tex=G%2CH" alt="[公式]"> 值，最终求解节点划分的增益。</p>
<h4><span id="114-加权分位数缩略图xgboost-直方图算法"><strong>1.1.4 加权分位数缩略图</strong>[XGBoost 直方图算法]</span></h4><ul>
<li><strong>第一个 for 循环：</strong>对特征 k <strong>根据该特征分布的分位数找到切割点的候选集合【==直方图==】</strong> <img src="https://www.zhihu.com/equation?tex=S_k%3D%5C%7Bs_%7Bk1%7D%2Cs_%7Bk2%7D%2C...%2Cs_%7Bkl%7D+%5C%7D" alt="[公式]"> 。XGBoost 支持 Global 策略和 Local 策略。</li>
<li><strong>第二个 for 循环：</strong>针对每个特征的候选集合，将样本映射到由该特征对应的候选点集构成的分桶区间中，即 <img src="https://www.zhihu.com/equation?tex=%7Bs_%7Bk%2Cv%7D%E2%89%A5x_%7Bjk%7D%3Es_%7Bk%2Cv%E2%88%921%7D%7D" alt="[公式]"> ，对每个桶统计 <img src="https://www.zhihu.com/equation?tex=G%2CH+" alt="[公式]"> 值，最后在这些统计量上寻找最佳分裂点。</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-161382c979557b8bae1563a459cd1ed4_1440w.jpg" alt="img" style="zoom:33%;"></p>
<p>事实上， <strong>XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值 <img src="https://www.zhihu.com/equation?tex=h_i+" alt="[公式]"> 作为样本的权重进行划分</strong>，如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-5f16246289eaa2a3ae72f971db198457_1440w.jpg" alt="img"></p>
<h5><span id="那么问题来了为什么要用-进行样本加权">==那么问题来了：为什么要用 <img src="https://www.zhihu.com/equation?tex=h_i" alt="[公式]"> 进行样本加权？==</span></h5><p>我们知道模型的目标函数为：</p>
<p><img src="https://www.zhihu.com/equation?tex=+Obj%5E%7B%28t%29%7D+%5Capprox+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%5B+g_if_t%28x_i%29+%2B+%5Cfrac12h_if_t%5E2%28x_i%29+%5Cright%5D+%2B+%5Csum_%7Bi%3D1%7D%5Et++%5COmega%28f_i%29+%5C%5C" alt="[公式]"></p>
<p>我们稍作整理，便可以看出 <img src="https://www.zhihu.com/equation?tex=h_i" alt="[公式]"> 有对 loss 加权的作用。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++Obj%5E%7B%28t%29%7D+%26+%5Capprox+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%5B+g_if_t%28x_i%29+%2B+%5Cfrac12h_if_t%5E2%28x_i%29+%5Cright%5D+%2B+%5Csum_%7Bi%3D1%7D%5Et++%5COmega%28f_i%29+%5C%5C+%5C%5C++++%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5B+g_i+f_t%28x_i%29+%2B+%5Cfrac%7B1%7D%7B2%7Dh_i+f_t%5E2%28x_i%29+%5Ccolor%7Bred%7D%7B%2B+%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7Bg_i%5E2%7D%7Bh_i%7D%7D%5D%2B%5COmega%28f_t%29+%5Ccolor%7Bred%7D%7B%2B+C%7D+%5C%5C++++%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Ccolor%7Bred%7D%7B%5Cfrac%7B1%7D%7B2%7Dh_i%7D+%5Cleft%5B+f_t%28x_i%29+-+%5Cleft%28+-%5Cfrac%7Bg_i%7D%7Bh_i%7D+%5Cright%29+%5Cright%5D%5E2+%2B+%5COmega%28f_t%29+%2B+C+%5Cend%7Balign%7D+%5C%5C" alt="[公式]"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7Bg_i%5E2%7D%7Bh_i%7D" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]"> 皆为常数。我们可以看到 <img src="https://www.zhihu.com/equation?tex=h_i" alt="[公式]"> 就是平方损失函数中样本的权重。</p>
<p>对于样本权值相同的数据集来说，找到候选分位点已经有了解决方案（GK 算法），但是当样本权值不一样时，该如何找到候选分位点呢？（作者给出了一个 Weighted Quantile Sketch 算法，这里将不做介绍。）</p>
<h4><span id="xgboost的近似直方图算法也类似于lightgbm这里的直方图算法-为什么慢"><strong><font color="red"> xgboost的近似直方图算法也类似于lightgbm这里的直方图算法? 为什么慢？</font></strong></span></h4><ul>
<li><strong>xgboost在每一层都动态构建直方图</strong>， 因为<strong>xgboost的直方图算法不是针对某个特定的feature</strong>，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而<strong>lightgbm中对每个特征都有一个直方图</strong>，所以构建一次直方图就够了。</li>
<li><strong>lightgbm有一些工程上的cache优化</strong></li>
</ul>
<h4><span id="115-稀疏感知算法缺失值的处理"><strong>1.1.5 稀疏感知算法</strong>【<strong>缺失值的处理</strong>】</span></h4><blockquote>
<ul>
<li><strong>特征值缺失的样本无需遍历只需直接分配到左右节点</strong></li>
<li><strong>如果训练中没有数据缺失，预测时出现了数据缺失，则默认被分类到右节点.</strong>？<ul>
<li>看c++源码是默认向左方向</li>
</ul>
</li>
</ul>
</blockquote>
<p>在决策树的第一篇文章中我们介绍 CART 树在应对数据缺失时的分裂策略【<strong>缺失代理</strong>】，XGBoost 也给出了其解决方案。XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。</p>
<p><strong>XGBoost提出的是在计算分割后的分数时，遇到缺失值，分别将缺失值带入左右两个分割节点，然后取最大值的方向为其默认方向。</strong>至于如何学到缺省值的分支，其实很简单，<strong>分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。</strong></p>
<p>在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而<strong>特征值缺失的样本无需遍历只需直接分配到左右节点</strong>，故算法所需遍历的样本量减少，下图可以看到稀疏感知算法比 basic 算法速度块了超过 50 倍。</p>
<p><img src="https://pic1.zhimg.com/80/v2-e065bea4b424ea2d13b25ed2e7004aa8_1440w.jpg" alt="img" style="zoom:67%;"></p>
<h4><span id="116-缩减与列采样"><strong>1.1.6 缩减与列采样</strong></span></h4><p>除了在目标函数中引入正则项，为了防止过拟合，XGBoost还引入了缩减(shrinkage)和列抽样（column subsampling），通过在每一步的boosting中引入缩减系数，降低每个树和叶子对结果的影响；列采样是借鉴随机森林中的思想，根据反馈，列采样有时甚至比行抽样效果更好，同时，通过列采样能加速计算。</p>
<h3><span id="12-工程实现">1.2 工程实现</span></h3><h4><span id="121-块结构设计"><strong>1.2.1 块结构设计</strong></span></h4><p>我们知道，决策树的学习<strong>最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序</strong>。而 <strong><font color="red"> XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</font></strong></p>
<blockquote>
<p>  预排序 + 块设计【独立】 + 稀疏矩阵存储 </p>
</blockquote>
<ul>
<li><strong>每一个块结构包括一个或多个已经排序好的特征</strong>；</li>
<li><strong>缺失特征值将不进行排序</strong>；</li>
<li>每个特征会存储指向<strong>样本梯度统计值</strong>的索引，方便计算一阶导和二阶导数值；</li>
</ul>
<p>这种块结构存储的特征之间相互独立，方便计算机进行并行计算。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个<strong>特征的增益计算可以同时进行</strong>，这也是 Xgboost 能够实现分布式或者多线程计算的原因。</p>
<h4><span id="122-缓存访问优化算法索引访问梯度统计-gt-缓存空间不连续"><strong>1.2.2 缓存访问优化算法</strong>【索引访问梯度统计 -&gt; 缓存空间不连续】</span></h4><p>块结构的设计可以减少节点分裂时的计算量，但<strong>特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续</strong>，这样会造成缓存命中率低，从而影响到算法的效率。</p>
<p>为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。此外适当调整块大小，也可以有助于缓存优化。</p>
<p>于exact greedy算法中, 使用<strong>缓存预取（cache-aware prefetching）</strong>。具体来说，<strong>对每个线程分配一个连续的buffer</strong>，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化）</p>
<h4><span id="123-核外块计算"><strong>1.2.3 “核外”块计算</strong></span></h4><p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，<strong>XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行</strong>。</p>
<p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p>
<ul>
<li><strong>块压缩：</strong>对 Block 进行按列压缩，并在读取时进行解压；</li>
<li><strong>块拆分：</strong>将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li>
</ul>
<h4><span id="124-xgboost损失函数">==1.2.4 <strong>XGBoost损失函数</strong>==</span></h4><blockquote>
<p>  <a href="https://blog.csdn.net/qq_32103261/article/details/106664227">不平衡处理：xgboost 中scale_pos_weight、给样本设置权重weight、 自定义损失函数 和 简单复制正样本的区别</a></p>
</blockquote>
<p><strong>损失函数</strong>：损失函数描述了预测值和真实标签的差异，通过对损失函数的优化来获得对学习任务的一个近似求解方法。boosting类算法的损失函数的作用： Boosting的框架, 无论是GBDT还是Adaboost, 其在每一轮迭代中, <strong>根本没有理会损失函数具体是什么, 仅仅用到了损失函数的一阶导数通过随机梯度下降来参数更新</strong>。XGBoost是用了牛顿法进行的梯度更新。通过对损失进行分解得到一阶导数和二阶导数并通过牛顿法来迭代更新梯度。</p>
<h5><span id="1自定义损失函数">（1）==<strong>自定义损失函数</strong>==</span></h5><p><strong>XGBOOST是一个非常灵活的模型</strong>，允许使用者根据实际使用场景调整<a href="https://so.csdn.net/so/search?q=损失函数&amp;spm=1001.2101.3001.7020">损失函数</a>，对于常见的二分类问题一般使用的binary：logistic损失函数，其形式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D-%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cleft%28y%5E%7B%28i%29%7D+%5Clog+h_%7B%5Ctheta%7D%5Cleft%28x%5E%7B%28i%29%7D%5Cright%29%2B%5Cleft%281-y%5E%7B%28i%29%7D%5Cright%29+%5Clog+%5Cleft%281-h_%7B%5Ctheta%7D%5Cleft%28x%5E%7B%28i%29%7D%5Cright%29%5Cright%29%5Cright%29+%EF%BC%883%EF%BC%89%5C%5C" alt="[公式]"></p>
<p>这个损失函数对于正类错分和负类错分给予的惩罚时相同的，但是<strong>对于不平衡数据集，或者某些特殊情况（两类错分代价不一样）的时候这样的损失函数就不再合理了。</strong></p>
<p>基于XGBoost的损失函数的分解求导，可以知道XGBoost的除正则项以外的核心影响因子是损失函数的1阶导和2阶导，所以对于任意的学习任务的损失函数，可以对其求一阶导数和二阶导数带入到XGBoost的自定义损失函数范式里面进行处理。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">custom_obj</span>(<span class="params">pred, dtrain</span>):<span class="comment"># pred 和 dtrain 的顺序不能弄反</span></span><br><span class="line">    <span class="comment"># STEP1 获得label</span></span><br><span class="line">    label = dtrain.get_label()</span><br><span class="line">    <span class="comment"># STEP2 如果是二分类任务，需要让预测值通过sigmoid函数获得0～1之间的预测值</span></span><br><span class="line">    <span class="comment"># 如果是回归任务则下述任务不需要通过sigmoid</span></span><br><span class="line">    <span class="comment"># 分类任务sigmoid化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    sigmoid_pred = sigmoid(原始预测值)</span><br><span class="line">    <span class="comment">#回归任务</span></span><br><span class="line">    pred = 原始预测值</span><br><span class="line">    <span class="comment"># STEP3 一阶导和二阶导</span></span><br><span class="line">    grad = 一阶导</span><br><span class="line">    hess = 二阶导</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> grad, hess</span><br></pre></td></tr></table></figure></p>
<p>非平衡分类学习任务，例如首笔首期30+的风险建模任务，首期30+的逾期率比例相对ever30+的逾期率为1/3左右，<strong>通过修正占比少的正样本权重来对影响正样本对损失函数的贡献度，可以进一步提升模型的效果</strong>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_binary_cross_entropy</span>(<span class="params">pred, dtrain,imbalance_alpha=<span class="number">10</span></span>):</span><br><span class="line">    <span class="comment"># retrieve data from dtrain matrix</span></span><br><span class="line">    label = dtrain.get_label()</span><br><span class="line">    <span class="comment"># compute the prediction with sigmoid</span></span><br><span class="line">    sigmoid_pred = <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-pred))</span><br><span class="line">    <span class="comment"># gradient</span></span><br><span class="line">    grad = -(imbalance_alpha ** label) * (label - sigmoid_pred)</span><br><span class="line">    hess = (imbalance_alpha ** label) * sigmoid_pred * (<span class="number">1.0</span> - sigmoid_pred)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> grad, hess </span><br></pre></td></tr></table></figure>
<h5><span id="2focal-loss">（2）Focal Loss</span></h5><p><strong>Focal Loss for Dense Object Detection 是ICCV2017的Best student paper,文章思路很简单但非常具有开拓性意义，效果也非常令人称赞。</strong></p>
<ul>
<li>大家还可以看知乎的讨论：<a href="https://www.zhihu.com/question/63581984">如何评价 Kaiming 的 Focal Loss for Dense Object Detection？</a></li>
<li>[机器学习] XGBoost 自定义损失函数-FocalLoss：<a href="https://blog.csdn.net/zwqjoy/article/details/109311133">https://blog.csdn.net/zwqjoy/article/details/109311133</a></li>
</ul>
<p>Focal Loss的引入主要是为了解决难易样本数量不平衡（注意，有区别于正负样本数量不平衡）的问题，实际可以使用的范围非常广泛，为了方便解释，拿目标检测的应用场景来说明</p>
<p><strong>==Focal Loss的主要思想就是改变损失函数.Focal loss是在交叉熵损失函数基础上进行的修改==</strong></p>
<p>单阶段的目标检测器通常会产生高达100k的候选目标，只有极少数是正样本，正负样本数量非常不平衡。我们在计算分类的时候常用的损失——交叉熵。<img src="https://private.codecogs.com/gif.latex?y%7B%7D%27" alt="y{}&#39;">是经过激活函数的输出，所以在0-1之间。可见普通的交叉熵对于正样本而言，输出概率越大损失越小。对于负样本而言，输出概率越小则损失越小。此时的损失函数在大量简单样本的迭代过程中比较缓慢且可能无法优化至最优。</p>
<p>为了解决<strong>正负样本不平衡</strong>的问题，我们通常会在交叉熵损失的前面加上一个参数<strong>平衡因子alpha</strong>，用来平衡正负样本本身的比例不均. 文中alpha取0.25，即正样本要比负样本占比小，这是因为负例易分。</p>
<h3><span id="13-优缺点">1.3 优缺点</span></h3><h4><span id="131-优点"><strong>1.3.1 优点</strong></span></h4><ol>
<li><strong>精度更高：</strong>GBDT 只用到一阶<strong>泰勒展开</strong>，而 XGBoost 对损失函数进行了二阶泰勒展开。<strong>XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数</strong>；</li>
<li><strong>灵活性更强：</strong>GBDT 以 CART 作为<strong>基分类器</strong>，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 <strong>XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）</strong>）。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li>
<li><strong>正则化：</strong>XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</li>
<li><strong>Shrinkage（缩减）：</strong>相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li>
<li><strong>列抽样：</strong>XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</li>
<li><strong>缺失值处理：</strong>XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；</li>
<li><strong>可以并行化操作：</strong>块结构可以很好的支持并行计算。</li>
</ol>
<h4><span id="132-缺点"><strong>1.3.2 缺点</strong></span></h4><ol>
<li>虽然利用<strong>预排序</strong>和<strong>近似算法</strong>可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要<strong>==遍历数据集==</strong>；</li>
<li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要<strong>==存储特征对应样本的梯度统计值的索引==</strong>，相当于消耗了两倍的内存。</li>
</ol>
<h2><span id="二-xgboost常用参数">二、XGBoost常用参数</span></h2><h4><span id="xgboost的参数一共分为三类">XGBoost的参数一共分为三类：</span></h4><p><a href="https://xgboost.apachecn.org/#/">完整参数请戳官方文档</a></p>
<p>1、<strong>通用参数</strong>：宏观函数控制。</p>
<p>2、<strong>Booster参数</strong>：控制每一步的booster(tree/regression)。booster参数一般可以调控模型的效果和计算代价。我们所说的调参，很这是大程度上都是在调整booster参数。</p>
<p>3、<strong>学习目标参数</strong>：控制训练目标的表现。我们对于问题的划分主要体现在学习目标参数上。比如我们要做分类还是回归，做二分类还是多分类，这都是目标参数所提供的。</p>
<h4><span id="通用参数">通用参数</span></h4><ol>
<li><strong>booster</strong>：我们有两种参数选择，<code>gbtree</code>、<code>dart</code>和<code>gblinear</code>。gbtree、dart是采用树的结构来运行数据，而gblinear是基于线性模型。</li>
<li><strong>silent</strong>：静默模式，为<code>1</code>时模型运行不输出。</li>
<li><strong>nthread</strong>: 使用线程数，一般我们设置成<code>-1</code>,使用所有线程。如果有需要，我们设置成多少就是用多少线程。</li>
</ol>
<h4><span id="booster参数">Booster参数</span></h4><ol>
<li><p><strong>==n_estimator==</strong>: 也作<code>num_boosting_rounds</code>这是生成的<strong>最大树的数目</strong>，也是最大的迭代次数。</p>
</li>
<li><p><strong>==learning_rate==</strong>: 有时也叫作<code>eta</code>，系统默认值为<code>0.3</code>,。<strong>每一步迭代的步长</strong>，很重要。太大了运行准确率不高，太小了运行速度慢。我们一般使用比默认值小一点，<code>0.1</code>左右就很好。</p>
</li>
<li><p><strong>==gamma==</strong>：系统默认为<code>0</code>,我们也常用<code>0</code>。在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。<code>gamma</code>指定了节点分裂所需的<strong>最小损失函数下降值</strong>。 这个参数的值越大，算法越保守。因为<code>gamma</code>值越大的时候，损失函数下降更多才可以分裂节点。所以树生成的时候更不容易分裂节点。范围: <code>[0,∞]</code></p>
</li>
<li><p><strong>==subsample==</strong>：系统默认为<code>1</code>。这个参数控制对于每棵树，<strong>随机采样的比例</strong>。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：<code>0.5-1</code>，<code>0.5</code>代表平均采样，防止过拟合. 范围: <code>(0,1]</code>，<strong>注意不可取0</strong></p>
</li>
<li><p><strong>colsample_bytree</strong>：系统默认值为1。我们一般设置成0.8左右。用来控制每棵<strong>随机采样的列数的占比</strong>(每一列是一个特征)。 典型值：<code>0.5-1</code>范围: <code>(0,1]</code></p>
</li>
<li><p><strong>colsample_bylevel</strong>：默认为1,我们也设置为1.这个就相比于前一个更加细致了，它指的是每棵树每次节点分裂的时候列采样的比例</p>
</li>
<li><p><strong>max_depth</strong>： 系统默认值为<code>6</code>，我们常用<code>3-10</code>之间的数字。这个值为<strong>树的最大深度</strong>。这个值是用来控制过拟合的。<code>max_depth</code>越大，模型学习的更加具体。设置为<code>0</code>代表没有限制，范围: <code>[0,∞]</code></p>
</li>
<li><p><strong>==max_delta_step==</strong>：默认<code>0</code>,我们常用<code>0</code>.这个参数限制了<strong>每棵树权重改变的最大步长</strong>，如果这个参数的值为<code>0</code>,则意味着没有约束。如果他被赋予了某一个正值，则是这个算法更加保守。通常，这个参数我们不需要设置，但是<strong>==当个类别的样本极不平衡的时候，这个参数对逻辑回归优化器是很有帮助的。==</strong></p>
</li>
<li><p><strong>==lambda==</strong>:也称<code>reg_lambda</code>,默认值为<code>0</code>。<strong>权重的L2正则化项</strong>。(和Ridge regression类似)。这个参数是用来控制XGBoost的正则化部分的。这个参数在减少过拟合上很有帮助。</p>
</li>
<li><p><strong>alpha</strong>:也称<code>reg_alpha</code>默认为<code>0</code>,权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。</p>
</li>
<li><p><strong>==scale_pos_weight==</strong>：默认为<code>1</code>在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。通常可以将其设置为<strong>负样本的数目与正样本数目的比值</strong>。<strong>xgboost中==scale_pos_weight、对样本进行weight设置和简单复制正样本==得到的结果是一样的，本质上都是改变了训练的损失函数。通过自定义设置损失函数可得到验证。实际上基本思想都是通过过采样的方法处理不平衡数据。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (label == <span class="number">1.0</span>f) &#123;</span><br><span class="line">    w *= scale_pos_weight;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 见源码 src/objective/regression_obj.cu</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>  在DMatrix里边设置每个样本的weight 是 怎样改变训练过程的呢，其实是改变训练的损失函数，源代码里的代码如下，可以看到对不同的样本赋予不同的权重实际上是影响了该样本在训练过程中贡献的损失，进而改变了一阶导和二阶导。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_out_gpair[_idx] = GradientPair(Loss::FirstOrderGradient(p, label) * w,</span><br><span class="line">                   Loss::SecondOrderGradient(p, label) * w);</span><br><span class="line"><span class="comment"># 见源码 src/objective/regression_obj.cu</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4><span id="学习目标参数">学习目标参数</span></h4><h5><span id="objective-缺省值reglinear">objective [缺省值=reg:linear]</span></h5><ul>
<li><code>reg:linear</code>– <strong>线性回归</strong></li>
<li><code>reg:logistic</code> – <strong>逻辑回归</strong></li>
<li><code>binary:logistic</code> – 二分类逻辑回归，输出为概率</li>
<li><code>binary:logitraw</code> – 二分类逻辑回归，输出的结果为wTx</li>
<li><code>count:poisson</code> – 计数问题的poisson回归，输出结果为poisson分布。在poisson回归中，max_delta_step的缺省值为0.7 (used to safeguard optimization)</li>
<li><code>multi:softmax</code> – 设置 XGBoost 使用softmax目标函数做多分类，需要设置参数num_class（类别个数）</li>
<li><code>multi:softprob</code> – 如同softmax，但是输出结果为ndata*nclass的向量，其中的值是每个数据分为每个类的概率。</li>
</ul>
<h5><span id="eval_metric-缺省值通过目标函数选择">eval_metric [缺省值=通过目标函数选择]</span></h5><ul>
<li><code>rmse</code>: <strong>均方根误差</strong></li>
<li><code>mae</code>: <strong>平均绝对值误差</strong></li>
<li><code>logloss</code>: negative log-likelihood</li>
<li><code>error</code>: 二分类错误率。其值通过错误分类数目与全部分类数目比值得到。对于预测，预测值大于0.5被认为是正类，其它归为负类。 error@t: 不同的划分阈值可以通过 ‘t’进行设置</li>
<li><code>merror</code>: 多分类错误率，计算公式为(wrong cases)/(all cases)</li>
<li><code>mlogloss</code>: ==多分类log损失==</li>
<li><code>auc</code>: 曲线下的面积</li>
<li><code>ndcg</code>: Normalized Discounted Cumulative Gain</li>
<li><code>map</code>: 平均正确率</li>
</ul>
<p>一般来说，我们都会使用<code>xgboost.train(params, dtrain)</code>函数来训练我们的模型。这里的<code>params</code>指的是<code>booster</code>参数。</p>
<h1><span id="xgboostqampa">XGBoostQ&amp;A</span></h1><ul>
<li>推荐收藏 | 又有10道XGBoost面试题送给你：<a href="https://cloud.tencent.com/developer/article/1518305">https://cloud.tencent.com/developer/article/1518305</a></li>
</ul>
<h3><span id="1-xgboost模型如果过拟合了怎么解决"><strong>1、XGBoost模型如果过拟合了怎么解决?</strong></span></h3><ul>
<li><strong>正则项</strong>：叶子结点的数目和叶子结点权重的L2模的平方</li>
<li><strong>列抽样</strong>：训练的时候只用一部分特征，不仅可以降低过拟合，还可以加速</li>
<li><strong>子采样</strong>：每轮计算可以不使用全部样本</li>
<li><strong>shrinkage</strong>: 步长(学习率)，消弱训练出的每棵树的影响，让后面的训练有更大的学习空间</li>
</ul>
<p>当出现过拟合时，有两类参数可以缓解：</p>
<p>第一类参数：用于<strong>直接控制模型的复杂度</strong>。包括<code>max_depth,min_child_weight,gamma</code> 等参数</p>
<p>第二类参数：用于<strong>增加随机性</strong>，从而使得模型在训练时对于噪音不敏感。包括<code>subsample,colsample_bytree</code></p>
<p>还有就是直接减小<code>learning rate</code>，但需要同时增加<code>estimator</code> 参数。</p>
<h3><span id="2-怎么理解决策树-xgboost能处理缺失值而有的模型svm对缺失值比较敏感呢">2、怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感呢?</span></h3><blockquote>
<p>   微调的回答 - 知乎 <a href="https://www.zhihu.com/question/58230411/answer/242037063">https://www.zhihu.com/question/58230411/answer/242037063</a></p>
<p>  XGBoost是一种<strong>boosting</strong>的集成学习模型：支持的弱学习器（即单个的学习器，也称基学习器）有<strong>树模型</strong>和<strong>线性模型</strong>（<strong>gblinear</strong>），默认为<strong>gbtree</strong>。</p>
<ul>
<li><p><strong>gblinear</strong>，<strong>由于线性模型不支持缺失值，会将缺失值填充为0</strong>；</p>
</li>
<li><p><strong>gbtree</strong>或者<strong>dart</strong>，则支持缺失值；</p>
</li>
</ul>
</blockquote>
<ul>
<li>工具包自动处理数据缺失<strong>不代表</strong>具体的算法可以<strong>处理缺失项</strong></li>
<li>对于有缺失的数据：以<a href="https://www.zhihu.com/search?q=决策树&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A242037063}">决策树</a>为原型的模型<strong>优于</strong>依赖距离度量的模型</li>
</ul>
<h3><span id="3-histogram-vs-pre-sorted">3、 Histogram VS Pre-sorted</span></h3><h4><span id="pre-sorted">Pre-sorted</span></h4><p><strong>预排序还是有一定优点的，如果不用预排序的话，在分裂节点的时候，选中某一个特征后，需要对A按特征值大小进行排序，然后计算每个阈值的增益，这个过程需要花费很多时间</strong>。</p>
<p>预排序算法在计算最优分裂时，各个特征的增益可以并行计算，并且能精确地找到分割点。但是<strong>预排序后需要保存特征值及排序后的索引，因此需要消耗两倍于训练数据的内存，时间消耗大</strong>。另外预排序后，<strong>特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化，时间消耗也大</strong>。最后，在每一层，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样。</p>
<h4><span id="historgram">Historgram</span></h4><p>首先需要指出的是，XGBoost在寻找树的分裂节点的也是支持直方图算法的，就是论文中提到的近视搜索算法（Approximate Algorithm）。<strong>只是，无论特征值是否为0，直方图算法都需要对特征的分箱值进行索引，因此对于大部分实际应用场景当中的稀疏数据优化不足。</strong></p>
<p>回过头来，为了能够发挥直方图算法的优化威力，LightGBM提出了另外两个新技术：<strong>单边梯度采样（Gradient-based One-Side Sampling</strong>）和<strong>互斥特征合并（Exclusive Feature Bundling）</strong>，<strong><font color="red"> 在减少维度和下采样上面做了优化以后才能够将直方图算法发挥得淋漓尽致。</font></strong></p>
<h3><span id="4-xgboost中的树如何剪枝">4、<strong>Xgboost中的树如何剪枝？</strong></span></h3><p><strong>在loss中增加了正则项</strong>：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度在每次分裂时，如果分裂后增益小于设置的阈值，则不分裂，则对应于Gain需要大于0才会分裂。(预剪枝)</p>
<p>则对于目标函数来说，分裂后的收益为：<strong>MAX</strong>【<strong>obj1 - obj2 （分裂后越小越好）</strong>】</p>
<p><img src="https://www.zhihu.com/equation?tex=Gain%3D%5Cfrac12+%5Cleft%5B+%5Cfrac%7BG_L%5E2%7D%7BH_L%2B%5Clambda%7D+%2B+%5Cfrac%7BG_R%5E2%7D%7BH_R%2B%5Clambda%7D+-+%5Cfrac%7B%28G_L%2BG_R%29%5E2%7D%7BH_L%2BH_R%2B%5Clambda%7D%5Cright%5D+-+%5Cgamma+%5C%5C" alt="[公式]"></p>
<p>注意<strong>该特征收益也可作为特征重要性输出的重要依据</strong>。</p>
<p>我们可以发现对于所有的分裂点 <img src="https://www.zhihu.com/equation?tex=a" alt="[公式]"> ，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和 <img src="https://www.zhihu.com/equation?tex=G_L" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=G_R" alt="[公式]"> 。然后用上面的公式计算每个分割方案的分数就可以了。<font color="red">观察分裂后的收益，我们会发现节点划分不一定会使得结果变好，因为我们有一个引入<strong>新叶子的惩罚项（gamma)</strong>，也就是说引入的分割带来的<strong>增益如果小于一个阀值</strong>的时候，我们可以剪掉这个分割。 </font></p>
<ul>
<li>当一次分裂后，计算新生成的左、右叶子节点样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会收回此次分裂。</li>
<li>完成完整一棵树的分裂之后，再从底到顶反向检查是否有不满足分裂条件的结点，进行回溯剪枝。</li>
</ul>
<h3><span id="5-xgboost采样是有放回还是无放回的">5、<strong>Xgboost采样是有放回还是无放回的？</strong></span></h3><p>xgboost时基于boosting的方法，样本是不放回的 ，每轮样本不重复。</p>
<h3><span id="6-xgboost在工程上有哪些优化为什么要做这些工程化优化">6、<strong>Xgboost在工程上有哪些优化？为什么要做这些工程化优化？</strong></span></h3><h4><span id="块结构设计"><strong>块结构设计</strong></span></h4><p>我们知道，决策树的学习<strong>最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序</strong>。而 <strong><font color="red"> XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</font></strong></p>
<blockquote>
<p>  预排序 + 块设计【独立】 + 稀疏矩阵存储 </p>
</blockquote>
<ul>
<li><strong>每一个块结构包括一个或多个已经排序好的特征</strong>；</li>
<li><strong>缺失特征值将不进行排序</strong>；</li>
<li>每个特征会存储指向<strong>样本梯度统计值</strong>的索引，方便计算一阶导和二阶导数值；</li>
</ul>
<p>这种块结构存储的特征之间相互独立，方便计算机进行并行计算。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个<strong>特征的增益计算可以同时进行</strong>，这也是 Xgboost 能够实现分布式或者多线程计算的原因。</p>
<h4><span id="缓存访问优化算法索引访问梯度统计-gt-缓存空间不连续"><strong>缓存访问优化算法</strong>【索引访问梯度统计 -&gt; 缓存空间不连续】</span></h4><p>块结构的设计可以减少节点分裂时的计算量，但<strong>特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续</strong>，这样会造成缓存命中率低，从而影响到算法的效率。</p>
<p>为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。此外适当调整块大小，也可以有助于缓存优化。</p>
<p>于exact greedy算法中, 使用<strong>缓存预取（cache-aware prefetching）</strong>。具体来说，<strong>对每个线程分配一个连续的buffer</strong>，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化）</p>
<h4><span id="核外块计算"><strong>“核外”块计算</strong></span></h4><p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，<strong>XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行</strong>。</p>
<p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p>
<ul>
<li><strong>块压缩：</strong>对 Block 进行按列压缩，并在读取时进行解压；</li>
<li><strong>块拆分：</strong>将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li>
</ul>
<h3><span id="7-xgboost与gbdt有什么联系和不同基模型-算法-工程设计">7、<strong>Xgboost与GBDT有什么联系和不同？</strong>【基模型、算法、工程设计】</span></h3><ol>
<li><strong>基分类器</strong>：GBDT 以 CART 作为基分类器，而Xgboost的基分类器不仅支持CART决策树，还支持线性分类器，此时Xgboost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。</li>
<li><strong>导数信息</strong>：GBDT只用了一阶导数信息，Xgboost中对损失函数进行二阶泰勒展开，引入二阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶和二阶可导即可。</li>
<li><strong>正则项</strong>：Xgboost的目标函数加入正则项(叶子结点的数目和叶子结点权重的L2模的平方)，相当于分裂预剪枝过程，降低过拟合。</li>
<li><strong>列抽样</strong>：Xgboost支持列采样，与随机森林类似，用于防止过拟合且加速。(列采样就是训练的时候随机使用一部分特征)，也同时支持子采样，即每轮迭代计算可以不使用全部样本，对样本数据进行采样。</li>
<li><strong>缺失值处理</strong>：Xgboost可以处理缺失值(具体，查看上方问答)</li>
<li><strong>并行化</strong>：Xgboost可以在特征维度进行并行化，在训练前预先将每个特征按照特征值大小进行预排序，按块的形式存储，后续可以重复使用这个结构，减小计算量，分裂时可以用多线程并行计算每个特征的增益，最终选增益最大的那个特征去做分裂，提高训练速度。</li>
</ol>
<h3><span id="8-xgboost特征重要性">8、<strong><font color="red"> XGBoost特征重要性</font></strong></span></h3><blockquote>
<p>  <strong>何时使用shap value分析特征重要性？</strong> -  知乎 <a href="https://www.zhihu.com/question/527570173/answer/2472253431">https://www.zhihu.com/question/527570173/answer/2472253431</a></p>
</blockquote>
<p>这一思路，通常被用来做<strong>特征筛选</strong>。剔除贡献度不高的尾部特征，增强模型的<a href="https://www.zhihu.com/search?q=鲁棒性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;355884348&quot;}">鲁棒性</a>的同时，起到特征降维的作用。另一个方面，则是用来做<strong>模型的可解释性</strong>。我们期望的结果是：重要的特征是符合业务直觉的；符合业务直觉的特征排名靠前。</p>
<h4><span id="xgb内置的三种特征重要性计算方法">XGB内置的三种特征重要性计算方法</span></h4><ul>
<li><strong>weight</strong>：<code>xgb.plot_importance</code>,<strong>子树模型分裂时，用到的特征次数。这里计算的是所有的树。</strong></li>
<li><strong>gain</strong>:<code>model.feature_importances_</code>,信息增益的泛化概念。这里是指，<strong>节点分裂时，该特征带来信息增益（目标函数）优化的平均值。</strong></li>
<li><strong>cover</strong>:<code>model = XGBRFClassifier(importance_type = &#39;cover&#39;)</code> 这个计算方法，需要在定义模型时定义。之后再调用<code>model.feature_importances_</code> 得到的便是基于<code>cover</code>得到的贡献度。<strong>树模型在分裂时，特征下的叶子结点涵盖的样本数除以特征用来分裂的次数。分裂越靠近根部，cover 值越大。</strong></li>
</ul>
<h4><span id="其他重要性计算方法">其他重要性计算方法</span></h4><ul>
<li><strong>permutation</strong>:<strong>如果这个特征很重要，那么我们打散所有样本中的该特征，则最后的优化目标将折损。这里的折损程度，就是特征的重要程度。</strong></li>
<li><strong>shap</strong>:<strong>轮流去掉每一个特征，算出剩下特征的贡献情况，以此来推导出被去除特征的边际贡献。该方法是目前唯一的逻辑严密的特征解释方法</strong></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成学习</category>
      </categories>
  </entry>
</search>
